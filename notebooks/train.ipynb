{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cd770d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import joblib\n",
    "import torch.optim as optim\n",
    "from visual_utils import train_and_get_losses\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24a137b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/cleaned_data.csv\")\n",
    "scaler = joblib.load(\"minmax.save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489a0075",
   "metadata": {},
   "source": [
    "## Splitting data, converting to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6992715b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9231/3186168220.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32)\n",
      "/tmp/ipykernel_9231/3186168220.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test = torch.tensor(X_test, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "bool_columns = df.select_dtypes(include='bool').columns\n",
    "df[bool_columns] = df[bool_columns].astype(int)\n",
    "\n",
    "X = df.drop('price', axis=1).values \n",
    "y = df['price'].values\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28f33c4",
   "metadata": {},
   "source": [
    "## Defining model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60846252",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NeuralNetwork1, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "class NeuralNetwork2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NeuralNetwork2, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69ea38e",
   "metadata": {},
   "source": [
    "## Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8cf27e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train.shape[1]\n",
    "hidden_size = 128\n",
    "output_size = 1  \n",
    "\n",
    "model = NeuralNetwork1(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d31025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork2(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19d3387",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3170e83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.004\n",
    "criterion = nn.MSELoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate) \n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "train_losses = [] \n",
    "test_losses = []  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf27461",
   "metadata": {},
   "source": [
    "## Training loop + eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25450533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [2/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [3/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [4/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [5/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [6/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [7/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [8/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [9/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [10/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [11/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [12/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [13/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [14/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [15/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [16/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [17/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [18/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [19/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [20/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [21/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [22/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [23/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [24/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [25/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [26/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [27/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [28/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [29/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [30/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [31/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [32/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [33/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [34/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [35/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [36/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [37/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [38/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [39/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [40/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [41/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [42/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [43/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [44/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [45/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [46/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [47/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [48/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [49/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [50/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [51/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [52/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [53/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [54/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [55/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [56/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [57/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [58/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [59/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [60/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [61/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [62/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [63/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [64/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [65/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [66/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [67/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [68/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [69/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [70/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [71/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [72/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [73/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [74/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [75/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [76/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [77/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [78/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [79/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [80/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [81/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [82/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [83/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [84/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [85/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [86/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [87/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [88/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [89/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [90/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [91/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [92/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [93/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [94/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [95/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [96/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [97/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [98/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [99/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Epoch [100/100], Training Loss: 9324785.056335526, Test Loss: 300100320.0\n",
      "Train RMSE : 3053.6511025877735, Test RMSE : 17323.403822574823\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        batch_X = X_train[i:i+batch_size]\n",
    "        batch_y = y_train[i:i+batch_size]\n",
    "\n",
    "        optimizer.zero_grad()  \n",
    "        outputs = model(batch_X)  \n",
    "        loss = criterion(outputs, batch_y.view(-1, 1)) \n",
    "\n",
    "        loss.backward()  \n",
    "        optimizer.step() \n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_outputs = model(X_test)\n",
    "        loss = criterion(test_outputs, y_test.view(-1, 1))\n",
    "        model.train()\n",
    "\n",
    "    train_losses.append(epoch_loss / len(X_train))\n",
    "    test_losses.append(loss.item())\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {train_losses[-1]}, Test Loss: {test_losses[-1]}')\n",
    "\n",
    "print(f\"Train RMSE : {sqrt(train_losses[-1])}, Test RMSE : {sqrt(test_losses[-1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9424fa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_vals = [0.1, 0.001, 0.2, 0.002, 0.3, 0.003, 0.4, 0.004]\n",
    "batch_vals = [32, 64, 128, 211, 256, 512]\n",
    "res_rmse = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b8aa580",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train.shape[1]\n",
    "hidden_size = 128\n",
    "output_size = 1  \n",
    "\n",
    "for lr in lr_vals:\n",
    "    for batch in batch_vals:\n",
    "        model = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "        res = train_and_get_losses(lr, batch, model, X_train, X_test, y_train, y_test)\n",
    "        res_rmse[(lr,batch)] = res[1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7de6eb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "for comb in res_rmse:\n",
    "    res_rmse[comb] = sqrt(res_rmse[comb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "532afd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dict = dict(sorted(res_rmse.items(), key=lambda item: item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c428d354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0.004, 32): 3661.9059245152653,\n",
       " (0.002, 32): 3668.066384350207,\n",
       " (0.001, 32): 3669.345309452355,\n",
       " (0.1, 32): 3674.1856240533084,\n",
       " (0.4, 32): 3679.356737257207,\n",
       " (0.2, 32): 3679.964266130855,\n",
       " (0.3, 32): 3688.413615634776,\n",
       " (0.003, 32): 3695.299446594281,\n",
       " (0.002, 64): 3721.2899645149932,\n",
       " (0.003, 64): 3739.222111616265,\n",
       " (0.004, 64): 3739.4598540430943,\n",
       " (0.1, 64): 3743.0721339562774,\n",
       " (0.3, 64): 3743.1792636741297,\n",
       " (0.4, 64): 3746.1737813401023,\n",
       " (0.2, 64): 3750.3058541937617,\n",
       " (0.001, 64): 3752.4312118944968,\n",
       " (0.1, 128): 3972.7842881284155,\n",
       " (0.3, 128): 3986.4845415478535,\n",
       " (0.001, 128): 3986.8656862252083,\n",
       " (0.2, 128): 3989.9437339391143,\n",
       " (0.002, 128): 3990.0556387098163,\n",
       " (0.004, 128): 3995.3310250841546,\n",
       " (0.003, 128): 4014.529735847027,\n",
       " (0.4, 128): 4059.343666160824,\n",
       " (0.001, 256): 4692.576264697251,\n",
       " (0.1, 256): 4707.039196777524,\n",
       " (0.4, 256): 4707.984282046829,\n",
       " (0.004, 256): 4722.7813838881,\n",
       " (0.2, 256): 4727.638734082798,\n",
       " (0.3, 256): 4738.234692372255,\n",
       " (0.003, 256): 4740.795080996436,\n",
       " (0.1, 211): 4752.566464553652,\n",
       " (0.2, 211): 4821.541039958075,\n",
       " (0.001, 211): 4833.738097994139,\n",
       " (0.3, 211): 4838.9250872482,\n",
       " (0.004, 211): 4857.523443072611,\n",
       " (0.003, 211): 4863.463786233017,\n",
       " (0.4, 211): 4909.268173567217,\n",
       " (0.002, 256): 4911.014559131341,\n",
       " (0.002, 211): 5012.256178608592,\n",
       " (0.2, 512): 5972.616177187347,\n",
       " (0.004, 512): 6045.696651338041,\n",
       " (0.001, 512): 6072.646540018611,\n",
       " (0.4, 512): 6111.880234428682,\n",
       " (0.1, 512): 6136.386558879745,\n",
       " (0.3, 512): 6136.764293990767,\n",
       " (0.002, 512): 6150.785640875481,\n",
       " (0.003, 512): 6183.599275502901}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7114b699",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = transform_raw_input_to_df(\"Opel\", \"Insignia\", \"2.0D\", 2010, 239000, df_mean, sc_mean)\n",
    "with torch.no_grad():\n",
    "    inputs = torch.tensor(data, dtype=torch.float32)\n",
    "    predictions = model.eval(inputs).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f1d758",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict, 'model_nonans.0.0.1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caffeb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
