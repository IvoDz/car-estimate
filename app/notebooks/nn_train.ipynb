{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4cd770d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import joblib\n",
    "import torch.optim as optim\n",
    "from training_utils import train_and_get_losses, plot_learning_curve\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24a137b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/cleaned_data.csv\")\n",
    "scaler = joblib.load(\"scalers/minmax.save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489a0075",
   "metadata": {},
   "source": [
    "## Splitting data, converting to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6992715b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9313/3186168220.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32)\n",
      "/tmp/ipykernel_9313/3186168220.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test = torch.tensor(X_test, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "bool_columns = df.select_dtypes(include='bool').columns\n",
    "df[bool_columns] = df[bool_columns].astype(int)\n",
    "\n",
    "X = df.drop('price', axis=1).values \n",
    "y = df['price'].values\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28f33c4",
   "metadata": {},
   "source": [
    "## Defining model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60846252",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.leaky_relu1 = nn.LeakyReLU(0.01)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.leaky_relu2 = nn.LeakyReLU(0.01)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.leaky_relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.leaky_relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69ea38e",
   "metadata": {},
   "source": [
    "## Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8cf27e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train.shape[1]\n",
    "hidden_size = 128\n",
    "output_size = 1  \n",
    "\n",
    "model = NeuralNetwork(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19d3387",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3170e83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.004\n",
    "criterion = nn.MSELoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate) \n",
    "\n",
    "num_epochs = 40\n",
    "batch_size = 32\n",
    "train_losses = [] \n",
    "test_losses = []  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf27461",
   "metadata": {},
   "source": [
    "## Training loop + eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25450533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40], Training Loss: 4531047.723890765, Test Loss: 82660344.0\n",
      "Epoch [2/40], Training Loss: 1911832.8331852378, Test Loss: 51679176.0\n",
      "Epoch [3/40], Training Loss: 1321029.388898762, Test Loss: 39604948.0\n",
      "Epoch [4/40], Training Loss: 1039054.8939043896, Test Loss: 32497762.0\n",
      "Epoch [5/40], Training Loss: 870333.7070226882, Test Loss: 28100664.0\n",
      "Epoch [6/40], Training Loss: 759396.0613707718, Test Loss: 25064528.0\n",
      "Epoch [7/40], Training Loss: 679267.5734405544, Test Loss: 22873072.0\n",
      "Epoch [8/40], Training Loss: 618367.6571441265, Test Loss: 21239494.0\n",
      "Epoch [9/40], Training Loss: 567800.5149428351, Test Loss: 19994162.0\n",
      "Epoch [10/40], Training Loss: 527379.879894556, Test Loss: 19030200.0\n",
      "Epoch [11/40], Training Loss: 493767.576905989, Test Loss: 18278700.0\n",
      "Epoch [12/40], Training Loss: 464297.493528227, Test Loss: 17678112.0\n",
      "Epoch [13/40], Training Loss: 440122.2535913157, Test Loss: 17206002.0\n",
      "Epoch [14/40], Training Loss: 420701.5011107162, Test Loss: 16923962.0\n",
      "Epoch [15/40], Training Loss: 404367.4485442213, Test Loss: 16432659.0\n",
      "Epoch [16/40], Training Loss: 390727.0392304958, Test Loss: 16402463.0\n",
      "Epoch [17/40], Training Loss: 378579.09396288724, Test Loss: 15965239.0\n",
      "Epoch [18/40], Training Loss: 368057.4380590605, Test Loss: 15853293.0\n",
      "Epoch [19/40], Training Loss: 358830.0810896866, Test Loss: 15541715.0\n",
      "Epoch [20/40], Training Loss: 350653.69934763934, Test Loss: 15417702.0\n",
      "Epoch [21/40], Training Loss: 343281.17431209644, Test Loss: 15310476.0\n",
      "Epoch [22/40], Training Loss: 336997.71363144956, Test Loss: 14997144.0\n",
      "Epoch [23/40], Training Loss: 330886.1024524614, Test Loss: 15042649.0\n",
      "Epoch [24/40], Training Loss: 325942.5757952728, Test Loss: 14832747.0\n",
      "Epoch [25/40], Training Loss: 320813.5022103252, Test Loss: 14808012.0\n",
      "Epoch [26/40], Training Loss: 316483.7744838872, Test Loss: 14719031.0\n",
      "Epoch [27/40], Training Loss: 312343.92871423496, Test Loss: 14636100.0\n",
      "Epoch [28/40], Training Loss: 308664.60588827677, Test Loss: 14586896.0\n",
      "Epoch [29/40], Training Loss: 305040.06988626265, Test Loss: 14790689.0\n",
      "Epoch [30/40], Training Loss: 302198.3889617025, Test Loss: 14624410.0\n",
      "Epoch [31/40], Training Loss: 298838.2175781944, Test Loss: 14670472.0\n",
      "Epoch [32/40], Training Loss: 296050.8792022096, Test Loss: 14671317.0\n",
      "Epoch [33/40], Training Loss: 293141.05953809014, Test Loss: 14808479.0\n",
      "Epoch [34/40], Training Loss: 290542.16652967833, Test Loss: 14841350.0\n",
      "Epoch [35/40], Training Loss: 287937.2357717256, Test Loss: 14746200.0\n",
      "Epoch [36/40], Training Loss: 285473.75629035605, Test Loss: 14961798.0\n",
      "Epoch [37/40], Training Loss: 282930.2143126888, Test Loss: 14942004.0\n",
      "Epoch [38/40], Training Loss: 280756.93672620103, Test Loss: 14856878.0\n",
      "Epoch [39/40], Training Loss: 278321.97109176, Test Loss: 15120225.0\n",
      "Epoch [40/40], Training Loss: 276295.701609798, Test Loss: 14933015.0\n",
      "Train RMSE : 525.6383753207123, Test RMSE : 3864.3259438096056\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        batch_X = X_train[i:i+batch_size]\n",
    "        batch_y = y_train[i:i+batch_size]\n",
    "\n",
    "        optimizer.zero_grad()  \n",
    "        outputs = model(batch_X)  \n",
    "        loss = criterion(outputs, batch_y.view(-1, 1)) \n",
    "\n",
    "        loss.backward()  \n",
    "        optimizer.step() \n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_outputs = model(X_test)\n",
    "        loss = criterion(test_outputs, y_test.view(-1, 1))\n",
    "        model.train()\n",
    "\n",
    "    train_losses.append(epoch_loss / len(X_train))\n",
    "    test_losses.append(loss.item())\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {train_losses[-1]}, Test Loss: {test_losses[-1]}')\n",
    "\n",
    "print(f\"Train RMSE : {sqrt(train_losses[-1])}, Test RMSE : {sqrt(test_losses[-1])}\")\n",
    "\n",
    "plot_learning_curve(train_losses, test_losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b71d4c2",
   "metadata": {},
   "source": [
    "## Testing different network + learning rate + batch size combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9424fa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_vals = [0.1, 0.001, 0.2, 0.002, 0.3, 0.003, 0.4, 0.004]\n",
    "batch_vals = [32, 64, 128, 211, 256, 512]\n",
    "res_rmse = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7b8aa580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Training Loss: 7112453.418103193, Test Loss: 147780544.0\n",
      "Epoch [2/100], Training Loss: 4041989.8180202595, Test Loss: 116578888.0\n",
      "Epoch [3/100], Training Loss: 3223717.925122919, Test Loss: 95075936.0\n",
      "Epoch [4/100], Training Loss: 2631642.048219892, Test Loss: 79484472.0\n",
      "Epoch [5/100], Training Loss: 2196641.50097743, Test Loss: 67981656.0\n",
      "Epoch [6/100], Training Loss: 1883549.684852793, Test Loss: 59695352.0\n",
      "Epoch [7/100], Training Loss: 1661119.2263491498, Test Loss: 53716588.0\n",
      "Epoch [8/100], Training Loss: 1496862.2647947397, Test Loss: 49111004.0\n",
      "Epoch [9/100], Training Loss: 1365511.0611338192, Test Loss: 45283116.0\n",
      "Epoch [10/100], Training Loss: 1254198.9274924472, Test Loss: 41954264.0\n",
      "Epoch [11/100], Training Loss: 1157265.0034506249, Test Loss: 38988188.0\n",
      "Epoch [12/100], Training Loss: 1072035.7704223683, Test Loss: 36324044.0\n",
      "Epoch [13/100], Training Loss: 996993.8193975475, Test Loss: 33944420.0\n",
      "Epoch [14/100], Training Loss: 930983.1860671762, Test Loss: 31839126.0\n",
      "Epoch [15/100], Training Loss: 872932.5759285587, Test Loss: 29985284.0\n",
      "Epoch [16/100], Training Loss: 821952.2369231681, Test Loss: 28350142.0\n",
      "Epoch [17/100], Training Loss: 777218.6909691369, Test Loss: 26908602.0\n",
      "Epoch [18/100], Training Loss: 737874.57555832, Test Loss: 25633308.0\n",
      "Epoch [19/100], Training Loss: 703070.0730258871, Test Loss: 24507110.0\n",
      "Epoch [20/100], Training Loss: 671938.3542296073, Test Loss: 23500452.0\n",
      "Epoch [21/100], Training Loss: 643795.368661513, Test Loss: 22601106.0\n",
      "Epoch [22/100], Training Loss: 618190.9771932942, Test Loss: 21800814.0\n",
      "Epoch [23/100], Training Loss: 594755.6917096144, Test Loss: 21088902.0\n",
      "Epoch [24/100], Training Loss: 573257.1607946804, Test Loss: 20453266.0\n",
      "Epoch [25/100], Training Loss: 553551.741262366, Test Loss: 19885942.0\n",
      "Epoch [26/100], Training Loss: 535455.1446967004, Test Loss: 19380806.0\n",
      "Epoch [27/100], Training Loss: 518732.7610775428, Test Loss: 18930170.0\n",
      "Epoch [28/100], Training Loss: 503200.5755064866, Test Loss: 18523314.0\n",
      "Epoch [29/100], Training Loss: 488759.3845817783, Test Loss: 18153506.0\n",
      "Epoch [30/100], Training Loss: 475381.80660950183, Test Loss: 17815954.0\n",
      "Epoch [31/100], Training Loss: 462942.7865869913, Test Loss: 17511448.0\n",
      "Epoch [32/100], Training Loss: 451385.82787601446, Test Loss: 17225614.0\n",
      "Epoch [33/100], Training Loss: 440570.2804114093, Test Loss: 16973916.0\n",
      "Epoch [34/100], Training Loss: 430459.49050707894, Test Loss: 16736524.0\n",
      "Epoch [35/100], Training Loss: 420970.1141816243, Test Loss: 16537783.0\n",
      "Epoch [36/100], Training Loss: 412111.37193442334, Test Loss: 16342611.0\n",
      "Epoch [37/100], Training Loss: 403766.60693234997, Test Loss: 16193208.0\n",
      "Epoch [38/100], Training Loss: 395966.63227148866, Test Loss: 16032893.0\n",
      "Epoch [39/100], Training Loss: 388615.8591241633, Test Loss: 15908594.0\n",
      "Epoch [40/100], Training Loss: 381741.57809075294, Test Loss: 15787993.0\n",
      "Epoch [41/100], Training Loss: 375192.65990610747, Test Loss: 15691171.0\n",
      "Epoch [42/100], Training Loss: 369039.93684467743, Test Loss: 15594948.0\n",
      "Epoch [43/100], Training Loss: 363216.33064540016, Test Loss: 15529949.0\n",
      "Epoch [44/100], Training Loss: 357751.2345314259, Test Loss: 15456841.0\n",
      "Epoch [45/100], Training Loss: 352554.8707126355, Test Loss: 15400828.0\n",
      "Epoch [46/100], Training Loss: 347642.059230792, Test Loss: 15328757.0\n",
      "Epoch [47/100], Training Loss: 343005.33402938215, Test Loss: 15286968.0\n",
      "Epoch [48/100], Training Loss: 338652.51953379536, Test Loss: 15196382.0\n",
      "Epoch [49/100], Training Loss: 334504.1673479059, Test Loss: 15185655.0\n",
      "Epoch [50/100], Training Loss: 330607.90562614775, Test Loss: 15077649.0\n",
      "Epoch [51/100], Training Loss: 326844.1545857769, Test Loss: 15083666.0\n",
      "Epoch [52/100], Training Loss: 323376.6107161898, Test Loss: 15000795.0\n",
      "Epoch [53/100], Training Loss: 320026.9217315325, Test Loss: 14991707.0\n",
      "Epoch [54/100], Training Loss: 316876.19265742553, Test Loss: 14922115.0\n",
      "Epoch [55/100], Training Loss: 313854.75913749187, Test Loss: 14919576.0\n",
      "Epoch [56/100], Training Loss: 311004.3153471358, Test Loss: 14854334.0\n",
      "Epoch [57/100], Training Loss: 308292.79472113616, Test Loss: 14862070.0\n",
      "Epoch [58/100], Training Loss: 305731.67012099404, Test Loss: 14803576.0\n",
      "Epoch [59/100], Training Loss: 303269.13189384516, Test Loss: 14800661.0\n",
      "Epoch [60/100], Training Loss: 300949.39084621763, Test Loss: 14759548.0\n",
      "Epoch [61/100], Training Loss: 298720.2236204905, Test Loss: 14715230.0\n",
      "Epoch [62/100], Training Loss: 296569.77941916947, Test Loss: 14743696.0\n",
      "Epoch [63/100], Training Loss: 294557.217411587, Test Loss: 14700424.0\n",
      "Epoch [64/100], Training Loss: 292608.59023828566, Test Loss: 14679981.0\n",
      "Epoch [65/100], Training Loss: 290727.58630264795, Test Loss: 14653293.0\n",
      "Epoch [66/100], Training Loss: 288921.8596350927, Test Loss: 14616899.0\n",
      "Epoch [67/100], Training Loss: 287188.9571707837, Test Loss: 14648946.0\n",
      "Epoch [68/100], Training Loss: 285534.1257960133, Test Loss: 14584161.0\n",
      "Epoch [69/100], Training Loss: 283924.7357495113, Test Loss: 14617600.0\n",
      "Epoch [70/100], Training Loss: 282378.7951691251, Test Loss: 14549223.0\n",
      "Epoch [71/100], Training Loss: 280852.0839849535, Test Loss: 14580991.0\n",
      "Epoch [72/100], Training Loss: 279402.4081511759, Test Loss: 14543274.0\n",
      "Epoch [73/100], Training Loss: 277933.44690036133, Test Loss: 14530849.0\n",
      "Epoch [74/100], Training Loss: 276574.44673005154, Test Loss: 14555693.0\n",
      "Epoch [75/100], Training Loss: 275200.625066643, Test Loss: 14500760.0\n",
      "Epoch [76/100], Training Loss: 273889.5062163083, Test Loss: 14524723.0\n",
      "Epoch [77/100], Training Loss: 272560.19172072154, Test Loss: 14489303.0\n",
      "Epoch [78/100], Training Loss: 271273.0967767016, Test Loss: 14475761.0\n",
      "Epoch [79/100], Training Loss: 269992.2286705468, Test Loss: 14520129.0\n",
      "Epoch [80/100], Training Loss: 268800.6153182572, Test Loss: 14443760.0\n",
      "Epoch [81/100], Training Loss: 267559.83221151, Test Loss: 14480153.0\n",
      "Epoch [82/100], Training Loss: 266466.2197477934, Test Loss: 14452436.0\n",
      "Epoch [83/100], Training Loss: 265287.5801863041, Test Loss: 14429923.0\n",
      "Epoch [84/100], Training Loss: 264199.6741788105, Test Loss: 14464098.0\n",
      "Epoch [85/100], Training Loss: 263129.18748519046, Test Loss: 14425718.0\n",
      "Epoch [86/100], Training Loss: 262066.4930358095, Test Loss: 14400441.0\n",
      "Epoch [87/100], Training Loss: 261042.87576269178, Test Loss: 14449650.0\n",
      "Epoch [88/100], Training Loss: 260101.8535520704, Test Loss: 14401982.0\n",
      "Epoch [89/100], Training Loss: 259112.29536905396, Test Loss: 14412866.0\n",
      "Epoch [90/100], Training Loss: 258181.90000222143, Test Loss: 14400777.0\n",
      "Epoch [91/100], Training Loss: 257269.50873022925, Test Loss: 14368520.0\n",
      "Epoch [92/100], Training Loss: 256307.10849105503, Test Loss: 14427443.0\n",
      "Epoch [93/100], Training Loss: 255452.21295391268, Test Loss: 14368768.0\n",
      "Epoch [94/100], Training Loss: 254581.1829830875, Test Loss: 14381717.0\n",
      "Epoch [95/100], Training Loss: 253728.37649946686, Test Loss: 14382824.0\n",
      "Epoch [96/100], Training Loss: 252885.26901546118, Test Loss: 14331272.0\n",
      "Epoch [97/100], Training Loss: 252019.15737367453, Test Loss: 14374975.0\n",
      "Epoch [98/100], Training Loss: 251202.38818420118, Test Loss: 14353366.0\n",
      "Epoch [99/100], Training Loss: 250357.96442376045, Test Loss: 14327641.0\n",
      "Epoch [100/100], Training Loss: 249520.88635892424, Test Loss: 14376654.0\n",
      "Epoch [1/100], Training Loss: 4370000.311355962, Test Loss: 226398720.0\n",
      "Epoch [2/100], Training Loss: 2629046.6299389848, Test Loss: 143550832.0\n",
      "Epoch [3/100], Training Loss: 2053607.4674486108, Test Loss: 123781264.0\n",
      "Epoch [4/100], Training Loss: 1773467.797997749, Test Loss: 107978712.0\n",
      "Epoch [5/100], Training Loss: 1542262.9666488953, Test Loss: 95043688.0\n",
      "Epoch [6/100], Training Loss: 1352931.7656536936, Test Loss: 84572136.0\n",
      "Epoch [7/100], Training Loss: 1198501.5798234702, Test Loss: 75990832.0\n",
      "Epoch [8/100], Training Loss: 1073036.9548012558, Test Loss: 68970328.0\n",
      "Epoch [9/100], Training Loss: 971833.9851904508, Test Loss: 63256628.0\n",
      "Epoch [10/100], Training Loss: 890457.8126295835, Test Loss: 58638876.0\n",
      "Epoch [11/100], Training Loss: 824596.0873763403, Test Loss: 54849644.0\n",
      "Epoch [12/100], Training Loss: 769941.4171553818, Test Loss: 51637560.0\n",
      "Epoch [13/100], Training Loss: 722830.9653456549, Test Loss: 48819480.0\n",
      "Epoch [14/100], Training Loss: 681081.9043895503, Test Loss: 46290056.0\n",
      "Epoch [15/100], Training Loss: 643431.0107813518, Test Loss: 43976404.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/100], Training Loss: 609092.3230258871, Test Loss: 41842800.0\n",
      "Epoch [17/100], Training Loss: 577589.2669569338, Test Loss: 39860660.0\n",
      "Epoch [18/100], Training Loss: 548608.0315739589, Test Loss: 38011696.0\n",
      "Epoch [19/100], Training Loss: 521918.32983827975, Test Loss: 36292472.0\n",
      "Epoch [20/100], Training Loss: 497382.8394052485, Test Loss: 34698088.0\n",
      "Epoch [21/100], Training Loss: 474836.1639713287, Test Loss: 33228006.0\n",
      "Epoch [22/100], Training Loss: 454110.7870979207, Test Loss: 31871748.0\n",
      "Epoch [23/100], Training Loss: 435078.9593922161, Test Loss: 30623376.0\n",
      "Epoch [24/100], Training Loss: 417614.2968426041, Test Loss: 29476422.0\n",
      "Epoch [25/100], Training Loss: 401580.3681357739, Test Loss: 28420736.0\n",
      "Epoch [26/100], Training Loss: 386820.19776671997, Test Loss: 27445092.0\n",
      "Epoch [27/100], Training Loss: 373250.9425537587, Test Loss: 26546634.0\n",
      "Epoch [28/100], Training Loss: 360764.7962650317, Test Loss: 25718412.0\n",
      "Epoch [29/100], Training Loss: 349245.69138380425, Test Loss: 24952074.0\n",
      "Epoch [30/100], Training Loss: 338523.72488300456, Test Loss: 24238558.0\n",
      "Epoch [31/100], Training Loss: 328482.38857295184, Test Loss: 23575512.0\n",
      "Epoch [32/100], Training Loss: 319042.83623600495, Test Loss: 22957556.0\n",
      "Epoch [33/100], Training Loss: 310153.1996771518, Test Loss: 22383700.0\n",
      "Epoch [34/100], Training Loss: 301762.9588146437, Test Loss: 21850568.0\n",
      "Epoch [35/100], Training Loss: 293831.5399117351, Test Loss: 21356118.0\n",
      "Epoch [36/100], Training Loss: 286338.1149665304, Test Loss: 20896078.0\n",
      "Epoch [37/100], Training Loss: 279252.00322848174, Test Loss: 20472144.0\n",
      "Epoch [38/100], Training Loss: 272553.51526864525, Test Loss: 20074984.0\n",
      "Epoch [39/100], Training Loss: 266222.5940702565, Test Loss: 19708578.0\n",
      "Epoch [40/100], Training Loss: 260221.92752206622, Test Loss: 19364902.0\n",
      "Epoch [41/100], Training Loss: 254542.6693620046, Test Loss: 19046674.0\n",
      "Epoch [42/100], Training Loss: 249147.47306143, Test Loss: 18743588.0\n",
      "Epoch [43/100], Training Loss: 244037.30251466145, Test Loss: 18463948.0\n",
      "Epoch [44/100], Training Loss: 239176.0029471003, Test Loss: 18198690.0\n",
      "Epoch [45/100], Training Loss: 234542.9346158403, Test Loss: 17949650.0\n",
      "Epoch [46/100], Training Loss: 230128.63655885315, Test Loss: 17719530.0\n",
      "Epoch [47/100], Training Loss: 225906.14211243408, Test Loss: 17501882.0\n",
      "Epoch [48/100], Training Loss: 221886.28800130324, Test Loss: 17298414.0\n",
      "Epoch [49/100], Training Loss: 218037.25342100585, Test Loss: 17111482.0\n",
      "Epoch [50/100], Training Loss: 214355.8686096795, Test Loss: 16934160.0\n",
      "Epoch [51/100], Training Loss: 210824.06613944672, Test Loss: 16769866.0\n",
      "Epoch [52/100], Training Loss: 207462.83114152006, Test Loss: 16617725.0\n",
      "Epoch [53/100], Training Loss: 204242.2771162846, Test Loss: 16475356.0\n",
      "Epoch [54/100], Training Loss: 201178.38538889875, Test Loss: 16342696.0\n",
      "Epoch [55/100], Training Loss: 198235.0649843019, Test Loss: 16224731.0\n",
      "Epoch [56/100], Training Loss: 195426.55452876014, Test Loss: 16108995.0\n",
      "Epoch [57/100], Training Loss: 192718.82918665957, Test Loss: 16016273.0\n",
      "Epoch [58/100], Training Loss: 190140.52884900183, Test Loss: 15915723.0\n",
      "Epoch [59/100], Training Loss: 187655.8259137492, Test Loss: 15834614.0\n",
      "Epoch [60/100], Training Loss: 185290.17411586992, Test Loss: 15749418.0\n",
      "Epoch [61/100], Training Loss: 183012.77960428884, Test Loss: 15683195.0\n",
      "Epoch [62/100], Training Loss: 180847.57563236775, Test Loss: 15611299.0\n",
      "Epoch [63/100], Training Loss: 178742.31545820745, Test Loss: 15555902.0\n",
      "Epoch [64/100], Training Loss: 176741.5350393934, Test Loss: 15497229.0\n",
      "Epoch [65/100], Training Loss: 174800.8592278301, Test Loss: 15448703.0\n",
      "Epoch [66/100], Training Loss: 172962.75914489664, Test Loss: 15393951.0\n",
      "Epoch [67/100], Training Loss: 171179.59287068303, Test Loss: 15345493.0\n",
      "Epoch [68/100], Training Loss: 169481.2621512351, Test Loss: 15296678.0\n",
      "Epoch [69/100], Training Loss: 167839.67282743912, Test Loss: 15249074.0\n",
      "Epoch [70/100], Training Loss: 166275.06348113265, Test Loss: 15199938.0\n",
      "Epoch [71/100], Training Loss: 164766.5129657603, Test Loss: 15154511.0\n",
      "Epoch [72/100], Training Loss: 163322.0130620224, Test Loss: 15108839.0\n",
      "Epoch [73/100], Training Loss: 161923.8143697056, Test Loss: 15066429.0\n",
      "Epoch [74/100], Training Loss: 160592.84969048042, Test Loss: 15016269.0\n",
      "Epoch [75/100], Training Loss: 159298.76026301758, Test Loss: 15005568.0\n",
      "Epoch [76/100], Training Loss: 158075.07903856406, Test Loss: 14939440.0\n",
      "Epoch [77/100], Training Loss: 156877.1965967656, Test Loss: 14929218.0\n",
      "Epoch [78/100], Training Loss: 155738.93292755168, Test Loss: 14876503.0\n",
      "Epoch [79/100], Training Loss: 154627.61306350335, Test Loss: 14858635.0\n",
      "Epoch [80/100], Training Loss: 153573.04884929804, Test Loss: 14819743.0\n",
      "Epoch [81/100], Training Loss: 152556.4137713998, Test Loss: 14798860.0\n",
      "Epoch [82/100], Training Loss: 151572.53936378178, Test Loss: 14765130.0\n",
      "Epoch [83/100], Training Loss: 150630.7458903501, Test Loss: 14742013.0\n",
      "Epoch [84/100], Training Loss: 149735.45587494818, Test Loss: 14726285.0\n",
      "Epoch [85/100], Training Loss: 148852.43756294058, Test Loss: 14684510.0\n",
      "Epoch [86/100], Training Loss: 148017.56277027426, Test Loss: 14695712.0\n",
      "Epoch [87/100], Training Loss: 147205.91733309638, Test Loss: 14667000.0\n",
      "Epoch [88/100], Training Loss: 146421.62839879154, Test Loss: 14612504.0\n",
      "Epoch [89/100], Training Loss: 145656.71481991588, Test Loss: 14634276.0\n",
      "Epoch [90/100], Training Loss: 144920.39427462828, Test Loss: 14608792.0\n",
      "Epoch [91/100], Training Loss: 144209.2609590664, Test Loss: 14593492.0\n",
      "Epoch [92/100], Training Loss: 143524.99185474793, Test Loss: 14584647.0\n",
      "Epoch [93/100], Training Loss: 142849.22951839346, Test Loss: 14566093.0\n",
      "Epoch [94/100], Training Loss: 142187.73603459512, Test Loss: 14556027.0\n",
      "Epoch [95/100], Training Loss: 141536.55373644925, Test Loss: 14536116.0\n",
      "Epoch [96/100], Training Loss: 140917.14032047865, Test Loss: 14537048.0\n",
      "Epoch [97/100], Training Loss: 140286.86564776968, Test Loss: 14521855.0\n",
      "Epoch [98/100], Training Loss: 139692.5563799538, Test Loss: 14468643.0\n",
      "Epoch [99/100], Training Loss: 139104.44430128546, Test Loss: 14496028.0\n",
      "Epoch [100/100], Training Loss: 138535.90058349623, Test Loss: 14476425.0\n",
      "Epoch [1/100], Training Loss: 2317354.6600319888, Test Loss: 291899808.0\n",
      "Epoch [2/100], Training Loss: 2056645.5963509271, Test Loss: 226635072.0\n",
      "Epoch [3/100], Training Loss: 1462936.8077720513, Test Loss: 160935248.0\n",
      "Epoch [4/100], Training Loss: 1159581.4847461644, Test Loss: 141536816.0\n",
      "Epoch [5/100], Training Loss: 1045394.9773117707, Test Loss: 129736552.0\n",
      "Epoch [6/100], Training Loss: 958286.4880042651, Test Loss: 119610112.0\n",
      "Epoch [7/100], Training Loss: 879970.9119128013, Test Loss: 110200120.0\n",
      "Epoch [8/100], Training Loss: 807065.5217107991, Test Loss: 101765056.0\n",
      "Epoch [9/100], Training Loss: 743175.6529826432, Test Loss: 94503136.0\n",
      "Epoch [10/100], Training Loss: 687681.7036905397, Test Loss: 88210088.0\n",
      "Epoch [11/100], Training Loss: 639387.7584266335, Test Loss: 82721568.0\n",
      "Epoch [12/100], Training Loss: 597291.9027308809, Test Loss: 77910288.0\n",
      "Epoch [13/100], Training Loss: 560663.7696818908, Test Loss: 73697416.0\n",
      "Epoch [14/100], Training Loss: 528829.0904567265, Test Loss: 69997680.0\n",
      "Epoch [15/100], Training Loss: 501130.243232036, Test Loss: 66738232.0\n",
      "Epoch [16/100], Training Loss: 476932.03767549316, Test Loss: 63868628.0\n",
      "Epoch [17/100], Training Loss: 455678.5846810023, Test Loss: 61324708.0\n",
      "Epoch [18/100], Training Loss: 436843.4765712932, Test Loss: 59049272.0\n",
      "Epoch [19/100], Training Loss: 419972.87222320953, Test Loss: 57004148.0\n",
      "Epoch [20/100], Training Loss: 404743.86440376757, Test Loss: 55137800.0\n",
      "Epoch [21/100], Training Loss: 390767.5868135774, Test Loss: 53414584.0\n",
      "Epoch [22/100], Training Loss: 377807.749244713, Test Loss: 51814544.0\n",
      "Epoch [23/100], Training Loss: 365706.1828683135, Test Loss: 50312780.0\n",
      "Epoch [24/100], Training Loss: 354323.26491321606, Test Loss: 48899052.0\n",
      "Epoch [25/100], Training Loss: 343583.7853207748, Test Loss: 47567756.0\n",
      "Epoch [26/100], Training Loss: 333453.46922575677, Test Loss: 46312736.0\n",
      "Epoch [27/100], Training Loss: 323866.2030093004, Test Loss: 45122556.0\n",
      "Epoch [28/100], Training Loss: 314768.1019489367, Test Loss: 43987164.0\n",
      "Epoch [29/100], Training Loss: 306114.3027664238, Test Loss: 42899072.0\n",
      "Epoch [30/100], Training Loss: 297894.7243054321, Test Loss: 41860952.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/100], Training Loss: 290081.47698596056, Test Loss: 40869132.0\n",
      "Epoch [32/100], Training Loss: 282684.623126592, Test Loss: 39928016.0\n",
      "Epoch [33/100], Training Loss: 275676.6139446715, Test Loss: 39031432.0\n",
      "Epoch [34/100], Training Loss: 269032.67543391977, Test Loss: 38178456.0\n",
      "Epoch [35/100], Training Loss: 262744.23505716486, Test Loss: 37368628.0\n",
      "Epoch [36/100], Training Loss: 256792.66548190272, Test Loss: 36598812.0\n",
      "Epoch [37/100], Training Loss: 251132.0315147207, Test Loss: 35864744.0\n",
      "Epoch [38/100], Training Loss: 245704.53788282684, Test Loss: 35162400.0\n",
      "Epoch [39/100], Training Loss: 240489.3101119602, Test Loss: 34487024.0\n",
      "Epoch [40/100], Training Loss: 235478.50891534862, Test Loss: 33836484.0\n",
      "Epoch [41/100], Training Loss: 230675.56945678574, Test Loss: 33214936.0\n",
      "Epoch [42/100], Training Loss: 226121.77815295302, Test Loss: 32624576.0\n",
      "Epoch [43/100], Training Loss: 221803.3724897814, Test Loss: 32063340.0\n",
      "Epoch [44/100], Training Loss: 217687.74032936437, Test Loss: 31527240.0\n",
      "Epoch [45/100], Training Loss: 213720.30329956757, Test Loss: 31007756.0\n",
      "Epoch [46/100], Training Loss: 209854.3800130324, Test Loss: 30502784.0\n",
      "Epoch [47/100], Training Loss: 206098.74338013152, Test Loss: 30012240.0\n",
      "Epoch [48/100], Training Loss: 202431.87411883182, Test Loss: 29533020.0\n",
      "Epoch [49/100], Training Loss: 198875.13171613056, Test Loss: 29070884.0\n",
      "Epoch [50/100], Training Loss: 195456.03044843316, Test Loss: 28629106.0\n",
      "Epoch [51/100], Training Loss: 192191.1627865648, Test Loss: 28211024.0\n",
      "Epoch [52/100], Training Loss: 189078.22910372607, Test Loss: 27811810.0\n",
      "Epoch [53/100], Training Loss: 186101.38258989397, Test Loss: 27426094.0\n",
      "Epoch [54/100], Training Loss: 183213.26701617203, Test Loss: 27046922.0\n",
      "Epoch [55/100], Training Loss: 180390.6661039038, Test Loss: 26683290.0\n",
      "Epoch [56/100], Training Loss: 177660.4997630472, Test Loss: 26335500.0\n",
      "Epoch [57/100], Training Loss: 175037.99161779517, Test Loss: 26004392.0\n",
      "Epoch [58/100], Training Loss: 172505.0905752029, Test Loss: 25686598.0\n",
      "Epoch [59/100], Training Loss: 170053.55837924292, Test Loss: 25382506.0\n",
      "Epoch [60/100], Training Loss: 167671.39129790888, Test Loss: 25080008.0\n",
      "Epoch [61/100], Training Loss: 165350.66003198863, Test Loss: 24785452.0\n",
      "Epoch [62/100], Training Loss: 163076.24083288905, Test Loss: 24496556.0\n",
      "Epoch [63/100], Training Loss: 160803.50337657722, Test Loss: 24203216.0\n",
      "Epoch [64/100], Training Loss: 158490.0032284817, Test Loss: 23906500.0\n",
      "Epoch [65/100], Training Loss: 156185.4021977371, Test Loss: 23616668.0\n",
      "Epoch [66/100], Training Loss: 153946.60402227356, Test Loss: 23338012.0\n",
      "Epoch [67/100], Training Loss: 151777.01910431846, Test Loss: 23065590.0\n",
      "Epoch [68/100], Training Loss: 149640.2237130502, Test Loss: 22799794.0\n",
      "Epoch [69/100], Training Loss: 147549.4597180262, Test Loss: 22543222.0\n",
      "Epoch [70/100], Training Loss: 145523.59641016528, Test Loss: 22298694.0\n",
      "Epoch [71/100], Training Loss: 143550.17845506783, Test Loss: 22058314.0\n",
      "Epoch [72/100], Training Loss: 141616.6457852023, Test Loss: 21830056.0\n",
      "Epoch [73/100], Training Loss: 139757.5959066406, Test Loss: 21609930.0\n",
      "Epoch [74/100], Training Loss: 137963.52025946332, Test Loss: 21400908.0\n",
      "Epoch [75/100], Training Loss: 136198.4888039808, Test Loss: 21197004.0\n",
      "Epoch [76/100], Training Loss: 134462.82148569397, Test Loss: 20999706.0\n",
      "Epoch [77/100], Training Loss: 132752.59818731117, Test Loss: 20807500.0\n",
      "Epoch [78/100], Training Loss: 131091.96155441029, Test Loss: 20626318.0\n",
      "Epoch [79/100], Training Loss: 129517.06048219893, Test Loss: 20454344.0\n",
      "Epoch [80/100], Training Loss: 128015.53249215095, Test Loss: 20293996.0\n",
      "Epoch [81/100], Training Loss: 126574.48264320834, Test Loss: 20140176.0\n",
      "Epoch [82/100], Training Loss: 125172.66666666667, Test Loss: 19990406.0\n",
      "Epoch [83/100], Training Loss: 123791.26583140809, Test Loss: 19841962.0\n",
      "Epoch [84/100], Training Loss: 122404.60656951602, Test Loss: 19692478.0\n",
      "Epoch [85/100], Training Loss: 120993.45795568982, Test Loss: 19538958.0\n",
      "Epoch [86/100], Training Loss: 119597.05443990284, Test Loss: 19394846.0\n",
      "Epoch [87/100], Training Loss: 118242.74791185357, Test Loss: 19256094.0\n",
      "Epoch [88/100], Training Loss: 116917.09184882412, Test Loss: 19119210.0\n",
      "Epoch [89/100], Training Loss: 115621.89763639595, Test Loss: 18987736.0\n",
      "Epoch [90/100], Training Loss: 114380.92900302114, Test Loss: 18863664.0\n",
      "Epoch [91/100], Training Loss: 113185.8773028849, Test Loss: 18747402.0\n",
      "Epoch [92/100], Training Loss: 112015.92301996327, Test Loss: 18630060.0\n",
      "Epoch [93/100], Training Loss: 110879.80537290445, Test Loss: 18520998.0\n",
      "Epoch [94/100], Training Loss: 109796.48159173035, Test Loss: 18418634.0\n",
      "Epoch [95/100], Training Loss: 108767.7630324033, Test Loss: 18324026.0\n",
      "Epoch [96/100], Training Loss: 107786.91696285765, Test Loss: 18234072.0\n",
      "Epoch [97/100], Training Loss: 106839.74798590131, Test Loss: 18148372.0\n",
      "Epoch [98/100], Training Loss: 105920.12232687637, Test Loss: 18067138.0\n",
      "Epoch [99/100], Training Loss: 105014.20795568982, Test Loss: 17985266.0\n",
      "Epoch [100/100], Training Loss: 104115.51060363723, Test Loss: 17904934.0\n",
      "Epoch [1/100], Training Loss: 1497879.4914993187, Test Loss: 298808384.0\n",
      "Epoch [2/100], Training Loss: 1469215.0957881643, Test Loss: 285837088.0\n",
      "Epoch [3/100], Training Loss: 1348417.6719388661, Test Loss: 249082656.0\n",
      "Epoch [4/100], Training Loss: 1124343.5151945974, Test Loss: 199684736.0\n",
      "Epoch [5/100], Training Loss: 906628.4281736864, Test Loss: 165645600.0\n",
      "Epoch [6/100], Training Loss: 784422.7012617736, Test Loss: 149722928.0\n",
      "Epoch [7/100], Training Loss: 721540.4179847166, Test Loss: 140125712.0\n",
      "Epoch [8/100], Training Loss: 676670.7285113441, Test Loss: 132431360.0\n",
      "Epoch [9/100], Training Loss: 638293.860316332, Test Loss: 125679104.0\n",
      "Epoch [10/100], Training Loss: 603715.1666370475, Test Loss: 119592472.0\n",
      "Epoch [11/100], Training Loss: 572042.2524731947, Test Loss: 114061472.0\n",
      "Epoch [12/100], Training Loss: 542904.6715241988, Test Loss: 109032912.0\n",
      "Epoch [13/100], Training Loss: 516118.11859487, Test Loss: 104470576.0\n",
      "Epoch [14/100], Training Loss: 491545.996090279, Test Loss: 100336608.0\n",
      "Epoch [15/100], Training Loss: 469056.2473787098, Test Loss: 96595840.0\n",
      "Epoch [16/100], Training Loss: 448523.0199632723, Test Loss: 93211704.0\n",
      "Epoch [17/100], Training Loss: 429806.87305254425, Test Loss: 90142232.0\n",
      "Epoch [18/100], Training Loss: 412742.797701558, Test Loss: 87344336.0\n",
      "Epoch [19/100], Training Loss: 397169.0518334222, Test Loss: 84781712.0\n",
      "Epoch [20/100], Training Loss: 382942.84070848883, Test Loss: 82424536.0\n",
      "Epoch [21/100], Training Loss: 369940.8980510633, Test Loss: 80249968.0\n",
      "Epoch [22/100], Training Loss: 358050.02499851905, Test Loss: 78238104.0\n",
      "Epoch [23/100], Training Loss: 347166.2178780878, Test Loss: 76371040.0\n",
      "Epoch [24/100], Training Loss: 337190.9316983591, Test Loss: 74633240.0\n",
      "Epoch [25/100], Training Loss: 328031.89254191105, Test Loss: 73009392.0\n",
      "Epoch [26/100], Training Loss: 319598.30199632724, Test Loss: 71485496.0\n",
      "Epoch [27/100], Training Loss: 311800.78834192286, Test Loss: 70048608.0\n",
      "Epoch [28/100], Training Loss: 304555.96208755404, Test Loss: 68687704.0\n",
      "Epoch [29/100], Training Loss: 297790.81653930456, Test Loss: 67394248.0\n",
      "Epoch [30/100], Training Loss: 291438.7587524436, Test Loss: 66159980.0\n",
      "Epoch [31/100], Training Loss: 285440.4886855044, Test Loss: 64976852.0\n",
      "Epoch [32/100], Training Loss: 279739.29901220306, Test Loss: 63836396.0\n",
      "Epoch [33/100], Training Loss: 274287.90513002785, Test Loss: 62733764.0\n",
      "Epoch [34/100], Training Loss: 269052.9082099513, Test Loss: 61664920.0\n",
      "Epoch [35/100], Training Loss: 264006.5462274293, Test Loss: 60625464.0\n",
      "Epoch [36/100], Training Loss: 259116.29677133315, Test Loss: 59611576.0\n",
      "Epoch [37/100], Training Loss: 254361.71955156684, Test Loss: 58620420.0\n",
      "Epoch [38/100], Training Loss: 249723.55947514958, Test Loss: 57650596.0\n",
      "Epoch [39/100], Training Loss: 245185.32702446537, Test Loss: 56700008.0\n",
      "Epoch [40/100], Training Loss: 240740.28560956105, Test Loss: 55769228.0\n",
      "Epoch [41/100], Training Loss: 236383.70175048872, Test Loss: 54857732.0\n",
      "Epoch [42/100], Training Loss: 232105.86863929863, Test Loss: 53963120.0\n",
      "Epoch [43/100], Training Loss: 227904.93514898408, Test Loss: 53085804.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/100], Training Loss: 223783.01928203306, Test Loss: 52224732.0\n",
      "Epoch [45/100], Training Loss: 219739.2824477223, Test Loss: 51380684.0\n",
      "Epoch [46/100], Training Loss: 215774.3894022866, Test Loss: 50554288.0\n",
      "Epoch [47/100], Training Loss: 211891.527397666, Test Loss: 49744704.0\n",
      "Epoch [48/100], Training Loss: 208090.78067057638, Test Loss: 48951508.0\n",
      "Epoch [49/100], Training Loss: 204367.50730110775, Test Loss: 48173892.0\n",
      "Epoch [50/100], Training Loss: 200719.90384159706, Test Loss: 47411792.0\n",
      "Epoch [51/100], Training Loss: 197147.35317812927, Test Loss: 46664296.0\n",
      "Epoch [52/100], Training Loss: 193648.66434156743, Test Loss: 45930976.0\n",
      "Epoch [53/100], Training Loss: 190221.82969018424, Test Loss: 45210824.0\n",
      "Epoch [54/100], Training Loss: 186867.7078224039, Test Loss: 44503900.0\n",
      "Epoch [55/100], Training Loss: 183587.43595610448, Test Loss: 43810060.0\n",
      "Epoch [56/100], Training Loss: 180381.53117410105, Test Loss: 43129648.0\n",
      "Epoch [57/100], Training Loss: 177248.53899354304, Test Loss: 42462500.0\n",
      "Epoch [58/100], Training Loss: 174186.77942657424, Test Loss: 41807844.0\n",
      "Epoch [59/100], Training Loss: 171194.84556972334, Test Loss: 41165680.0\n",
      "Epoch [60/100], Training Loss: 168272.849294325, Test Loss: 40535772.0\n",
      "Epoch [61/100], Training Loss: 165420.05712413363, Test Loss: 39917944.0\n",
      "Epoch [62/100], Training Loss: 162635.1599468337, Test Loss: 39312920.0\n",
      "Epoch [63/100], Training Loss: 159916.83536964635, Test Loss: 38720516.0\n",
      "Epoch [64/100], Training Loss: 157263.5975393934, Test Loss: 38140968.0\n",
      "Epoch [65/100], Training Loss: 154673.1216197204, Test Loss: 37573816.0\n",
      "Epoch [66/100], Training Loss: 152145.24585517743, Test Loss: 37018712.0\n",
      "Epoch [67/100], Training Loss: 149678.89983098602, Test Loss: 36475984.0\n",
      "Epoch [68/100], Training Loss: 147272.8216282359, Test Loss: 35945408.0\n",
      "Epoch [69/100], Training Loss: 144925.24074773415, Test Loss: 35426128.0\n",
      "Epoch [70/100], Training Loss: 142632.62119140045, Test Loss: 34918512.0\n",
      "Epoch [71/100], Training Loss: 140395.06979717396, Test Loss: 34421872.0\n",
      "Epoch [72/100], Training Loss: 138212.23190654896, Test Loss: 33936724.0\n",
      "Epoch [73/100], Training Loss: 136083.6129922324, Test Loss: 33462308.0\n",
      "Epoch [74/100], Training Loss: 134007.13032148755, Test Loss: 32998920.0\n",
      "Epoch [75/100], Training Loss: 131984.14573396184, Test Loss: 32547210.0\n",
      "Epoch [76/100], Training Loss: 130014.41162865762, Test Loss: 32106526.0\n",
      "Epoch [77/100], Training Loss: 128094.4815465144, Test Loss: 31678084.0\n",
      "Epoch [78/100], Training Loss: 126224.69085810144, Test Loss: 31260754.0\n",
      "Epoch [79/100], Training Loss: 124403.23663697063, Test Loss: 30854360.0\n",
      "Epoch [80/100], Training Loss: 122629.83850811378, Test Loss: 30459026.0\n",
      "Epoch [81/100], Training Loss: 120902.16336818668, Test Loss: 30074170.0\n",
      "Epoch [82/100], Training Loss: 119217.40847559756, Test Loss: 29697806.0\n",
      "Epoch [83/100], Training Loss: 117572.22846448577, Test Loss: 29329492.0\n",
      "Epoch [84/100], Training Loss: 115964.91698750167, Test Loss: 28970406.0\n",
      "Epoch [85/100], Training Loss: 114397.09158456622, Test Loss: 28620654.0\n",
      "Epoch [86/100], Training Loss: 112868.33676000847, Test Loss: 28279908.0\n",
      "Epoch [87/100], Training Loss: 111376.91007259919, Test Loss: 27948602.0\n",
      "Epoch [88/100], Training Loss: 109923.02203093993, Test Loss: 27626566.0\n",
      "Epoch [89/100], Training Loss: 108504.78382477896, Test Loss: 27314038.0\n",
      "Epoch [90/100], Training Loss: 107122.74151389691, Test Loss: 27010932.0\n",
      "Epoch [91/100], Training Loss: 105775.24594472891, Test Loss: 26716746.0\n",
      "Epoch [92/100], Training Loss: 104462.05343817569, Test Loss: 26430796.0\n",
      "Epoch [93/100], Training Loss: 103183.01379532648, Test Loss: 26153412.0\n",
      "Epoch [94/100], Training Loss: 101934.97069120794, Test Loss: 25884340.0\n",
      "Epoch [95/100], Training Loss: 100718.99914312873, Test Loss: 25623182.0\n",
      "Epoch [96/100], Training Loss: 99532.10018451772, Test Loss: 25369524.0\n",
      "Epoch [97/100], Training Loss: 98373.4566545321, Test Loss: 25123564.0\n",
      "Epoch [98/100], Training Loss: 97241.96284793185, Test Loss: 24883722.0\n",
      "Epoch [99/100], Training Loss: 96133.4544652642, Test Loss: 24650774.0\n",
      "Epoch [100/100], Training Loss: 95048.84959838353, Test Loss: 24424452.0\n",
      "Epoch [1/100], Training Loss: 1164445.7129316984, Test Loss: 299409120.0\n",
      "Epoch [2/100], Training Loss: 1151710.8048101417, Test Loss: 291790656.0\n",
      "Epoch [3/100], Training Loss: 1092655.5820152834, Test Loss: 267630112.0\n",
      "Epoch [4/100], Training Loss: 963920.4857532136, Test Loss: 226837280.0\n",
      "Epoch [5/100], Training Loss: 797250.2017652983, Test Loss: 185586576.0\n",
      "Epoch [6/100], Training Loss: 664881.4354599846, Test Loss: 160219856.0\n",
      "Epoch [7/100], Training Loss: 593440.6018600793, Test Loss: 147577664.0\n",
      "Epoch [8/100], Training Loss: 553778.0598305787, Test Loss: 139378784.0\n",
      "Epoch [9/100], Training Loss: 524212.696878147, Test Loss: 132609744.0\n",
      "Epoch [10/100], Training Loss: 498466.1844677448, Test Loss: 126525000.0\n",
      "Epoch [11/100], Training Loss: 474832.664652568, Test Loss: 120880216.0\n",
      "Epoch [12/100], Training Loss: 452675.4640127955, Test Loss: 115574488.0\n",
      "Epoch [13/100], Training Loss: 431733.58971624903, Test Loss: 110569016.0\n",
      "Epoch [14/100], Training Loss: 411923.5121142112, Test Loss: 105856320.0\n",
      "Epoch [15/100], Training Loss: 393245.815769208, Test Loss: 101437584.0\n",
      "Epoch [16/100], Training Loss: 375714.519282033, Test Loss: 97313800.0\n",
      "Epoch [17/100], Training Loss: 359347.71281322197, Test Loss: 93488536.0\n",
      "Epoch [18/100], Training Loss: 344155.22563829157, Test Loss: 89953912.0\n",
      "Epoch [19/100], Training Loss: 330108.5712931698, Test Loss: 86691344.0\n",
      "Epoch [20/100], Training Loss: 317145.6296427937, Test Loss: 83678744.0\n",
      "Epoch [21/100], Training Loss: 305188.42746282805, Test Loss: 80891152.0\n",
      "Epoch [22/100], Training Loss: 294147.45702268824, Test Loss: 78305088.0\n",
      "Epoch [23/100], Training Loss: 283934.13387832476, Test Loss: 75899472.0\n",
      "Epoch [24/100], Training Loss: 274467.91078727564, Test Loss: 73655232.0\n",
      "Epoch [25/100], Training Loss: 265674.71891475626, Test Loss: 71555152.0\n",
      "Epoch [26/100], Training Loss: 257490.6660742847, Test Loss: 69587368.0\n",
      "Epoch [27/100], Training Loss: 249861.3674545347, Test Loss: 67741912.0\n",
      "Epoch [28/100], Training Loss: 242735.61720277235, Test Loss: 66008920.0\n",
      "Epoch [29/100], Training Loss: 236061.91754043006, Test Loss: 64378260.0\n",
      "Epoch [30/100], Training Loss: 229799.78425448728, Test Loss: 62844296.0\n",
      "Epoch [31/100], Training Loss: 223922.30507671347, Test Loss: 61402760.0\n",
      "Epoch [32/100], Training Loss: 218391.24210651027, Test Loss: 60041632.0\n",
      "Epoch [33/100], Training Loss: 213167.324803033, Test Loss: 58753124.0\n",
      "Epoch [34/100], Training Loss: 208220.2024761566, Test Loss: 57529600.0\n",
      "Epoch [35/100], Training Loss: 203515.89858420708, Test Loss: 56361084.0\n",
      "Epoch [36/100], Training Loss: 199021.94194656715, Test Loss: 55242928.0\n",
      "Epoch [37/100], Training Loss: 194715.35146022154, Test Loss: 54170356.0\n",
      "Epoch [38/100], Training Loss: 190577.08950891535, Test Loss: 53139380.0\n",
      "Epoch [39/100], Training Loss: 186593.71956637639, Test Loss: 52145756.0\n",
      "Epoch [40/100], Training Loss: 182750.06101534268, Test Loss: 51184592.0\n",
      "Epoch [41/100], Training Loss: 179035.8226408388, Test Loss: 50253264.0\n",
      "Epoch [42/100], Training Loss: 175445.520170606, Test Loss: 49351596.0\n",
      "Epoch [43/100], Training Loss: 171971.05124104023, Test Loss: 48477464.0\n",
      "Epoch [44/100], Training Loss: 168606.97506071915, Test Loss: 47628228.0\n",
      "Epoch [45/100], Training Loss: 165344.68787394112, Test Loss: 46803236.0\n",
      "Epoch [46/100], Training Loss: 162178.28647591968, Test Loss: 46001268.0\n",
      "Epoch [47/100], Training Loss: 159103.53355843847, Test Loss: 45221000.0\n",
      "Epoch [48/100], Training Loss: 156117.51910431846, Test Loss: 44461212.0\n",
      "Epoch [49/100], Training Loss: 153214.31810911675, Test Loss: 43720168.0\n",
      "Epoch [50/100], Training Loss: 150391.00242876608, Test Loss: 42997372.0\n",
      "Epoch [51/100], Training Loss: 147643.6603281796, Test Loss: 42291384.0\n",
      "Epoch [52/100], Training Loss: 144971.91967300515, Test Loss: 41602100.0\n",
      "Epoch [53/100], Training Loss: 142374.63823233222, Test Loss: 40929684.0\n",
      "Epoch [54/100], Training Loss: 139850.37391149814, Test Loss: 40274412.0\n",
      "Epoch [55/100], Training Loss: 137397.0386825425, Test Loss: 39635188.0\n",
      "Epoch [56/100], Training Loss: 135011.5426218826, Test Loss: 39011040.0\n",
      "Epoch [57/100], Training Loss: 132687.90284935726, Test Loss: 38401780.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [58/100], Training Loss: 130426.43759255968, Test Loss: 37806472.0\n",
      "Epoch [59/100], Training Loss: 128224.34837983531, Test Loss: 37226456.0\n",
      "Epoch [60/100], Training Loss: 126080.7397666015, Test Loss: 36661584.0\n",
      "Epoch [61/100], Training Loss: 123995.29879746461, Test Loss: 36111744.0\n",
      "Epoch [62/100], Training Loss: 121966.24773413897, Test Loss: 35576580.0\n",
      "Epoch [63/100], Training Loss: 119994.1753450625, Test Loss: 35056080.0\n",
      "Epoch [64/100], Training Loss: 118077.43581541377, Test Loss: 34549636.0\n",
      "Epoch [65/100], Training Loss: 116214.76903027072, Test Loss: 34056216.0\n",
      "Epoch [66/100], Training Loss: 114402.66992476748, Test Loss: 33575324.0\n",
      "Epoch [67/100], Training Loss: 112639.42787749541, Test Loss: 33106366.0\n",
      "Epoch [68/100], Training Loss: 110924.74012203068, Test Loss: 32649818.0\n",
      "Epoch [69/100], Training Loss: 109256.53794206504, Test Loss: 32205096.0\n",
      "Epoch [70/100], Training Loss: 107635.86452224394, Test Loss: 31772594.0\n",
      "Epoch [71/100], Training Loss: 106063.3374207689, Test Loss: 31352548.0\n",
      "Epoch [72/100], Training Loss: 104537.42870683016, Test Loss: 30944698.0\n",
      "Epoch [73/100], Training Loss: 103056.99182512885, Test Loss: 30549010.0\n",
      "Epoch [74/100], Training Loss: 101618.62300811564, Test Loss: 30164112.0\n",
      "Epoch [75/100], Training Loss: 100218.23955926782, Test Loss: 29790090.0\n",
      "Epoch [76/100], Training Loss: 98855.80694271666, Test Loss: 29426656.0\n",
      "Epoch [77/100], Training Loss: 97529.63876547597, Test Loss: 29072886.0\n",
      "Epoch [78/100], Training Loss: 96238.98489425982, Test Loss: 28728286.0\n",
      "Epoch [79/100], Training Loss: 94981.74438718085, Test Loss: 28392708.0\n",
      "Epoch [80/100], Training Loss: 93758.14910254131, Test Loss: 28066710.0\n",
      "Epoch [81/100], Training Loss: 92566.60689532611, Test Loss: 27749628.0\n",
      "Epoch [82/100], Training Loss: 91406.59297434986, Test Loss: 27441198.0\n",
      "Epoch [83/100], Training Loss: 90276.02488004265, Test Loss: 27140724.0\n",
      "Epoch [84/100], Training Loss: 89175.16503761626, Test Loss: 26849004.0\n",
      "Epoch [85/100], Training Loss: 88104.34974231385, Test Loss: 26565592.0\n",
      "Epoch [86/100], Training Loss: 87061.7637580712, Test Loss: 26289968.0\n",
      "Epoch [87/100], Training Loss: 86046.32515846218, Test Loss: 26022588.0\n",
      "Epoch [88/100], Training Loss: 85056.38102008175, Test Loss: 25762516.0\n",
      "Epoch [89/100], Training Loss: 84089.3540667022, Test Loss: 25508476.0\n",
      "Epoch [90/100], Training Loss: 83144.1181802026, Test Loss: 25261474.0\n",
      "Epoch [91/100], Training Loss: 82220.43765179788, Test Loss: 25020708.0\n",
      "Epoch [92/100], Training Loss: 81318.29992299035, Test Loss: 24786636.0\n",
      "Epoch [93/100], Training Loss: 80436.81191872519, Test Loss: 24558926.0\n",
      "Epoch [94/100], Training Loss: 79573.77471713761, Test Loss: 24336606.0\n",
      "Epoch [95/100], Training Loss: 78728.2030685386, Test Loss: 24120200.0\n",
      "Epoch [96/100], Training Loss: 77900.19708548072, Test Loss: 23909794.0\n",
      "Epoch [97/100], Training Loss: 77089.88774361709, Test Loss: 23705496.0\n",
      "Epoch [98/100], Training Loss: 76294.98128072981, Test Loss: 23506784.0\n",
      "Epoch [99/100], Training Loss: 75517.7731177063, Test Loss: 23313108.0\n",
      "Epoch [100/100], Training Loss: 74756.68698536817, Test Loss: 23123628.0\n",
      "Epoch [1/100], Training Loss: 582417.364848054, Test Loss: 300021664.0\n",
      "Epoch [2/100], Training Loss: 581830.631834607, Test Loss: 299326592.0\n",
      "Epoch [3/100], Training Loss: 578988.065635922, Test Loss: 296804160.0\n",
      "Epoch [4/100], Training Loss: 571164.9376221788, Test Loss: 290946112.0\n",
      "Epoch [5/100], Training Loss: 555539.4794147266, Test Loss: 280460544.0\n",
      "Epoch [6/100], Training Loss: 530204.5622889639, Test Loss: 264788144.0\n",
      "Epoch [7/100], Training Loss: 495142.32190036133, Test Loss: 244559968.0\n",
      "Epoch [8/100], Training Loss: 452904.80137432617, Test Loss: 221782976.0\n",
      "Epoch [9/100], Training Loss: 408459.83484390733, Test Loss: 199442736.0\n",
      "Epoch [10/100], Training Loss: 367741.50678277353, Test Loss: 180425392.0\n",
      "Epoch [11/100], Training Loss: 335166.0223920384, Test Loss: 166177008.0\n",
      "Epoch [12/100], Training Loss: 311656.889520763, Test Loss: 156243280.0\n",
      "Epoch [13/100], Training Loss: 295168.1734494402, Test Loss: 149194128.0\n",
      "Epoch [14/100], Training Loss: 282954.12641431193, Test Loss: 143759664.0\n",
      "Epoch [15/100], Training Loss: 273053.9825839701, Test Loss: 139185776.0\n",
      "Epoch [16/100], Training Loss: 264412.81677625736, Test Loss: 135100288.0\n",
      "Epoch [17/100], Training Loss: 256525.31200758248, Test Loss: 131326664.0\n",
      "Epoch [18/100], Training Loss: 249145.45631182985, Test Loss: 127775208.0\n",
      "Epoch [19/100], Training Loss: 242141.46792251643, Test Loss: 124395376.0\n",
      "Epoch [20/100], Training Loss: 235435.74811918725, Test Loss: 121156488.0\n",
      "Epoch [21/100], Training Loss: 228980.29927137017, Test Loss: 118039384.0\n",
      "Epoch [22/100], Training Loss: 222745.42882530656, Test Loss: 115032352.0\n",
      "Epoch [23/100], Training Loss: 216713.38261951305, Test Loss: 112128496.0\n",
      "Epoch [24/100], Training Loss: 210874.26811207866, Test Loss: 109324264.0\n",
      "Epoch [25/100], Training Loss: 205224.5959362597, Test Loss: 106618800.0\n",
      "Epoch [26/100], Training Loss: 199764.59877969316, Test Loss: 104012096.0\n",
      "Epoch [27/100], Training Loss: 194495.22018837745, Test Loss: 101504352.0\n",
      "Epoch [28/100], Training Loss: 189417.08595462353, Test Loss: 99095064.0\n",
      "Epoch [29/100], Training Loss: 184531.15644807773, Test Loss: 96783096.0\n",
      "Epoch [30/100], Training Loss: 179839.3102304366, Test Loss: 94568432.0\n",
      "Epoch [31/100], Training Loss: 175342.35175641254, Test Loss: 92450432.0\n",
      "Epoch [32/100], Training Loss: 171039.9578224039, Test Loss: 90428008.0\n",
      "Epoch [33/100], Training Loss: 166930.94721876667, Test Loss: 88499432.0\n",
      "Epoch [34/100], Training Loss: 163012.4722469048, Test Loss: 86661904.0\n",
      "Epoch [35/100], Training Loss: 159279.56376991884, Test Loss: 84911656.0\n",
      "Epoch [36/100], Training Loss: 155725.64445234288, Test Loss: 83244544.0\n",
      "Epoch [37/100], Training Loss: 152343.2033647296, Test Loss: 81655888.0\n",
      "Epoch [38/100], Training Loss: 149123.85735442213, Test Loss: 80140712.0\n",
      "Epoch [39/100], Training Loss: 146058.47900005925, Test Loss: 78693016.0\n",
      "Epoch [40/100], Training Loss: 143133.41271251702, Test Loss: 77305064.0\n",
      "Epoch [41/100], Training Loss: 140324.42485634738, Test Loss: 75966264.0\n",
      "Epoch [42/100], Training Loss: 137615.40690717375, Test Loss: 74673696.0\n",
      "Epoch [43/100], Training Loss: 135010.39772525325, Test Loss: 73423808.0\n",
      "Epoch [44/100], Training Loss: 132498.0901605355, Test Loss: 72214528.0\n",
      "Epoch [45/100], Training Loss: 130071.06593211302, Test Loss: 71039984.0\n",
      "Epoch [46/100], Training Loss: 127751.88863219004, Test Loss: 69917568.0\n",
      "Epoch [47/100], Training Loss: 125538.03566139446, Test Loss: 68837576.0\n",
      "Epoch [48/100], Training Loss: 123415.81991588176, Test Loss: 67799104.0\n",
      "Epoch [49/100], Training Loss: 121379.26070730407, Test Loss: 66798292.0\n",
      "Epoch [50/100], Training Loss: 119419.44908476986, Test Loss: 65830292.0\n",
      "Epoch [51/100], Training Loss: 117529.27030389194, Test Loss: 64894416.0\n",
      "Epoch [52/100], Training Loss: 115705.08168947337, Test Loss: 63989824.0\n",
      "Epoch [53/100], Training Loss: 113944.56134115277, Test Loss: 63113552.0\n",
      "Epoch [54/100], Training Loss: 112241.6385285232, Test Loss: 62264068.0\n",
      "Epoch [55/100], Training Loss: 110588.99733428114, Test Loss: 61438508.0\n",
      "Epoch [56/100], Training Loss: 108984.54688703276, Test Loss: 60636996.0\n",
      "Epoch [57/100], Training Loss: 107427.06285172679, Test Loss: 59858760.0\n",
      "Epoch [58/100], Training Loss: 105912.14702920443, Test Loss: 59100668.0\n",
      "Epoch [59/100], Training Loss: 104436.23173982585, Test Loss: 58361728.0\n",
      "Epoch [60/100], Training Loss: 102998.01078135181, Test Loss: 57642908.0\n",
      "Epoch [61/100], Training Loss: 101596.2433505124, Test Loss: 56941544.0\n",
      "Epoch [62/100], Training Loss: 100229.1495764469, Test Loss: 56256304.0\n",
      "Epoch [63/100], Training Loss: 98895.10360760619, Test Loss: 55587632.0\n",
      "Epoch [64/100], Training Loss: 97591.13559623245, Test Loss: 54933576.0\n",
      "Epoch [65/100], Training Loss: 96315.37385225993, Test Loss: 54294120.0\n",
      "Epoch [66/100], Training Loss: 95067.82382560275, Test Loss: 53668536.0\n",
      "Epoch [67/100], Training Loss: 93846.11219714471, Test Loss: 53056040.0\n",
      "Epoch [68/100], Training Loss: 92648.74474261004, Test Loss: 52454952.0\n",
      "Epoch [69/100], Training Loss: 91475.33534743202, Test Loss: 51865372.0\n",
      "Epoch [70/100], Training Loss: 90324.85919080624, Test Loss: 51286260.0\n",
      "Epoch [71/100], Training Loss: 89197.0976837865, Test Loss: 50718548.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [72/100], Training Loss: 88091.55464723654, Test Loss: 50160928.0\n",
      "Epoch [73/100], Training Loss: 87007.14069071738, Test Loss: 49613552.0\n",
      "Epoch [74/100], Training Loss: 85942.8450921154, Test Loss: 49075920.0\n",
      "Epoch [75/100], Training Loss: 84897.08713938748, Test Loss: 48545312.0\n",
      "Epoch [76/100], Training Loss: 83866.54368817013, Test Loss: 48022096.0\n",
      "Epoch [77/100], Training Loss: 82852.61678810498, Test Loss: 47508416.0\n",
      "Epoch [78/100], Training Loss: 81859.35596232451, Test Loss: 47004872.0\n",
      "Epoch [79/100], Training Loss: 80886.75114033528, Test Loss: 46510012.0\n",
      "Epoch [80/100], Training Loss: 79932.41691842901, Test Loss: 46023784.0\n",
      "Epoch [81/100], Training Loss: 78996.53598720455, Test Loss: 45545080.0\n",
      "Epoch [82/100], Training Loss: 78078.42224986671, Test Loss: 45073888.0\n",
      "Epoch [83/100], Training Loss: 77176.9870268349, Test Loss: 44609804.0\n",
      "Epoch [84/100], Training Loss: 76292.23328001896, Test Loss: 44153760.0\n",
      "Epoch [85/100], Training Loss: 75423.82797227653, Test Loss: 43705796.0\n",
      "Epoch [86/100], Training Loss: 74571.3929269593, Test Loss: 43265800.0\n",
      "Epoch [87/100], Training Loss: 73733.82690598899, Test Loss: 42833112.0\n",
      "Epoch [88/100], Training Loss: 72911.04211835792, Test Loss: 42407156.0\n",
      "Epoch [89/100], Training Loss: 72102.79355488419, Test Loss: 41988304.0\n",
      "Epoch [90/100], Training Loss: 71308.76843788875, Test Loss: 41575648.0\n",
      "Epoch [91/100], Training Loss: 70528.54890113145, Test Loss: 41169404.0\n",
      "Epoch [92/100], Training Loss: 69761.73544221313, Test Loss: 40769392.0\n",
      "Epoch [93/100], Training Loss: 69008.49238789172, Test Loss: 40375712.0\n",
      "Epoch [94/100], Training Loss: 68268.61133819087, Test Loss: 39987820.0\n",
      "Epoch [95/100], Training Loss: 67541.30383271133, Test Loss: 39605896.0\n",
      "Epoch [96/100], Training Loss: 66827.15964694034, Test Loss: 39229076.0\n",
      "Epoch [97/100], Training Loss: 66124.72827439132, Test Loss: 38858136.0\n",
      "Epoch [98/100], Training Loss: 65434.13079793851, Test Loss: 38493324.0\n",
      "Epoch [99/100], Training Loss: 64755.06842011729, Test Loss: 38133732.0\n",
      "Epoch [100/100], Training Loss: 64087.191280137435, Test Loss: 37779860.0\n",
      "Epoch [1/100], Training Loss: 7121858.887565902, Test Loss: 147950256.0\n",
      "Epoch [2/100], Training Loss: 4047366.274509804, Test Loss: 116754224.0\n",
      "Epoch [3/100], Training Loss: 3215013.396718204, Test Loss: 94598528.0\n",
      "Epoch [4/100], Training Loss: 2615580.700787868, Test Loss: 79035880.0\n",
      "Epoch [5/100], Training Loss: 2185951.951246964, Test Loss: 67801360.0\n",
      "Epoch [6/100], Training Loss: 1886574.1334636575, Test Loss: 59988668.0\n",
      "Epoch [7/100], Training Loss: 1681524.472306143, Test Loss: 54487756.0\n",
      "Epoch [8/100], Training Loss: 1532801.8089271963, Test Loss: 50299740.0\n",
      "Epoch [9/100], Training Loss: 1414035.6172620105, Test Loss: 46831544.0\n",
      "Epoch [10/100], Training Loss: 1313705.158965701, Test Loss: 43875304.0\n",
      "Epoch [11/100], Training Loss: 1227411.7115692198, Test Loss: 41267588.0\n",
      "Epoch [12/100], Training Loss: 1151037.4422723772, Test Loss: 38938272.0\n",
      "Epoch [13/100], Training Loss: 1084394.067087258, Test Loss: 36900248.0\n",
      "Epoch [14/100], Training Loss: 1025665.8973105858, Test Loss: 35086360.0\n",
      "Epoch [15/100], Training Loss: 973540.1022747468, Test Loss: 33479712.0\n",
      "Epoch [16/100], Training Loss: 927064.4869083585, Test Loss: 32017534.0\n",
      "Epoch [17/100], Training Loss: 884238.5600971506, Test Loss: 30644990.0\n",
      "Epoch [18/100], Training Loss: 843530.4029085955, Test Loss: 29367674.0\n",
      "Epoch [19/100], Training Loss: 807595.2221580475, Test Loss: 28266622.0\n",
      "Epoch [20/100], Training Loss: 775919.089849535, Test Loss: 27263124.0\n",
      "Epoch [21/100], Training Loss: 745964.9764231977, Test Loss: 26315220.0\n",
      "Epoch [22/100], Training Loss: 718869.3765179788, Test Loss: 25483832.0\n",
      "Epoch [23/100], Training Loss: 694349.1864226053, Test Loss: 24719932.0\n",
      "Epoch [24/100], Training Loss: 670809.8890468574, Test Loss: 23964126.0\n",
      "Epoch [25/100], Training Loss: 648258.6680735738, Test Loss: 23254866.0\n",
      "Epoch [26/100], Training Loss: 626908.5797494224, Test Loss: 22610984.0\n",
      "Epoch [27/100], Training Loss: 606948.1109827617, Test Loss: 22004812.0\n",
      "Epoch [28/100], Training Loss: 588699.8939636278, Test Loss: 21496956.0\n",
      "Epoch [29/100], Training Loss: 572344.7247497187, Test Loss: 21034306.0\n",
      "Epoch [30/100], Training Loss: 557085.6130857177, Test Loss: 20614084.0\n",
      "Epoch [31/100], Training Loss: 542384.9530685386, Test Loss: 20182566.0\n",
      "Epoch [32/100], Training Loss: 527676.1051774184, Test Loss: 19762780.0\n",
      "Epoch [33/100], Training Loss: 512929.3059726912, Test Loss: 19367878.0\n",
      "Epoch [34/100], Training Loss: 499487.49001836387, Test Loss: 19010358.0\n",
      "Epoch [35/100], Training Loss: 486924.71381286654, Test Loss: 18687116.0\n",
      "Epoch [36/100], Training Loss: 475610.1279322907, Test Loss: 18417576.0\n",
      "Epoch [37/100], Training Loss: 465404.61137521476, Test Loss: 18176344.0\n",
      "Epoch [38/100], Training Loss: 456111.2198625674, Test Loss: 17976378.0\n",
      "Epoch [39/100], Training Loss: 447317.79094840353, Test Loss: 17768658.0\n",
      "Epoch [40/100], Training Loss: 438831.9216648895, Test Loss: 17589152.0\n",
      "Epoch [41/100], Training Loss: 430722.6040148688, Test Loss: 17416666.0\n",
      "Epoch [42/100], Training Loss: 423344.0164756235, Test Loss: 17281124.0\n",
      "Epoch [43/100], Training Loss: 416473.3488093123, Test Loss: 17148478.0\n",
      "Epoch [44/100], Training Loss: 409984.4397991825, Test Loss: 17038490.0\n",
      "Epoch [45/100], Training Loss: 403883.71042888454, Test Loss: 16925014.0\n",
      "Epoch [46/100], Training Loss: 397911.8980658729, Test Loss: 16825876.0\n",
      "Epoch [47/100], Training Loss: 392193.2104733132, Test Loss: 16717462.0\n",
      "Epoch [48/100], Training Loss: 386748.3792651502, Test Loss: 16619218.0\n",
      "Epoch [49/100], Training Loss: 381719.83727267344, Test Loss: 16529150.0\n",
      "Epoch [50/100], Training Loss: 376941.6670220958, Test Loss: 16402329.0\n",
      "Epoch [51/100], Training Loss: 372375.8062022392, Test Loss: 16353739.0\n",
      "Epoch [52/100], Training Loss: 368043.4710399265, Test Loss: 16252718.0\n",
      "Epoch [53/100], Training Loss: 363999.38667732954, Test Loss: 16190111.0\n",
      "Epoch [54/100], Training Loss: 360092.4747497186, Test Loss: 16107289.0\n",
      "Epoch [55/100], Training Loss: 356366.0167570049, Test Loss: 16017919.0\n",
      "Epoch [56/100], Training Loss: 352774.8200343581, Test Loss: 15991489.0\n",
      "Epoch [57/100], Training Loss: 349400.1610538475, Test Loss: 15874453.0\n",
      "Epoch [58/100], Training Loss: 346020.9537719922, Test Loss: 15858495.0\n",
      "Epoch [59/100], Training Loss: 342774.2036164919, Test Loss: 15752229.0\n",
      "Epoch [60/100], Training Loss: 339557.5130027842, Test Loss: 15721504.0\n",
      "Epoch [61/100], Training Loss: 336523.40297523845, Test Loss: 15647122.0\n",
      "Epoch [62/100], Training Loss: 333538.12953172205, Test Loss: 15615606.0\n",
      "Epoch [63/100], Training Loss: 330746.2739914697, Test Loss: 15563113.0\n",
      "Epoch [64/100], Training Loss: 328020.67472306144, Test Loss: 15548468.0\n",
      "Epoch [65/100], Training Loss: 325359.88350808603, Test Loss: 15554629.0\n",
      "Epoch [66/100], Training Loss: 322748.08628043364, Test Loss: 15479326.0\n",
      "Epoch [67/100], Training Loss: 320099.25872282445, Test Loss: 15490082.0\n",
      "Epoch [68/100], Training Loss: 317550.8803092234, Test Loss: 15413313.0\n",
      "Epoch [69/100], Training Loss: 315101.15740329365, Test Loss: 15416169.0\n",
      "Epoch [70/100], Training Loss: 312736.1799878562, Test Loss: 15361698.0\n",
      "Epoch [71/100], Training Loss: 310484.42556720576, Test Loss: 15355965.0\n",
      "Epoch [72/100], Training Loss: 308303.29936763225, Test Loss: 15336720.0\n",
      "Epoch [73/100], Training Loss: 306205.85658432555, Test Loss: 15269904.0\n",
      "Epoch [74/100], Training Loss: 304058.6902064451, Test Loss: 15296872.0\n",
      "Epoch [75/100], Training Loss: 301995.65898791543, Test Loss: 15247896.0\n",
      "Epoch [76/100], Training Loss: 299943.2154863456, Test Loss: 15221292.0\n",
      "Epoch [77/100], Training Loss: 297928.5278641668, Test Loss: 15204682.0\n",
      "Epoch [78/100], Training Loss: 295931.18556365144, Test Loss: 15128477.0\n",
      "Epoch [79/100], Training Loss: 294009.6804099283, Test Loss: 15151549.0\n",
      "Epoch [80/100], Training Loss: 292140.2455941591, Test Loss: 15117184.0\n",
      "Epoch [81/100], Training Loss: 290382.68103933416, Test Loss: 15093586.0\n",
      "Epoch [82/100], Training Loss: 288653.8010114922, Test Loss: 15106866.0\n",
      "Epoch [83/100], Training Loss: 287009.1450595344, Test Loss: 15042516.0\n",
      "Epoch [84/100], Training Loss: 285368.71336117527, Test Loss: 15081088.0\n",
      "Epoch [85/100], Training Loss: 283820.7962946508, Test Loss: 15037643.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [86/100], Training Loss: 282316.8989692554, Test Loss: 15054169.0\n",
      "Epoch [87/100], Training Loss: 280843.6313681062, Test Loss: 15067752.0\n",
      "Epoch [88/100], Training Loss: 279440.0856140039, Test Loss: 15011274.0\n",
      "Epoch [89/100], Training Loss: 278003.2504961199, Test Loss: 15043968.0\n",
      "Epoch [90/100], Training Loss: 276596.4228348439, Test Loss: 15034519.0\n",
      "Epoch [91/100], Training Loss: 275270.40646288724, Test Loss: 15007415.0\n",
      "Epoch [92/100], Training Loss: 273913.7210177122, Test Loss: 15062881.0\n",
      "Epoch [93/100], Training Loss: 272654.57437355607, Test Loss: 14991679.0\n",
      "Epoch [94/100], Training Loss: 271343.2247608258, Test Loss: 15033107.0\n",
      "Epoch [95/100], Training Loss: 270113.3382352941, Test Loss: 14988899.0\n",
      "Epoch [96/100], Training Loss: 268899.5796790771, Test Loss: 15010429.0\n",
      "Epoch [97/100], Training Loss: 267718.19124311354, Test Loss: 15042666.0\n",
      "Epoch [98/100], Training Loss: 266572.25091819203, Test Loss: 14983411.0\n",
      "Epoch [99/100], Training Loss: 265423.8738707719, Test Loss: 15005803.0\n",
      "Epoch [100/100], Training Loss: 264293.2735027546, Test Loss: 14989082.0\n",
      "Epoch [1/100], Training Loss: 4362557.100882649, Test Loss: 225098608.0\n",
      "Epoch [2/100], Training Loss: 2617878.0995201706, Test Loss: 143234960.0\n",
      "Epoch [3/100], Training Loss: 2048309.9732243351, Test Loss: 123419952.0\n",
      "Epoch [4/100], Training Loss: 1767317.1698359102, Test Loss: 107579664.0\n",
      "Epoch [5/100], Training Loss: 1535798.313962443, Test Loss: 94645368.0\n",
      "Epoch [6/100], Training Loss: 1346562.2181150406, Test Loss: 84184488.0\n",
      "Epoch [7/100], Training Loss: 1192391.68396422, Test Loss: 75620168.0\n",
      "Epoch [8/100], Training Loss: 1067314.0302114803, Test Loss: 68620128.0\n",
      "Epoch [9/100], Training Loss: 966478.8883952373, Test Loss: 62929644.0\n",
      "Epoch [10/100], Training Loss: 885450.760973876, Test Loss: 58339296.0\n",
      "Epoch [11/100], Training Loss: 820017.590901013, Test Loss: 54577712.0\n",
      "Epoch [12/100], Training Loss: 765699.5374681595, Test Loss: 51386248.0\n",
      "Epoch [13/100], Training Loss: 718930.882293703, Test Loss: 48587416.0\n",
      "Epoch [14/100], Training Loss: 677431.5526923761, Test Loss: 46065960.0\n",
      "Epoch [15/100], Training Loss: 639923.7430839405, Test Loss: 43762192.0\n",
      "Epoch [16/100], Training Loss: 605702.5033469581, Test Loss: 41631524.0\n",
      "Epoch [17/100], Training Loss: 574297.665866951, Test Loss: 39651532.0\n",
      "Epoch [18/100], Training Loss: 545392.6088205675, Test Loss: 37809640.0\n",
      "Epoch [19/100], Training Loss: 518827.2048456845, Test Loss: 36097008.0\n",
      "Epoch [20/100], Training Loss: 494438.3241810319, Test Loss: 34510660.0\n",
      "Epoch [21/100], Training Loss: 472037.60805047094, Test Loss: 33050848.0\n",
      "Epoch [22/100], Training Loss: 451449.2491854748, Test Loss: 31703082.0\n",
      "Epoch [23/100], Training Loss: 432525.02322137315, Test Loss: 30461350.0\n",
      "Epoch [24/100], Training Loss: 415134.9169776672, Test Loss: 29318354.0\n",
      "Epoch [25/100], Training Loss: 399157.0428292163, Test Loss: 28265024.0\n",
      "Epoch [26/100], Training Loss: 384485.172042533, Test Loss: 27292640.0\n",
      "Epoch [27/100], Training Loss: 371013.34368520824, Test Loss: 26396450.0\n",
      "Epoch [28/100], Training Loss: 358609.24395770393, Test Loss: 25570302.0\n",
      "Epoch [29/100], Training Loss: 347137.9748533855, Test Loss: 24807184.0\n",
      "Epoch [30/100], Training Loss: 336453.20555654285, Test Loss: 24096272.0\n",
      "Epoch [31/100], Training Loss: 326455.4146969966, Test Loss: 23435910.0\n",
      "Epoch [32/100], Training Loss: 317052.6796250222, Test Loss: 22821646.0\n",
      "Epoch [33/100], Training Loss: 308202.24161779514, Test Loss: 22250748.0\n",
      "Epoch [34/100], Training Loss: 299870.5, Test Loss: 21721540.0\n",
      "Epoch [35/100], Training Loss: 292001.5025028138, Test Loss: 21231874.0\n",
      "Epoch [36/100], Training Loss: 284580.3044547124, Test Loss: 20777734.0\n",
      "Epoch [37/100], Training Loss: 277584.0857028612, Test Loss: 20356376.0\n",
      "Epoch [38/100], Training Loss: 270976.7877495409, Test Loss: 19962876.0\n",
      "Epoch [39/100], Training Loss: 264716.8791096499, Test Loss: 19600046.0\n",
      "Epoch [40/100], Training Loss: 258787.83340738108, Test Loss: 19259060.0\n",
      "Epoch [41/100], Training Loss: 253167.80974172146, Test Loss: 18945410.0\n",
      "Epoch [42/100], Training Loss: 247836.1279693146, Test Loss: 18651500.0\n",
      "Epoch [43/100], Training Loss: 242776.4861530715, Test Loss: 18377922.0\n",
      "Epoch [44/100], Training Loss: 237959.74862271192, Test Loss: 18115174.0\n",
      "Epoch [45/100], Training Loss: 233361.59275220663, Test Loss: 17876076.0\n",
      "Epoch [46/100], Training Loss: 228970.26515016882, Test Loss: 17643436.0\n",
      "Epoch [47/100], Training Loss: 224777.3642112434, Test Loss: 17432956.0\n",
      "Epoch [48/100], Training Loss: 220781.80204075587, Test Loss: 17228874.0\n",
      "Epoch [49/100], Training Loss: 216968.83481428825, Test Loss: 17046188.0\n",
      "Epoch [50/100], Training Loss: 213321.08748000712, Test Loss: 16868514.0\n",
      "Epoch [51/100], Training Loss: 209834.50225105148, Test Loss: 16709720.0\n",
      "Epoch [52/100], Training Loss: 206502.50678277353, Test Loss: 16555347.0\n",
      "Epoch [53/100], Training Loss: 203313.31350334696, Test Loss: 16421554.0\n",
      "Epoch [54/100], Training Loss: 200283.80187785084, Test Loss: 16286206.0\n",
      "Epoch [55/100], Training Loss: 197363.68698536817, Test Loss: 16179656.0\n",
      "Epoch [56/100], Training Loss: 194583.56747230614, Test Loss: 16056002.0\n",
      "Epoch [57/100], Training Loss: 191915.62768793319, Test Loss: 15966130.0\n",
      "Epoch [58/100], Training Loss: 189364.49201765298, Test Loss: 15865811.0\n",
      "Epoch [59/100], Training Loss: 186906.51683845744, Test Loss: 15784674.0\n",
      "Epoch [60/100], Training Loss: 184567.39251229193, Test Loss: 15697264.0\n",
      "Epoch [61/100], Training Loss: 182314.60354836797, Test Loss: 15632178.0\n",
      "Epoch [62/100], Training Loss: 180148.377036313, Test Loss: 15553422.0\n",
      "Epoch [63/100], Training Loss: 178075.6375362834, Test Loss: 15501801.0\n",
      "Epoch [64/100], Training Loss: 176093.5369350157, Test Loss: 15431688.0\n",
      "Epoch [65/100], Training Loss: 174182.83160061607, Test Loss: 15390686.0\n",
      "Epoch [66/100], Training Loss: 172367.71202831584, Test Loss: 15325181.0\n",
      "Epoch [67/100], Training Loss: 170612.61949825246, Test Loss: 15287659.0\n",
      "Epoch [68/100], Training Loss: 168947.34238937267, Test Loss: 15218542.0\n",
      "Epoch [69/100], Training Loss: 167328.49793406788, Test Loss: 15186087.0\n",
      "Epoch [70/100], Training Loss: 165784.22649724543, Test Loss: 15119818.0\n",
      "Epoch [71/100], Training Loss: 164290.99007760204, Test Loss: 15090505.0\n",
      "Epoch [72/100], Training Loss: 162872.63997245423, Test Loss: 15025531.0\n",
      "Epoch [73/100], Training Loss: 161493.52415437475, Test Loss: 15000274.0\n",
      "Epoch [74/100], Training Loss: 160181.71069545642, Test Loss: 14935553.0\n",
      "Epoch [75/100], Training Loss: 158904.8322078076, Test Loss: 14945866.0\n",
      "Epoch [76/100], Training Loss: 157705.99109946092, Test Loss: 14848134.0\n",
      "Epoch [77/100], Training Loss: 156514.39519282032, Test Loss: 14867922.0\n",
      "Epoch [78/100], Training Loss: 155393.0119809253, Test Loss: 14794952.0\n",
      "Epoch [79/100], Training Loss: 154295.72343166874, Test Loss: 14798602.0\n",
      "Epoch [80/100], Training Loss: 153246.3364063148, Test Loss: 14737050.0\n",
      "Epoch [81/100], Training Loss: 152213.51636455188, Test Loss: 14737089.0\n",
      "Epoch [82/100], Training Loss: 151241.9327424323, Test Loss: 14684018.0\n",
      "Epoch [83/100], Training Loss: 150280.57514365262, Test Loss: 14682695.0\n",
      "Epoch [84/100], Training Loss: 149354.94428647592, Test Loss: 14646688.0\n",
      "Epoch [85/100], Training Loss: 148465.92054676855, Test Loss: 14626664.0\n",
      "Epoch [86/100], Training Loss: 147609.75374681596, Test Loss: 14605950.0\n",
      "Epoch [87/100], Training Loss: 146771.39728837155, Test Loss: 14637372.0\n",
      "Epoch [88/100], Training Loss: 145981.33636188615, Test Loss: 14552565.0\n",
      "Epoch [89/100], Training Loss: 145187.0000814525, Test Loss: 14583152.0\n",
      "Epoch [90/100], Training Loss: 144447.0108109709, Test Loss: 14535665.0\n",
      "Epoch [91/100], Training Loss: 143702.03694982524, Test Loss: 14548774.0\n",
      "Epoch [92/100], Training Loss: 142992.9824432794, Test Loss: 14506316.0\n",
      "Epoch [93/100], Training Loss: 142289.0833407381, Test Loss: 14514286.0\n",
      "Epoch [94/100], Training Loss: 141605.8318301641, Test Loss: 14472223.0\n",
      "Epoch [95/100], Training Loss: 140935.04967122801, Test Loss: 14481621.0\n",
      "Epoch [96/100], Training Loss: 140286.68415674428, Test Loss: 14442244.0\n",
      "Epoch [97/100], Training Loss: 139630.82615810673, Test Loss: 14461633.0\n",
      "Epoch [98/100], Training Loss: 139022.0028286239, Test Loss: 14398460.0\n",
      "Epoch [99/100], Training Loss: 138402.38000562764, Test Loss: 14428009.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Training Loss: 137819.28082607666, Test Loss: 14442947.0\n",
      "Epoch [1/100], Training Loss: 2316234.0723890765, Test Loss: 291172256.0\n",
      "Epoch [2/100], Training Loss: 2031819.0751732718, Test Loss: 220678480.0\n",
      "Epoch [3/100], Training Loss: 1421049.8143474914, Test Loss: 157575968.0\n",
      "Epoch [4/100], Training Loss: 1141202.284461821, Test Loss: 139597952.0\n",
      "Epoch [5/100], Training Loss: 1029887.2682897933, Test Loss: 127738072.0\n",
      "Epoch [6/100], Training Loss: 941640.1085243765, Test Loss: 117453464.0\n",
      "Epoch [7/100], Training Loss: 862656.22700077, Test Loss: 108068152.0\n",
      "Epoch [8/100], Training Loss: 789509.6330786091, Test Loss: 99562112.0\n",
      "Epoch [9/100], Training Loss: 725257.5533439962, Test Loss: 92318064.0\n",
      "Epoch [10/100], Training Loss: 669991.3360582904, Test Loss: 86074456.0\n",
      "Epoch [11/100], Training Loss: 622148.8549256561, Test Loss: 80640344.0\n",
      "Epoch [12/100], Training Loss: 580637.1307387003, Test Loss: 75897296.0\n",
      "Epoch [13/100], Training Loss: 544688.6901249926, Test Loss: 71751824.0\n",
      "Epoch [14/100], Training Loss: 513538.33825010364, Test Loss: 68121280.0\n",
      "Epoch [15/100], Training Loss: 486528.8430780167, Test Loss: 64943692.0\n",
      "Epoch [16/100], Training Loss: 463049.40631479176, Test Loss: 62162416.0\n",
      "Epoch [17/100], Training Loss: 442489.06486582546, Test Loss: 59696536.0\n",
      "Epoch [18/100], Training Loss: 424256.4240270126, Test Loss: 57493164.0\n",
      "Epoch [19/100], Training Loss: 407873.86256738345, Test Loss: 55496340.0\n",
      "Epoch [20/100], Training Loss: 392964.3039511877, Test Loss: 53663868.0\n",
      "Epoch [21/100], Training Loss: 379202.82187074225, Test Loss: 51966840.0\n",
      "Epoch [22/100], Training Loss: 366422.3297790415, Test Loss: 50386788.0\n",
      "Epoch [23/100], Training Loss: 354473.2832178188, Test Loss: 48907992.0\n",
      "Epoch [24/100], Training Loss: 343257.4215390083, Test Loss: 47516072.0\n",
      "Epoch [25/100], Training Loss: 332715.99360227474, Test Loss: 46211636.0\n",
      "Epoch [26/100], Training Loss: 322785.6622830401, Test Loss: 44979552.0\n",
      "Epoch [27/100], Training Loss: 313422.0532551389, Test Loss: 43809432.0\n",
      "Epoch [28/100], Training Loss: 304577.72945915523, Test Loss: 42697736.0\n",
      "Epoch [29/100], Training Loss: 296224.78259581776, Test Loss: 41640860.0\n",
      "Epoch [30/100], Training Loss: 288317.63929861976, Test Loss: 40635136.0\n",
      "Epoch [31/100], Training Loss: 280814.82163378946, Test Loss: 39677792.0\n",
      "Epoch [32/100], Training Loss: 273669.6659558083, Test Loss: 38759668.0\n",
      "Epoch [33/100], Training Loss: 266856.7090219774, Test Loss: 37882804.0\n",
      "Epoch [34/100], Training Loss: 260379.61945382383, Test Loss: 37050604.0\n",
      "Epoch [35/100], Training Loss: 254219.81043777027, Test Loss: 36257920.0\n",
      "Epoch [36/100], Training Loss: 248356.29625022213, Test Loss: 35504072.0\n",
      "Epoch [37/100], Training Loss: 242770.0001777146, Test Loss: 34784628.0\n",
      "Epoch [38/100], Training Loss: 237463.96978851964, Test Loss: 34100124.0\n",
      "Epoch [39/100], Training Loss: 232396.30750547952, Test Loss: 33445866.0\n",
      "Epoch [40/100], Training Loss: 227557.15597417214, Test Loss: 32820160.0\n",
      "Epoch [41/100], Training Loss: 222910.18908832414, Test Loss: 32212672.0\n",
      "Epoch [42/100], Training Loss: 218401.17620401634, Test Loss: 31621008.0\n",
      "Epoch [43/100], Training Loss: 214039.70976245482, Test Loss: 31055058.0\n",
      "Epoch [44/100], Training Loss: 209869.72756353297, Test Loss: 30514662.0\n",
      "Epoch [45/100], Training Loss: 205880.7488300456, Test Loss: 29997524.0\n",
      "Epoch [46/100], Training Loss: 202058.4316983591, Test Loss: 29500116.0\n",
      "Epoch [47/100], Training Loss: 198392.62022392038, Test Loss: 29026036.0\n",
      "Epoch [48/100], Training Loss: 194875.46940347136, Test Loss: 28570048.0\n",
      "Epoch [49/100], Training Loss: 191479.80119661157, Test Loss: 28131194.0\n",
      "Epoch [50/100], Training Loss: 188186.72957763166, Test Loss: 27701784.0\n",
      "Epoch [51/100], Training Loss: 184981.61945382383, Test Loss: 27284306.0\n",
      "Epoch [52/100], Training Loss: 181877.23019963273, Test Loss: 26882862.0\n",
      "Epoch [53/100], Training Loss: 178907.3468692613, Test Loss: 26505770.0\n",
      "Epoch [54/100], Training Loss: 176075.8247734139, Test Loss: 26144154.0\n",
      "Epoch [55/100], Training Loss: 173347.27572418697, Test Loss: 25798974.0\n",
      "Epoch [56/100], Training Loss: 170698.20558616196, Test Loss: 25462654.0\n",
      "Epoch [57/100], Training Loss: 168112.24933357027, Test Loss: 25132290.0\n",
      "Epoch [58/100], Training Loss: 165539.822700077, Test Loss: 24799500.0\n",
      "Epoch [59/100], Training Loss: 162930.0649843019, Test Loss: 24460388.0\n",
      "Epoch [60/100], Training Loss: 160323.6326935608, Test Loss: 24129992.0\n",
      "Epoch [61/100], Training Loss: 157836.96730051536, Test Loss: 23822442.0\n",
      "Epoch [62/100], Training Loss: 155484.46789289734, Test Loss: 23528958.0\n",
      "Epoch [63/100], Training Loss: 153209.59513654406, Test Loss: 23244760.0\n",
      "Epoch [64/100], Training Loss: 151000.9080919377, Test Loss: 22974134.0\n",
      "Epoch [65/100], Training Loss: 148890.78526153663, Test Loss: 22720058.0\n",
      "Epoch [66/100], Training Loss: 146875.50829334755, Test Loss: 22476080.0\n",
      "Epoch [67/100], Training Loss: 144919.63076831942, Test Loss: 22239738.0\n",
      "Epoch [68/100], Training Loss: 142973.71473846337, Test Loss: 21998968.0\n",
      "Epoch [69/100], Training Loss: 141025.43240921746, Test Loss: 21769608.0\n",
      "Epoch [70/100], Training Loss: 139130.84011610688, Test Loss: 21543700.0\n",
      "Epoch [71/100], Training Loss: 137323.119809253, Test Loss: 21337868.0\n",
      "Epoch [72/100], Training Loss: 135578.89135714708, Test Loss: 21135024.0\n",
      "Epoch [73/100], Training Loss: 133875.78105562465, Test Loss: 20940650.0\n",
      "Epoch [74/100], Training Loss: 132170.74053669805, Test Loss: 20741586.0\n",
      "Epoch [75/100], Training Loss: 130421.31810911676, Test Loss: 20542148.0\n",
      "Epoch [76/100], Training Loss: 128659.79651679403, Test Loss: 20345294.0\n",
      "Epoch [77/100], Training Loss: 126982.20072862982, Test Loss: 20166118.0\n",
      "Epoch [78/100], Training Loss: 125409.73650850067, Test Loss: 19994590.0\n",
      "Epoch [79/100], Training Loss: 123917.24323203602, Test Loss: 19840032.0\n",
      "Epoch [80/100], Training Loss: 122491.06009715064, Test Loss: 19682766.0\n",
      "Epoch [81/100], Training Loss: 121094.95582311474, Test Loss: 19537448.0\n",
      "Epoch [82/100], Training Loss: 119717.24527575381, Test Loss: 19387412.0\n",
      "Epoch [83/100], Training Loss: 118358.0739588887, Test Loss: 19249678.0\n",
      "Epoch [84/100], Training Loss: 117031.1247112138, Test Loss: 19111174.0\n",
      "Epoch [85/100], Training Loss: 115732.9243528227, Test Loss: 18981638.0\n",
      "Epoch [86/100], Training Loss: 114436.80848290978, Test Loss: 18847738.0\n",
      "Epoch [87/100], Training Loss: 113177.55509152301, Test Loss: 18728406.0\n",
      "Epoch [88/100], Training Loss: 111984.10248208045, Test Loss: 18612388.0\n",
      "Epoch [89/100], Training Loss: 110850.81043777027, Test Loss: 18508412.0\n",
      "Epoch [90/100], Training Loss: 109757.98563473728, Test Loss: 18402844.0\n",
      "Epoch [91/100], Training Loss: 108710.25502043718, Test Loss: 18309676.0\n",
      "Epoch [92/100], Training Loss: 107700.02730880873, Test Loss: 18213366.0\n",
      "Epoch [93/100], Training Loss: 106725.9527723476, Test Loss: 18126552.0\n",
      "Epoch [94/100], Training Loss: 105768.4760973876, Test Loss: 18034458.0\n",
      "Epoch [95/100], Training Loss: 104820.30775724187, Test Loss: 17950142.0\n",
      "Epoch [96/100], Training Loss: 103889.62134944613, Test Loss: 17865728.0\n",
      "Epoch [97/100], Training Loss: 102987.58817605593, Test Loss: 17786188.0\n",
      "Epoch [98/100], Training Loss: 102111.30279604289, Test Loss: 17712054.0\n",
      "Epoch [99/100], Training Loss: 101273.84137491854, Test Loss: 17636974.0\n",
      "Epoch [100/100], Training Loss: 100469.61344114685, Test Loss: 17572974.0\n",
      "Epoch [1/100], Training Loss: 1497705.1814465967, Test Loss: 298661728.0\n",
      "Epoch [2/100], Training Loss: 1465733.9323499792, Test Loss: 284193856.0\n",
      "Epoch [3/100], Training Loss: 1332026.416918429, Test Loss: 243961664.0\n",
      "Epoch [4/100], Training Loss: 1093181.2001658669, Test Loss: 193142128.0\n",
      "Epoch [5/100], Training Loss: 879008.5061311533, Test Loss: 161578144.0\n",
      "Epoch [6/100], Training Loss: 767823.1100053315, Test Loss: 147124800.0\n",
      "Epoch [7/100], Training Loss: 708648.2303181092, Test Loss: 137782240.0\n",
      "Epoch [8/100], Training Loss: 664076.7610923523, Test Loss: 130069648.0\n",
      "Epoch [9/100], Training Loss: 625330.4562525917, Test Loss: 123259712.0\n",
      "Epoch [10/100], Training Loss: 590320.0097150643, Test Loss: 117127160.0\n",
      "Epoch [11/100], Training Loss: 558314.0622001067, Test Loss: 111577272.0\n",
      "Epoch [12/100], Training Loss: 528963.153841597, Test Loss: 106534480.0\n",
      "Epoch [13/100], Training Loss: 501659.7215804751, Test Loss: 101834840.0\n",
      "Epoch [14/100], Training Loss: 476092.18529707956, Test Loss: 97583008.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/100], Training Loss: 453216.4191694805, Test Loss: 93861560.0\n",
      "Epoch [16/100], Training Loss: 432806.032817961, Test Loss: 90536208.0\n",
      "Epoch [17/100], Training Loss: 414413.37716959894, Test Loss: 87533864.0\n",
      "Epoch [18/100], Training Loss: 397772.47722291335, Test Loss: 84804168.0\n",
      "Epoch [19/100], Training Loss: 382681.239618506, Test Loss: 82310128.0\n",
      "Epoch [20/100], Training Loss: 368994.7692672235, Test Loss: 80025784.0\n",
      "Epoch [21/100], Training Loss: 356576.454238493, Test Loss: 77927272.0\n",
      "Epoch [22/100], Training Loss: 345298.3672768201, Test Loss: 75990120.0\n",
      "Epoch [23/100], Training Loss: 335037.1400983354, Test Loss: 74201504.0\n",
      "Epoch [24/100], Training Loss: 325692.3388424856, Test Loss: 72542440.0\n",
      "Epoch [25/100], Training Loss: 317158.51027782715, Test Loss: 70997224.0\n",
      "Epoch [26/100], Training Loss: 309328.3726082578, Test Loss: 69548112.0\n",
      "Epoch [27/100], Training Loss: 302104.3889580001, Test Loss: 68183968.0\n",
      "Epoch [28/100], Training Loss: 295403.7984124163, Test Loss: 66894120.0\n",
      "Epoch [29/100], Training Loss: 289146.9055150761, Test Loss: 65665612.0\n",
      "Epoch [30/100], Training Loss: 283259.5547805225, Test Loss: 64491152.0\n",
      "Epoch [31/100], Training Loss: 277677.67381967895, Test Loss: 63360600.0\n",
      "Epoch [32/100], Training Loss: 272358.0839571856, Test Loss: 62269816.0\n",
      "Epoch [33/100], Training Loss: 267262.4245051123, Test Loss: 61213496.0\n",
      "Epoch [34/100], Training Loss: 262349.72781205573, Test Loss: 60187896.0\n",
      "Epoch [35/100], Training Loss: 257590.44519170962, Test Loss: 59189140.0\n",
      "Epoch [36/100], Training Loss: 252954.9196878147, Test Loss: 58212832.0\n",
      "Epoch [37/100], Training Loss: 248425.900887092, Test Loss: 57257568.0\n",
      "Epoch [38/100], Training Loss: 243990.23884100467, Test Loss: 56321860.0\n",
      "Epoch [39/100], Training Loss: 239634.67602630175, Test Loss: 55406340.0\n",
      "Epoch [40/100], Training Loss: 235356.63992062083, Test Loss: 54507508.0\n",
      "Epoch [41/100], Training Loss: 231154.45443101713, Test Loss: 53625924.0\n",
      "Epoch [42/100], Training Loss: 227025.68824417985, Test Loss: 52760796.0\n",
      "Epoch [43/100], Training Loss: 222973.37740655174, Test Loss: 51913688.0\n",
      "Epoch [44/100], Training Loss: 219007.67460458505, Test Loss: 51086524.0\n",
      "Epoch [45/100], Training Loss: 215123.993528227, Test Loss: 50276716.0\n",
      "Epoch [46/100], Training Loss: 211326.73785616967, Test Loss: 49487076.0\n",
      "Epoch [47/100], Training Loss: 207619.0522925182, Test Loss: 48720516.0\n",
      "Epoch [48/100], Training Loss: 203996.11148628636, Test Loss: 47972560.0\n",
      "Epoch [49/100], Training Loss: 200465.08662105325, Test Loss: 47242216.0\n",
      "Epoch [50/100], Training Loss: 197025.21754487292, Test Loss: 46526648.0\n",
      "Epoch [51/100], Training Loss: 193670.96754487292, Test Loss: 45826664.0\n",
      "Epoch [52/100], Training Loss: 190400.0952105918, Test Loss: 45145064.0\n",
      "Epoch [53/100], Training Loss: 187213.8611530715, Test Loss: 44481056.0\n",
      "Epoch [54/100], Training Loss: 184108.78510973876, Test Loss: 43832732.0\n",
      "Epoch [55/100], Training Loss: 181086.6081504354, Test Loss: 43198112.0\n",
      "Epoch [56/100], Training Loss: 178149.49885596233, Test Loss: 42580936.0\n",
      "Epoch [57/100], Training Loss: 175293.76649783782, Test Loss: 41980568.0\n",
      "Epoch [58/100], Training Loss: 172522.69170776315, Test Loss: 41394384.0\n",
      "Epoch [59/100], Training Loss: 169835.37532581008, Test Loss: 40822492.0\n",
      "Epoch [60/100], Training Loss: 167227.08523265802, Test Loss: 40265196.0\n",
      "Epoch [61/100], Training Loss: 164697.4018728526, Test Loss: 39722844.0\n",
      "Epoch [62/100], Training Loss: 162241.32015376014, Test Loss: 39193244.0\n",
      "Epoch [63/100], Training Loss: 159854.5954382886, Test Loss: 38677768.0\n",
      "Epoch [64/100], Training Loss: 157534.1039528538, Test Loss: 38173912.0\n",
      "Epoch [65/100], Training Loss: 155267.6774086806, Test Loss: 37681648.0\n",
      "Epoch [66/100], Training Loss: 153056.0071378556, Test Loss: 37200064.0\n",
      "Epoch [67/100], Training Loss: 150904.014250373, Test Loss: 36730372.0\n",
      "Epoch [68/100], Training Loss: 148812.14158663724, Test Loss: 36272808.0\n",
      "Epoch [69/100], Training Loss: 146777.8674100048, Test Loss: 35827004.0\n",
      "Epoch [70/100], Training Loss: 144800.52658638402, Test Loss: 35393216.0\n",
      "Epoch [71/100], Training Loss: 142877.80963693015, Test Loss: 34972564.0\n",
      "Epoch [72/100], Training Loss: 141008.66096243038, Test Loss: 34565020.0\n",
      "Epoch [73/100], Training Loss: 139185.8915811415, Test Loss: 34165820.0\n",
      "Epoch [74/100], Training Loss: 137405.9473829444, Test Loss: 33774696.0\n",
      "Epoch [75/100], Training Loss: 135665.01999323853, Test Loss: 33392172.0\n",
      "Epoch [76/100], Training Loss: 133962.3790469407, Test Loss: 33017436.0\n",
      "Epoch [77/100], Training Loss: 132290.79653438038, Test Loss: 32648142.0\n",
      "Epoch [78/100], Training Loss: 130643.57109671191, Test Loss: 32286312.0\n",
      "Epoch [79/100], Training Loss: 129028.49240848624, Test Loss: 31933918.0\n",
      "Epoch [80/100], Training Loss: 127452.18933962368, Test Loss: 31590374.0\n",
      "Epoch [81/100], Training Loss: 125921.19100107, Test Loss: 31256270.0\n",
      "Epoch [82/100], Training Loss: 124432.12583859073, Test Loss: 30932148.0\n",
      "Epoch [83/100], Training Loss: 122984.69858494758, Test Loss: 30616518.0\n",
      "Epoch [84/100], Training Loss: 121575.5271403501, Test Loss: 30309170.0\n",
      "Epoch [85/100], Training Loss: 120194.372146385, Test Loss: 30005908.0\n",
      "Epoch [86/100], Training Loss: 118828.45403625007, Test Loss: 29708950.0\n",
      "Epoch [87/100], Training Loss: 117478.28175907825, Test Loss: 29415548.0\n",
      "Epoch [88/100], Training Loss: 116155.60588549997, Test Loss: 29131694.0\n",
      "Epoch [89/100], Training Loss: 114872.21653180721, Test Loss: 28854728.0\n",
      "Epoch [90/100], Training Loss: 113629.57379829764, Test Loss: 28588826.0\n",
      "Epoch [91/100], Training Loss: 112426.95163664031, Test Loss: 28329656.0\n",
      "Epoch [92/100], Training Loss: 111259.7318120224, Test Loss: 28077402.0\n",
      "Epoch [93/100], Training Loss: 110122.39802792341, Test Loss: 27831864.0\n",
      "Epoch [94/100], Training Loss: 109011.81347280226, Test Loss: 27591852.0\n",
      "Epoch [95/100], Training Loss: 107928.53540361575, Test Loss: 27357414.0\n",
      "Epoch [96/100], Training Loss: 106869.87002399148, Test Loss: 27129762.0\n",
      "Epoch [97/100], Training Loss: 105834.58056255923, Test Loss: 26907842.0\n",
      "Epoch [98/100], Training Loss: 104819.6971470329, Test Loss: 26693006.0\n",
      "Epoch [99/100], Training Loss: 103822.26271677477, Test Loss: 26483332.0\n",
      "Epoch [100/100], Training Loss: 102843.49261975371, Test Loss: 26279784.0\n",
      "Epoch [1/100], Training Loss: 1164417.6150701973, Test Loss: 299368384.0\n",
      "Epoch [2/100], Training Loss: 1150757.6650672352, Test Loss: 291175904.0\n",
      "Epoch [3/100], Training Loss: 1087411.6708725786, Test Loss: 265347616.0\n",
      "Epoch [4/100], Training Loss: 951179.5926781589, Test Loss: 222686176.0\n",
      "Epoch [5/100], Training Loss: 780518.1204904923, Test Loss: 181551712.0\n",
      "Epoch [6/100], Training Loss: 652275.4497956282, Test Loss: 157778080.0\n",
      "Epoch [7/100], Training Loss: 585764.9321722647, Test Loss: 145946832.0\n",
      "Epoch [8/100], Training Loss: 547749.7371008827, Test Loss: 137918048.0\n",
      "Epoch [9/100], Training Loss: 518409.50583496236, Test Loss: 131148840.0\n",
      "Epoch [10/100], Training Loss: 492534.38042769977, Test Loss: 125020608.0\n",
      "Epoch [11/100], Training Loss: 468673.55227770866, Test Loss: 119320416.0\n",
      "Epoch [12/100], Training Loss: 446258.90669984004, Test Loss: 113950400.0\n",
      "Epoch [13/100], Training Loss: 424887.4137788046, Test Loss: 108784648.0\n",
      "Epoch [14/100], Training Loss: 404214.2453646111, Test Loss: 103855720.0\n",
      "Epoch [15/100], Training Loss: 384745.4006279249, Test Loss: 99298232.0\n",
      "Epoch [16/100], Training Loss: 366830.06030448433, Test Loss: 95145320.0\n",
      "Epoch [17/100], Training Loss: 350420.7663053137, Test Loss: 91344768.0\n",
      "Epoch [18/100], Training Loss: 335374.51312126056, Test Loss: 87860464.0\n",
      "Epoch [19/100], Training Loss: 321568.115277531, Test Loss: 84661816.0\n",
      "Epoch [20/100], Training Loss: 308898.7693856999, Test Loss: 81716712.0\n",
      "Epoch [21/100], Training Loss: 297269.43474912626, Test Loss: 78999600.0\n",
      "Epoch [22/100], Training Loss: 286582.26953379536, Test Loss: 76488968.0\n",
      "Epoch [23/100], Training Loss: 276751.6417273858, Test Loss: 74163376.0\n",
      "Epoch [24/100], Training Loss: 267689.9941946567, Test Loss: 72004312.0\n",
      "Epoch [25/100], Training Loss: 259316.10579941946, Test Loss: 69994016.0\n",
      "Epoch [26/100], Training Loss: 251563.24115869912, Test Loss: 68116816.0\n",
      "Epoch [27/100], Training Loss: 244366.84793554884, Test Loss: 66364740.0\n",
      "Epoch [28/100], Training Loss: 237669.088798057, Test Loss: 64727008.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/100], Training Loss: 231420.52639061667, Test Loss: 63190752.0\n",
      "Epoch [30/100], Training Loss: 225568.65268645223, Test Loss: 61743420.0\n",
      "Epoch [31/100], Training Loss: 220067.97215804752, Test Loss: 60379340.0\n",
      "Epoch [32/100], Training Loss: 214885.36117528583, Test Loss: 59090076.0\n",
      "Epoch [33/100], Training Loss: 209980.4388365618, Test Loss: 57868832.0\n",
      "Epoch [34/100], Training Loss: 205333.3502754576, Test Loss: 56710408.0\n",
      "Epoch [35/100], Training Loss: 200914.64889520762, Test Loss: 55611140.0\n",
      "Epoch [36/100], Training Loss: 196705.45879983413, Test Loss: 54563380.0\n",
      "Epoch [37/100], Training Loss: 192678.69533795392, Test Loss: 53561380.0\n",
      "Epoch [38/100], Training Loss: 188824.03080386232, Test Loss: 52601076.0\n",
      "Epoch [39/100], Training Loss: 185128.99141046146, Test Loss: 51674768.0\n",
      "Epoch [40/100], Training Loss: 181581.53557253716, Test Loss: 50783376.0\n",
      "Epoch [41/100], Training Loss: 178167.88898761923, Test Loss: 49927448.0\n",
      "Epoch [42/100], Training Loss: 174878.0738107932, Test Loss: 49101376.0\n",
      "Epoch [43/100], Training Loss: 171700.67934364078, Test Loss: 48302948.0\n",
      "Epoch [44/100], Training Loss: 168632.2606480659, Test Loss: 47529852.0\n",
      "Epoch [45/100], Training Loss: 165670.48978141104, Test Loss: 46782508.0\n",
      "Epoch [46/100], Training Loss: 162808.32012321544, Test Loss: 46059492.0\n",
      "Epoch [47/100], Training Loss: 160040.39736982406, Test Loss: 45360044.0\n",
      "Epoch [48/100], Training Loss: 157361.46780404003, Test Loss: 44683932.0\n",
      "Epoch [49/100], Training Loss: 154764.217641135, Test Loss: 44028756.0\n",
      "Epoch [50/100], Training Loss: 152243.83531781292, Test Loss: 43393312.0\n",
      "Epoch [51/100], Training Loss: 149799.7474083289, Test Loss: 42773464.0\n",
      "Epoch [52/100], Training Loss: 147430.90681831646, Test Loss: 42169996.0\n",
      "Epoch [53/100], Training Loss: 145132.1785439251, Test Loss: 41581064.0\n",
      "Epoch [54/100], Training Loss: 142897.17386410758, Test Loss: 41007052.0\n",
      "Epoch [55/100], Training Loss: 140726.52911557374, Test Loss: 40448152.0\n",
      "Epoch [56/100], Training Loss: 138621.63894319057, Test Loss: 39904056.0\n",
      "Epoch [57/100], Training Loss: 136580.9981636159, Test Loss: 39377136.0\n",
      "Epoch [58/100], Training Loss: 134603.8599609028, Test Loss: 38867492.0\n",
      "Epoch [59/100], Training Loss: 132687.03246253185, Test Loss: 38373716.0\n",
      "Epoch [60/100], Training Loss: 130828.02120727445, Test Loss: 37893656.0\n",
      "Epoch [61/100], Training Loss: 129023.35655470648, Test Loss: 37427716.0\n",
      "Epoch [62/100], Training Loss: 127271.3707126355, Test Loss: 36974196.0\n",
      "Epoch [63/100], Training Loss: 125559.83431076357, Test Loss: 36530452.0\n",
      "Epoch [64/100], Training Loss: 123879.26183282981, Test Loss: 36095492.0\n",
      "Epoch [65/100], Training Loss: 122227.68005449914, Test Loss: 35668732.0\n",
      "Epoch [66/100], Training Loss: 120608.44085066051, Test Loss: 35250224.0\n",
      "Epoch [67/100], Training Loss: 119027.46833718382, Test Loss: 34840892.0\n",
      "Epoch [68/100], Training Loss: 117489.41022451277, Test Loss: 34442700.0\n",
      "Epoch [69/100], Training Loss: 115996.42035424442, Test Loss: 34055916.0\n",
      "Epoch [70/100], Training Loss: 114547.17593744447, Test Loss: 33679256.0\n",
      "Epoch [71/100], Training Loss: 113144.55920857769, Test Loss: 33313466.0\n",
      "Epoch [72/100], Training Loss: 111785.35720632663, Test Loss: 32957206.0\n",
      "Epoch [73/100], Training Loss: 110461.1825128843, Test Loss: 32609490.0\n",
      "Epoch [74/100], Training Loss: 109165.02707185593, Test Loss: 32268640.0\n",
      "Epoch [75/100], Training Loss: 107890.91469699662, Test Loss: 31933668.0\n",
      "Epoch [76/100], Training Loss: 106633.98039215687, Test Loss: 31603546.0\n",
      "Epoch [77/100], Training Loss: 105394.54884189325, Test Loss: 31279346.0\n",
      "Epoch [78/100], Training Loss: 104177.95397192109, Test Loss: 30963194.0\n",
      "Epoch [79/100], Training Loss: 102989.77578342515, Test Loss: 30656630.0\n",
      "Epoch [80/100], Training Loss: 101839.25614596292, Test Loss: 30359960.0\n",
      "Epoch [81/100], Training Loss: 100727.30655766839, Test Loss: 30071816.0\n",
      "Epoch [82/100], Training Loss: 99648.6660742847, Test Loss: 29791752.0\n",
      "Epoch [83/100], Training Loss: 98598.37166044666, Test Loss: 29517984.0\n",
      "Epoch [84/100], Training Loss: 97571.46365736627, Test Loss: 29250378.0\n",
      "Epoch [85/100], Training Loss: 96567.31313310823, Test Loss: 28989344.0\n",
      "Epoch [86/100], Training Loss: 95588.73496830756, Test Loss: 28736974.0\n",
      "Epoch [87/100], Training Loss: 94637.62685859842, Test Loss: 28492108.0\n",
      "Epoch [88/100], Training Loss: 93710.94621171731, Test Loss: 28254288.0\n",
      "Epoch [89/100], Training Loss: 92807.18399383922, Test Loss: 28022198.0\n",
      "Epoch [90/100], Training Loss: 91920.4724246194, Test Loss: 27794880.0\n",
      "Epoch [91/100], Training Loss: 91045.7859131568, Test Loss: 27570146.0\n",
      "Epoch [92/100], Training Loss: 90175.50950773059, Test Loss: 27344536.0\n",
      "Epoch [93/100], Training Loss: 89311.88768437889, Test Loss: 27124128.0\n",
      "Epoch [94/100], Training Loss: 88465.63686985368, Test Loss: 26908950.0\n",
      "Epoch [95/100], Training Loss: 87639.63076831942, Test Loss: 26700000.0\n",
      "Epoch [96/100], Training Loss: 86829.86007937918, Test Loss: 26494642.0\n",
      "Epoch [97/100], Training Loss: 86032.46958118594, Test Loss: 26293750.0\n",
      "Epoch [98/100], Training Loss: 85247.01806765002, Test Loss: 26094398.0\n",
      "Epoch [99/100], Training Loss: 84466.54327350276, Test Loss: 25896452.0\n",
      "Epoch [100/100], Training Loss: 83685.7270303892, Test Loss: 25695748.0\n",
      "Epoch [1/100], Training Loss: 582425.0781351816, Test Loss: 300035776.0\n",
      "Epoch [2/100], Training Loss: 581911.8801018897, Test Loss: 299409568.0\n",
      "Epoch [3/100], Training Loss: 579267.3182868314, Test Loss: 297017984.0\n",
      "Epoch [4/100], Training Loss: 571709.9015461169, Test Loss: 291287616.0\n",
      "Epoch [5/100], Training Loss: 556274.1344707067, Test Loss: 280855616.0\n",
      "Epoch [6/100], Training Loss: 530936.7075410224, Test Loss: 265118160.0\n",
      "Epoch [7/100], Training Loss: 495632.8009004206, Test Loss: 244706960.0\n",
      "Epoch [8/100], Training Loss: 452969.00041466736, Test Loss: 221685680.0\n",
      "Epoch [9/100], Training Loss: 408067.0751732717, Test Loss: 199136000.0\n",
      "Epoch [10/100], Training Loss: 367044.3414489663, Test Loss: 180024272.0\n",
      "Epoch [11/100], Training Loss: 334405.51578697946, Test Loss: 165799760.0\n",
      "Epoch [12/100], Training Loss: 311004.71867780347, Test Loss: 155942800.0\n",
      "Epoch [13/100], Training Loss: 294663.81328120374, Test Loss: 148964400.0\n",
      "Epoch [14/100], Training Loss: 282563.14341567445, Test Loss: 143576720.0\n",
      "Epoch [15/100], Training Loss: 272732.8764883597, Test Loss: 139030176.0\n",
      "Epoch [16/100], Training Loss: 264131.92867721105, Test Loss: 134960384.0\n",
      "Epoch [17/100], Training Loss: 256267.56424382442, Test Loss: 131195792.0\n",
      "Epoch [18/100], Training Loss: 248900.65943960665, Test Loss: 127649240.0\n",
      "Epoch [19/100], Training Loss: 241902.8024406137, Test Loss: 124271408.0\n",
      "Epoch [20/100], Training Loss: 235198.17783306676, Test Loss: 121032336.0\n",
      "Epoch [21/100], Training Loss: 228739.9168295717, Test Loss: 117913360.0\n",
      "Epoch [22/100], Training Loss: 222499.1197203957, Test Loss: 114903072.0\n",
      "Epoch [23/100], Training Loss: 216458.63681061548, Test Loss: 111994984.0\n",
      "Epoch [24/100], Training Loss: 210609.21367217583, Test Loss: 109185520.0\n",
      "Epoch [25/100], Training Loss: 204947.22587524436, Test Loss: 106473568.0\n",
      "Epoch [26/100], Training Loss: 199470.2451276583, Test Loss: 103854496.0\n",
      "Epoch [27/100], Training Loss: 194156.25638291572, Test Loss: 101309696.0\n",
      "Epoch [28/100], Training Loss: 188972.89805106333, Test Loss: 98837120.0\n",
      "Epoch [29/100], Training Loss: 183930.56762040165, Test Loss: 96449488.0\n",
      "Epoch [30/100], Training Loss: 179085.26651264736, Test Loss: 94165864.0\n",
      "Epoch [31/100], Training Loss: 174441.56436230082, Test Loss: 91982904.0\n",
      "Epoch [32/100], Training Loss: 170021.4776375807, Test Loss: 89911784.0\n",
      "Epoch [33/100], Training Loss: 165832.43101711984, Test Loss: 87959400.0\n",
      "Epoch [34/100], Training Loss: 161869.18049878563, Test Loss: 86108720.0\n",
      "Epoch [35/100], Training Loss: 158117.29850127362, Test Loss: 84355616.0\n",
      "Epoch [36/100], Training Loss: 154564.75801196613, Test Loss: 82690872.0\n",
      "Epoch [37/100], Training Loss: 151193.82761684735, Test Loss: 81106696.0\n",
      "Epoch [38/100], Training Loss: 147993.63355251466, Test Loss: 79598744.0\n",
      "Epoch [39/100], Training Loss: 144952.04359931283, Test Loss: 78159936.0\n",
      "Epoch [40/100], Training Loss: 142056.98975179196, Test Loss: 76785080.0\n",
      "Epoch [41/100], Training Loss: 139299.4630649843, Test Loss: 75469664.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/100], Training Loss: 136668.69995853325, Test Loss: 74209608.0\n",
      "Epoch [43/100], Training Loss: 134156.04739055742, Test Loss: 73000952.0\n",
      "Epoch [44/100], Training Loss: 131752.40542621881, Test Loss: 71840304.0\n",
      "Epoch [45/100], Training Loss: 129451.89005390675, Test Loss: 70726008.0\n",
      "Epoch [46/100], Training Loss: 127247.89360819856, Test Loss: 69653840.0\n",
      "Epoch [47/100], Training Loss: 125133.21201350632, Test Loss: 68619632.0\n",
      "Epoch [48/100], Training Loss: 123101.04590960252, Test Loss: 67622376.0\n",
      "Epoch [49/100], Training Loss: 121147.16568923643, Test Loss: 66661220.0\n",
      "Epoch [50/100], Training Loss: 119269.03216634085, Test Loss: 65733184.0\n",
      "Epoch [51/100], Training Loss: 117459.48557549909, Test Loss: 64834340.0\n",
      "Epoch [52/100], Training Loss: 115713.37930217404, Test Loss: 63964188.0\n",
      "Epoch [53/100], Training Loss: 114027.48936674367, Test Loss: 63121600.0\n",
      "Epoch [54/100], Training Loss: 112398.30555061904, Test Loss: 62304900.0\n",
      "Epoch [55/100], Training Loss: 110820.5037616255, Test Loss: 61511052.0\n",
      "Epoch [56/100], Training Loss: 109288.93608198567, Test Loss: 60739476.0\n",
      "Epoch [57/100], Training Loss: 107801.66506723536, Test Loss: 59988876.0\n",
      "Epoch [58/100], Training Loss: 106356.34298915941, Test Loss: 59258124.0\n",
      "Epoch [59/100], Training Loss: 104949.55073751554, Test Loss: 58547288.0\n",
      "Epoch [60/100], Training Loss: 103578.5294710029, Test Loss: 57855160.0\n",
      "Epoch [61/100], Training Loss: 102240.42461939459, Test Loss: 57180188.0\n",
      "Epoch [62/100], Training Loss: 100936.35187488893, Test Loss: 56522556.0\n",
      "Epoch [63/100], Training Loss: 99664.16326047035, Test Loss: 55881176.0\n",
      "Epoch [64/100], Training Loss: 98422.02203660921, Test Loss: 55254712.0\n",
      "Epoch [65/100], Training Loss: 97208.88525561283, Test Loss: 54642284.0\n",
      "Epoch [66/100], Training Loss: 96022.91345299449, Test Loss: 54043568.0\n",
      "Epoch [67/100], Training Loss: 94862.30412890231, Test Loss: 53458068.0\n",
      "Epoch [68/100], Training Loss: 93726.95266868077, Test Loss: 52885256.0\n",
      "Epoch [69/100], Training Loss: 92615.3745631183, Test Loss: 52323592.0\n",
      "Epoch [70/100], Training Loss: 91525.72122504591, Test Loss: 51772212.0\n",
      "Epoch [71/100], Training Loss: 90456.41608909424, Test Loss: 51230720.0\n",
      "Epoch [72/100], Training Loss: 89406.82684675079, Test Loss: 50698928.0\n",
      "Epoch [73/100], Training Loss: 88376.79912327469, Test Loss: 50175984.0\n",
      "Epoch [74/100], Training Loss: 87365.34375925596, Test Loss: 49662256.0\n",
      "Epoch [75/100], Training Loss: 86372.24880042652, Test Loss: 49155808.0\n",
      "Epoch [76/100], Training Loss: 85397.09436644749, Test Loss: 48658688.0\n",
      "Epoch [77/100], Training Loss: 84439.92926959303, Test Loss: 48170736.0\n",
      "Epoch [78/100], Training Loss: 83501.05266275695, Test Loss: 47693116.0\n",
      "Epoch [79/100], Training Loss: 82579.74432794265, Test Loss: 47225328.0\n",
      "Epoch [80/100], Training Loss: 81675.15324921509, Test Loss: 46765924.0\n",
      "Epoch [81/100], Training Loss: 80787.85474794147, Test Loss: 46314304.0\n",
      "Epoch [82/100], Training Loss: 79917.85711746935, Test Loss: 45870968.0\n",
      "Epoch [83/100], Training Loss: 79064.14252710147, Test Loss: 45436080.0\n",
      "Epoch [84/100], Training Loss: 78226.2491558557, Test Loss: 45009512.0\n",
      "Epoch [85/100], Training Loss: 77404.09158225224, Test Loss: 44590728.0\n",
      "Epoch [86/100], Training Loss: 76597.81956045258, Test Loss: 44180036.0\n",
      "Epoch [87/100], Training Loss: 75807.17801078135, Test Loss: 43776272.0\n",
      "Epoch [88/100], Training Loss: 75031.74112908004, Test Loss: 43381372.0\n",
      "Epoch [89/100], Training Loss: 74271.40098335406, Test Loss: 42993672.0\n",
      "Epoch [90/100], Training Loss: 73526.13079793852, Test Loss: 42612584.0\n",
      "Epoch [91/100], Training Loss: 72795.67952135537, Test Loss: 42237628.0\n",
      "Epoch [92/100], Training Loss: 72079.13417451573, Test Loss: 41869112.0\n",
      "Epoch [93/100], Training Loss: 71376.33979029678, Test Loss: 41507104.0\n",
      "Epoch [94/100], Training Loss: 70686.98110301522, Test Loss: 41151224.0\n",
      "Epoch [95/100], Training Loss: 70011.07055269237, Test Loss: 40801524.0\n",
      "Epoch [96/100], Training Loss: 69348.11278952668, Test Loss: 40458216.0\n",
      "Epoch [97/100], Training Loss: 68697.59836502577, Test Loss: 40121012.0\n",
      "Epoch [98/100], Training Loss: 68059.0294413838, Test Loss: 39789864.0\n",
      "Epoch [99/100], Training Loss: 67432.87625140691, Test Loss: 39464628.0\n",
      "Epoch [100/100], Training Loss: 66818.92921035484, Test Loss: 39144628.0\n",
      "Epoch [1/100], Training Loss: 7288259.157692079, Test Loss: 151080672.0\n",
      "Epoch [2/100], Training Loss: 4142468.0399265448, Test Loss: 120185432.0\n",
      "Epoch [3/100], Training Loss: 3347295.24803033, Test Loss: 99081880.0\n",
      "Epoch [4/100], Training Loss: 2761806.077068894, Test Loss: 83510560.0\n",
      "Epoch [5/100], Training Loss: 2321666.8346661925, Test Loss: 71729400.0\n",
      "Epoch [6/100], Training Loss: 1994213.995201706, Test Loss: 62938760.0\n",
      "Epoch [7/100], Training Loss: 1754524.4429536164, Test Loss: 56478852.0\n",
      "Epoch [8/100], Training Loss: 1577845.928914164, Test Loss: 51606924.0\n",
      "Epoch [9/100], Training Loss: 1440798.14421539, Test Loss: 47670872.0\n",
      "Epoch [10/100], Training Loss: 1326971.4543865884, Test Loss: 44307808.0\n",
      "Epoch [11/100], Training Loss: 1228515.078875659, Test Loss: 41325948.0\n",
      "Epoch [12/100], Training Loss: 1141690.2575084413, Test Loss: 38642036.0\n",
      "Epoch [13/100], Training Loss: 1064541.7228837155, Test Loss: 36207976.0\n",
      "Epoch [14/100], Training Loss: 995920.9872785972, Test Loss: 34013892.0\n",
      "Epoch [15/100], Training Loss: 934868.2827142942, Test Loss: 32052094.0\n",
      "Epoch [16/100], Training Loss: 880612.0453912683, Test Loss: 30309710.0\n",
      "Epoch [17/100], Training Loss: 832354.8244031752, Test Loss: 28759156.0\n",
      "Epoch [18/100], Training Loss: 789488.8453142586, Test Loss: 27374874.0\n",
      "Epoch [19/100], Training Loss: 751426.8535187489, Test Loss: 26138030.0\n",
      "Epoch [20/100], Training Loss: 717532.1914134234, Test Loss: 25035040.0\n",
      "Epoch [21/100], Training Loss: 687049.0354392512, Test Loss: 24043356.0\n",
      "Epoch [22/100], Training Loss: 659368.852793081, Test Loss: 23150394.0\n",
      "Epoch [23/100], Training Loss: 634067.1461628458, Test Loss: 22339354.0\n",
      "Epoch [24/100], Training Loss: 610821.8102674604, Test Loss: 21609804.0\n",
      "Epoch [25/100], Training Loss: 589395.8325114034, Test Loss: 20953770.0\n",
      "Epoch [26/100], Training Loss: 569589.1843936971, Test Loss: 20362748.0\n",
      "Epoch [27/100], Training Loss: 551283.019252414, Test Loss: 19829586.0\n",
      "Epoch [28/100], Training Loss: 534335.3403974883, Test Loss: 19352474.0\n",
      "Epoch [29/100], Training Loss: 518625.3592944731, Test Loss: 18922148.0\n",
      "Epoch [30/100], Training Loss: 504021.4238122742, Test Loss: 18528228.0\n",
      "Epoch [31/100], Training Loss: 490407.9756531011, Test Loss: 18167606.0\n",
      "Epoch [32/100], Training Loss: 477718.9356895326, Test Loss: 17843550.0\n",
      "Epoch [33/100], Training Loss: 465874.22540874354, Test Loss: 17530672.0\n",
      "Epoch [34/100], Training Loss: 454799.86976482434, Test Loss: 17255156.0\n",
      "Epoch [35/100], Training Loss: 444397.45283158583, Test Loss: 16997846.0\n",
      "Epoch [36/100], Training Loss: 434707.469810734, Test Loss: 16767840.0\n",
      "Epoch [37/100], Training Loss: 425577.75925596827, Test Loss: 16557852.0\n",
      "Epoch [38/100], Training Loss: 416975.8983842782, Test Loss: 16375730.0\n",
      "Epoch [39/100], Training Loss: 408915.99977785675, Test Loss: 16207057.0\n",
      "Epoch [40/100], Training Loss: 401348.74980747583, Test Loss: 16051503.0\n",
      "Epoch [41/100], Training Loss: 394213.827068894, Test Loss: 15911147.0\n",
      "Epoch [42/100], Training Loss: 387488.68092826253, Test Loss: 15784218.0\n",
      "Epoch [43/100], Training Loss: 381117.9618802204, Test Loss: 15677163.0\n",
      "Epoch [44/100], Training Loss: 375086.051181802, Test Loss: 15566876.0\n",
      "Epoch [45/100], Training Loss: 369392.4242935845, Test Loss: 15486417.0\n",
      "Epoch [46/100], Training Loss: 363994.2394926248, Test Loss: 15396389.0\n",
      "Epoch [47/100], Training Loss: 358890.2791525976, Test Loss: 15325339.0\n",
      "Epoch [48/100], Training Loss: 354044.3004635389, Test Loss: 15257442.0\n",
      "Epoch [49/100], Training Loss: 349472.5535735442, Test Loss: 15210554.0\n",
      "Epoch [50/100], Training Loss: 345079.012943546, Test Loss: 15131459.0\n",
      "Epoch [51/100], Training Loss: 340960.0408077128, Test Loss: 15086315.0\n",
      "Epoch [52/100], Training Loss: 337013.51418754813, Test Loss: 14997946.0\n",
      "Epoch [53/100], Training Loss: 333302.64614063146, Test Loss: 14985495.0\n",
      "Epoch [54/100], Training Loss: 329760.0922560867, Test Loss: 14901483.0\n",
      "Epoch [55/100], Training Loss: 326405.62286002014, Test Loss: 14882714.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [56/100], Training Loss: 323207.907358865, Test Loss: 14804283.0\n",
      "Epoch [57/100], Training Loss: 320205.1526012973, Test Loss: 14796020.0\n",
      "Epoch [58/100], Training Loss: 317292.29439902847, Test Loss: 14727570.0\n",
      "Epoch [59/100], Training Loss: 314555.0161535158, Test Loss: 14707621.0\n",
      "Epoch [60/100], Training Loss: 311927.86982406257, Test Loss: 14655581.0\n",
      "Epoch [61/100], Training Loss: 309437.40246801137, Test Loss: 14640192.0\n",
      "Epoch [62/100], Training Loss: 307052.5408077128, Test Loss: 14587735.0\n",
      "Epoch [63/100], Training Loss: 304795.7042162787, Test Loss: 14582317.0\n",
      "Epoch [64/100], Training Loss: 302617.53035587346, Test Loss: 14537559.0\n",
      "Epoch [65/100], Training Loss: 300543.297864463, Test Loss: 14527088.0\n",
      "Epoch [66/100], Training Loss: 298508.7329208874, Test Loss: 14488342.0\n",
      "Epoch [67/100], Training Loss: 296595.7264972454, Test Loss: 14493131.0\n",
      "Epoch [68/100], Training Loss: 294723.98182498076, Test Loss: 14475666.0\n",
      "Epoch [69/100], Training Loss: 292937.0520888869, Test Loss: 14488650.0\n",
      "Epoch [70/100], Training Loss: 291102.02747911855, Test Loss: 14471855.0\n",
      "Epoch [71/100], Training Loss: 289363.71944049525, Test Loss: 14463228.0\n",
      "Epoch [72/100], Training Loss: 287681.09318908834, Test Loss: 14496587.0\n",
      "Epoch [73/100], Training Loss: 286108.8171687104, Test Loss: 14421444.0\n",
      "Epoch [74/100], Training Loss: 284454.70337213436, Test Loss: 14475041.0\n",
      "Epoch [75/100], Training Loss: 282918.5977504295, Test Loss: 14448763.0\n",
      "Epoch [76/100], Training Loss: 281351.96191354183, Test Loss: 14447487.0\n",
      "Epoch [77/100], Training Loss: 279924.9612323026, Test Loss: 14466203.0\n",
      "Epoch [78/100], Training Loss: 278445.0104629465, Test Loss: 14462480.0\n",
      "Epoch [79/100], Training Loss: 277051.95150242874, Test Loss: 14428206.0\n",
      "Epoch [80/100], Training Loss: 275652.11836902436, Test Loss: 14491358.0\n",
      "Epoch [81/100], Training Loss: 274388.144119128, Test Loss: 14438572.0\n",
      "Epoch [82/100], Training Loss: 273054.34892408625, Test Loss: 14467585.0\n",
      "Epoch [83/100], Training Loss: 271861.3475134767, Test Loss: 14463498.0\n",
      "Epoch [84/100], Training Loss: 270662.05107073043, Test Loss: 14443997.0\n",
      "Epoch [85/100], Training Loss: 269483.53386943904, Test Loss: 14461846.0\n",
      "Epoch [86/100], Training Loss: 268284.2547982939, Test Loss: 14464508.0\n",
      "Epoch [87/100], Training Loss: 267156.30903456546, Test Loss: 14443142.0\n",
      "Epoch [88/100], Training Loss: 266019.9792147977, Test Loss: 14498231.0\n",
      "Epoch [89/100], Training Loss: 264919.4009981636, Test Loss: 14429505.0\n",
      "Epoch [90/100], Training Loss: 263758.5063607014, Test Loss: 14498706.0\n",
      "Epoch [91/100], Training Loss: 262688.872715627, Test Loss: 14461621.0\n",
      "Epoch [92/100], Training Loss: 261564.32528064094, Test Loss: 14443622.0\n",
      "Epoch [93/100], Training Loss: 260544.39881005272, Test Loss: 14470277.0\n",
      "Epoch [94/100], Training Loss: 259418.9919324981, Test Loss: 14468564.0\n",
      "Epoch [95/100], Training Loss: 258387.77239944317, Test Loss: 14452405.0\n",
      "Epoch [96/100], Training Loss: 257302.5722928144, Test Loss: 14491058.0\n",
      "Epoch [97/100], Training Loss: 256340.25908195606, Test Loss: 14433056.0\n",
      "Epoch [98/100], Training Loss: 255314.37968351995, Test Loss: 14496761.0\n",
      "Epoch [99/100], Training Loss: 254438.11892438243, Test Loss: 14499570.0\n",
      "Epoch [100/100], Training Loss: 253521.70147651207, Test Loss: 14456727.0\n",
      "Epoch [1/100], Training Loss: 4410120.771755228, Test Loss: 235159504.0\n",
      "Epoch [2/100], Training Loss: 2722059.8162431135, Test Loss: 146243472.0\n",
      "Epoch [3/100], Training Loss: 2096726.3294828506, Test Loss: 126665416.0\n",
      "Epoch [4/100], Training Loss: 1822927.4604585036, Test Loss: 111213416.0\n",
      "Epoch [5/100], Training Loss: 1595363.2286002014, Test Loss: 98339416.0\n",
      "Epoch [6/100], Training Loss: 1406119.0401042593, Test Loss: 87802944.0\n",
      "Epoch [7/100], Training Loss: 1249969.5020437178, Test Loss: 79093960.0\n",
      "Epoch [8/100], Training Loss: 1121309.7538652923, Test Loss: 71881240.0\n",
      "Epoch [9/100], Training Loss: 1016028.6583140809, Test Loss: 65924684.0\n",
      "Epoch [10/100], Training Loss: 930269.4448196196, Test Loss: 61043388.0\n",
      "Epoch [11/100], Training Loss: 860448.4667377525, Test Loss: 57043912.0\n",
      "Epoch [12/100], Training Loss: 802837.0777205141, Test Loss: 53685308.0\n",
      "Epoch [13/100], Training Loss: 753880.6492506368, Test Loss: 50781620.0\n",
      "Epoch [14/100], Training Loss: 710994.8685504413, Test Loss: 48195804.0\n",
      "Epoch [15/100], Training Loss: 672535.9970973283, Test Loss: 45852068.0\n",
      "Epoch [16/100], Training Loss: 637563.0887388188, Test Loss: 43693988.0\n",
      "Epoch [17/100], Training Loss: 605462.3169835911, Test Loss: 41688448.0\n",
      "Epoch [18/100], Training Loss: 575834.7453942302, Test Loss: 39817940.0\n",
      "Epoch [19/100], Training Loss: 548457.2565013921, Test Loss: 38068544.0\n",
      "Epoch [20/100], Training Loss: 523143.0182157455, Test Loss: 36434668.0\n",
      "Epoch [21/100], Training Loss: 499781.3061726201, Test Loss: 34915116.0\n",
      "Epoch [22/100], Training Loss: 478222.73484983115, Test Loss: 33505944.0\n",
      "Epoch [23/100], Training Loss: 458307.40821041405, Test Loss: 32200394.0\n",
      "Epoch [24/100], Training Loss: 439939.2029796813, Test Loss: 30991156.0\n",
      "Epoch [25/100], Training Loss: 422974.3620046206, Test Loss: 29872128.0\n",
      "Epoch [26/100], Training Loss: 407310.70555654285, Test Loss: 28838500.0\n",
      "Epoch [27/100], Training Loss: 392844.4630649843, Test Loss: 27881190.0\n",
      "Epoch [28/100], Training Loss: 379468.2600556839, Test Loss: 26993828.0\n",
      "Epoch [29/100], Training Loss: 367112.69862567383, Test Loss: 26172504.0\n",
      "Epoch [30/100], Training Loss: 355678.6295687459, Test Loss: 25410614.0\n",
      "Epoch [31/100], Training Loss: 345028.4843463065, Test Loss: 24703874.0\n",
      "Epoch [32/100], Training Loss: 335052.5322403886, Test Loss: 24040454.0\n",
      "Epoch [33/100], Training Loss: 325671.3936526272, Test Loss: 23422796.0\n",
      "Epoch [34/100], Training Loss: 316826.7352200699, Test Loss: 22844012.0\n",
      "Epoch [35/100], Training Loss: 308473.23806350335, Test Loss: 22303592.0\n",
      "Epoch [36/100], Training Loss: 300558.01011492213, Test Loss: 21798032.0\n",
      "Epoch [37/100], Training Loss: 293075.16736271547, Test Loss: 21331074.0\n",
      "Epoch [38/100], Training Loss: 285988.09236715833, Test Loss: 20896368.0\n",
      "Epoch [39/100], Training Loss: 279268.72258752445, Test Loss: 20490796.0\n",
      "Epoch [40/100], Training Loss: 272897.4151412831, Test Loss: 20109716.0\n",
      "Epoch [41/100], Training Loss: 266855.5527368047, Test Loss: 19756568.0\n",
      "Epoch [42/100], Training Loss: 261110.087746579, Test Loss: 19421730.0\n",
      "Epoch [43/100], Training Loss: 255652.3056246668, Test Loss: 19111792.0\n",
      "Epoch [44/100], Training Loss: 250461.81682068598, Test Loss: 18814926.0\n",
      "Epoch [45/100], Training Loss: 245524.2041792548, Test Loss: 18543774.0\n",
      "Epoch [46/100], Training Loss: 240823.27954505064, Test Loss: 18279246.0\n",
      "Epoch [47/100], Training Loss: 236321.51178840117, Test Loss: 18038340.0\n",
      "Epoch [48/100], Training Loss: 232003.41832533618, Test Loss: 17805848.0\n",
      "Epoch [49/100], Training Loss: 227879.10991647415, Test Loss: 17594766.0\n",
      "Epoch [50/100], Training Loss: 223938.18503050768, Test Loss: 17388176.0\n",
      "Epoch [51/100], Training Loss: 220169.49028493572, Test Loss: 17204212.0\n",
      "Epoch [52/100], Training Loss: 216551.19123570877, Test Loss: 17024074.0\n",
      "Epoch [53/100], Training Loss: 213097.55750547952, Test Loss: 16865202.0\n",
      "Epoch [54/100], Training Loss: 209776.43273502754, Test Loss: 16707174.0\n",
      "Epoch [55/100], Training Loss: 206599.2749244713, Test Loss: 16567853.0\n",
      "Epoch [56/100], Training Loss: 203552.45863692908, Test Loss: 16430753.0\n",
      "Epoch [57/100], Training Loss: 200646.87472602335, Test Loss: 16307842.0\n",
      "Epoch [58/100], Training Loss: 197857.6797731177, Test Loss: 16192678.0\n",
      "Epoch [59/100], Training Loss: 195180.77550204372, Test Loss: 16081888.0\n",
      "Epoch [60/100], Training Loss: 192609.68483798354, Test Loss: 15988166.0\n",
      "Epoch [61/100], Training Loss: 190145.33696167287, Test Loss: 15893990.0\n",
      "Epoch [62/100], Training Loss: 187770.6206237782, Test Loss: 15811942.0\n",
      "Epoch [63/100], Training Loss: 185488.27467270897, Test Loss: 15731741.0\n",
      "Epoch [64/100], Training Loss: 183286.09402582786, Test Loss: 15661757.0\n",
      "Epoch [65/100], Training Loss: 181169.26799360226, Test Loss: 15594509.0\n",
      "Epoch [66/100], Training Loss: 179137.2382264084, Test Loss: 15533020.0\n",
      "Epoch [67/100], Training Loss: 177187.9315502636, Test Loss: 15476016.0\n",
      "Epoch [68/100], Training Loss: 175306.93095788165, Test Loss: 15422001.0\n",
      "Epoch [69/100], Training Loss: 173490.7019874415, Test Loss: 15372170.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/100], Training Loss: 171757.26634233753, Test Loss: 15328426.0\n",
      "Epoch [71/100], Training Loss: 170093.4613174575, Test Loss: 15281532.0\n",
      "Epoch [72/100], Training Loss: 168480.92294591552, Test Loss: 15235668.0\n",
      "Epoch [73/100], Training Loss: 166939.9274109946, Test Loss: 15190294.0\n",
      "Epoch [74/100], Training Loss: 165455.56879035602, Test Loss: 15153606.0\n",
      "Epoch [75/100], Training Loss: 164039.04368076535, Test Loss: 15103010.0\n",
      "Epoch [76/100], Training Loss: 162664.4388365618, Test Loss: 15059916.0\n",
      "Epoch [77/100], Training Loss: 161357.9703512825, Test Loss: 15027218.0\n",
      "Epoch [78/100], Training Loss: 160088.98759700256, Test Loss: 14980259.0\n",
      "Epoch [79/100], Training Loss: 158883.0592604111, Test Loss: 14963192.0\n",
      "Epoch [80/100], Training Loss: 157709.36299686038, Test Loss: 14896606.0\n",
      "Epoch [81/100], Training Loss: 156596.24789704403, Test Loss: 14898909.0\n",
      "Epoch [82/100], Training Loss: 155510.0962842841, Test Loss: 14843523.0\n",
      "Epoch [83/100], Training Loss: 154466.96389431905, Test Loss: 14834113.0\n",
      "Epoch [84/100], Training Loss: 153461.02283632488, Test Loss: 14792842.0\n",
      "Epoch [85/100], Training Loss: 152495.26358776138, Test Loss: 14776903.0\n",
      "Epoch [86/100], Training Loss: 151553.84783928678, Test Loss: 14738404.0\n",
      "Epoch [87/100], Training Loss: 150656.4407321841, Test Loss: 14727087.0\n",
      "Epoch [88/100], Training Loss: 149763.2171450151, Test Loss: 14693906.0\n",
      "Epoch [89/100], Training Loss: 148921.20156536935, Test Loss: 14669136.0\n",
      "Epoch [90/100], Training Loss: 148095.02604259228, Test Loss: 14668400.0\n",
      "Epoch [91/100], Training Loss: 147293.54467300515, Test Loss: 14625483.0\n",
      "Epoch [92/100], Training Loss: 146510.24375037025, Test Loss: 14607168.0\n",
      "Epoch [93/100], Training Loss: 145747.37842100585, Test Loss: 14623621.0\n",
      "Epoch [94/100], Training Loss: 145012.3215375274, Test Loss: 14571183.0\n",
      "Epoch [95/100], Training Loss: 144282.7447203957, Test Loss: 14580339.0\n",
      "Epoch [96/100], Training Loss: 143583.99618654107, Test Loss: 14541070.0\n",
      "Epoch [97/100], Training Loss: 142894.02941916947, Test Loss: 14540291.0\n",
      "Epoch [98/100], Training Loss: 142204.4886632901, Test Loss: 14512942.0\n",
      "Epoch [99/100], Training Loss: 141575.64204579112, Test Loss: 14506787.0\n",
      "Epoch [100/100], Training Loss: 140931.05603933416, Test Loss: 14486544.0\n",
      "Epoch [1/100], Training Loss: 2319053.2172264676, Test Loss: 292873408.0\n",
      "Epoch [2/100], Training Loss: 2083889.2918665956, Test Loss: 232926752.0\n",
      "Epoch [3/100], Training Loss: 1509480.6191576328, Test Loss: 164899568.0\n",
      "Epoch [4/100], Training Loss: 1180210.999111427, Test Loss: 143552048.0\n",
      "Epoch [5/100], Training Loss: 1061304.1796102126, Test Loss: 131746288.0\n",
      "Epoch [6/100], Training Loss: 975119.0611930573, Test Loss: 121781928.0\n",
      "Epoch [7/100], Training Loss: 899063.3682838695, Test Loss: 112814384.0\n",
      "Epoch [8/100], Training Loss: 830007.287482969, Test Loss: 104713800.0\n",
      "Epoch [9/100], Training Loss: 767648.2932290741, Test Loss: 97482976.0\n",
      "Epoch [10/100], Training Loss: 712029.4772821516, Test Loss: 91108312.0\n",
      "Epoch [11/100], Training Loss: 662885.307031574, Test Loss: 85498856.0\n",
      "Epoch [12/100], Training Loss: 619551.8418340145, Test Loss: 80538368.0\n",
      "Epoch [13/100], Training Loss: 581314.8440258278, Test Loss: 76135064.0\n",
      "Epoch [14/100], Training Loss: 547534.7347905929, Test Loss: 72214832.0\n",
      "Epoch [15/100], Training Loss: 517678.3237959836, Test Loss: 68720304.0\n",
      "Epoch [16/100], Training Loss: 491297.8732302589, Test Loss: 65608484.0\n",
      "Epoch [17/100], Training Loss: 467918.09170072863, Test Loss: 62829304.0\n",
      "Epoch [18/100], Training Loss: 447106.09087139386, Test Loss: 60344384.0\n",
      "Epoch [19/100], Training Loss: 428478.88430780167, Test Loss: 58109652.0\n",
      "Epoch [20/100], Training Loss: 411676.12469640427, Test Loss: 56076148.0\n",
      "Epoch [21/100], Training Loss: 396328.19566376397, Test Loss: 54203888.0\n",
      "Epoch [22/100], Training Loss: 382145.4243824418, Test Loss: 52466808.0\n",
      "Epoch [23/100], Training Loss: 368922.5170309816, Test Loss: 50841912.0\n",
      "Epoch [24/100], Training Loss: 356511.5718263136, Test Loss: 49310904.0\n",
      "Epoch [25/100], Training Loss: 344823.7482969018, Test Loss: 47861480.0\n",
      "Epoch [26/100], Training Loss: 333767.2117173153, Test Loss: 46484736.0\n",
      "Epoch [27/100], Training Loss: 323274.55310704344, Test Loss: 45172084.0\n",
      "Epoch [28/100], Training Loss: 313292.87370416446, Test Loss: 43915852.0\n",
      "Epoch [29/100], Training Loss: 303772.8800426515, Test Loss: 42709480.0\n",
      "Epoch [30/100], Training Loss: 294688.5186896511, Test Loss: 41552848.0\n",
      "Epoch [31/100], Training Loss: 286017.63319708547, Test Loss: 40440960.0\n",
      "Epoch [32/100], Training Loss: 277737.1492802559, Test Loss: 39371736.0\n",
      "Epoch [33/100], Training Loss: 269833.8916533381, Test Loss: 38344136.0\n",
      "Epoch [34/100], Training Loss: 262288.752562052, Test Loss: 37356312.0\n",
      "Epoch [35/100], Training Loss: 255076.83371838162, Test Loss: 36409112.0\n",
      "Epoch [36/100], Training Loss: 248188.527397666, Test Loss: 35502688.0\n",
      "Epoch [37/100], Training Loss: 241612.28256619867, Test Loss: 34636304.0\n",
      "Epoch [38/100], Training Loss: 235330.36076061844, Test Loss: 33807540.0\n",
      "Epoch [39/100], Training Loss: 229326.7807001955, Test Loss: 33013100.0\n",
      "Epoch [40/100], Training Loss: 223597.36840234583, Test Loss: 32252314.0\n",
      "Epoch [41/100], Training Loss: 218139.68046916652, Test Loss: 31524086.0\n",
      "Epoch [42/100], Training Loss: 212935.1844381257, Test Loss: 30829002.0\n",
      "Epoch [43/100], Training Loss: 207972.85925004442, Test Loss: 30165222.0\n",
      "Epoch [44/100], Training Loss: 203242.38273798945, Test Loss: 29531916.0\n",
      "Epoch [45/100], Training Loss: 198728.91075765653, Test Loss: 28927238.0\n",
      "Epoch [46/100], Training Loss: 194417.90311592916, Test Loss: 28350406.0\n",
      "Epoch [47/100], Training Loss: 190304.88854333275, Test Loss: 27799534.0\n",
      "Epoch [48/100], Training Loss: 186379.1975297672, Test Loss: 27274040.0\n",
      "Epoch [49/100], Training Loss: 182626.62416326048, Test Loss: 26772922.0\n",
      "Epoch [50/100], Training Loss: 179034.8946744861, Test Loss: 26293968.0\n",
      "Epoch [51/100], Training Loss: 175597.68349031455, Test Loss: 25835828.0\n",
      "Epoch [52/100], Training Loss: 172302.81879035602, Test Loss: 25398438.0\n",
      "Epoch [53/100], Training Loss: 169139.05586161956, Test Loss: 24979942.0\n",
      "Epoch [54/100], Training Loss: 166094.08639891, Test Loss: 24579428.0\n",
      "Epoch [55/100], Training Loss: 163160.82447722292, Test Loss: 24195114.0\n",
      "Epoch [56/100], Training Loss: 160332.9702920443, Test Loss: 23827896.0\n",
      "Epoch [57/100], Training Loss: 157603.18197974053, Test Loss: 23472858.0\n",
      "Epoch [58/100], Training Loss: 154966.1772406848, Test Loss: 23135822.0\n",
      "Epoch [59/100], Training Loss: 152419.10286712873, Test Loss: 22811210.0\n",
      "Epoch [60/100], Training Loss: 149958.43771103607, Test Loss: 22499866.0\n",
      "Epoch [61/100], Training Loss: 147578.0203483206, Test Loss: 22200338.0\n",
      "Epoch [62/100], Training Loss: 145279.07099697884, Test Loss: 21914016.0\n",
      "Epoch [63/100], Training Loss: 143051.90847698596, Test Loss: 21638242.0\n",
      "Epoch [64/100], Training Loss: 140893.27163675136, Test Loss: 21373856.0\n",
      "Epoch [65/100], Training Loss: 138804.5205852734, Test Loss: 21120140.0\n",
      "Epoch [66/100], Training Loss: 136779.3798353178, Test Loss: 20876136.0\n",
      "Epoch [67/100], Training Loss: 134816.09661749896, Test Loss: 20641174.0\n",
      "Epoch [68/100], Training Loss: 132911.2654759789, Test Loss: 20415016.0\n",
      "Epoch [69/100], Training Loss: 131064.41574847461, Test Loss: 20198818.0\n",
      "Epoch [70/100], Training Loss: 129272.66845862212, Test Loss: 19989542.0\n",
      "Epoch [71/100], Training Loss: 127534.69354599847, Test Loss: 19789854.0\n",
      "Epoch [72/100], Training Loss: 125849.92568568213, Test Loss: 19597022.0\n",
      "Epoch [73/100], Training Loss: 124213.23872993306, Test Loss: 19412706.0\n",
      "Epoch [74/100], Training Loss: 122621.97525324329, Test Loss: 19235326.0\n",
      "Epoch [75/100], Training Loss: 121072.57575084415, Test Loss: 19065022.0\n",
      "Epoch [76/100], Training Loss: 119563.19079142231, Test Loss: 18900436.0\n",
      "Epoch [77/100], Training Loss: 118098.11909839464, Test Loss: 18741202.0\n",
      "Epoch [78/100], Training Loss: 116667.52717552277, Test Loss: 18588784.0\n",
      "Epoch [79/100], Training Loss: 115277.42586339671, Test Loss: 18439322.0\n",
      "Epoch [80/100], Training Loss: 113923.97313547776, Test Loss: 18300054.0\n",
      "Epoch [81/100], Training Loss: 112608.98513121261, Test Loss: 18160870.0\n",
      "Epoch [82/100], Training Loss: 111330.86004976009, Test Loss: 18031552.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [83/100], Training Loss: 110085.52185889462, Test Loss: 17903706.0\n",
      "Epoch [84/100], Training Loss: 108872.32252236242, Test Loss: 17783100.0\n",
      "Epoch [85/100], Training Loss: 107690.40978022628, Test Loss: 17664914.0\n",
      "Epoch [86/100], Training Loss: 106538.11187133464, Test Loss: 17549104.0\n",
      "Epoch [87/100], Training Loss: 105416.40074640128, Test Loss: 17439512.0\n",
      "Epoch [88/100], Training Loss: 104325.48105858658, Test Loss: 17331940.0\n",
      "Epoch [89/100], Training Loss: 103261.34276701618, Test Loss: 17230076.0\n",
      "Epoch [90/100], Training Loss: 102225.31039334163, Test Loss: 17132154.0\n",
      "Epoch [91/100], Training Loss: 101217.44570819264, Test Loss: 17036748.0\n",
      "Epoch [92/100], Training Loss: 100234.89448196196, Test Loss: 16947368.0\n",
      "Epoch [93/100], Training Loss: 99280.2159232273, Test Loss: 16860876.0\n",
      "Epoch [94/100], Training Loss: 98346.25531662816, Test Loss: 16777708.0\n",
      "Epoch [95/100], Training Loss: 97439.4391623719, Test Loss: 16700070.0\n",
      "Epoch [96/100], Training Loss: 96558.30667614478, Test Loss: 16623899.0\n",
      "Epoch [97/100], Training Loss: 95698.85655470648, Test Loss: 16552700.0\n",
      "Epoch [98/100], Training Loss: 94861.08593981399, Test Loss: 16483954.0\n",
      "Epoch [99/100], Training Loss: 94045.165866951, Test Loss: 16416947.0\n",
      "Epoch [100/100], Training Loss: 93248.43052840472, Test Loss: 16357243.0\n",
      "Epoch [1/100], Training Loss: 1497752.0971506427, Test Loss: 298677888.0\n",
      "Epoch [2/100], Training Loss: 1465760.1687103845, Test Loss: 284179296.0\n",
      "Epoch [3/100], Training Loss: 1331815.630353652, Test Loss: 243888048.0\n",
      "Epoch [4/100], Training Loss: 1092775.4261003495, Test Loss: 193068048.0\n",
      "Epoch [5/100], Training Loss: 878859.813044251, Test Loss: 161593520.0\n",
      "Epoch [6/100], Training Loss: 768114.9782595817, Test Loss: 147211024.0\n",
      "Epoch [7/100], Training Loss: 709232.5101593507, Test Loss: 137911552.0\n",
      "Epoch [8/100], Training Loss: 664853.2110656952, Test Loss: 130228024.0\n",
      "Epoch [9/100], Training Loss: 626241.0845329068, Test Loss: 123437040.0\n",
      "Epoch [10/100], Training Loss: 591317.4837983532, Test Loss: 117314360.0\n",
      "Epoch [11/100], Training Loss: 559347.4187548131, Test Loss: 111760768.0\n",
      "Epoch [12/100], Training Loss: 529807.3460103074, Test Loss: 106625992.0\n",
      "Epoch [13/100], Training Loss: 501967.2846395356, Test Loss: 101879000.0\n",
      "Epoch [14/100], Training Loss: 476542.33659143414, Test Loss: 97686104.0\n",
      "Epoch [15/100], Training Loss: 453860.02653871215, Test Loss: 93980344.0\n",
      "Epoch [16/100], Training Loss: 433503.3599905219, Test Loss: 90658592.0\n",
      "Epoch [17/100], Training Loss: 415125.0565724779, Test Loss: 87657288.0\n",
      "Epoch [18/100], Training Loss: 398474.9149931876, Test Loss: 84927520.0\n",
      "Epoch [19/100], Training Loss: 383377.0865470055, Test Loss: 82433008.0\n",
      "Epoch [20/100], Training Loss: 369679.8361471477, Test Loss: 80146752.0\n",
      "Epoch [21/100], Training Loss: 357245.4668562289, Test Loss: 78046280.0\n",
      "Epoch [22/100], Training Loss: 345960.10615484865, Test Loss: 76110384.0\n",
      "Epoch [23/100], Training Loss: 335703.6388839524, Test Loss: 74324744.0\n",
      "Epoch [24/100], Training Loss: 326370.2804336236, Test Loss: 72670024.0\n",
      "Epoch [25/100], Training Loss: 317854.18695574906, Test Loss: 71128912.0\n",
      "Epoch [26/100], Training Loss: 310038.9001836384, Test Loss: 69683336.0\n",
      "Epoch [27/100], Training Loss: 302827.1618387536, Test Loss: 68321672.0\n",
      "Epoch [28/100], Training Loss: 296135.2430839405, Test Loss: 67033040.0\n",
      "Epoch [29/100], Training Loss: 289886.69838872104, Test Loss: 65807204.0\n",
      "Epoch [30/100], Training Loss: 284006.4588294532, Test Loss: 64635396.0\n",
      "Epoch [31/100], Training Loss: 278429.74965938035, Test Loss: 63506336.0\n",
      "Epoch [32/100], Training Loss: 273111.41906211124, Test Loss: 62414668.0\n",
      "Epoch [33/100], Training Loss: 268009.73165536515, Test Loss: 61358836.0\n",
      "Epoch [34/100], Training Loss: 263088.73727535765, Test Loss: 60332152.0\n",
      "Epoch [35/100], Training Loss: 258308.05459725432, Test Loss: 59328024.0\n",
      "Epoch [36/100], Training Loss: 253645.19894778152, Test Loss: 58346600.0\n",
      "Epoch [37/100], Training Loss: 249089.7093848113, Test Loss: 57386576.0\n",
      "Epoch [38/100], Training Loss: 244629.78767549316, Test Loss: 56446984.0\n",
      "Epoch [39/100], Training Loss: 240257.2668458622, Test Loss: 55527500.0\n",
      "Epoch [40/100], Training Loss: 235965.5082489189, Test Loss: 54628472.0\n",
      "Epoch [41/100], Training Loss: 231761.79234050115, Test Loss: 53749060.0\n",
      "Epoch [42/100], Training Loss: 227648.2484005687, Test Loss: 52888652.0\n",
      "Epoch [43/100], Training Loss: 223621.2867573011, Test Loss: 52046404.0\n",
      "Epoch [44/100], Training Loss: 219683.47494224276, Test Loss: 51224952.0\n",
      "Epoch [45/100], Training Loss: 215834.47412771755, Test Loss: 50424956.0\n",
      "Epoch [46/100], Training Loss: 212079.0149576447, Test Loss: 49647040.0\n",
      "Epoch [47/100], Training Loss: 208414.0166163142, Test Loss: 48887008.0\n",
      "Epoch [48/100], Training Loss: 204834.80586161956, Test Loss: 48145196.0\n",
      "Epoch [49/100], Training Loss: 201344.61674367634, Test Loss: 47419848.0\n",
      "Epoch [50/100], Training Loss: 197946.75162164564, Test Loss: 46710068.0\n",
      "Epoch [51/100], Training Loss: 194635.4649606066, Test Loss: 46021288.0\n",
      "Epoch [52/100], Training Loss: 191408.25901161067, Test Loss: 45351580.0\n",
      "Epoch [53/100], Training Loss: 188266.61658817605, Test Loss: 44698280.0\n",
      "Epoch [54/100], Training Loss: 185211.72232095254, Test Loss: 44059840.0\n",
      "Epoch [55/100], Training Loss: 182240.18710384457, Test Loss: 43437312.0\n",
      "Epoch [56/100], Training Loss: 179345.37059045673, Test Loss: 42828708.0\n",
      "Epoch [57/100], Training Loss: 176526.2764979859, Test Loss: 42234484.0\n",
      "Epoch [58/100], Training Loss: 173780.60091226824, Test Loss: 41651968.0\n",
      "Epoch [59/100], Training Loss: 171102.67299404656, Test Loss: 41080556.0\n",
      "Epoch [60/100], Training Loss: 168487.0462687341, Test Loss: 40521380.0\n",
      "Epoch [61/100], Training Loss: 165936.23237756206, Test Loss: 39972600.0\n",
      "Epoch [62/100], Training Loss: 163448.7619003984, Test Loss: 39436356.0\n",
      "Epoch [63/100], Training Loss: 161026.91563138663, Test Loss: 38913068.0\n",
      "Epoch [64/100], Training Loss: 158669.66201230304, Test Loss: 38401036.0\n",
      "Epoch [65/100], Training Loss: 156376.52133176723, Test Loss: 37903956.0\n",
      "Epoch [66/100], Training Loss: 154149.68011466294, Test Loss: 37419004.0\n",
      "Epoch [67/100], Training Loss: 151987.06360527832, Test Loss: 36947656.0\n",
      "Epoch [68/100], Training Loss: 149885.7197241039, Test Loss: 36489020.0\n",
      "Epoch [69/100], Training Loss: 147843.69690416497, Test Loss: 36042452.0\n",
      "Epoch [70/100], Training Loss: 145856.53081491977, Test Loss: 35608756.0\n",
      "Epoch [71/100], Training Loss: 143921.5171852706, Test Loss: 35187336.0\n",
      "Epoch [72/100], Training Loss: 142040.08382450708, Test Loss: 34776016.0\n",
      "Epoch [73/100], Training Loss: 140210.91755894202, Test Loss: 34375632.0\n",
      "Epoch [74/100], Training Loss: 138433.00905326253, Test Loss: 33985652.0\n",
      "Epoch [75/100], Training Loss: 136701.3105827419, Test Loss: 33605212.0\n",
      "Epoch [76/100], Training Loss: 135013.05708329167, Test Loss: 33232322.0\n",
      "Epoch [77/100], Training Loss: 133358.06117188427, Test Loss: 32863920.0\n",
      "Epoch [78/100], Training Loss: 131723.84813964294, Test Loss: 32503298.0\n",
      "Epoch [79/100], Training Loss: 130115.91343471395, Test Loss: 32150178.0\n",
      "Epoch [80/100], Training Loss: 128539.4940208759, Test Loss: 31804776.0\n",
      "Epoch [81/100], Training Loss: 127004.25881515276, Test Loss: 31469760.0\n",
      "Epoch [82/100], Training Loss: 125513.45005317553, Test Loss: 31143888.0\n",
      "Epoch [83/100], Training Loss: 124067.69657362567, Test Loss: 30828054.0\n",
      "Epoch [84/100], Training Loss: 122661.73361971299, Test Loss: 30519386.0\n",
      "Epoch [85/100], Training Loss: 121285.98053469877, Test Loss: 30218558.0\n",
      "Epoch [86/100], Training Loss: 119934.47254633538, Test Loss: 29922812.0\n",
      "Epoch [87/100], Training Loss: 118610.17183103415, Test Loss: 29638638.0\n",
      "Epoch [88/100], Training Loss: 117317.18823816346, Test Loss: 29360312.0\n",
      "Epoch [89/100], Training Loss: 116062.18763652553, Test Loss: 29092952.0\n",
      "Epoch [90/100], Training Loss: 114842.30335417777, Test Loss: 28831478.0\n",
      "Epoch [91/100], Training Loss: 113654.28567435282, Test Loss: 28578264.0\n",
      "Epoch [92/100], Training Loss: 112490.99498557921, Test Loss: 28329962.0\n",
      "Epoch [93/100], Training Loss: 111351.00638985768, Test Loss: 28084558.0\n",
      "Epoch [94/100], Training Loss: 110223.78134441088, Test Loss: 27838892.0\n",
      "Epoch [95/100], Training Loss: 109110.62907956801, Test Loss: 27599780.0\n",
      "Epoch [96/100], Training Loss: 108021.31910043096, Test Loss: 27365756.0\n",
      "Epoch [97/100], Training Loss: 106959.24458387018, Test Loss: 27139684.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [98/100], Training Loss: 105923.32773856333, Test Loss: 26920146.0\n",
      "Epoch [99/100], Training Loss: 104909.64697505701, Test Loss: 26706840.0\n",
      "Epoch [100/100], Training Loss: 103914.07620762617, Test Loss: 26496256.0\n",
      "Epoch [1/100], Training Loss: 1164524.6106273325, Test Loss: 299480864.0\n",
      "Epoch [2/100], Training Loss: 1152489.8733487353, Test Loss: 292161184.0\n",
      "Epoch [3/100], Training Loss: 1094964.9760085302, Test Loss: 268461120.0\n",
      "Epoch [4/100], Training Loss: 967902.0280789053, Test Loss: 227992880.0\n",
      "Epoch [5/100], Training Loss: 801588.7039867307, Test Loss: 186561792.0\n",
      "Epoch [6/100], Training Loss: 667810.4287660683, Test Loss: 160747184.0\n",
      "Epoch [7/100], Training Loss: 595006.2060304484, Test Loss: 147876736.0\n",
      "Epoch [8/100], Training Loss: 554832.2902671642, Test Loss: 139612112.0\n",
      "Epoch [9/100], Training Loss: 525120.9077661276, Test Loss: 132822440.0\n",
      "Epoch [10/100], Training Loss: 499324.90539659973, Test Loss: 126729688.0\n",
      "Epoch [11/100], Training Loss: 475672.91819205025, Test Loss: 121081480.0\n",
      "Epoch [12/100], Training Loss: 453508.6340856584, Test Loss: 115773832.0\n",
      "Epoch [13/100], Training Loss: 432562.0131508797, Test Loss: 110766208.0\n",
      "Epoch [14/100], Training Loss: 412744.56181505835, Test Loss: 106050520.0\n",
      "Epoch [15/100], Training Loss: 394055.3045435697, Test Loss: 101627744.0\n",
      "Epoch [16/100], Training Loss: 376507.07825365796, Test Loss: 97498392.0\n",
      "Epoch [17/100], Training Loss: 360118.458148214, Test Loss: 93667064.0\n",
      "Epoch [18/100], Training Loss: 344902.5026953379, Test Loss: 90126272.0\n",
      "Epoch [19/100], Training Loss: 330832.03743854037, Test Loss: 86858832.0\n",
      "Epoch [20/100], Training Loss: 317847.56981221493, Test Loss: 83841944.0\n",
      "Epoch [21/100], Training Loss: 305871.6898288016, Test Loss: 81051176.0\n",
      "Epoch [22/100], Training Loss: 294815.84041229787, Test Loss: 78462752.0\n",
      "Epoch [23/100], Training Loss: 284592.0988093123, Test Loss: 76055760.0\n",
      "Epoch [24/100], Training Loss: 275118.34997926664, Test Loss: 73810768.0\n",
      "Epoch [25/100], Training Loss: 266318.76831941237, Test Loss: 71710808.0\n",
      "Epoch [26/100], Training Loss: 258127.28262543687, Test Loss: 69741992.0\n",
      "Epoch [27/100], Training Loss: 250484.7176115159, Test Loss: 67893528.0\n",
      "Epoch [28/100], Training Loss: 243340.79248859663, Test Loss: 66156580.0\n",
      "Epoch [29/100], Training Loss: 236653.4686333748, Test Loss: 64524492.0\n",
      "Epoch [30/100], Training Loss: 230377.87133463658, Test Loss: 62986548.0\n",
      "Epoch [31/100], Training Loss: 224477.38261951305, Test Loss: 61536272.0\n",
      "Epoch [32/100], Training Loss: 218916.52378413602, Test Loss: 60165644.0\n",
      "Epoch [33/100], Training Loss: 213655.47692672236, Test Loss: 58866516.0\n",
      "Epoch [34/100], Training Loss: 208667.5016882886, Test Loss: 57632920.0\n",
      "Epoch [35/100], Training Loss: 203919.87441502282, Test Loss: 56456880.0\n",
      "Epoch [36/100], Training Loss: 199383.47325395414, Test Loss: 55331584.0\n",
      "Epoch [37/100], Training Loss: 195042.53243291273, Test Loss: 54253232.0\n",
      "Epoch [38/100], Training Loss: 190879.12043125406, Test Loss: 53216912.0\n",
      "Epoch [39/100], Training Loss: 186872.30661690657, Test Loss: 52218404.0\n",
      "Epoch [40/100], Training Loss: 183007.6662519993, Test Loss: 51253956.0\n",
      "Epoch [41/100], Training Loss: 179277.49232865352, Test Loss: 50321856.0\n",
      "Epoch [42/100], Training Loss: 175670.3191754043, Test Loss: 49418328.0\n",
      "Epoch [43/100], Training Loss: 172177.4925656063, Test Loss: 48541308.0\n",
      "Epoch [44/100], Training Loss: 168793.06581363664, Test Loss: 47689652.0\n",
      "Epoch [45/100], Training Loss: 165511.79396955157, Test Loss: 46861588.0\n",
      "Epoch [46/100], Training Loss: 162329.98424263965, Test Loss: 46056712.0\n",
      "Epoch [47/100], Training Loss: 159242.79319945502, Test Loss: 45273432.0\n",
      "Epoch [48/100], Training Loss: 156245.89716249038, Test Loss: 44510300.0\n",
      "Epoch [49/100], Training Loss: 153334.44250933002, Test Loss: 43766340.0\n",
      "Epoch [50/100], Training Loss: 150503.37017949173, Test Loss: 43040576.0\n",
      "Epoch [51/100], Training Loss: 147751.47479414727, Test Loss: 42332444.0\n",
      "Epoch [52/100], Training Loss: 145077.8853740892, Test Loss: 41642156.0\n",
      "Epoch [53/100], Training Loss: 142479.80113737338, Test Loss: 40968836.0\n",
      "Epoch [54/100], Training Loss: 139955.86505538772, Test Loss: 40312568.0\n",
      "Epoch [55/100], Training Loss: 137503.32207807593, Test Loss: 39672468.0\n",
      "Epoch [56/100], Training Loss: 135117.40903974883, Test Loss: 39047028.0\n",
      "Epoch [57/100], Training Loss: 132795.9391031337, Test Loss: 38436728.0\n",
      "Epoch [58/100], Training Loss: 130536.52117765535, Test Loss: 37841932.0\n",
      "Epoch [59/100], Training Loss: 128339.69622652687, Test Loss: 37262816.0\n",
      "Epoch [60/100], Training Loss: 126203.79029678337, Test Loss: 36698412.0\n",
      "Epoch [61/100], Training Loss: 124125.58432557313, Test Loss: 36149220.0\n",
      "Epoch [62/100], Training Loss: 122104.69747052899, Test Loss: 35614864.0\n",
      "Epoch [63/100], Training Loss: 120140.1162845803, Test Loss: 35094928.0\n",
      "Epoch [64/100], Training Loss: 118231.20016586695, Test Loss: 34588348.0\n",
      "Epoch [65/100], Training Loss: 116374.04762751021, Test Loss: 34095152.0\n",
      "Epoch [66/100], Training Loss: 114566.17439725135, Test Loss: 33614764.0\n",
      "Epoch [67/100], Training Loss: 112806.89893963627, Test Loss: 33146772.0\n",
      "Epoch [68/100], Training Loss: 111095.49789704401, Test Loss: 32690784.0\n",
      "Epoch [69/100], Training Loss: 109430.77430247024, Test Loss: 32247112.0\n",
      "Epoch [70/100], Training Loss: 107809.5073159173, Test Loss: 31815324.0\n",
      "Epoch [71/100], Training Loss: 106231.53438777324, Test Loss: 31395374.0\n",
      "Epoch [72/100], Training Loss: 104697.3763995024, Test Loss: 30987058.0\n",
      "Epoch [73/100], Training Loss: 103207.58426633493, Test Loss: 30590200.0\n",
      "Epoch [74/100], Training Loss: 101759.18026183284, Test Loss: 30204028.0\n",
      "Epoch [75/100], Training Loss: 100351.75019252414, Test Loss: 29828988.0\n",
      "Epoch [76/100], Training Loss: 98984.151412831, Test Loss: 29463844.0\n",
      "Epoch [77/100], Training Loss: 97653.10651027782, Test Loss: 29108568.0\n",
      "Epoch [78/100], Training Loss: 96357.66548190273, Test Loss: 28763398.0\n",
      "Epoch [79/100], Training Loss: 95096.26597950359, Test Loss: 28427784.0\n",
      "Epoch [80/100], Training Loss: 93868.01178840116, Test Loss: 28101374.0\n",
      "Epoch [81/100], Training Loss: 92672.39902849357, Test Loss: 27784136.0\n",
      "Epoch [82/100], Training Loss: 91507.77797523844, Test Loss: 27475438.0\n",
      "Epoch [83/100], Training Loss: 90374.87672531248, Test Loss: 27175118.0\n",
      "Epoch [84/100], Training Loss: 89272.71251703099, Test Loss: 26883342.0\n",
      "Epoch [85/100], Training Loss: 88199.60855399561, Test Loss: 26599560.0\n",
      "Epoch [86/100], Training Loss: 87153.58799834133, Test Loss: 26323786.0\n",
      "Epoch [87/100], Training Loss: 86134.20099520171, Test Loss: 26056032.0\n",
      "Epoch [88/100], Training Loss: 85141.40702565014, Test Loss: 25795952.0\n",
      "Epoch [89/100], Training Loss: 84172.35614003909, Test Loss: 25543266.0\n",
      "Epoch [90/100], Training Loss: 83224.90693679285, Test Loss: 25297048.0\n",
      "Epoch [91/100], Training Loss: 82298.72015875837, Test Loss: 25057770.0\n",
      "Epoch [92/100], Training Loss: 81393.99152893786, Test Loss: 24825196.0\n",
      "Epoch [93/100], Training Loss: 80509.31876073692, Test Loss: 24599072.0\n",
      "Epoch [94/100], Training Loss: 79644.73816717019, Test Loss: 24378444.0\n",
      "Epoch [95/100], Training Loss: 78798.39736982406, Test Loss: 24163246.0\n",
      "Epoch [96/100], Training Loss: 77969.19495290563, Test Loss: 23953802.0\n",
      "Epoch [97/100], Training Loss: 77156.40880279604, Test Loss: 23749500.0\n",
      "Epoch [98/100], Training Loss: 76361.54297731176, Test Loss: 23550152.0\n",
      "Epoch [99/100], Training Loss: 75583.74551270659, Test Loss: 23355992.0\n",
      "Epoch [100/100], Training Loss: 74821.479829394, Test Loss: 23166312.0\n",
      "Epoch [1/100], Training Loss: 582420.6262662165, Test Loss: 300019296.0\n",
      "Epoch [2/100], Training Loss: 581776.7222320952, Test Loss: 299249376.0\n",
      "Epoch [3/100], Training Loss: 578609.2975534624, Test Loss: 296431904.0\n",
      "Epoch [4/100], Training Loss: 569866.7150050353, Test Loss: 289888736.0\n",
      "Epoch [5/100], Training Loss: 552453.7863870624, Test Loss: 278236192.0\n",
      "Epoch [6/100], Training Loss: 524440.8459214502, Test Loss: 261004688.0\n",
      "Epoch [7/100], Training Loss: 486221.8067650021, Test Loss: 239168496.0\n",
      "Epoch [8/100], Training Loss: 441232.2928736449, Test Loss: 215276736.0\n",
      "Epoch [9/100], Training Loss: 395492.0272495705, Test Loss: 192794384.0\n",
      "Epoch [10/100], Training Loss: 355491.9229903442, Test Loss: 174631152.0\n",
      "Epoch [11/100], Training Loss: 325095.38866180915, Test Loss: 161671200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/100], Training Loss: 303936.20520111365, Test Loss: 152813248.0\n",
      "Epoch [13/100], Training Loss: 289093.723357621, Test Loss: 146406032.0\n",
      "Epoch [14/100], Training Loss: 277757.707718737, Test Loss: 141280752.0\n",
      "Epoch [15/100], Training Loss: 268243.63722528284, Test Loss: 136833568.0\n",
      "Epoch [16/100], Training Loss: 259739.13772880755, Test Loss: 132787768.0\n",
      "Epoch [17/100], Training Loss: 251870.8081274806, Test Loss: 129012720.0\n",
      "Epoch [18/100], Training Loss: 244452.87506664297, Test Loss: 125439432.0\n",
      "Epoch [19/100], Training Loss: 237381.69397547538, Test Loss: 122027688.0\n",
      "Epoch [20/100], Training Loss: 230595.03252177004, Test Loss: 118752984.0\n",
      "Epoch [21/100], Training Loss: 224054.87921331674, Test Loss: 115600368.0\n",
      "Epoch [22/100], Training Loss: 217738.7455719448, Test Loss: 112561176.0\n",
      "Epoch [23/100], Training Loss: 211634.08565843257, Test Loss: 109630848.0\n",
      "Epoch [24/100], Training Loss: 205735.6031040815, Test Loss: 106807904.0\n",
      "Epoch [25/100], Training Loss: 200043.04839760676, Test Loss: 104092192.0\n",
      "Epoch [26/100], Training Loss: 194557.52360642143, Test Loss: 101483936.0\n",
      "Epoch [27/100], Training Loss: 189280.01066287543, Test Loss: 98982656.0\n",
      "Epoch [28/100], Training Loss: 184211.27753095195, Test Loss: 96586928.0\n",
      "Epoch [29/100], Training Loss: 179352.98477578344, Test Loss: 94296712.0\n",
      "Epoch [30/100], Training Loss: 174705.8579468041, Test Loss: 92111184.0\n",
      "Epoch [31/100], Training Loss: 170269.13050174754, Test Loss: 90028576.0\n",
      "Epoch [32/100], Training Loss: 166040.6400094781, Test Loss: 88046832.0\n",
      "Epoch [33/100], Training Loss: 162017.02837509627, Test Loss: 86162648.0\n",
      "Epoch [34/100], Training Loss: 158192.00236952788, Test Loss: 84371352.0\n",
      "Epoch [35/100], Training Loss: 154557.3501569812, Test Loss: 82667816.0\n",
      "Epoch [36/100], Training Loss: 151103.96801137374, Test Loss: 81046544.0\n",
      "Epoch [37/100], Training Loss: 147821.81861264142, Test Loss: 79501712.0\n",
      "Epoch [38/100], Training Loss: 144700.18363841006, Test Loss: 78027632.0\n",
      "Epoch [39/100], Training Loss: 141727.73484983118, Test Loss: 76618608.0\n",
      "Epoch [40/100], Training Loss: 138893.46792251643, Test Loss: 75269736.0\n",
      "Epoch [41/100], Training Loss: 136187.64717729992, Test Loss: 73976232.0\n",
      "Epoch [42/100], Training Loss: 133600.87009063445, Test Loss: 72733896.0\n",
      "Epoch [43/100], Training Loss: 131124.2305550619, Test Loss: 71539240.0\n",
      "Epoch [44/100], Training Loss: 128749.63995023991, Test Loss: 70388800.0\n",
      "Epoch [45/100], Training Loss: 126470.01765298264, Test Loss: 69279640.0\n",
      "Epoch [46/100], Training Loss: 124278.78514306025, Test Loss: 68209112.0\n",
      "Epoch [47/100], Training Loss: 122170.03826787512, Test Loss: 67175448.0\n",
      "Epoch [48/100], Training Loss: 120139.02707185593, Test Loss: 66176532.0\n",
      "Epoch [49/100], Training Loss: 118180.77602037793, Test Loss: 65210588.0\n",
      "Epoch [50/100], Training Loss: 116290.56288134589, Test Loss: 64275552.0\n",
      "Epoch [51/100], Training Loss: 114463.8284461821, Test Loss: 63369624.0\n",
      "Epoch [52/100], Training Loss: 112696.70540844736, Test Loss: 62491432.0\n",
      "Epoch [53/100], Training Loss: 110985.68094307209, Test Loss: 61639984.0\n",
      "Epoch [54/100], Training Loss: 109328.14276405427, Test Loss: 60814152.0\n",
      "Epoch [55/100], Training Loss: 107719.27421361294, Test Loss: 60011244.0\n",
      "Epoch [56/100], Training Loss: 106154.53373615307, Test Loss: 59229716.0\n",
      "Epoch [57/100], Training Loss: 104632.65458207452, Test Loss: 58469240.0\n",
      "Epoch [58/100], Training Loss: 103151.47384633611, Test Loss: 57728264.0\n",
      "Epoch [59/100], Training Loss: 101707.59327054085, Test Loss: 57005908.0\n",
      "Epoch [60/100], Training Loss: 100297.7951543155, Test Loss: 56300408.0\n",
      "Epoch [61/100], Training Loss: 98919.86754339199, Test Loss: 55610680.0\n",
      "Epoch [62/100], Training Loss: 97572.74284698773, Test Loss: 54935664.0\n",
      "Epoch [63/100], Training Loss: 96254.78514306025, Test Loss: 54274344.0\n",
      "Epoch [64/100], Training Loss: 94964.53373615307, Test Loss: 53626192.0\n",
      "Epoch [65/100], Training Loss: 93701.42147977016, Test Loss: 52991044.0\n",
      "Epoch [66/100], Training Loss: 92464.14714768082, Test Loss: 52368556.0\n",
      "Epoch [67/100], Training Loss: 91251.25798234702, Test Loss: 51757988.0\n",
      "Epoch [68/100], Training Loss: 90061.45595640069, Test Loss: 51158640.0\n",
      "Epoch [69/100], Training Loss: 88894.42521177656, Test Loss: 50570252.0\n",
      "Epoch [70/100], Training Loss: 87749.48190273088, Test Loss: 49992012.0\n",
      "Epoch [71/100], Training Loss: 86625.89396362775, Test Loss: 49423752.0\n",
      "Epoch [72/100], Training Loss: 85523.2030093004, Test Loss: 48865392.0\n",
      "Epoch [73/100], Training Loss: 84441.45631182987, Test Loss: 48317112.0\n",
      "Epoch [74/100], Training Loss: 83380.27214027605, Test Loss: 47778148.0\n",
      "Epoch [75/100], Training Loss: 82338.83075647177, Test Loss: 47248108.0\n",
      "Epoch [76/100], Training Loss: 81316.2085184527, Test Loss: 46726900.0\n",
      "Epoch [77/100], Training Loss: 80312.31881997512, Test Loss: 46214336.0\n",
      "Epoch [78/100], Training Loss: 79327.04519874415, Test Loss: 45710384.0\n",
      "Epoch [79/100], Training Loss: 78359.84467744803, Test Loss: 45215012.0\n",
      "Epoch [80/100], Training Loss: 77410.44416799952, Test Loss: 44727572.0\n",
      "Epoch [81/100], Training Loss: 76478.65470055092, Test Loss: 44248008.0\n",
      "Epoch [82/100], Training Loss: 75563.72631953083, Test Loss: 43775712.0\n",
      "Epoch [83/100], Training Loss: 74665.07552870091, Test Loss: 43310664.0\n",
      "Epoch [84/100], Training Loss: 73781.98471654522, Test Loss: 42852724.0\n",
      "Epoch [85/100], Training Loss: 72914.55494342752, Test Loss: 42402224.0\n",
      "Epoch [86/100], Training Loss: 72062.45696345004, Test Loss: 41958440.0\n",
      "Epoch [87/100], Training Loss: 71225.42065043541, Test Loss: 41521828.0\n",
      "Epoch [88/100], Training Loss: 70403.57111545524, Test Loss: 41091908.0\n",
      "Epoch [89/100], Training Loss: 69596.48030329957, Test Loss: 40668784.0\n",
      "Epoch [90/100], Training Loss: 68803.72240980985, Test Loss: 40251964.0\n",
      "Epoch [91/100], Training Loss: 68024.94022865944, Test Loss: 39841308.0\n",
      "Epoch [92/100], Training Loss: 67259.79598365026, Test Loss: 39437120.0\n",
      "Epoch [93/100], Training Loss: 66507.99241751082, Test Loss: 39039360.0\n",
      "Epoch [94/100], Training Loss: 65769.1309756531, Test Loss: 38647860.0\n",
      "Epoch [95/100], Training Loss: 65042.85385936852, Test Loss: 38262528.0\n",
      "Epoch [96/100], Training Loss: 64328.79959718026, Test Loss: 37883316.0\n",
      "Epoch [97/100], Training Loss: 63626.84580297376, Test Loss: 37510500.0\n",
      "Epoch [98/100], Training Loss: 62936.86973520526, Test Loss: 37143920.0\n",
      "Epoch [99/100], Training Loss: 62258.891771814466, Test Loss: 36783084.0\n",
      "Epoch [100/100], Training Loss: 61592.64889520763, Test Loss: 36428068.0\n",
      "Epoch [1/100], Training Loss: 7129773.912505183, Test Loss: 148068144.0\n",
      "Epoch [2/100], Training Loss: 4052136.645459392, Test Loss: 116956200.0\n",
      "Epoch [3/100], Training Loss: 3237011.168888099, Test Loss: 95503000.0\n",
      "Epoch [4/100], Training Loss: 2645656.7517327173, Test Loss: 79908320.0\n",
      "Epoch [5/100], Training Loss: 2209670.4781114864, Test Loss: 68357952.0\n",
      "Epoch [6/100], Training Loss: 1894345.546383508, Test Loss: 60000580.0\n",
      "Epoch [7/100], Training Loss: 1669564.891297909, Test Loss: 53962688.0\n",
      "Epoch [8/100], Training Loss: 1503867.9447307624, Test Loss: 49333648.0\n",
      "Epoch [9/100], Training Loss: 1372343.3931046738, Test Loss: 45508888.0\n",
      "Epoch [10/100], Training Loss: 1261179.971713761, Test Loss: 42186220.0\n",
      "Epoch [11/100], Training Loss: 1164420.7107991232, Test Loss: 39226400.0\n",
      "Epoch [12/100], Training Loss: 1079354.714945797, Test Loss: 36570056.0\n",
      "Epoch [13/100], Training Loss: 1004340.4258930158, Test Loss: 34188208.0\n",
      "Epoch [14/100], Training Loss: 938181.1428677212, Test Loss: 32078048.0\n",
      "Epoch [15/100], Training Loss: 879872.3220632664, Test Loss: 30214860.0\n",
      "Epoch [16/100], Training Loss: 828511.1056217048, Test Loss: 28570192.0\n",
      "Epoch [17/100], Training Loss: 783382.9581037853, Test Loss: 27117352.0\n",
      "Epoch [18/100], Training Loss: 743674.0910787275, Test Loss: 25832242.0\n",
      "Epoch [19/100], Training Loss: 708538.3171761151, Test Loss: 24694688.0\n",
      "Epoch [20/100], Training Loss: 677114.6937385226, Test Loss: 23676636.0\n",
      "Epoch [21/100], Training Loss: 648693.6732421066, Test Loss: 22769232.0\n",
      "Epoch [22/100], Training Loss: 622785.2307253717, Test Loss: 21951072.0\n",
      "Epoch [23/100], Training Loss: 599099.3422412772, Test Loss: 21224698.0\n",
      "Epoch [24/100], Training Loss: 577386.3625081453, Test Loss: 20573310.0\n",
      "Epoch [25/100], Training Loss: 557456.7331245187, Test Loss: 20001444.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/100], Training Loss: 539095.0037320064, Test Loss: 19479664.0\n",
      "Epoch [27/100], Training Loss: 522130.58977548726, Test Loss: 19016406.0\n",
      "Epoch [28/100], Training Loss: 506408.08953853446, Test Loss: 18600248.0\n",
      "Epoch [29/100], Training Loss: 491874.0609264854, Test Loss: 18222140.0\n",
      "Epoch [30/100], Training Loss: 478365.72812629584, Test Loss: 17876290.0\n",
      "Epoch [31/100], Training Loss: 465783.2802336947, Test Loss: 17559024.0\n",
      "Epoch [32/100], Training Loss: 454080.6538045732, Test Loss: 17268786.0\n",
      "Epoch [33/100], Training Loss: 443134.8888765476, Test Loss: 17009074.0\n",
      "Epoch [34/100], Training Loss: 432861.2546131746, Test Loss: 16767782.0\n",
      "Epoch [35/100], Training Loss: 423216.28422486817, Test Loss: 16560319.0\n",
      "Epoch [36/100], Training Loss: 414155.48680469167, Test Loss: 16363979.0\n",
      "Epoch [37/100], Training Loss: 405687.60140098335, Test Loss: 16190842.0\n",
      "Epoch [38/100], Training Loss: 397743.52753095195, Test Loss: 16053892.0\n",
      "Epoch [39/100], Training Loss: 390289.6311237486, Test Loss: 15916462.0\n",
      "Epoch [40/100], Training Loss: 383275.26997808187, Test Loss: 15801762.0\n",
      "Epoch [41/100], Training Loss: 376707.25141431193, Test Loss: 15703482.0\n",
      "Epoch [42/100], Training Loss: 370468.8437666607, Test Loss: 15606609.0\n",
      "Epoch [43/100], Training Loss: 364616.8692983236, Test Loss: 15542183.0\n",
      "Epoch [44/100], Training Loss: 359086.1264587406, Test Loss: 15463602.0\n",
      "Epoch [45/100], Training Loss: 353878.79688703275, Test Loss: 15412038.0\n",
      "Epoch [46/100], Training Loss: 348935.6798471655, Test Loss: 15354671.0\n",
      "Epoch [47/100], Training Loss: 344310.1261329305, Test Loss: 15301672.0\n",
      "Epoch [48/100], Training Loss: 339902.9114092767, Test Loss: 15223065.0\n",
      "Epoch [49/100], Training Loss: 335748.2755983058, Test Loss: 15202203.0\n",
      "Epoch [50/100], Training Loss: 331857.6688029441, Test Loss: 15105429.0\n",
      "Epoch [51/100], Training Loss: 328141.17471195426, Test Loss: 15110262.0\n",
      "Epoch [52/100], Training Loss: 324631.9021273917, Test Loss: 15025170.0\n",
      "Epoch [53/100], Training Loss: 321346.58749851905, Test Loss: 15020900.0\n",
      "Epoch [54/100], Training Loss: 318195.7394074699, Test Loss: 14940292.0\n",
      "Epoch [55/100], Training Loss: 315230.26672368345, Test Loss: 14943002.0\n",
      "Epoch [56/100], Training Loss: 312371.36719166517, Test Loss: 14874520.0\n",
      "Epoch [57/100], Training Loss: 309683.73898910015, Test Loss: 14883560.0\n",
      "Epoch [58/100], Training Loss: 307087.59656566556, Test Loss: 14812479.0\n",
      "Epoch [59/100], Training Loss: 304545.91285691014, Test Loss: 14825814.0\n",
      "Epoch [60/100], Training Loss: 302176.7203401753, Test Loss: 14750202.0\n",
      "Epoch [61/100], Training Loss: 299920.41104644275, Test Loss: 14766000.0\n",
      "Epoch [62/100], Training Loss: 297758.0350616077, Test Loss: 14707308.0\n",
      "Epoch [63/100], Training Loss: 295658.7191109828, Test Loss: 14669337.0\n",
      "Epoch [64/100], Training Loss: 293673.4975971506, Test Loss: 14682138.0\n",
      "Epoch [65/100], Training Loss: 291742.7706185949, Test Loss: 14647712.0\n",
      "Epoch [66/100], Training Loss: 289912.4574706771, Test Loss: 14612207.0\n",
      "Epoch [67/100], Training Loss: 288111.5299375037, Test Loss: 14606740.0\n",
      "Epoch [68/100], Training Loss: 286430.50700121437, Test Loss: 14561630.0\n",
      "Epoch [69/100], Training Loss: 284787.6217863278, Test Loss: 14603209.0\n",
      "Epoch [70/100], Training Loss: 283233.6634418873, Test Loss: 14540088.0\n",
      "Epoch [71/100], Training Loss: 281595.7630398081, Test Loss: 14525590.0\n",
      "Epoch [72/100], Training Loss: 280128.2311289319, Test Loss: 14549461.0\n",
      "Epoch [73/100], Training Loss: 278639.834133049, Test Loss: 14504490.0\n",
      "Epoch [74/100], Training Loss: 277201.1238670695, Test Loss: 14500820.0\n",
      "Epoch [75/100], Training Loss: 275796.6601356555, Test Loss: 14482751.0\n",
      "Epoch [76/100], Training Loss: 274470.7636469996, Test Loss: 14482514.0\n",
      "Epoch [77/100], Training Loss: 273162.4596180617, Test Loss: 14494188.0\n",
      "Epoch [78/100], Training Loss: 271898.3673286535, Test Loss: 14425117.0\n",
      "Epoch [79/100], Training Loss: 270671.5004627984, Test Loss: 14479792.0\n",
      "Epoch [80/100], Training Loss: 269497.3305306262, Test Loss: 14454329.0\n",
      "Epoch [81/100], Training Loss: 268283.49448714533, Test Loss: 14444151.0\n",
      "Epoch [82/100], Training Loss: 267165.6134485516, Test Loss: 14459311.0\n",
      "Epoch [83/100], Training Loss: 266063.15838442626, Test Loss: 14460383.0\n",
      "Epoch [84/100], Training Loss: 264989.04310319293, Test Loss: 14384781.0\n",
      "Epoch [85/100], Training Loss: 263912.8948855222, Test Loss: 14454732.0\n",
      "Epoch [86/100], Training Loss: 262922.38848779694, Test Loss: 14414379.0\n",
      "Epoch [87/100], Training Loss: 261875.06129672413, Test Loss: 14417240.0\n",
      "Epoch [88/100], Training Loss: 260894.37810260055, Test Loss: 14414275.0\n",
      "Epoch [89/100], Training Loss: 259889.03944893667, Test Loss: 14406065.0\n",
      "Epoch [90/100], Training Loss: 258918.11584029382, Test Loss: 14343117.0\n",
      "Epoch [91/100], Training Loss: 257936.21715612226, Test Loss: 14417541.0\n",
      "Epoch [92/100], Training Loss: 257004.16135744328, Test Loss: 14384187.0\n",
      "Epoch [93/100], Training Loss: 256111.71071767077, Test Loss: 14382049.0\n",
      "Epoch [94/100], Training Loss: 255187.4550789349, Test Loss: 14392619.0\n",
      "Epoch [95/100], Training Loss: 254292.18943634856, Test Loss: 14392681.0\n",
      "Epoch [96/100], Training Loss: 253475.8786690658, Test Loss: 14332950.0\n",
      "Epoch [97/100], Training Loss: 252588.7221025117, Test Loss: 14402295.0\n",
      "Epoch [98/100], Training Loss: 251796.51701987442, Test Loss: 14371662.0\n",
      "Epoch [99/100], Training Loss: 250968.42457496593, Test Loss: 14370916.0\n",
      "Epoch [100/100], Training Loss: 250228.28630560986, Test Loss: 14392215.0\n",
      "Epoch [1/100], Training Loss: 4372516.26183283, Test Loss: 227140800.0\n",
      "Epoch [2/100], Training Loss: 2637189.6706356257, Test Loss: 143752208.0\n",
      "Epoch [3/100], Training Loss: 2056746.7331319235, Test Loss: 123981464.0\n",
      "Epoch [4/100], Training Loss: 1776996.058408862, Test Loss: 108209656.0\n",
      "Epoch [5/100], Training Loss: 1546143.74112908, Test Loss: 95287200.0\n",
      "Epoch [6/100], Training Loss: 1356885.196611575, Test Loss: 84813984.0\n",
      "Epoch [7/100], Training Loss: 1202265.9772525325, Test Loss: 76216656.0\n",
      "Epoch [8/100], Training Loss: 1076284.6834903145, Test Loss: 69163224.0\n",
      "Epoch [9/100], Training Loss: 974353.7902375452, Test Loss: 63408860.0\n",
      "Epoch [10/100], Training Loss: 892255.457555832, Test Loss: 58760112.0\n",
      "Epoch [11/100], Training Loss: 825988.1543155026, Test Loss: 54971744.0\n",
      "Epoch [12/100], Training Loss: 771390.7476452817, Test Loss: 51768776.0\n",
      "Epoch [13/100], Training Loss: 724552.2360642143, Test Loss: 48966968.0\n",
      "Epoch [14/100], Training Loss: 683046.7608553995, Test Loss: 46442648.0\n",
      "Epoch [15/100], Training Loss: 645549.2927551685, Test Loss: 44137680.0\n",
      "Epoch [16/100], Training Loss: 611331.919643386, Test Loss: 42005744.0\n",
      "Epoch [17/100], Training Loss: 579865.2809963864, Test Loss: 40023092.0\n",
      "Epoch [18/100], Training Loss: 550853.1576920799, Test Loss: 38173656.0\n",
      "Epoch [19/100], Training Loss: 524160.9334162668, Test Loss: 36452888.0\n",
      "Epoch [20/100], Training Loss: 499638.1957230022, Test Loss: 34857972.0\n",
      "Epoch [21/100], Training Loss: 477060.6331378473, Test Loss: 33383124.0\n",
      "Epoch [22/100], Training Loss: 456262.23173982586, Test Loss: 32024432.0\n",
      "Epoch [23/100], Training Loss: 437115.79411764705, Test Loss: 30769020.0\n",
      "Epoch [24/100], Training Loss: 419528.7803151472, Test Loss: 29612448.0\n",
      "Epoch [25/100], Training Loss: 403369.1777145904, Test Loss: 28548234.0\n",
      "Epoch [26/100], Training Loss: 388542.3357028612, Test Loss: 27567220.0\n",
      "Epoch [27/100], Training Loss: 374901.90151649783, Test Loss: 26663306.0\n",
      "Epoch [28/100], Training Loss: 362345.03386943904, Test Loss: 25828858.0\n",
      "Epoch [29/100], Training Loss: 350743.3250696049, Test Loss: 25057280.0\n",
      "Epoch [30/100], Training Loss: 339960.5464427463, Test Loss: 24338678.0\n",
      "Epoch [31/100], Training Loss: 329874.88418932527, Test Loss: 23672960.0\n",
      "Epoch [32/100], Training Loss: 320404.40444878855, Test Loss: 23052482.0\n",
      "Epoch [33/100], Training Loss: 311510.4463598128, Test Loss: 22478400.0\n",
      "Epoch [34/100], Training Loss: 303132.9682038979, Test Loss: 21943826.0\n",
      "Epoch [35/100], Training Loss: 295217.95080267754, Test Loss: 21446982.0\n",
      "Epoch [36/100], Training Loss: 287744.563666252, Test Loss: 20986006.0\n",
      "Epoch [37/100], Training Loss: 280685.8939488182, Test Loss: 20555526.0\n",
      "Epoch [38/100], Training Loss: 274023.3023221373, Test Loss: 20154908.0\n",
      "Epoch [39/100], Training Loss: 267735.1440672946, Test Loss: 19784206.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/100], Training Loss: 261775.41733309638, Test Loss: 19438532.0\n",
      "Epoch [41/100], Training Loss: 256124.7213731414, Test Loss: 19115846.0\n",
      "Epoch [42/100], Training Loss: 250750.63749185475, Test Loss: 18817276.0\n",
      "Epoch [43/100], Training Loss: 245651.28249215093, Test Loss: 18537474.0\n",
      "Epoch [44/100], Training Loss: 240783.66437118655, Test Loss: 18273158.0\n",
      "Epoch [45/100], Training Loss: 236145.8254398436, Test Loss: 18025932.0\n",
      "Epoch [46/100], Training Loss: 231702.56519163557, Test Loss: 17793568.0\n",
      "Epoch [47/100], Training Loss: 227480.76590545583, Test Loss: 17578378.0\n",
      "Epoch [48/100], Training Loss: 223438.0172827439, Test Loss: 17373344.0\n",
      "Epoch [49/100], Training Loss: 219588.89498548664, Test Loss: 17184022.0\n",
      "Epoch [50/100], Training Loss: 215893.59595106926, Test Loss: 17005022.0\n",
      "Epoch [51/100], Training Loss: 212364.35735442213, Test Loss: 16842364.0\n",
      "Epoch [52/100], Training Loss: 208994.24484627688, Test Loss: 16687913.0\n",
      "Epoch [53/100], Training Loss: 205769.75010366685, Test Loss: 16547757.0\n",
      "Epoch [54/100], Training Loss: 202673.95317220545, Test Loss: 16412929.0\n",
      "Epoch [55/100], Training Loss: 199719.88390794385, Test Loss: 16292857.0\n",
      "Epoch [56/100], Training Loss: 196891.72897044013, Test Loss: 16173668.0\n",
      "Epoch [57/100], Training Loss: 194199.14922101772, Test Loss: 16078161.0\n",
      "Epoch [58/100], Training Loss: 191614.28049286178, Test Loss: 15970235.0\n",
      "Epoch [59/100], Training Loss: 189130.20133582133, Test Loss: 15890227.0\n",
      "Epoch [60/100], Training Loss: 186739.26137373378, Test Loss: 15803213.0\n",
      "Epoch [61/100], Training Loss: 184440.50299152895, Test Loss: 15732347.0\n",
      "Epoch [62/100], Training Loss: 182230.302825662, Test Loss: 15658365.0\n",
      "Epoch [63/100], Training Loss: 180113.67381967892, Test Loss: 15597132.0\n",
      "Epoch [64/100], Training Loss: 178091.0628517268, Test Loss: 15530507.0\n",
      "Epoch [65/100], Training Loss: 176155.6239263077, Test Loss: 15483346.0\n",
      "Epoch [66/100], Training Loss: 174291.26996327232, Test Loss: 15416298.0\n",
      "Epoch [67/100], Training Loss: 172507.32368491203, Test Loss: 15374164.0\n",
      "Epoch [68/100], Training Loss: 170795.84559564007, Test Loss: 15313777.0\n",
      "Epoch [69/100], Training Loss: 169158.9808216338, Test Loss: 15274034.0\n",
      "Epoch [70/100], Training Loss: 167582.5181639121, Test Loss: 15210142.0\n",
      "Epoch [71/100], Training Loss: 166075.40159054557, Test Loss: 15180152.0\n",
      "Epoch [72/100], Training Loss: 164626.3433149695, Test Loss: 15113700.0\n",
      "Epoch [73/100], Training Loss: 163239.51144778152, Test Loss: 15102866.0\n",
      "Epoch [74/100], Training Loss: 161908.3804795332, Test Loss: 15013626.0\n",
      "Epoch [75/100], Training Loss: 160619.72185445175, Test Loss: 15026449.0\n",
      "Epoch [76/100], Training Loss: 159386.53894170962, Test Loss: 14938907.0\n",
      "Epoch [77/100], Training Loss: 158194.06470292044, Test Loss: 14942338.0\n",
      "Epoch [78/100], Training Loss: 157039.5034654345, Test Loss: 14864917.0\n",
      "Epoch [79/100], Training Loss: 155938.215538179, Test Loss: 14869728.0\n",
      "Epoch [80/100], Training Loss: 154872.32195219476, Test Loss: 14795614.0\n",
      "Epoch [81/100], Training Loss: 153849.9834503288, Test Loss: 14802895.0\n",
      "Epoch [82/100], Training Loss: 152857.86498133998, Test Loss: 14734481.0\n",
      "Epoch [83/100], Training Loss: 151892.5534624726, Test Loss: 14733332.0\n",
      "Epoch [84/100], Training Loss: 150962.71923316154, Test Loss: 14686973.0\n",
      "Epoch [85/100], Training Loss: 150070.00780463242, Test Loss: 14659733.0\n",
      "Epoch [86/100], Training Loss: 149203.46188022036, Test Loss: 14647518.0\n",
      "Epoch [87/100], Training Loss: 148374.72832622475, Test Loss: 14637194.0\n",
      "Epoch [88/100], Training Loss: 147563.42339760676, Test Loss: 14562473.0\n",
      "Epoch [89/100], Training Loss: 146789.59398880397, Test Loss: 14612879.0\n",
      "Epoch [90/100], Training Loss: 146034.12227504296, Test Loss: 14551351.0\n",
      "Epoch [91/100], Training Loss: 145303.04296250222, Test Loss: 14567001.0\n",
      "Epoch [92/100], Training Loss: 144589.91341597063, Test Loss: 14517328.0\n",
      "Epoch [93/100], Training Loss: 143892.53043362361, Test Loss: 14532339.0\n",
      "Epoch [94/100], Training Loss: 143208.92014691074, Test Loss: 14490223.0\n",
      "Epoch [95/100], Training Loss: 142558.65444878858, Test Loss: 14491335.0\n",
      "Epoch [96/100], Training Loss: 141904.16430454358, Test Loss: 14467598.0\n",
      "Epoch [97/100], Training Loss: 141273.99891890291, Test Loss: 14477871.0\n",
      "Epoch [98/100], Training Loss: 140648.08545850363, Test Loss: 14390080.0\n",
      "Epoch [99/100], Training Loss: 140041.2881642083, Test Loss: 14448437.0\n",
      "Epoch [100/100], Training Loss: 139442.52540578163, Test Loss: 14395756.0\n",
      "Epoch [1/100], Training Loss: 2319970.54961199, Test Loss: 293342272.0\n",
      "Epoch [2/100], Training Loss: 2096265.6889994668, Test Loss: 235809808.0\n",
      "Epoch [3/100], Training Loss: 1531901.545169125, Test Loss: 166947776.0\n",
      "Epoch [4/100], Training Loss: 1190482.213139032, Test Loss: 144516416.0\n",
      "Epoch [5/100], Training Loss: 1068643.561400391, Test Loss: 132674520.0\n",
      "Epoch [6/100], Training Loss: 982729.7416029857, Test Loss: 122767624.0\n",
      "Epoch [7/100], Training Loss: 907200.650198448, Test Loss: 113855376.0\n",
      "Epoch [8/100], Training Loss: 838539.7722883716, Test Loss: 105782136.0\n",
      "Epoch [9/100], Training Loss: 776017.745631183, Test Loss: 98409120.0\n",
      "Epoch [10/100], Training Loss: 718419.7238315266, Test Loss: 91772552.0\n",
      "Epoch [11/100], Training Loss: 667769.191754043, Test Loss: 86069384.0\n",
      "Epoch [12/100], Training Loss: 623961.1531307386, Test Loss: 81077584.0\n",
      "Epoch [13/100], Training Loss: 585641.2301403945, Test Loss: 76679920.0\n",
      "Epoch [14/100], Training Loss: 552102.2177596114, Test Loss: 72796432.0\n",
      "Epoch [15/100], Training Loss: 522679.3379539127, Test Loss: 69353504.0\n",
      "Epoch [16/100], Training Loss: 496808.54120016587, Test Loss: 66292344.0\n",
      "Epoch [17/100], Training Loss: 473939.30774243234, Test Loss: 63565060.0\n",
      "Epoch [18/100], Training Loss: 453597.3373615307, Test Loss: 61123456.0\n",
      "Epoch [19/100], Training Loss: 435381.7746578994, Test Loss: 58925540.0\n",
      "Epoch [20/100], Training Loss: 418959.1539600735, Test Loss: 56933960.0\n",
      "Epoch [21/100], Training Loss: 404038.1237485931, Test Loss: 55114560.0\n",
      "Epoch [22/100], Training Loss: 390331.98643445293, Test Loss: 53432988.0\n",
      "Epoch [23/100], Training Loss: 377584.05414371187, Test Loss: 51862792.0\n",
      "Epoch [24/100], Training Loss: 365667.79011906876, Test Loss: 50389792.0\n",
      "Epoch [25/100], Training Loss: 354473.77833066764, Test Loss: 49001128.0\n",
      "Epoch [26/100], Training Loss: 343925.7067116877, Test Loss: 47691380.0\n",
      "Epoch [27/100], Training Loss: 333955.65381197794, Test Loss: 46452028.0\n",
      "Epoch [28/100], Training Loss: 324513.43960665836, Test Loss: 45271812.0\n",
      "Epoch [29/100], Training Loss: 315546.0938333037, Test Loss: 44148876.0\n",
      "Epoch [30/100], Training Loss: 306985.68283869437, Test Loss: 43070300.0\n",
      "Epoch [31/100], Training Loss: 298811.2360642142, Test Loss: 42036916.0\n",
      "Epoch [32/100], Training Loss: 290988.4648421302, Test Loss: 41041756.0\n",
      "Epoch [33/100], Training Loss: 283518.8336591434, Test Loss: 40087628.0\n",
      "Epoch [34/100], Training Loss: 276421.17469344236, Test Loss: 39175780.0\n",
      "Epoch [35/100], Training Loss: 269677.03767549316, Test Loss: 38307860.0\n",
      "Epoch [36/100], Training Loss: 263248.0481014158, Test Loss: 37478400.0\n",
      "Epoch [37/100], Training Loss: 257109.25608672472, Test Loss: 36684316.0\n",
      "Epoch [38/100], Training Loss: 251259.2047272081, Test Loss: 35926624.0\n",
      "Epoch [39/100], Training Loss: 245682.06930869023, Test Loss: 35200752.0\n",
      "Epoch [40/100], Training Loss: 240357.29239973935, Test Loss: 34504760.0\n",
      "Epoch [41/100], Training Loss: 235274.53255138913, Test Loss: 33840716.0\n",
      "Epoch [42/100], Training Loss: 230428.195959955, Test Loss: 33207682.0\n",
      "Epoch [43/100], Training Loss: 225799.01332859427, Test Loss: 32603888.0\n",
      "Epoch [44/100], Training Loss: 221374.80717966944, Test Loss: 32025704.0\n",
      "Epoch [45/100], Training Loss: 217135.53255138913, Test Loss: 31470182.0\n",
      "Epoch [46/100], Training Loss: 213049.15585569575, Test Loss: 30933768.0\n",
      "Epoch [47/100], Training Loss: 209090.73639002428, Test Loss: 30414780.0\n",
      "Epoch [48/100], Training Loss: 205240.96398317636, Test Loss: 29908240.0\n",
      "Epoch [49/100], Training Loss: 201474.42666311236, Test Loss: 29416458.0\n",
      "Epoch [50/100], Training Loss: 197835.8872993306, Test Loss: 28942592.0\n",
      "Epoch [51/100], Training Loss: 194348.20425330254, Test Loss: 28489666.0\n",
      "Epoch [52/100], Training Loss: 191013.52642023578, Test Loss: 28055790.0\n",
      "Epoch [53/100], Training Loss: 187785.2117765535, Test Loss: 27633504.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [54/100], Training Loss: 184616.73511640306, Test Loss: 27216910.0\n",
      "Epoch [55/100], Training Loss: 181484.6997215805, Test Loss: 26807198.0\n",
      "Epoch [56/100], Training Loss: 178418.3413304899, Test Loss: 26405812.0\n",
      "Epoch [57/100], Training Loss: 175465.2062970203, Test Loss: 26021646.0\n",
      "Epoch [58/100], Training Loss: 172640.52467270897, Test Loss: 25652426.0\n",
      "Epoch [59/100], Training Loss: 169936.83890172382, Test Loss: 25301610.0\n",
      "Epoch [60/100], Training Loss: 167351.11578105562, Test Loss: 24966264.0\n",
      "Epoch [61/100], Training Loss: 164864.70446063622, Test Loss: 24646802.0\n",
      "Epoch [62/100], Training Loss: 162450.3409158225, Test Loss: 24335908.0\n",
      "Epoch [63/100], Training Loss: 160084.07455127066, Test Loss: 24033460.0\n",
      "Epoch [64/100], Training Loss: 157756.29210354836, Test Loss: 23735106.0\n",
      "Epoch [65/100], Training Loss: 155477.6176766779, Test Loss: 23441936.0\n",
      "Epoch [66/100], Training Loss: 153254.76440969136, Test Loss: 23158268.0\n",
      "Epoch [67/100], Training Loss: 151104.68580060423, Test Loss: 22892158.0\n",
      "Epoch [68/100], Training Loss: 149042.15724779337, Test Loss: 22636114.0\n",
      "Epoch [69/100], Training Loss: 147065.3045139506, Test Loss: 22393142.0\n",
      "Epoch [70/100], Training Loss: 145159.78330667614, Test Loss: 22157538.0\n",
      "Epoch [71/100], Training Loss: 143315.50613115338, Test Loss: 21931238.0\n",
      "Epoch [72/100], Training Loss: 141525.78777916, Test Loss: 21713808.0\n",
      "Epoch [73/100], Training Loss: 139787.80220366092, Test Loss: 21504104.0\n",
      "Epoch [74/100], Training Loss: 138101.55070789644, Test Loss: 21302474.0\n",
      "Epoch [75/100], Training Loss: 136457.66731828684, Test Loss: 21106784.0\n",
      "Epoch [76/100], Training Loss: 134851.43679284403, Test Loss: 20919120.0\n",
      "Epoch [77/100], Training Loss: 133270.4191990996, Test Loss: 20734968.0\n",
      "Epoch [78/100], Training Loss: 131705.24216574847, Test Loss: 20552888.0\n",
      "Epoch [79/100], Training Loss: 130164.53977844915, Test Loss: 20376368.0\n",
      "Epoch [80/100], Training Loss: 128655.03006338487, Test Loss: 20203944.0\n",
      "Epoch [81/100], Training Loss: 127170.65167940287, Test Loss: 20036922.0\n",
      "Epoch [82/100], Training Loss: 125728.31929388069, Test Loss: 19876176.0\n",
      "Epoch [83/100], Training Loss: 124334.95862211955, Test Loss: 19724734.0\n",
      "Epoch [84/100], Training Loss: 122993.55281085243, Test Loss: 19578630.0\n",
      "Epoch [85/100], Training Loss: 121704.52462828031, Test Loss: 19443508.0\n",
      "Epoch [86/100], Training Loss: 120460.52187370416, Test Loss: 19311856.0\n",
      "Epoch [87/100], Training Loss: 119256.16385285232, Test Loss: 19187566.0\n",
      "Epoch [88/100], Training Loss: 118087.62565902494, Test Loss: 19068620.0\n",
      "Epoch [89/100], Training Loss: 116949.18482317399, Test Loss: 18953448.0\n",
      "Epoch [90/100], Training Loss: 115840.97209880932, Test Loss: 18844866.0\n",
      "Epoch [91/100], Training Loss: 114767.71034002725, Test Loss: 18738766.0\n",
      "Epoch [92/100], Training Loss: 113721.04090397488, Test Loss: 18637672.0\n",
      "Epoch [93/100], Training Loss: 112706.7139979859, Test Loss: 18539736.0\n",
      "Epoch [94/100], Training Loss: 111715.33244476038, Test Loss: 18444886.0\n",
      "Epoch [95/100], Training Loss: 110743.43876251407, Test Loss: 18355642.0\n",
      "Epoch [96/100], Training Loss: 109793.95243172797, Test Loss: 18266790.0\n",
      "Epoch [97/100], Training Loss: 108865.32338131627, Test Loss: 18186626.0\n",
      "Epoch [98/100], Training Loss: 107955.05332918666, Test Loss: 18105210.0\n",
      "Epoch [99/100], Training Loss: 107058.87248978141, Test Loss: 18026416.0\n",
      "Epoch [100/100], Training Loss: 106177.75576091463, Test Loss: 17950762.0\n",
      "Epoch [1/100], Training Loss: 1497760.4170369054, Test Loss: 298697216.0\n",
      "Epoch [2/100], Training Loss: 1466216.475327291, Test Loss: 284373728.0\n",
      "Epoch [3/100], Training Loss: 1333601.3923345774, Test Loss: 244402544.0\n",
      "Epoch [4/100], Training Loss: 1095758.0546176175, Test Loss: 193637552.0\n",
      "Epoch [5/100], Training Loss: 881115.708311119, Test Loss: 161871008.0\n",
      "Epoch [6/100], Training Loss: 769167.7030981577, Test Loss: 147338128.0\n",
      "Epoch [7/100], Training Loss: 709864.131271844, Test Loss: 138002352.0\n",
      "Epoch [8/100], Training Loss: 665382.2527101475, Test Loss: 130309248.0\n",
      "Epoch [9/100], Training Loss: 626743.2187666608, Test Loss: 123515784.0\n",
      "Epoch [10/100], Training Loss: 591817.0591789586, Test Loss: 117393456.0\n",
      "Epoch [11/100], Training Loss: 559864.4549493514, Test Loss: 111848296.0\n",
      "Epoch [12/100], Training Loss: 530571.055980096, Test Loss: 106830896.0\n",
      "Epoch [13/100], Training Loss: 503762.8296901842, Test Loss: 102300968.0\n",
      "Epoch [14/100], Training Loss: 479290.5311296724, Test Loss: 98216280.0\n",
      "Epoch [15/100], Training Loss: 457006.1512943546, Test Loss: 94536752.0\n",
      "Epoch [16/100], Training Loss: 436757.6127006694, Test Loss: 91217264.0\n",
      "Epoch [17/100], Training Loss: 418371.6917244239, Test Loss: 88208432.0\n",
      "Epoch [18/100], Training Loss: 401657.17741839937, Test Loss: 85466160.0\n",
      "Epoch [19/100], Training Loss: 386445.9412357088, Test Loss: 82954744.0\n",
      "Epoch [20/100], Training Loss: 372591.21082874236, Test Loss: 80646072.0\n",
      "Epoch [21/100], Training Loss: 359961.2163971329, Test Loss: 78516728.0\n",
      "Epoch [22/100], Training Loss: 348437.5648362064, Test Loss: 76545952.0\n",
      "Epoch [23/100], Training Loss: 337910.4999703809, Test Loss: 74716936.0\n",
      "Epoch [24/100], Training Loss: 328278.1148036254, Test Loss: 73013904.0\n",
      "Epoch [25/100], Training Loss: 319443.4156744269, Test Loss: 71421648.0\n",
      "Epoch [26/100], Training Loss: 311307.5612819146, Test Loss: 69926448.0\n",
      "Epoch [27/100], Training Loss: 303781.47959244123, Test Loss: 68516640.0\n",
      "Epoch [28/100], Training Loss: 296783.10408151173, Test Loss: 67181480.0\n",
      "Epoch [29/100], Training Loss: 290232.16862152715, Test Loss: 65909880.0\n",
      "Epoch [30/100], Training Loss: 284064.60425922635, Test Loss: 64694792.0\n",
      "Epoch [31/100], Training Loss: 278224.80360316334, Test Loss: 63526600.0\n",
      "Epoch [32/100], Training Loss: 272658.1415293081, Test Loss: 62397920.0\n",
      "Epoch [33/100], Training Loss: 267319.6834151098, Test Loss: 61303556.0\n",
      "Epoch [34/100], Training Loss: 262169.6340717745, Test Loss: 60239044.0\n",
      "Epoch [35/100], Training Loss: 257181.2923516083, Test Loss: 59201236.0\n",
      "Epoch [36/100], Training Loss: 252331.77235501452, Test Loss: 58188692.0\n",
      "Epoch [37/100], Training Loss: 247601.63554439903, Test Loss: 57198904.0\n",
      "Epoch [38/100], Training Loss: 242970.30972691192, Test Loss: 56228160.0\n",
      "Epoch [39/100], Training Loss: 238429.09812807298, Test Loss: 55276696.0\n",
      "Epoch [40/100], Training Loss: 233974.9991854748, Test Loss: 54344808.0\n",
      "Epoch [41/100], Training Loss: 229606.28943782952, Test Loss: 53431312.0\n",
      "Epoch [42/100], Training Loss: 225318.3189680706, Test Loss: 52536032.0\n",
      "Epoch [43/100], Training Loss: 221115.94748533855, Test Loss: 51659268.0\n",
      "Epoch [44/100], Training Loss: 217000.93119483444, Test Loss: 50801280.0\n",
      "Epoch [45/100], Training Loss: 212971.0433179314, Test Loss: 49961280.0\n",
      "Epoch [46/100], Training Loss: 209022.40985427404, Test Loss: 49137704.0\n",
      "Epoch [47/100], Training Loss: 205153.3873585688, Test Loss: 48329820.0\n",
      "Epoch [48/100], Training Loss: 201363.2607517327, Test Loss: 47538540.0\n",
      "Epoch [49/100], Training Loss: 197653.6987293407, Test Loss: 46763412.0\n",
      "Epoch [50/100], Training Loss: 194025.26383211894, Test Loss: 46004052.0\n",
      "Epoch [51/100], Training Loss: 190476.61281914578, Test Loss: 45259648.0\n",
      "Epoch [52/100], Training Loss: 187005.45671909247, Test Loss: 44530076.0\n",
      "Epoch [53/100], Training Loss: 183615.64358598425, Test Loss: 43814880.0\n",
      "Epoch [54/100], Training Loss: 180306.65577424323, Test Loss: 43113780.0\n",
      "Epoch [55/100], Training Loss: 177076.27085184527, Test Loss: 42427088.0\n",
      "Epoch [56/100], Training Loss: 173922.3272762277, Test Loss: 41754324.0\n",
      "Epoch [57/100], Training Loss: 170842.66469699662, Test Loss: 41094652.0\n",
      "Epoch [58/100], Training Loss: 167837.3408639891, Test Loss: 40447692.0\n",
      "Epoch [59/100], Training Loss: 164906.92957689118, Test Loss: 39814104.0\n",
      "Epoch [60/100], Training Loss: 162050.57505479534, Test Loss: 39193704.0\n",
      "Epoch [61/100], Training Loss: 159263.4795665245, Test Loss: 38586608.0\n",
      "Epoch [62/100], Training Loss: 156545.1384952017, Test Loss: 37993340.0\n",
      "Epoch [63/100], Training Loss: 153893.70400339138, Test Loss: 37413316.0\n",
      "Epoch [64/100], Training Loss: 151308.9272610479, Test Loss: 36846392.0\n",
      "Epoch [65/100], Training Loss: 148790.1454381035, Test Loss: 36292876.0\n",
      "Epoch [66/100], Training Loss: 146334.485428792, Test Loss: 35752524.0\n",
      "Epoch [67/100], Training Loss: 143941.57042218323, Test Loss: 35224264.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [68/100], Training Loss: 141607.87909229496, Test Loss: 34707892.0\n",
      "Epoch [69/100], Training Loss: 139332.3330510263, Test Loss: 34203304.0\n",
      "Epoch [70/100], Training Loss: 137115.62646498837, Test Loss: 33711132.0\n",
      "Epoch [71/100], Training Loss: 134957.6239813807, Test Loss: 33230928.0\n",
      "Epoch [72/100], Training Loss: 132856.11511479947, Test Loss: 32762920.0\n",
      "Epoch [73/100], Training Loss: 130809.79497132935, Test Loss: 32306950.0\n",
      "Epoch [74/100], Training Loss: 128818.30147883545, Test Loss: 31863422.0\n",
      "Epoch [75/100], Training Loss: 126879.87441469017, Test Loss: 31431316.0\n",
      "Epoch [76/100], Training Loss: 124992.81776716658, Test Loss: 31011172.0\n",
      "Epoch [77/100], Training Loss: 123156.31258146698, Test Loss: 30602896.0\n",
      "Epoch [78/100], Training Loss: 121369.02890543432, Test Loss: 30205556.0\n",
      "Epoch [79/100], Training Loss: 119628.69084192518, Test Loss: 29818520.0\n",
      "Epoch [80/100], Training Loss: 117933.6630704916, Test Loss: 29440530.0\n",
      "Epoch [81/100], Training Loss: 116280.2586013399, Test Loss: 29071768.0\n",
      "Epoch [82/100], Training Loss: 114663.88424393549, Test Loss: 28711836.0\n",
      "Epoch [83/100], Training Loss: 113086.19467904467, Test Loss: 28360990.0\n",
      "Epoch [84/100], Training Loss: 111545.75725482791, Test Loss: 28019340.0\n",
      "Epoch [85/100], Training Loss: 110043.96471474494, Test Loss: 27687326.0\n",
      "Epoch [86/100], Training Loss: 108579.17010965546, Test Loss: 27364346.0\n",
      "Epoch [87/100], Training Loss: 107152.48004436385, Test Loss: 27050862.0\n",
      "Epoch [88/100], Training Loss: 105763.520287694, Test Loss: 26746920.0\n",
      "Epoch [89/100], Training Loss: 104410.79061218048, Test Loss: 26452206.0\n",
      "Epoch [90/100], Training Loss: 103093.55488372652, Test Loss: 26167096.0\n",
      "Epoch [91/100], Training Loss: 101810.16517992677, Test Loss: 25890540.0\n",
      "Epoch [92/100], Training Loss: 100558.19505425848, Test Loss: 25622332.0\n",
      "Epoch [93/100], Training Loss: 99336.38400142912, Test Loss: 25361876.0\n",
      "Epoch [94/100], Training Loss: 98143.52818164653, Test Loss: 25109010.0\n",
      "Epoch [95/100], Training Loss: 96978.66932451795, Test Loss: 24863536.0\n",
      "Epoch [96/100], Training Loss: 95840.84362573862, Test Loss: 24624932.0\n",
      "Epoch [97/100], Training Loss: 94728.77490619077, Test Loss: 24392008.0\n",
      "Epoch [98/100], Training Loss: 93641.57433930898, Test Loss: 24165716.0\n",
      "Epoch [99/100], Training Loss: 92579.82590565021, Test Loss: 23944336.0\n",
      "Epoch [100/100], Training Loss: 91543.2071469496, Test Loss: 23729956.0\n",
      "Epoch [1/100], Training Loss: 1164365.3404419168, Test Loss: 299318368.0\n",
      "Epoch [2/100], Training Loss: 1149989.6773887803, Test Loss: 290739008.0\n",
      "Epoch [3/100], Training Loss: 1084033.702268823, Test Loss: 263962208.0\n",
      "Epoch [4/100], Training Loss: 943806.1422901487, Test Loss: 220389104.0\n",
      "Epoch [5/100], Training Loss: 771609.6117528583, Test Loss: 179504272.0\n",
      "Epoch [6/100], Training Loss: 646006.8062318583, Test Loss: 156585296.0\n",
      "Epoch [7/100], Training Loss: 581880.084592145, Test Loss: 145094784.0\n",
      "Epoch [8/100], Training Loss: 544459.4028789763, Test Loss: 137103232.0\n",
      "Epoch [9/100], Training Loss: 515080.36395948107, Test Loss: 130301960.0\n",
      "Epoch [10/100], Training Loss: 489029.06486582546, Test Loss: 124126888.0\n",
      "Epoch [11/100], Training Loss: 464953.31295539363, Test Loss: 118369896.0\n",
      "Epoch [12/100], Training Loss: 442185.9029678337, Test Loss: 112863976.0\n",
      "Epoch [13/100], Training Loss: 420204.4367039867, Test Loss: 107597696.0\n",
      "Epoch [14/100], Training Loss: 399410.1681180025, Test Loss: 102704352.0\n",
      "Epoch [15/100], Training Loss: 380081.69918843673, Test Loss: 98158056.0\n",
      "Epoch [16/100], Training Loss: 361961.2222024762, Test Loss: 93927832.0\n",
      "Epoch [17/100], Training Loss: 345223.21142112435, Test Loss: 90057040.0\n",
      "Epoch [18/100], Training Loss: 329990.29630946036, Test Loss: 86563912.0\n",
      "Epoch [19/100], Training Loss: 316213.2335762099, Test Loss: 83382464.0\n",
      "Epoch [20/100], Training Loss: 303670.8607309993, Test Loss: 80465144.0\n",
      "Epoch [21/100], Training Loss: 292203.3338072389, Test Loss: 77782216.0\n",
      "Epoch [22/100], Training Loss: 281706.6047035128, Test Loss: 75308592.0\n",
      "Epoch [23/100], Training Loss: 272079.1972039571, Test Loss: 73025720.0\n",
      "Epoch [24/100], Training Loss: 263233.1357147088, Test Loss: 70909512.0\n",
      "Epoch [25/100], Training Loss: 255087.1623718974, Test Loss: 68944272.0\n",
      "Epoch [26/100], Training Loss: 247581.1069249452, Test Loss: 67123552.0\n",
      "Epoch [27/100], Training Loss: 240647.29850127362, Test Loss: 65434380.0\n",
      "Epoch [28/100], Training Loss: 234227.90640364907, Test Loss: 63859972.0\n",
      "Epoch [29/100], Training Loss: 228271.83117113914, Test Loss: 62391560.0\n",
      "Epoch [30/100], Training Loss: 222733.31947159528, Test Loss: 61017940.0\n",
      "Epoch [31/100], Training Loss: 217551.16888809906, Test Loss: 59727644.0\n",
      "Epoch [32/100], Training Loss: 212667.77797523842, Test Loss: 58508812.0\n",
      "Epoch [33/100], Training Loss: 208060.95302410994, Test Loss: 57355800.0\n",
      "Epoch [34/100], Training Loss: 203688.9094247971, Test Loss: 56262164.0\n",
      "Epoch [35/100], Training Loss: 199525.40287897637, Test Loss: 55220144.0\n",
      "Epoch [36/100], Training Loss: 195550.43800722706, Test Loss: 54217808.0\n",
      "Epoch [37/100], Training Loss: 191740.22640838812, Test Loss: 53260192.0\n",
      "Epoch [38/100], Training Loss: 188079.70996978853, Test Loss: 52343656.0\n",
      "Epoch [39/100], Training Loss: 184565.2432912742, Test Loss: 51460912.0\n",
      "Epoch [40/100], Training Loss: 181190.08198566435, Test Loss: 50611908.0\n",
      "Epoch [41/100], Training Loss: 177938.01516497837, Test Loss: 49797656.0\n",
      "Epoch [42/100], Training Loss: 174801.80131508797, Test Loss: 49016092.0\n",
      "Epoch [43/100], Training Loss: 171781.79776079615, Test Loss: 48260512.0\n",
      "Epoch [44/100], Training Loss: 168871.41602985605, Test Loss: 47532104.0\n",
      "Epoch [45/100], Training Loss: 166061.92180558023, Test Loss: 46825396.0\n",
      "Epoch [46/100], Training Loss: 163336.13494461228, Test Loss: 46140948.0\n",
      "Epoch [47/100], Training Loss: 160686.5034061963, Test Loss: 45473476.0\n",
      "Epoch [48/100], Training Loss: 158113.45228363248, Test Loss: 44823936.0\n",
      "Epoch [49/100], Training Loss: 155614.82530655767, Test Loss: 44194092.0\n",
      "Epoch [50/100], Training Loss: 153193.12078668326, Test Loss: 43583264.0\n",
      "Epoch [51/100], Training Loss: 150853.09318168354, Test Loss: 42991540.0\n",
      "Epoch [52/100], Training Loss: 148592.6530418814, Test Loss: 42418532.0\n",
      "Epoch [53/100], Training Loss: 146412.7070671169, Test Loss: 41865784.0\n",
      "Epoch [54/100], Training Loss: 144308.26325454653, Test Loss: 41328936.0\n",
      "Epoch [55/100], Training Loss: 142272.4778152953, Test Loss: 40806684.0\n",
      "Epoch [56/100], Training Loss: 140300.08731710206, Test Loss: 40301168.0\n",
      "Epoch [57/100], Training Loss: 138387.32569160595, Test Loss: 39808660.0\n",
      "Epoch [58/100], Training Loss: 136532.2598187311, Test Loss: 39331180.0\n",
      "Epoch [59/100], Training Loss: 134728.15982465493, Test Loss: 38867836.0\n",
      "Epoch [60/100], Training Loss: 132966.93477874534, Test Loss: 38410244.0\n",
      "Epoch [61/100], Training Loss: 131243.28037438542, Test Loss: 37960460.0\n",
      "Epoch [62/100], Training Loss: 129557.310052722, Test Loss: 37520648.0\n",
      "Epoch [63/100], Training Loss: 127913.12694745572, Test Loss: 37093220.0\n",
      "Epoch [64/100], Training Loss: 126317.80972691192, Test Loss: 36677880.0\n",
      "Epoch [65/100], Training Loss: 124769.67815887685, Test Loss: 36275476.0\n",
      "Epoch [66/100], Training Loss: 123271.34843907351, Test Loss: 35889096.0\n",
      "Epoch [67/100], Training Loss: 121834.17291629643, Test Loss: 35520120.0\n",
      "Epoch [68/100], Training Loss: 120455.74432794265, Test Loss: 35164948.0\n",
      "Epoch [69/100], Training Loss: 119128.83970143949, Test Loss: 34822592.0\n",
      "Epoch [70/100], Training Loss: 117842.49292103549, Test Loss: 34490640.0\n",
      "Epoch [71/100], Training Loss: 116590.26811207867, Test Loss: 34168088.0\n",
      "Epoch [72/100], Training Loss: 115370.12878383981, Test Loss: 33854088.0\n",
      "Epoch [73/100], Training Loss: 114169.14969492328, Test Loss: 33544080.0\n",
      "Epoch [74/100], Training Loss: 112981.98234701736, Test Loss: 33238708.0\n",
      "Epoch [75/100], Training Loss: 111819.52994490847, Test Loss: 32938970.0\n",
      "Epoch [76/100], Training Loss: 110689.08370357206, Test Loss: 32648168.0\n",
      "Epoch [77/100], Training Loss: 109597.03897873349, Test Loss: 32365930.0\n",
      "Epoch [78/100], Training Loss: 108540.2806705764, Test Loss: 32090666.0\n",
      "Epoch [79/100], Training Loss: 107507.30596528642, Test Loss: 31820100.0\n",
      "Epoch [80/100], Training Loss: 106494.13903204787, Test Loss: 31553904.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [81/100], Training Loss: 105495.57893489722, Test Loss: 31291558.0\n",
      "Epoch [82/100], Training Loss: 104508.00432438837, Test Loss: 31032558.0\n",
      "Epoch [83/100], Training Loss: 103533.67851430603, Test Loss: 30779534.0\n",
      "Epoch [84/100], Training Loss: 102581.89751791955, Test Loss: 30533456.0\n",
      "Epoch [85/100], Training Loss: 101658.17783306676, Test Loss: 30295074.0\n",
      "Epoch [86/100], Training Loss: 100761.8818790356, Test Loss: 30061854.0\n",
      "Epoch [87/100], Training Loss: 99886.7288075351, Test Loss: 29835834.0\n",
      "Epoch [88/100], Training Loss: 99023.46762632545, Test Loss: 29610128.0\n",
      "Epoch [89/100], Training Loss: 98164.35199336532, Test Loss: 29388460.0\n",
      "Epoch [90/100], Training Loss: 97303.92358272614, Test Loss: 29167700.0\n",
      "Epoch [91/100], Training Loss: 96453.3117113915, Test Loss: 28950590.0\n",
      "Epoch [92/100], Training Loss: 95621.86783958296, Test Loss: 28740032.0\n",
      "Epoch [93/100], Training Loss: 94811.86298205082, Test Loss: 28537150.0\n",
      "Epoch [94/100], Training Loss: 94016.3485575499, Test Loss: 28337500.0\n",
      "Epoch [95/100], Training Loss: 93222.7214027605, Test Loss: 28138182.0\n",
      "Epoch [96/100], Training Loss: 92420.64699958533, Test Loss: 27937084.0\n",
      "Epoch [97/100], Training Loss: 91612.22771162845, Test Loss: 27735064.0\n",
      "Epoch [98/100], Training Loss: 90809.55677981162, Test Loss: 27537892.0\n",
      "Epoch [99/100], Training Loss: 90027.62283040104, Test Loss: 27347742.0\n",
      "Epoch [100/100], Training Loss: 89274.1734494402, Test Loss: 27166078.0\n",
      "Epoch [1/100], Training Loss: 582417.108939044, Test Loss: 300023904.0\n",
      "Epoch [2/100], Training Loss: 581861.0328771992, Test Loss: 299370176.0\n",
      "Epoch [3/100], Training Loss: 579200.1895622297, Test Loss: 297014304.0\n",
      "Epoch [4/100], Training Loss: 571898.7472306143, Test Loss: 291549568.0\n",
      "Epoch [5/100], Training Loss: 557309.6608020852, Test Loss: 281749984.0\n",
      "Epoch [6/100], Training Loss: 533571.4415022806, Test Loss: 267024592.0\n",
      "Epoch [7/100], Training Loss: 500471.21284284105, Test Loss: 247828080.0\n",
      "Epoch [8/100], Training Loss: 460082.3752147385, Test Loss: 225859904.0\n",
      "Epoch [9/100], Training Loss: 416740.3210710266, Test Loss: 203792480.0\n",
      "Epoch [10/100], Training Loss: 375931.87891712575, Test Loss: 184406480.0\n",
      "Epoch [11/100], Training Loss: 342208.07677270303, Test Loss: 169395488.0\n",
      "Epoch [12/100], Training Loss: 317189.6091463776, Test Loss: 158715168.0\n",
      "Epoch [13/100], Training Loss: 299489.8155322552, Test Loss: 151163024.0\n",
      "Epoch [14/100], Training Loss: 286554.0837628103, Test Loss: 145460336.0\n",
      "Epoch [15/100], Training Loss: 276302.52994490846, Test Loss: 140764928.0\n",
      "Epoch [16/100], Training Loss: 267517.8546294651, Test Loss: 136633168.0\n",
      "Epoch [17/100], Training Loss: 259589.49161779517, Test Loss: 132849720.0\n",
      "Epoch [18/100], Training Loss: 252219.13535927967, Test Loss: 129306656.0\n",
      "Epoch [19/100], Training Loss: 245250.6661927611, Test Loss: 125944704.0\n",
      "Epoch [20/100], Training Loss: 238594.07333688764, Test Loss: 122728368.0\n",
      "Epoch [21/100], Training Loss: 232193.91220899235, Test Loss: 119635416.0\n",
      "Epoch [22/100], Training Loss: 226015.26307683194, Test Loss: 116651808.0\n",
      "Epoch [23/100], Training Loss: 220036.50660505894, Test Loss: 113769040.0\n",
      "Epoch [24/100], Training Loss: 214244.8134589183, Test Loss: 110982048.0\n",
      "Epoch [25/100], Training Loss: 208633.6349742314, Test Loss: 108288976.0\n",
      "Epoch [26/100], Training Loss: 203201.69136899474, Test Loss: 105689168.0\n",
      "Epoch [27/100], Training Loss: 197949.09472187667, Test Loss: 103182456.0\n",
      "Epoch [28/100], Training Loss: 192875.21521236893, Test Loss: 100766784.0\n",
      "Epoch [29/100], Training Loss: 187968.29761270067, Test Loss: 98429784.0\n",
      "Epoch [30/100], Training Loss: 183197.53308453292, Test Loss: 96156344.0\n",
      "Epoch [31/100], Training Loss: 178549.19708548073, Test Loss: 93955368.0\n",
      "Epoch [32/100], Training Loss: 174058.5941591138, Test Loss: 91837088.0\n",
      "Epoch [33/100], Training Loss: 169750.0956104496, Test Loss: 89815320.0\n",
      "Epoch [34/100], Training Loss: 165655.31662816185, Test Loss: 87898384.0\n",
      "Epoch [35/100], Training Loss: 161778.380190747, Test Loss: 86088800.0\n",
      "Epoch [36/100], Training Loss: 158109.8837746579, Test Loss: 84376128.0\n",
      "Epoch [37/100], Training Loss: 154639.36757301108, Test Loss: 82750344.0\n",
      "Epoch [38/100], Training Loss: 151347.49197322433, Test Loss: 81203632.0\n",
      "Epoch [39/100], Training Loss: 148221.05586161956, Test Loss: 79730064.0\n",
      "Epoch [40/100], Training Loss: 145246.68325336176, Test Loss: 78322712.0\n",
      "Epoch [41/100], Training Loss: 142413.18737041645, Test Loss: 76977272.0\n",
      "Epoch [42/100], Training Loss: 139711.9094840353, Test Loss: 75689144.0\n",
      "Epoch [43/100], Training Loss: 137133.85913156802, Test Loss: 74453928.0\n",
      "Epoch [44/100], Training Loss: 134670.3891949529, Test Loss: 73268352.0\n",
      "Epoch [45/100], Training Loss: 132312.76867484153, Test Loss: 72130016.0\n",
      "Epoch [46/100], Training Loss: 130055.04768674841, Test Loss: 71036472.0\n",
      "Epoch [47/100], Training Loss: 127890.91096499022, Test Loss: 69984256.0\n",
      "Epoch [48/100], Training Loss: 125814.3117113915, Test Loss: 68969248.0\n",
      "Epoch [49/100], Training Loss: 123818.4316095018, Test Loss: 67990408.0\n",
      "Epoch [50/100], Training Loss: 121899.51732717257, Test Loss: 67046264.0\n",
      "Epoch [51/100], Training Loss: 120053.97381671701, Test Loss: 66135076.0\n",
      "Epoch [52/100], Training Loss: 118277.02482080445, Test Loss: 65254276.0\n",
      "Epoch [53/100], Training Loss: 116564.3376577217, Test Loss: 64402396.0\n",
      "Epoch [54/100], Training Loss: 114912.59688407085, Test Loss: 63577512.0\n",
      "Epoch [55/100], Training Loss: 113316.01350630887, Test Loss: 62777232.0\n",
      "Epoch [56/100], Training Loss: 111772.01611278953, Test Loss: 62000960.0\n",
      "Epoch [57/100], Training Loss: 110276.79675374682, Test Loss: 61247236.0\n",
      "Epoch [58/100], Training Loss: 108827.01771222084, Test Loss: 60515048.0\n",
      "Epoch [59/100], Training Loss: 107419.42491558557, Test Loss: 59802836.0\n",
      "Epoch [60/100], Training Loss: 106049.72004028197, Test Loss: 59108592.0\n",
      "Epoch [61/100], Training Loss: 104713.30525442805, Test Loss: 58431456.0\n",
      "Epoch [62/100], Training Loss: 103409.118298679, Test Loss: 57770788.0\n",
      "Epoch [63/100], Training Loss: 102134.96807061193, Test Loss: 57125880.0\n",
      "Epoch [64/100], Training Loss: 100888.47461643268, Test Loss: 56494600.0\n",
      "Epoch [65/100], Training Loss: 99665.93507493632, Test Loss: 55875004.0\n",
      "Epoch [66/100], Training Loss: 98467.4225460577, Test Loss: 55268872.0\n",
      "Epoch [67/100], Training Loss: 97293.75570167645, Test Loss: 54676056.0\n",
      "Epoch [68/100], Training Loss: 96143.39257153012, Test Loss: 54094744.0\n",
      "Epoch [69/100], Training Loss: 95015.45050648658, Test Loss: 53524768.0\n",
      "Epoch [70/100], Training Loss: 93910.90539659973, Test Loss: 52966616.0\n",
      "Epoch [71/100], Training Loss: 92828.8245956993, Test Loss: 52418264.0\n",
      "Epoch [72/100], Training Loss: 91767.31307387003, Test Loss: 51880084.0\n",
      "Epoch [73/100], Training Loss: 90726.11681772406, Test Loss: 51352712.0\n",
      "Epoch [74/100], Training Loss: 89704.97837805818, Test Loss: 50834708.0\n",
      "Epoch [75/100], Training Loss: 88703.0847698596, Test Loss: 50325328.0\n",
      "Epoch [76/100], Training Loss: 87720.14181624311, Test Loss: 49825552.0\n",
      "Epoch [77/100], Training Loss: 86755.92867721106, Test Loss: 49335128.0\n",
      "Epoch [78/100], Training Loss: 85809.21616018009, Test Loss: 48855512.0\n",
      "Epoch [79/100], Training Loss: 84879.48296901843, Test Loss: 48384352.0\n",
      "Epoch [80/100], Training Loss: 83966.49380960844, Test Loss: 47921068.0\n",
      "Epoch [81/100], Training Loss: 83070.3282980866, Test Loss: 47465600.0\n",
      "Epoch [82/100], Training Loss: 82190.40661098275, Test Loss: 47018036.0\n",
      "Epoch [83/100], Training Loss: 81325.87299330608, Test Loss: 46577812.0\n",
      "Epoch [84/100], Training Loss: 80476.40850660506, Test Loss: 46145100.0\n",
      "Epoch [85/100], Training Loss: 79640.95160239322, Test Loss: 45719180.0\n",
      "Epoch [86/100], Training Loss: 78819.50986315976, Test Loss: 45300800.0\n",
      "Epoch [87/100], Training Loss: 78012.98809312245, Test Loss: 44891124.0\n",
      "Epoch [88/100], Training Loss: 77221.66032817961, Test Loss: 44488548.0\n",
      "Epoch [89/100], Training Loss: 76445.69397547538, Test Loss: 44093372.0\n",
      "Epoch [90/100], Training Loss: 75684.46146555299, Test Loss: 43705868.0\n",
      "Epoch [91/100], Training Loss: 74937.7583081571, Test Loss: 43325296.0\n",
      "Epoch [92/100], Training Loss: 74205.85036431492, Test Loss: 42951560.0\n",
      "Epoch [93/100], Training Loss: 73487.9592441206, Test Loss: 42584156.0\n",
      "Epoch [94/100], Training Loss: 72782.79391031337, Test Loss: 42222492.0\n",
      "Epoch [95/100], Training Loss: 72089.82299626799, Test Loss: 41866084.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [96/100], Training Loss: 71408.7250755287, Test Loss: 41515288.0\n",
      "Epoch [97/100], Training Loss: 70738.6272140276, Test Loss: 41169456.0\n",
      "Epoch [98/100], Training Loss: 70078.67128724602, Test Loss: 40828608.0\n",
      "Epoch [99/100], Training Loss: 69428.51217344945, Test Loss: 40492532.0\n",
      "Epoch [100/100], Training Loss: 68787.84538830638, Test Loss: 40160940.0\n",
      "Epoch [1/100], Training Loss: 7220604.4142527105, Test Loss: 149692064.0\n",
      "Epoch [2/100], Training Loss: 4101697.8896984775, Test Loss: 118724120.0\n",
      "Epoch [3/100], Training Loss: 3285495.250162905, Test Loss: 96810632.0\n",
      "Epoch [4/100], Training Loss: 2687201.746460518, Test Loss: 81231376.0\n",
      "Epoch [5/100], Training Loss: 2253127.0754694627, Test Loss: 69778064.0\n",
      "Epoch [6/100], Training Loss: 1942804.2006990106, Test Loss: 61585884.0\n",
      "Epoch [7/100], Training Loss: 1723883.0972395001, Test Loss: 55677892.0\n",
      "Epoch [8/100], Training Loss: 1564199.9210947219, Test Loss: 51238004.0\n",
      "Epoch [9/100], Training Loss: 1440201.4633315562, Test Loss: 47665852.0\n",
      "Epoch [10/100], Training Loss: 1337385.7668680765, Test Loss: 44625580.0\n",
      "Epoch [11/100], Training Loss: 1247946.8767549316, Test Loss: 41920048.0\n",
      "Epoch [12/100], Training Loss: 1168859.4770748178, Test Loss: 39521784.0\n",
      "Epoch [13/100], Training Loss: 1099768.793732599, Test Loss: 37396036.0\n",
      "Epoch [14/100], Training Loss: 1038561.8514602216, Test Loss: 35491480.0\n",
      "Epoch [15/100], Training Loss: 984029.8442331615, Test Loss: 33807084.0\n",
      "Epoch [16/100], Training Loss: 935756.1804395474, Test Loss: 32307074.0\n",
      "Epoch [17/100], Training Loss: 892507.5669687815, Test Loss: 30955652.0\n",
      "Epoch [18/100], Training Loss: 853530.5723446478, Test Loss: 29736892.0\n",
      "Epoch [19/100], Training Loss: 818240.7539541497, Test Loss: 28622016.0\n",
      "Epoch [20/100], Training Loss: 785564.4607250756, Test Loss: 27575484.0\n",
      "Epoch [21/100], Training Loss: 755757.8399976304, Test Loss: 26635028.0\n",
      "Epoch [22/100], Training Loss: 728720.0455541733, Test Loss: 25764990.0\n",
      "Epoch [23/100], Training Loss: 702916.3118891061, Test Loss: 24945894.0\n",
      "Epoch [24/100], Training Loss: 678684.7937770274, Test Loss: 24163670.0\n",
      "Epoch [25/100], Training Loss: 655511.7672383153, Test Loss: 23430956.0\n",
      "Epoch [26/100], Training Loss: 633479.3079349565, Test Loss: 22740352.0\n",
      "Epoch [27/100], Training Loss: 612066.576905989, Test Loss: 22068976.0\n",
      "Epoch [28/100], Training Loss: 592180.6926870446, Test Loss: 21495882.0\n",
      "Epoch [29/100], Training Loss: 574698.1117972869, Test Loss: 20993570.0\n",
      "Epoch [30/100], Training Loss: 558754.0160683609, Test Loss: 20538750.0\n",
      "Epoch [31/100], Training Loss: 543658.997867425, Test Loss: 20104760.0\n",
      "Epoch [32/100], Training Loss: 529528.2639505954, Test Loss: 19719226.0\n",
      "Epoch [33/100], Training Loss: 516328.50162164564, Test Loss: 19363816.0\n",
      "Epoch [34/100], Training Loss: 504019.1534639536, Test Loss: 19049206.0\n",
      "Epoch [35/100], Training Loss: 492512.031899769, Test Loss: 18751598.0\n",
      "Epoch [36/100], Training Loss: 481367.6390468574, Test Loss: 18474422.0\n",
      "Epoch [37/100], Training Loss: 470787.8964590368, Test Loss: 18212292.0\n",
      "Epoch [38/100], Training Loss: 460988.1955156685, Test Loss: 17988824.0\n",
      "Epoch [39/100], Training Loss: 451864.543725194, Test Loss: 17776156.0\n",
      "Epoch [40/100], Training Loss: 443382.7949099579, Test Loss: 17600588.0\n",
      "Epoch [41/100], Training Loss: 435491.0317738878, Test Loss: 17419594.0\n",
      "Epoch [42/100], Training Loss: 427865.3223742669, Test Loss: 17257192.0\n",
      "Epoch [43/100], Training Loss: 420433.56522125466, Test Loss: 17103076.0\n",
      "Epoch [44/100], Training Loss: 413655.0729666489, Test Loss: 16977068.0\n",
      "Epoch [45/100], Training Loss: 407417.323751555, Test Loss: 16864564.0\n",
      "Epoch [46/100], Training Loss: 401627.0359205616, Test Loss: 16774130.0\n",
      "Epoch [47/100], Training Loss: 396162.3000858954, Test Loss: 16686112.0\n",
      "Epoch [48/100], Training Loss: 390999.5242802559, Test Loss: 16599175.0\n",
      "Epoch [49/100], Training Loss: 385948.4971195427, Test Loss: 16508159.0\n",
      "Epoch [50/100], Training Loss: 381185.24686037557, Test Loss: 16411873.0\n",
      "Epoch [51/100], Training Loss: 376630.7760055684, Test Loss: 16360433.0\n",
      "Epoch [52/100], Training Loss: 372265.05010070495, Test Loss: 16238228.0\n",
      "Epoch [53/100], Training Loss: 368096.95605266275, Test Loss: 16195062.0\n",
      "Epoch [54/100], Training Loss: 364142.78001155145, Test Loss: 16099000.0\n",
      "Epoch [55/100], Training Loss: 360262.5352763462, Test Loss: 16042465.0\n",
      "Epoch [56/100], Training Loss: 356507.27080741664, Test Loss: 15957915.0\n",
      "Epoch [57/100], Training Loss: 352884.026049997, Test Loss: 15885237.0\n",
      "Epoch [58/100], Training Loss: 349523.99121053255, Test Loss: 15865014.0\n",
      "Epoch [59/100], Training Loss: 346300.72741543746, Test Loss: 15759510.0\n",
      "Epoch [60/100], Training Loss: 343180.84649902256, Test Loss: 15756753.0\n",
      "Epoch [61/100], Training Loss: 340117.4815176826, Test Loss: 15649950.0\n",
      "Epoch [62/100], Training Loss: 337151.35574758606, Test Loss: 15635323.0\n",
      "Epoch [63/100], Training Loss: 334289.9350749363, Test Loss: 15557447.0\n",
      "Epoch [64/100], Training Loss: 331516.8074314318, Test Loss: 15549339.0\n",
      "Epoch [65/100], Training Loss: 328823.44530093006, Test Loss: 15488541.0\n",
      "Epoch [66/100], Training Loss: 326190.0635329661, Test Loss: 15463000.0\n",
      "Epoch [67/100], Training Loss: 323588.1290281974, Test Loss: 15455657.0\n",
      "Epoch [68/100], Training Loss: 321076.29035602155, Test Loss: 15391455.0\n",
      "Epoch [69/100], Training Loss: 318530.6463183461, Test Loss: 15386165.0\n",
      "Epoch [70/100], Training Loss: 316153.04642793676, Test Loss: 15325811.0\n",
      "Epoch [71/100], Training Loss: 313770.9009167111, Test Loss: 15318029.0\n",
      "Epoch [72/100], Training Loss: 311528.249770452, Test Loss: 15272450.0\n",
      "Epoch [73/100], Training Loss: 309279.94073958887, Test Loss: 15247580.0\n",
      "Epoch [74/100], Training Loss: 307186.42320508265, Test Loss: 15235475.0\n",
      "Epoch [75/100], Training Loss: 305094.53311415203, Test Loss: 15156432.0\n",
      "Epoch [76/100], Training Loss: 303064.747734139, Test Loss: 15197141.0\n",
      "Epoch [77/100], Training Loss: 301052.3772732658, Test Loss: 15129084.0\n",
      "Epoch [78/100], Training Loss: 299128.2764276405, Test Loss: 15127287.0\n",
      "Epoch [79/100], Training Loss: 297215.9962087554, Test Loss: 15088815.0\n",
      "Epoch [80/100], Training Loss: 295385.4817768497, Test Loss: 15048964.0\n",
      "Epoch [81/100], Training Loss: 293549.75796753744, Test Loss: 15038087.0\n",
      "Epoch [82/100], Training Loss: 291823.52379894553, Test Loss: 15039973.0\n",
      "Epoch [83/100], Training Loss: 290113.088272318, Test Loss: 14940452.0\n",
      "Epoch [84/100], Training Loss: 288485.59524761565, Test Loss: 14998042.0\n",
      "Epoch [85/100], Training Loss: 286897.8062614774, Test Loss: 14927342.0\n",
      "Epoch [86/100], Training Loss: 285385.3287868017, Test Loss: 14939575.0\n",
      "Epoch [87/100], Training Loss: 283894.57941620756, Test Loss: 14906324.0\n",
      "Epoch [88/100], Training Loss: 282447.26096647116, Test Loss: 14883801.0\n",
      "Epoch [89/100], Training Loss: 281029.27677566494, Test Loss: 14852874.0\n",
      "Epoch [90/100], Training Loss: 279669.48815236066, Test Loss: 14940260.0\n",
      "Epoch [91/100], Training Loss: 278332.7208029738, Test Loss: 14830308.0\n",
      "Epoch [92/100], Training Loss: 277024.7389446715, Test Loss: 14867890.0\n",
      "Epoch [93/100], Training Loss: 275727.4977859724, Test Loss: 14801844.0\n",
      "Epoch [94/100], Training Loss: 274492.5928188496, Test Loss: 14834791.0\n",
      "Epoch [95/100], Training Loss: 273274.3076683846, Test Loss: 14793883.0\n",
      "Epoch [96/100], Training Loss: 272090.22902227356, Test Loss: 14784016.0\n",
      "Epoch [97/100], Training Loss: 270909.1093944375, Test Loss: 14780224.0\n",
      "Epoch [98/100], Training Loss: 269784.3892060601, Test Loss: 14822748.0\n",
      "Epoch [99/100], Training Loss: 268615.7170635626, Test Loss: 14715351.0\n",
      "Epoch [100/100], Training Loss: 267422.55708340736, Test Loss: 14771923.0\n",
      "Epoch [1/100], Training Loss: 4382313.887091997, Test Loss: 228873792.0\n",
      "Epoch [2/100], Training Loss: 2652690.7078964515, Test Loss: 144213296.0\n",
      "Epoch [3/100], Training Loss: 2064562.764054262, Test Loss: 124532184.0\n",
      "Epoch [4/100], Training Loss: 1786417.2495705232, Test Loss: 108834232.0\n",
      "Epoch [5/100], Training Loss: 1556284.0138617381, Test Loss: 95919432.0\n",
      "Epoch [6/100], Training Loss: 1366955.8907647652, Test Loss: 85424568.0\n",
      "Epoch [7/100], Training Loss: 1211804.0306261478, Test Loss: 76785088.0\n",
      "Epoch [8/100], Training Loss: 1084949.3373615306, Test Loss: 69677520.0\n",
      "Epoch [9/100], Training Loss: 982167.0135063089, Test Loss: 63887384.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Training Loss: 899632.5546472365, Test Loss: 59212560.0\n",
      "Epoch [11/100], Training Loss: 833003.1552633138, Test Loss: 55393112.0\n",
      "Epoch [12/100], Training Loss: 777963.538534447, Test Loss: 52163944.0\n",
      "Epoch [13/100], Training Loss: 730725.6911912801, Test Loss: 49337828.0\n",
      "Epoch [14/100], Training Loss: 688892.514720692, Test Loss: 46803156.0\n",
      "Epoch [15/100], Training Loss: 651176.4304247379, Test Loss: 44488092.0\n",
      "Epoch [16/100], Training Loss: 616716.9312244535, Test Loss: 42346568.0\n",
      "Epoch [17/100], Training Loss: 585035.223357621, Test Loss: 40357144.0\n",
      "Epoch [18/100], Training Loss: 555874.0267460458, Test Loss: 38503372.0\n",
      "Epoch [19/100], Training Loss: 529013.9585332623, Test Loss: 36777452.0\n",
      "Epoch [20/100], Training Loss: 504287.16166103905, Test Loss: 35172352.0\n",
      "Epoch [21/100], Training Loss: 481475.54736093833, Test Loss: 33687208.0\n",
      "Epoch [22/100], Training Loss: 460478.465671465, Test Loss: 32315176.0\n",
      "Epoch [23/100], Training Loss: 441175.42912149755, Test Loss: 31051728.0\n",
      "Epoch [24/100], Training Loss: 423426.46593803685, Test Loss: 29887964.0\n",
      "Epoch [25/100], Training Loss: 407095.76162549615, Test Loss: 28813662.0\n",
      "Epoch [26/100], Training Loss: 392069.95486049406, Test Loss: 27822236.0\n",
      "Epoch [27/100], Training Loss: 378210.82132278895, Test Loss: 26904434.0\n",
      "Epoch [28/100], Training Loss: 365463.3465582608, Test Loss: 26057244.0\n",
      "Epoch [29/100], Training Loss: 353695.35371127306, Test Loss: 25273220.0\n",
      "Epoch [30/100], Training Loss: 342738.69290918787, Test Loss: 24542776.0\n",
      "Epoch [31/100], Training Loss: 332488.7429950832, Test Loss: 23863054.0\n",
      "Epoch [32/100], Training Loss: 322871.29348083644, Test Loss: 23228178.0\n",
      "Epoch [33/100], Training Loss: 313832.98397606774, Test Loss: 22639322.0\n",
      "Epoch [34/100], Training Loss: 305315.100556839, Test Loss: 22090448.0\n",
      "Epoch [35/100], Training Loss: 297290.2797671939, Test Loss: 21583054.0\n",
      "Epoch [36/100], Training Loss: 289709.14914696995, Test Loss: 21112016.0\n",
      "Epoch [37/100], Training Loss: 282546.1278360287, Test Loss: 20674294.0\n",
      "Epoch [38/100], Training Loss: 275770.93383093417, Test Loss: 20265360.0\n",
      "Epoch [39/100], Training Loss: 269353.39930987504, Test Loss: 19886210.0\n",
      "Epoch [40/100], Training Loss: 263274.9585184527, Test Loss: 19532910.0\n",
      "Epoch [41/100], Training Loss: 257494.99865233104, Test Loss: 19199766.0\n",
      "Epoch [42/100], Training Loss: 252011.2125466501, Test Loss: 18892394.0\n",
      "Epoch [43/100], Training Loss: 246806.26805284046, Test Loss: 18600730.0\n",
      "Epoch [44/100], Training Loss: 241857.19055446953, Test Loss: 18331706.0\n",
      "Epoch [45/100], Training Loss: 237146.78499496475, Test Loss: 18076954.0\n",
      "Epoch [46/100], Training Loss: 232655.51611278954, Test Loss: 17844684.0\n",
      "Epoch [47/100], Training Loss: 228382.0404004502, Test Loss: 17621418.0\n",
      "Epoch [48/100], Training Loss: 224301.18834784668, Test Loss: 17416250.0\n",
      "Epoch [49/100], Training Loss: 220403.7149013684, Test Loss: 17221596.0\n",
      "Epoch [50/100], Training Loss: 216671.89928025592, Test Loss: 17041654.0\n",
      "Epoch [51/100], Training Loss: 213094.2886825425, Test Loss: 16871746.0\n",
      "Epoch [52/100], Training Loss: 209680.90441916947, Test Loss: 16719932.0\n",
      "Epoch [53/100], Training Loss: 206411.83023813754, Test Loss: 16570436.0\n",
      "Epoch [54/100], Training Loss: 203277.33770215034, Test Loss: 16436855.0\n",
      "Epoch [55/100], Training Loss: 200284.49945204667, Test Loss: 16308609.0\n",
      "Epoch [56/100], Training Loss: 197420.28813458918, Test Loss: 16194079.0\n",
      "Epoch [57/100], Training Loss: 194690.83790948405, Test Loss: 16086965.0\n",
      "Epoch [58/100], Training Loss: 192062.41933238553, Test Loss: 15979924.0\n",
      "Epoch [59/100], Training Loss: 189549.93116521533, Test Loss: 15890180.0\n",
      "Epoch [60/100], Training Loss: 187143.60280789054, Test Loss: 15803977.0\n",
      "Epoch [61/100], Training Loss: 184825.75272495704, Test Loss: 15724731.0\n",
      "Epoch [62/100], Training Loss: 182602.82118950298, Test Loss: 15655131.0\n",
      "Epoch [63/100], Training Loss: 180476.59635092708, Test Loss: 15589710.0\n",
      "Epoch [64/100], Training Loss: 178429.24161779517, Test Loss: 15527293.0\n",
      "Epoch [65/100], Training Loss: 176464.36883182277, Test Loss: 15471140.0\n",
      "Epoch [66/100], Training Loss: 174578.1189354896, Test Loss: 15416902.0\n",
      "Epoch [67/100], Training Loss: 172755.25305817192, Test Loss: 15366756.0\n",
      "Epoch [68/100], Training Loss: 171015.22477193293, Test Loss: 15320367.0\n",
      "Epoch [69/100], Training Loss: 169351.41480066348, Test Loss: 15266481.0\n",
      "Epoch [70/100], Training Loss: 167742.5655026361, Test Loss: 15223990.0\n",
      "Epoch [71/100], Training Loss: 166209.9153782359, Test Loss: 15173600.0\n",
      "Epoch [72/100], Training Loss: 164720.7935252651, Test Loss: 15128235.0\n",
      "Epoch [73/100], Training Loss: 163312.65270126177, Test Loss: 15086543.0\n",
      "Epoch [74/100], Training Loss: 161941.16641490432, Test Loss: 15040271.0\n",
      "Epoch [75/100], Training Loss: 160650.36609205615, Test Loss: 15016003.0\n",
      "Epoch [76/100], Training Loss: 159391.23075499083, Test Loss: 14953794.0\n",
      "Epoch [77/100], Training Loss: 158188.51751229193, Test Loss: 14938992.0\n",
      "Epoch [78/100], Training Loss: 157027.74332089332, Test Loss: 14890873.0\n",
      "Epoch [79/100], Training Loss: 155907.8142734435, Test Loss: 14863370.0\n",
      "Epoch [80/100], Training Loss: 154834.63783987914, Test Loss: 14827397.0\n",
      "Epoch [81/100], Training Loss: 153786.62682157455, Test Loss: 14797013.0\n",
      "Epoch [82/100], Training Loss: 152769.3040992832, Test Loss: 14770370.0\n",
      "Epoch [83/100], Training Loss: 151800.7425359872, Test Loss: 14735930.0\n",
      "Epoch [84/100], Training Loss: 150842.67249422427, Test Loss: 14716595.0\n",
      "Epoch [85/100], Training Loss: 149932.25005183343, Test Loss: 14678161.0\n",
      "Epoch [86/100], Training Loss: 149024.35300781945, Test Loss: 14679400.0\n",
      "Epoch [87/100], Training Loss: 148178.6724868195, Test Loss: 14622103.0\n",
      "Epoch [88/100], Training Loss: 147330.92282743912, Test Loss: 14633919.0\n",
      "Epoch [89/100], Training Loss: 146528.22285409633, Test Loss: 14618882.0\n",
      "Epoch [90/100], Training Loss: 145730.15978763107, Test Loss: 14573042.0\n",
      "Epoch [91/100], Training Loss: 144965.9743720751, Test Loss: 14568176.0\n",
      "Epoch [92/100], Training Loss: 144218.32781677626, Test Loss: 14559497.0\n",
      "Epoch [93/100], Training Loss: 143518.5757656537, Test Loss: 14531056.0\n",
      "Epoch [94/100], Training Loss: 142805.55009330015, Test Loss: 14531888.0\n",
      "Epoch [95/100], Training Loss: 142163.8735708785, Test Loss: 14497558.0\n",
      "Epoch [96/100], Training Loss: 141494.55487678456, Test Loss: 14501402.0\n",
      "Epoch [97/100], Training Loss: 140871.61216752563, Test Loss: 14468588.0\n",
      "Epoch [98/100], Training Loss: 140239.50782684676, Test Loss: 14475288.0\n",
      "Epoch [99/100], Training Loss: 139654.52644985486, Test Loss: 14454359.0\n",
      "Epoch [100/100], Training Loss: 139068.465079083, Test Loss: 14440744.0\n",
      "Epoch [1/100], Training Loss: 2319231.829393993, Test Loss: 293116000.0\n",
      "Epoch [2/100], Training Loss: 2096151.4388958, Test Loss: 236242016.0\n",
      "Epoch [3/100], Training Loss: 1537529.3807238908, Test Loss: 167624496.0\n",
      "Epoch [4/100], Training Loss: 1194085.758900539, Test Loss: 144850544.0\n",
      "Epoch [5/100], Training Loss: 1071193.5252650909, Test Loss: 132990864.0\n",
      "Epoch [6/100], Training Loss: 985360.8314673301, Test Loss: 123105704.0\n",
      "Epoch [7/100], Training Loss: 910039.0538475208, Test Loss: 114217888.0\n",
      "Epoch [8/100], Training Loss: 841554.7292222024, Test Loss: 106160536.0\n",
      "Epoch [9/100], Training Loss: 779442.7749540904, Test Loss: 98929240.0\n",
      "Epoch [10/100], Training Loss: 723759.1647414253, Test Loss: 92524904.0\n",
      "Epoch [11/100], Training Loss: 674364.3771103608, Test Loss: 86878624.0\n",
      "Epoch [12/100], Training Loss: 630706.6948640484, Test Loss: 81880968.0\n",
      "Epoch [13/100], Training Loss: 592115.1656892364, Test Loss: 77440984.0\n",
      "Epoch [14/100], Training Loss: 557979.7020318701, Test Loss: 73485288.0\n",
      "Epoch [15/100], Training Loss: 527737.7514365263, Test Loss: 69947792.0\n",
      "Epoch [16/100], Training Loss: 500915.0574018127, Test Loss: 66783872.0\n",
      "Epoch [17/100], Training Loss: 477079.8891060956, Test Loss: 63951748.0\n",
      "Epoch [18/100], Training Loss: 455822.6174989633, Test Loss: 61413436.0\n",
      "Epoch [19/100], Training Loss: 436775.8026183283, Test Loss: 59126992.0\n",
      "Epoch [20/100], Training Loss: 419610.84805402526, Test Loss: 57057440.0\n",
      "Epoch [21/100], Training Loss: 404006.55417333095, Test Loss: 55157932.0\n",
      "Epoch [22/100], Training Loss: 389631.73917421955, Test Loss: 53399236.0\n",
      "Epoch [23/100], Training Loss: 376266.48676026304, Test Loss: 51756788.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/100], Training Loss: 363741.16545228363, Test Loss: 50212420.0\n",
      "Epoch [25/100], Training Loss: 351937.251347669, Test Loss: 48750572.0\n",
      "Epoch [26/100], Training Loss: 340760.18132812035, Test Loss: 47360608.0\n",
      "Epoch [27/100], Training Loss: 330147.96232450684, Test Loss: 46036760.0\n",
      "Epoch [28/100], Training Loss: 320058.23410935374, Test Loss: 44771408.0\n",
      "Epoch [29/100], Training Loss: 310438.55417333095, Test Loss: 43556984.0\n",
      "Epoch [30/100], Training Loss: 301245.1864226053, Test Loss: 42390124.0\n",
      "Epoch [31/100], Training Loss: 292457.07078964513, Test Loss: 41268924.0\n",
      "Epoch [32/100], Training Loss: 284056.26799360226, Test Loss: 40189912.0\n",
      "Epoch [33/100], Training Loss: 276023.88667732954, Test Loss: 39151840.0\n",
      "Epoch [34/100], Training Loss: 268348.3468988804, Test Loss: 38153628.0\n",
      "Epoch [35/100], Training Loss: 261016.73881879036, Test Loss: 37195552.0\n",
      "Epoch [36/100], Training Loss: 254008.52674604586, Test Loss: 36276008.0\n",
      "Epoch [37/100], Training Loss: 247307.05994905514, Test Loss: 35394964.0\n",
      "Epoch [38/100], Training Loss: 240903.24258041586, Test Loss: 34552168.0\n",
      "Epoch [39/100], Training Loss: 234783.08793910313, Test Loss: 33743968.0\n",
      "Epoch [40/100], Training Loss: 228932.8800426515, Test Loss: 32968494.0\n",
      "Epoch [41/100], Training Loss: 223348.46608613234, Test Loss: 32225132.0\n",
      "Epoch [42/100], Training Loss: 218019.6882293703, Test Loss: 31513770.0\n",
      "Epoch [43/100], Training Loss: 212930.92245720042, Test Loss: 30833446.0\n",
      "Epoch [44/100], Training Loss: 208073.10076417273, Test Loss: 30184320.0\n",
      "Epoch [45/100], Training Loss: 203437.23354659084, Test Loss: 29564042.0\n",
      "Epoch [46/100], Training Loss: 199006.93451217344, Test Loss: 28971836.0\n",
      "Epoch [47/100], Training Loss: 194768.49102541318, Test Loss: 28405606.0\n",
      "Epoch [48/100], Training Loss: 190715.00050352467, Test Loss: 27862656.0\n",
      "Epoch [49/100], Training Loss: 186837.40071678217, Test Loss: 27342474.0\n",
      "Epoch [50/100], Training Loss: 183131.70899235827, Test Loss: 26844918.0\n",
      "Epoch [51/100], Training Loss: 179588.40122030684, Test Loss: 26369420.0\n",
      "Epoch [52/100], Training Loss: 176195.25401338784, Test Loss: 25915268.0\n",
      "Epoch [53/100], Training Loss: 172941.50482791304, Test Loss: 25481132.0\n",
      "Epoch [54/100], Training Loss: 169814.080682424, Test Loss: 25065616.0\n",
      "Epoch [55/100], Training Loss: 166803.7912149754, Test Loss: 24667050.0\n",
      "Epoch [56/100], Training Loss: 163901.39938392275, Test Loss: 24285440.0\n",
      "Epoch [57/100], Training Loss: 161098.43957703927, Test Loss: 23918414.0\n",
      "Epoch [58/100], Training Loss: 158389.26446892956, Test Loss: 23568558.0\n",
      "Epoch [59/100], Training Loss: 155771.8705053018, Test Loss: 23232112.0\n",
      "Epoch [60/100], Training Loss: 153246.6860079379, Test Loss: 22911258.0\n",
      "Epoch [61/100], Training Loss: 150808.61957230023, Test Loss: 22603666.0\n",
      "Epoch [62/100], Training Loss: 148448.3542444168, Test Loss: 22308598.0\n",
      "Epoch [63/100], Training Loss: 146163.22400924115, Test Loss: 22024766.0\n",
      "Epoch [64/100], Training Loss: 143954.0993720751, Test Loss: 21752446.0\n",
      "Epoch [65/100], Training Loss: 141816.7952727919, Test Loss: 21490284.0\n",
      "Epoch [66/100], Training Loss: 139747.03207748357, Test Loss: 21238754.0\n",
      "Epoch [67/100], Training Loss: 137741.5496416089, Test Loss: 20996558.0\n",
      "Epoch [68/100], Training Loss: 135792.94206504355, Test Loss: 20764916.0\n",
      "Epoch [69/100], Training Loss: 133903.16708133405, Test Loss: 20541378.0\n",
      "Epoch [70/100], Training Loss: 132067.5378087791, Test Loss: 20327362.0\n",
      "Epoch [71/100], Training Loss: 130285.29401398021, Test Loss: 20119468.0\n",
      "Epoch [72/100], Training Loss: 128552.41271251703, Test Loss: 19920560.0\n",
      "Epoch [73/100], Training Loss: 126871.68186126414, Test Loss: 19728636.0\n",
      "Epoch [74/100], Training Loss: 125240.30771281323, Test Loss: 19544768.0\n",
      "Epoch [75/100], Training Loss: 123655.19544162076, Test Loss: 19368842.0\n",
      "Epoch [76/100], Training Loss: 122113.97737100882, Test Loss: 19198554.0\n",
      "Epoch [77/100], Training Loss: 120612.47944434572, Test Loss: 19034164.0\n",
      "Epoch [78/100], Training Loss: 119149.9344973639, Test Loss: 18875686.0\n",
      "Epoch [79/100], Training Loss: 117724.78548367988, Test Loss: 18721052.0\n",
      "Epoch [80/100], Training Loss: 116337.97067709258, Test Loss: 18575252.0\n",
      "Epoch [81/100], Training Loss: 114993.60307446241, Test Loss: 18432876.0\n",
      "Epoch [82/100], Training Loss: 113685.33084532907, Test Loss: 18295144.0\n",
      "Epoch [83/100], Training Loss: 112412.92846987737, Test Loss: 18164984.0\n",
      "Epoch [84/100], Training Loss: 111172.80036727682, Test Loss: 18037630.0\n",
      "Epoch [85/100], Training Loss: 109966.0443101712, Test Loss: 17914672.0\n",
      "Epoch [86/100], Training Loss: 108790.33306676145, Test Loss: 17795558.0\n",
      "Epoch [87/100], Training Loss: 107642.60996090279, Test Loss: 17678984.0\n",
      "Epoch [88/100], Training Loss: 106524.73782655055, Test Loss: 17567146.0\n",
      "Epoch [89/100], Training Loss: 105431.92963983177, Test Loss: 17458536.0\n",
      "Epoch [90/100], Training Loss: 104368.60248208045, Test Loss: 17355722.0\n",
      "Epoch [91/100], Training Loss: 103330.39230495824, Test Loss: 17254928.0\n",
      "Epoch [92/100], Training Loss: 102316.74106984184, Test Loss: 17159122.0\n",
      "Epoch [93/100], Training Loss: 101326.86672886678, Test Loss: 17066066.0\n",
      "Epoch [94/100], Training Loss: 100368.03304010426, Test Loss: 16978242.0\n",
      "Epoch [95/100], Training Loss: 99433.31917540431, Test Loss: 16893652.0\n",
      "Epoch [96/100], Training Loss: 98520.94030270718, Test Loss: 16811598.0\n",
      "Epoch [97/100], Training Loss: 97631.54553936378, Test Loss: 16734212.0\n",
      "Epoch [98/100], Training Loss: 96766.50114033528, Test Loss: 16659275.0\n",
      "Epoch [99/100], Training Loss: 95922.78484686927, Test Loss: 16588127.0\n",
      "Epoch [100/100], Training Loss: 95102.66537823589, Test Loss: 16522363.0\n",
      "Epoch [1/100], Training Loss: 1498114.2681120788, Test Loss: 299005856.0\n",
      "Epoch [2/100], Training Loss: 1473444.5115810675, Test Loss: 287723264.0\n",
      "Epoch [3/100], Training Loss: 1366267.9073514603, Test Loss: 254556768.0\n",
      "Epoch [4/100], Training Loss: 1158116.2751021858, Test Loss: 207005280.0\n",
      "Epoch [5/100], Training Loss: 938799.8483502162, Test Loss: 170575408.0\n",
      "Epoch [6/100], Training Loss: 804163.5149576447, Test Loss: 152685584.0\n",
      "Epoch [7/100], Training Loss: 735720.9321722647, Test Loss: 142612992.0\n",
      "Epoch [8/100], Training Loss: 689888.653515787, Test Loss: 134869232.0\n",
      "Epoch [9/100], Training Loss: 651646.9946093241, Test Loss: 128147984.0\n",
      "Epoch [10/100], Training Loss: 617398.6453409158, Test Loss: 122097592.0\n",
      "Epoch [11/100], Training Loss: 586023.5215923227, Test Loss: 116584504.0\n",
      "Epoch [12/100], Training Loss: 557068.8485279308, Test Loss: 111547552.0\n",
      "Epoch [13/100], Training Loss: 530320.7307623955, Test Loss: 106950408.0\n",
      "Epoch [14/100], Training Loss: 505625.56933830935, Test Loss: 102746888.0\n",
      "Epoch [15/100], Training Loss: 482567.2040755879, Test Loss: 98805504.0\n",
      "Epoch [16/100], Training Loss: 460775.44031751674, Test Loss: 95172568.0\n",
      "Epoch [17/100], Training Loss: 440851.68532669864, Test Loss: 91900960.0\n",
      "Epoch [18/100], Training Loss: 422609.4465967656, Test Loss: 88889176.0\n",
      "Epoch [19/100], Training Loss: 405691.03015224217, Test Loss: 86114408.0\n",
      "Epoch [20/100], Training Loss: 390442.85137136426, Test Loss: 83630096.0\n",
      "Epoch [21/100], Training Loss: 376763.63248622714, Test Loss: 81363336.0\n",
      "Epoch [22/100], Training Loss: 364348.49558675435, Test Loss: 79281920.0\n",
      "Epoch [23/100], Training Loss: 353042.6358628043, Test Loss: 77360832.0\n",
      "Epoch [24/100], Training Loss: 342730.12131982704, Test Loss: 75585912.0\n",
      "Epoch [25/100], Training Loss: 333313.47183223744, Test Loss: 73940608.0\n",
      "Epoch [26/100], Training Loss: 324700.9242343463, Test Loss: 72408688.0\n",
      "Epoch [27/100], Training Loss: 316804.96807061194, Test Loss: 70978792.0\n",
      "Epoch [28/100], Training Loss: 309532.74201765296, Test Loss: 69632504.0\n",
      "Epoch [29/100], Training Loss: 302804.2748059949, Test Loss: 68362968.0\n",
      "Epoch [30/100], Training Loss: 296548.81890883244, Test Loss: 67162200.0\n",
      "Epoch [31/100], Training Loss: 290706.3543628932, Test Loss: 66018508.0\n",
      "Epoch [32/100], Training Loss: 285203.3307268527, Test Loss: 64922964.0\n",
      "Epoch [33/100], Training Loss: 279991.1717019134, Test Loss: 63869872.0\n",
      "Epoch [34/100], Training Loss: 275022.90454875305, Test Loss: 62854536.0\n",
      "Epoch [35/100], Training Loss: 270257.7877254754, Test Loss: 61869312.0\n",
      "Epoch [36/100], Training Loss: 265666.94627055054, Test Loss: 60911612.0\n",
      "Epoch [37/100], Training Loss: 261222.83877214027, Test Loss: 59977972.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/100], Training Loss: 256894.35129176293, Test Loss: 59066156.0\n",
      "Epoch [39/100], Training Loss: 252665.1776812689, Test Loss: 58172340.0\n",
      "Epoch [40/100], Training Loss: 248521.37764720691, Test Loss: 57295044.0\n",
      "Epoch [41/100], Training Loss: 244445.83620638587, Test Loss: 56434260.0\n",
      "Epoch [42/100], Training Loss: 240440.84974971862, Test Loss: 55588880.0\n",
      "Epoch [43/100], Training Loss: 236508.94721876667, Test Loss: 54758036.0\n",
      "Epoch [44/100], Training Loss: 232642.60528849, Test Loss: 53943808.0\n",
      "Epoch [45/100], Training Loss: 228844.80706119305, Test Loss: 53149712.0\n",
      "Epoch [46/100], Training Loss: 225117.46650079972, Test Loss: 52369756.0\n",
      "Epoch [47/100], Training Loss: 221445.9906403649, Test Loss: 51605300.0\n",
      "Epoch [48/100], Training Loss: 217857.72366862153, Test Loss: 50859524.0\n",
      "Epoch [49/100], Training Loss: 214364.37171968486, Test Loss: 50128596.0\n",
      "Epoch [50/100], Training Loss: 210953.14254191102, Test Loss: 49419204.0\n",
      "Epoch [51/100], Training Loss: 207618.56750192525, Test Loss: 48723848.0\n",
      "Epoch [52/100], Training Loss: 204360.40813636634, Test Loss: 48046112.0\n",
      "Epoch [53/100], Training Loss: 201177.68714086842, Test Loss: 47384736.0\n",
      "Epoch [54/100], Training Loss: 198069.61017564125, Test Loss: 46738312.0\n",
      "Epoch [55/100], Training Loss: 195037.5366758486, Test Loss: 46108692.0\n",
      "Epoch [56/100], Training Loss: 192085.56085984243, Test Loss: 45495136.0\n",
      "Epoch [57/100], Training Loss: 189212.9569449381, Test Loss: 44895016.0\n",
      "Epoch [58/100], Training Loss: 186409.59652864168, Test Loss: 44308232.0\n",
      "Epoch [59/100], Training Loss: 183679.28742002844, Test Loss: 43736308.0\n",
      "Epoch [60/100], Training Loss: 181023.07749096618, Test Loss: 43178332.0\n",
      "Epoch [61/100], Training Loss: 178435.21032892007, Test Loss: 42632092.0\n",
      "Epoch [62/100], Training Loss: 175909.9471780404, Test Loss: 42097248.0\n",
      "Epoch [63/100], Training Loss: 173447.0356262218, Test Loss: 41574532.0\n",
      "Epoch [64/100], Training Loss: 171052.74091249038, Test Loss: 41063916.0\n",
      "Epoch [65/100], Training Loss: 168718.529675097, Test Loss: 40564164.0\n",
      "Epoch [66/100], Training Loss: 166442.106755561, Test Loss: 40074892.0\n",
      "Epoch [67/100], Training Loss: 164228.33644125608, Test Loss: 39597840.0\n",
      "Epoch [68/100], Training Loss: 162075.0028823085, Test Loss: 39132996.0\n",
      "Epoch [69/100], Training Loss: 159984.02933991526, Test Loss: 38680068.0\n",
      "Epoch [70/100], Training Loss: 157951.1033228116, Test Loss: 38239488.0\n",
      "Epoch [71/100], Training Loss: 155976.32642962478, Test Loss: 37811868.0\n",
      "Epoch [72/100], Training Loss: 154054.70405607086, Test Loss: 37395176.0\n",
      "Epoch [73/100], Training Loss: 152187.89700926884, Test Loss: 36992268.0\n",
      "Epoch [74/100], Training Loss: 150376.03790084706, Test Loss: 36600480.0\n",
      "Epoch [75/100], Training Loss: 148610.92218507495, Test Loss: 36217484.0\n",
      "Epoch [76/100], Training Loss: 146891.3475940036, Test Loss: 35843956.0\n",
      "Epoch [77/100], Training Loss: 145216.40247599463, Test Loss: 35479816.0\n",
      "Epoch [78/100], Training Loss: 143577.11868349588, Test Loss: 35123128.0\n",
      "Epoch [79/100], Training Loss: 141972.62093165485, Test Loss: 34773236.0\n",
      "Epoch [80/100], Training Loss: 140398.17570997906, Test Loss: 34429648.0\n",
      "Epoch [81/100], Training Loss: 138852.7679838835, Test Loss: 34091080.0\n",
      "Epoch [82/100], Training Loss: 137327.9135677685, Test Loss: 33758084.0\n",
      "Epoch [83/100], Training Loss: 135833.62094102651, Test Loss: 33430230.0\n",
      "Epoch [84/100], Training Loss: 134373.41331369217, Test Loss: 33113212.0\n",
      "Epoch [85/100], Training Loss: 132943.34957038422, Test Loss: 32799782.0\n",
      "Epoch [86/100], Training Loss: 131545.25610084008, Test Loss: 32495974.0\n",
      "Epoch [87/100], Training Loss: 130173.99598707496, Test Loss: 32196634.0\n",
      "Epoch [88/100], Training Loss: 128828.30097400554, Test Loss: 31903556.0\n",
      "Epoch [89/100], Training Loss: 127511.23313423745, Test Loss: 31615166.0\n",
      "Epoch [90/100], Training Loss: 126220.96036316716, Test Loss: 31333738.0\n",
      "Epoch [91/100], Training Loss: 124964.09219638573, Test Loss: 31059198.0\n",
      "Epoch [92/100], Training Loss: 123737.70382752799, Test Loss: 30792276.0\n",
      "Epoch [93/100], Training Loss: 122543.54584342234, Test Loss: 30533290.0\n",
      "Epoch [94/100], Training Loss: 121374.38627886011, Test Loss: 30280356.0\n",
      "Epoch [95/100], Training Loss: 120223.39757900895, Test Loss: 30030972.0\n",
      "Epoch [96/100], Training Loss: 119091.59942761093, Test Loss: 29787434.0\n",
      "Epoch [97/100], Training Loss: 117971.98298521637, Test Loss: 29543852.0\n",
      "Epoch [98/100], Training Loss: 116864.88063503346, Test Loss: 29303430.0\n",
      "Epoch [99/100], Training Loss: 115765.54600586458, Test Loss: 29064866.0\n",
      "Epoch [100/100], Training Loss: 114683.02837926145, Test Loss: 28834410.0\n",
      "Epoch [1/100], Training Loss: 1164337.950595344, Test Loss: 299276864.0\n",
      "Epoch [2/100], Training Loss: 1149196.9688999467, Test Loss: 290253312.0\n",
      "Epoch [3/100], Training Loss: 1080066.404596884, Test Loss: 262278832.0\n",
      "Epoch [4/100], Training Loss: 934713.5022806706, Test Loss: 217522880.0\n",
      "Epoch [5/100], Training Loss: 760575.8412416326, Test Loss: 176988272.0\n",
      "Epoch [6/100], Training Loss: 638434.7623955926, Test Loss: 155167024.0\n",
      "Epoch [7/100], Training Loss: 577355.0512410402, Test Loss: 144125568.0\n",
      "Epoch [8/100], Training Loss: 540817.9681298501, Test Loss: 136226128.0\n",
      "Epoch [9/100], Training Loss: 511576.9113204194, Test Loss: 129431216.0\n",
      "Epoch [10/100], Training Loss: 485480.10781351815, Test Loss: 123239048.0\n",
      "Epoch [11/100], Training Loss: 461318.87068301643, Test Loss: 117469608.0\n",
      "Epoch [12/100], Training Loss: 438627.84574373555, Test Loss: 112052312.0\n",
      "Epoch [13/100], Training Loss: 417237.2328653516, Test Loss: 106961312.0\n",
      "Epoch [14/100], Training Loss: 396918.22024761565, Test Loss: 102061440.0\n",
      "Epoch [15/100], Training Loss: 377010.1183579172, Test Loss: 97295848.0\n",
      "Epoch [16/100], Training Loss: 357994.68751851196, Test Loss: 92880928.0\n",
      "Epoch [17/100], Training Loss: 340679.3469581186, Test Loss: 88930912.0\n",
      "Epoch [18/100], Training Loss: 325209.752265861, Test Loss: 85403608.0\n",
      "Epoch [19/100], Training Loss: 311370.25377643504, Test Loss: 82215792.0\n",
      "Epoch [20/100], Training Loss: 298858.6666666667, Test Loss: 79302560.0\n",
      "Epoch [21/100], Training Loss: 287471.12682897935, Test Loss: 76634072.0\n",
      "Epoch [22/100], Training Loss: 277083.0382086369, Test Loss: 74181184.0\n",
      "Epoch [23/100], Training Loss: 267577.2184112316, Test Loss: 71918440.0\n",
      "Epoch [24/100], Training Loss: 258863.91647414252, Test Loss: 69825264.0\n",
      "Epoch [25/100], Training Loss: 250868.28860849477, Test Loss: 67890432.0\n",
      "Epoch [26/100], Training Loss: 243517.3558438481, Test Loss: 66107272.0\n",
      "Epoch [27/100], Training Loss: 236748.6325454653, Test Loss: 64453584.0\n",
      "Epoch [28/100], Training Loss: 230502.10935371128, Test Loss: 62917792.0\n",
      "Epoch [29/100], Training Loss: 224721.79622060305, Test Loss: 61486584.0\n",
      "Epoch [30/100], Training Loss: 219331.81671701913, Test Loss: 60144412.0\n",
      "Epoch [31/100], Training Loss: 214273.3006338487, Test Loss: 58883216.0\n",
      "Epoch [32/100], Training Loss: 209516.30744624135, Test Loss: 57691632.0\n",
      "Epoch [33/100], Training Loss: 205011.52680528405, Test Loss: 56564864.0\n",
      "Epoch [34/100], Training Loss: 200731.90510040874, Test Loss: 55493992.0\n",
      "Epoch [35/100], Training Loss: 196661.17102067414, Test Loss: 54471224.0\n",
      "Epoch [36/100], Training Loss: 192775.16095018067, Test Loss: 53499408.0\n",
      "Epoch [37/100], Training Loss: 189063.40477459866, Test Loss: 52569592.0\n",
      "Epoch [38/100], Training Loss: 185513.55215923226, Test Loss: 51678848.0\n",
      "Epoch [39/100], Training Loss: 182108.47591967302, Test Loss: 50827432.0\n",
      "Epoch [40/100], Training Loss: 178842.84130087082, Test Loss: 50015764.0\n",
      "Epoch [41/100], Training Loss: 175713.6590249393, Test Loss: 49234276.0\n",
      "Epoch [42/100], Training Loss: 172711.32717256088, Test Loss: 48483692.0\n",
      "Epoch [43/100], Training Loss: 169831.1569219833, Test Loss: 47762440.0\n",
      "Epoch [44/100], Training Loss: 167064.96949232864, Test Loss: 47064736.0\n",
      "Epoch [45/100], Training Loss: 164411.70925893015, Test Loss: 46394116.0\n",
      "Epoch [46/100], Training Loss: 161862.47331319234, Test Loss: 45748580.0\n",
      "Epoch [47/100], Training Loss: 159411.18926603874, Test Loss: 45128228.0\n",
      "Epoch [48/100], Training Loss: 157052.3509270778, Test Loss: 44529916.0\n",
      "Epoch [49/100], Training Loss: 154788.51940050945, Test Loss: 43954480.0\n",
      "Epoch [50/100], Training Loss: 152609.7859131568, Test Loss: 43400588.0\n",
      "Epoch [51/100], Training Loss: 150511.30430661692, Test Loss: 42867904.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [52/100], Training Loss: 148489.28179610212, Test Loss: 42352916.0\n",
      "Epoch [53/100], Training Loss: 146533.69053965996, Test Loss: 41853376.0\n",
      "Epoch [54/100], Training Loss: 144637.02719033233, Test Loss: 41368024.0\n",
      "Epoch [55/100], Training Loss: 142789.317575973, Test Loss: 40896324.0\n",
      "Epoch [56/100], Training Loss: 140986.85089745867, Test Loss: 40434184.0\n",
      "Epoch [57/100], Training Loss: 139232.4060186008, Test Loss: 39982992.0\n",
      "Epoch [58/100], Training Loss: 137538.12487411883, Test Loss: 39547540.0\n",
      "Epoch [59/100], Training Loss: 135910.26106273325, Test Loss: 39129900.0\n",
      "Epoch [60/100], Training Loss: 134351.2784787631, Test Loss: 38728784.0\n",
      "Epoch [61/100], Training Loss: 132857.43516379362, Test Loss: 38346208.0\n",
      "Epoch [62/100], Training Loss: 131424.18423079202, Test Loss: 37979452.0\n",
      "Epoch [63/100], Training Loss: 130049.28558734672, Test Loss: 37625264.0\n",
      "Epoch [64/100], Training Loss: 128724.35128250696, Test Loss: 37281752.0\n",
      "Epoch [65/100], Training Loss: 127439.35483679877, Test Loss: 36950832.0\n",
      "Epoch [66/100], Training Loss: 126196.47686748415, Test Loss: 36631392.0\n",
      "Epoch [67/100], Training Loss: 124994.45175048872, Test Loss: 36321124.0\n",
      "Epoch [68/100], Training Loss: 123833.41721461999, Test Loss: 36019744.0\n",
      "Epoch [69/100], Training Loss: 122710.12049049226, Test Loss: 35727848.0\n",
      "Epoch [70/100], Training Loss: 121615.9831763521, Test Loss: 35442400.0\n",
      "Epoch [71/100], Training Loss: 120550.79633907943, Test Loss: 35164400.0\n",
      "Epoch [72/100], Training Loss: 119517.48119187252, Test Loss: 34893440.0\n",
      "Epoch [73/100], Training Loss: 118512.66275694568, Test Loss: 34628388.0\n",
      "Epoch [74/100], Training Loss: 117532.74160298561, Test Loss: 34368568.0\n",
      "Epoch [75/100], Training Loss: 116578.88525561283, Test Loss: 34115100.0\n",
      "Epoch [76/100], Training Loss: 115649.4832652094, Test Loss: 33868736.0\n",
      "Epoch [77/100], Training Loss: 114743.90693679285, Test Loss: 33630004.0\n",
      "Epoch [78/100], Training Loss: 113862.59528463954, Test Loss: 33398930.0\n",
      "Epoch [79/100], Training Loss: 113003.84799478704, Test Loss: 33174708.0\n",
      "Epoch [80/100], Training Loss: 112162.38090160536, Test Loss: 32955196.0\n",
      "Epoch [81/100], Training Loss: 111335.00811563295, Test Loss: 32738772.0\n",
      "Epoch [82/100], Training Loss: 110521.24755642438, Test Loss: 32529454.0\n",
      "Epoch [83/100], Training Loss: 109718.85628813459, Test Loss: 32319966.0\n",
      "Epoch [84/100], Training Loss: 108916.6622830401, Test Loss: 32113338.0\n",
      "Epoch [85/100], Training Loss: 108104.16989514838, Test Loss: 31902016.0\n",
      "Epoch [86/100], Training Loss: 107299.25472424619, Test Loss: 31700848.0\n",
      "Epoch [87/100], Training Loss: 106526.39950239914, Test Loss: 31508182.0\n",
      "Epoch [88/100], Training Loss: 105783.49043303121, Test Loss: 31323644.0\n",
      "Epoch [89/100], Training Loss: 105061.0331733902, Test Loss: 31143718.0\n",
      "Epoch [90/100], Training Loss: 104355.06842011728, Test Loss: 30968360.0\n",
      "Epoch [91/100], Training Loss: 103661.82151531307, Test Loss: 30795124.0\n",
      "Epoch [92/100], Training Loss: 102978.71222084, Test Loss: 30624930.0\n",
      "Epoch [93/100], Training Loss: 102302.95059534388, Test Loss: 30454498.0\n",
      "Epoch [94/100], Training Loss: 101630.59528463954, Test Loss: 30285836.0\n",
      "Epoch [95/100], Training Loss: 100947.71707837214, Test Loss: 30110030.0\n",
      "Epoch [96/100], Training Loss: 100229.70594159114, Test Loss: 29927054.0\n",
      "Epoch [97/100], Training Loss: 99483.54949351342, Test Loss: 29740542.0\n",
      "Epoch [98/100], Training Loss: 98751.50701972633, Test Loss: 29565264.0\n",
      "Epoch [99/100], Training Loss: 98055.45459392216, Test Loss: 29396636.0\n",
      "Epoch [100/100], Training Loss: 97376.00598305788, Test Loss: 29229084.0\n",
      "Epoch [1/100], Training Loss: 582429.9915881761, Test Loss: 300040544.0\n",
      "Epoch [2/100], Training Loss: 581956.4452342871, Test Loss: 299470272.0\n",
      "Epoch [3/100], Training Loss: 579566.533736153, Test Loss: 297319104.0\n",
      "Epoch [4/100], Training Loss: 572808.3909720988, Test Loss: 292214720.0\n",
      "Epoch [5/100], Training Loss: 559080.5056572478, Test Loss: 282939424.0\n",
      "Epoch [6/100], Training Loss: 536491.2476749008, Test Loss: 268857312.0\n",
      "Epoch [7/100], Training Loss: 504670.7299330608, Test Loss: 250304256.0\n",
      "Epoch [8/100], Training Loss: 465395.214027605, Test Loss: 228799328.0\n",
      "Epoch [9/100], Training Loss: 422640.9895148392, Test Loss: 206843104.0\n",
      "Epoch [10/100], Training Loss: 381657.5103370653, Test Loss: 187165744.0\n",
      "Epoch [11/100], Training Loss: 347091.3514602216, Test Loss: 171613184.0\n",
      "Epoch [12/100], Training Loss: 320995.940998756, Test Loss: 160399680.0\n",
      "Epoch [13/100], Training Loss: 302418.3704756827, Test Loss: 152481216.0\n",
      "Epoch [14/100], Training Loss: 288949.6418458622, Test Loss: 146580816.0\n",
      "Epoch [15/100], Training Loss: 278436.3859960903, Test Loss: 141796432.0\n",
      "Epoch [16/100], Training Loss: 269546.57472898526, Test Loss: 137632448.0\n",
      "Epoch [17/100], Training Loss: 261590.85504413245, Test Loss: 133843936.0\n",
      "Epoch [18/100], Training Loss: 254230.32332207807, Test Loss: 130308888.0\n",
      "Epoch [19/100], Training Loss: 247289.88472246905, Test Loss: 126961256.0\n",
      "Epoch [20/100], Training Loss: 240670.19252413957, Test Loss: 123762088.0\n",
      "Epoch [21/100], Training Loss: 234310.41052070374, Test Loss: 120686920.0\n",
      "Epoch [22/100], Training Loss: 228171.84763935785, Test Loss: 117720048.0\n",
      "Epoch [23/100], Training Loss: 222229.24044784077, Test Loss: 114850944.0\n",
      "Epoch [24/100], Training Loss: 216463.64077957466, Test Loss: 112069512.0\n",
      "Epoch [25/100], Training Loss: 210854.20579349564, Test Loss: 109360944.0\n",
      "Epoch [26/100], Training Loss: 205349.3669806291, Test Loss: 106688656.0\n",
      "Epoch [27/100], Training Loss: 199891.07683194123, Test Loss: 104046224.0\n",
      "Epoch [28/100], Training Loss: 194479.46969966235, Test Loss: 101443104.0\n",
      "Epoch [29/100], Training Loss: 189162.284461821, Test Loss: 98913328.0\n",
      "Epoch [30/100], Training Loss: 184015.18014335644, Test Loss: 96481152.0\n",
      "Epoch [31/100], Training Loss: 179094.4387180854, Test Loss: 94175720.0\n",
      "Epoch [32/100], Training Loss: 174443.88341922872, Test Loss: 92008688.0\n",
      "Epoch [33/100], Training Loss: 170067.66874000354, Test Loss: 89973424.0\n",
      "Epoch [34/100], Training Loss: 165954.27332503998, Test Loss: 88058520.0\n",
      "Epoch [35/100], Training Loss: 162079.5606895326, Test Loss: 86249960.0\n",
      "Epoch [36/100], Training Loss: 158418.1901546117, Test Loss: 84534872.0\n",
      "Epoch [37/100], Training Loss: 154948.4833836858, Test Loss: 82905016.0\n",
      "Epoch [38/100], Training Loss: 151656.11207866832, Test Loss: 81354560.0\n",
      "Epoch [39/100], Training Loss: 148526.6737752503, Test Loss: 79876288.0\n",
      "Epoch [40/100], Training Loss: 145549.55725371718, Test Loss: 78465736.0\n",
      "Epoch [41/100], Training Loss: 142715.1645044725, Test Loss: 77117520.0\n",
      "Epoch [42/100], Training Loss: 140011.84408506606, Test Loss: 75825760.0\n",
      "Epoch [43/100], Training Loss: 137426.42687044607, Test Loss: 74583968.0\n",
      "Epoch [44/100], Training Loss: 134942.5443990285, Test Loss: 73387232.0\n",
      "Epoch [45/100], Training Loss: 132549.5423256916, Test Loss: 72233944.0\n",
      "Epoch [46/100], Training Loss: 130248.8174871157, Test Loss: 71118088.0\n",
      "Epoch [47/100], Training Loss: 128033.03429891594, Test Loss: 70037224.0\n",
      "Epoch [48/100], Training Loss: 125892.54688703276, Test Loss: 68988152.0\n",
      "Epoch [49/100], Training Loss: 123843.44695219478, Test Loss: 67986816.0\n",
      "Epoch [50/100], Training Loss: 121894.95314258634, Test Loss: 67027500.0\n",
      "Epoch [51/100], Training Loss: 120031.94526390616, Test Loss: 66105740.0\n",
      "Epoch [52/100], Training Loss: 118239.52680528404, Test Loss: 65213684.0\n",
      "Epoch [53/100], Training Loss: 116516.36703986731, Test Loss: 64356116.0\n",
      "Epoch [54/100], Training Loss: 114862.44819619691, Test Loss: 63527616.0\n",
      "Epoch [55/100], Training Loss: 113270.38611456667, Test Loss: 62727352.0\n",
      "Epoch [56/100], Training Loss: 111736.26917836622, Test Loss: 61954024.0\n",
      "Epoch [57/100], Training Loss: 110254.36419643386, Test Loss: 61205816.0\n",
      "Epoch [58/100], Training Loss: 108820.94603400273, Test Loss: 60480520.0\n",
      "Epoch [59/100], Training Loss: 107429.44872934069, Test Loss: 59776520.0\n",
      "Epoch [60/100], Training Loss: 106076.8411823944, Test Loss: 59092848.0\n",
      "Epoch [61/100], Training Loss: 104760.12866536343, Test Loss: 58429212.0\n",
      "Epoch [62/100], Training Loss: 103476.97932586932, Test Loss: 57783220.0\n",
      "Epoch [63/100], Training Loss: 102227.62099401694, Test Loss: 57154008.0\n",
      "Epoch [64/100], Training Loss: 101007.92678158877, Test Loss: 56540024.0\n",
      "Epoch [65/100], Training Loss: 99813.95249096617, Test Loss: 55936336.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [66/100], Training Loss: 98646.61110123807, Test Loss: 55347172.0\n",
      "Epoch [67/100], Training Loss: 97509.66565961733, Test Loss: 54771544.0\n",
      "Epoch [68/100], Training Loss: 96399.90000592382, Test Loss: 54209576.0\n",
      "Epoch [69/100], Training Loss: 95317.51199573485, Test Loss: 53662120.0\n",
      "Epoch [70/100], Training Loss: 94260.0312777679, Test Loss: 53126408.0\n",
      "Epoch [71/100], Training Loss: 93225.82619513062, Test Loss: 52603280.0\n",
      "Epoch [72/100], Training Loss: 92214.73099934838, Test Loss: 52091876.0\n",
      "Epoch [73/100], Training Loss: 91225.78449144008, Test Loss: 51590792.0\n",
      "Epoch [74/100], Training Loss: 90258.09940169421, Test Loss: 51098672.0\n",
      "Epoch [75/100], Training Loss: 89311.53758663587, Test Loss: 50617100.0\n",
      "Epoch [76/100], Training Loss: 88384.70600082933, Test Loss: 50147488.0\n",
      "Epoch [77/100], Training Loss: 87478.6215271607, Test Loss: 49688296.0\n",
      "Epoch [78/100], Training Loss: 86593.84005686866, Test Loss: 49240172.0\n",
      "Epoch [79/100], Training Loss: 85730.60813932824, Test Loss: 48801700.0\n",
      "Epoch [80/100], Training Loss: 84887.52905633552, Test Loss: 48373332.0\n",
      "Epoch [81/100], Training Loss: 84063.44790000592, Test Loss: 47955244.0\n",
      "Epoch [82/100], Training Loss: 83257.08062318583, Test Loss: 47545724.0\n",
      "Epoch [83/100], Training Loss: 82468.12807298146, Test Loss: 47144800.0\n",
      "Epoch [84/100], Training Loss: 81696.70742254605, Test Loss: 46751840.0\n",
      "Epoch [85/100], Training Loss: 80941.00693086903, Test Loss: 46365328.0\n",
      "Epoch [86/100], Training Loss: 80199.50654582074, Test Loss: 45986128.0\n",
      "Epoch [87/100], Training Loss: 79472.04004502103, Test Loss: 45617008.0\n",
      "Epoch [88/100], Training Loss: 78761.26532788342, Test Loss: 45256456.0\n",
      "Epoch [89/100], Training Loss: 78065.68544517506, Test Loss: 44903020.0\n",
      "Epoch [90/100], Training Loss: 77385.86398910017, Test Loss: 44556720.0\n",
      "Epoch [91/100], Training Loss: 76722.42331615425, Test Loss: 44217448.0\n",
      "Epoch [92/100], Training Loss: 76074.59510692494, Test Loss: 43884684.0\n",
      "Epoch [93/100], Training Loss: 75442.11361886145, Test Loss: 43559380.0\n",
      "Epoch [94/100], Training Loss: 74824.13044250933, Test Loss: 43241092.0\n",
      "Epoch [95/100], Training Loss: 74220.01871927019, Test Loss: 42930032.0\n",
      "Epoch [96/100], Training Loss: 73629.41176470589, Test Loss: 42626504.0\n",
      "Epoch [97/100], Training Loss: 73051.67063562585, Test Loss: 42329172.0\n",
      "Epoch [98/100], Training Loss: 72485.71861856525, Test Loss: 42038992.0\n",
      "Epoch [99/100], Training Loss: 71932.31040815117, Test Loss: 41755892.0\n",
      "Epoch [100/100], Training Loss: 71391.56661335229, Test Loss: 41478932.0\n",
      "Epoch [1/100], Training Loss: 7018397.48587169, Test Loss: 146135664.0\n",
      "Epoch [2/100], Training Loss: 3989116.092056158, Test Loss: 114702720.0\n",
      "Epoch [3/100], Training Loss: 3160958.0209703217, Test Loss: 93066480.0\n",
      "Epoch [4/100], Training Loss: 2566895.7620401634, Test Loss: 77496608.0\n",
      "Epoch [5/100], Training Loss: 2136408.5474201767, Test Loss: 66192424.0\n",
      "Epoch [6/100], Training Loss: 1831039.5413778804, Test Loss: 58127368.0\n",
      "Epoch [7/100], Training Loss: 1615045.8679284402, Test Loss: 52327596.0\n",
      "Epoch [8/100], Training Loss: 1454909.4218944376, Test Loss: 47817636.0\n",
      "Epoch [9/100], Training Loss: 1325878.6556779812, Test Loss: 44040172.0\n",
      "Epoch [10/100], Training Loss: 1216218.2317102067, Test Loss: 40738716.0\n",
      "Epoch [11/100], Training Loss: 1120817.190080564, Test Loss: 37796712.0\n",
      "Epoch [12/100], Training Loss: 1037362.2842100586, Test Loss: 35169496.0\n",
      "Epoch [13/100], Training Loss: 964377.0794828505, Test Loss: 32850304.0\n",
      "Epoch [14/100], Training Loss: 900414.0813636633, Test Loss: 30812586.0\n",
      "Epoch [15/100], Training Loss: 844346.0232658018, Test Loss: 29021782.0\n",
      "Epoch [16/100], Training Loss: 795258.7554795332, Test Loss: 27445444.0\n",
      "Epoch [17/100], Training Loss: 752371.6772554943, Test Loss: 26058654.0\n",
      "Epoch [18/100], Training Loss: 714769.4684704698, Test Loss: 24842648.0\n",
      "Epoch [19/100], Training Loss: 681413.3360434808, Test Loss: 23766604.0\n",
      "Epoch [20/100], Training Loss: 651470.5341212014, Test Loss: 22807794.0\n",
      "Epoch [21/100], Training Loss: 624305.5388010188, Test Loss: 21958872.0\n",
      "Epoch [22/100], Training Loss: 599591.2475712339, Test Loss: 21203754.0\n",
      "Epoch [23/100], Training Loss: 577033.8029663527, Test Loss: 20535226.0\n",
      "Epoch [24/100], Training Loss: 556423.0012958356, Test Loss: 19940038.0\n",
      "Epoch [25/100], Training Loss: 537511.5498711569, Test Loss: 19414024.0\n",
      "Epoch [26/100], Training Loss: 520071.39098690834, Test Loss: 18936592.0\n",
      "Epoch [27/100], Training Loss: 503925.3793317931, Test Loss: 18511826.0\n",
      "Epoch [28/100], Training Loss: 489026.91489692556, Test Loss: 18124706.0\n",
      "Epoch [29/100], Training Loss: 475215.2376044073, Test Loss: 17770806.0\n",
      "Epoch [30/100], Training Loss: 462459.1955156685, Test Loss: 17449648.0\n",
      "Epoch [31/100], Training Loss: 450532.8966959896, Test Loss: 17157360.0\n",
      "Epoch [32/100], Training Loss: 439420.7128280315, Test Loss: 16890836.0\n",
      "Epoch [33/100], Training Loss: 429063.2669717434, Test Loss: 16658045.0\n",
      "Epoch [34/100], Training Loss: 419398.4576446893, Test Loss: 16450856.0\n",
      "Epoch [35/100], Training Loss: 410405.96856673184, Test Loss: 16268194.0\n",
      "Epoch [36/100], Training Loss: 401991.25992980273, Test Loss: 16091431.0\n",
      "Epoch [37/100], Training Loss: 394152.6908210414, Test Loss: 15952002.0\n",
      "Epoch [38/100], Training Loss: 386825.23153249215, Test Loss: 15817704.0\n",
      "Epoch [39/100], Training Loss: 379980.7772791896, Test Loss: 15708910.0\n",
      "Epoch [40/100], Training Loss: 373514.72666015045, Test Loss: 15596254.0\n",
      "Epoch [41/100], Training Loss: 367473.9401175878, Test Loss: 15523283.0\n",
      "Epoch [42/100], Training Loss: 361773.62842100585, Test Loss: 15430149.0\n",
      "Epoch [43/100], Training Loss: 356412.5194079142, Test Loss: 15369855.0\n",
      "Epoch [44/100], Training Loss: 351324.67841804394, Test Loss: 15286291.0\n",
      "Epoch [45/100], Training Loss: 346532.33382204844, Test Loss: 15233196.0\n",
      "Epoch [46/100], Training Loss: 342033.88210858364, Test Loss: 15151768.0\n",
      "Epoch [47/100], Training Loss: 337775.41235708783, Test Loss: 15079162.0\n",
      "Epoch [48/100], Training Loss: 333749.14678484685, Test Loss: 15039710.0\n",
      "Epoch [49/100], Training Loss: 329920.90655174456, Test Loss: 14961622.0\n",
      "Epoch [50/100], Training Loss: 326318.12003509863, Test Loss: 14918218.0\n",
      "Epoch [51/100], Training Loss: 322909.81428455067, Test Loss: 14854262.0\n",
      "Epoch [52/100], Training Loss: 319652.84510692494, Test Loss: 14816852.0\n",
      "Epoch [53/100], Training Loss: 316588.603252177, Test Loss: 14757184.0\n",
      "Epoch [54/100], Training Loss: 313633.67691117234, Test Loss: 14718196.0\n",
      "Epoch [55/100], Training Loss: 310862.2172116581, Test Loss: 14666915.0\n",
      "Epoch [56/100], Training Loss: 308222.1454593922, Test Loss: 14641700.0\n",
      "Epoch [57/100], Training Loss: 305743.5800196967, Test Loss: 14601564.0\n",
      "Epoch [58/100], Training Loss: 303313.9373296902, Test Loss: 14545074.0\n",
      "Epoch [59/100], Training Loss: 300991.4748755998, Test Loss: 14578722.0\n",
      "Epoch [60/100], Training Loss: 298795.2505072271, Test Loss: 14471708.0\n",
      "Epoch [61/100], Training Loss: 296675.6085317813, Test Loss: 14520390.0\n",
      "Epoch [62/100], Training Loss: 294721.1975001481, Test Loss: 14437259.0\n",
      "Epoch [63/100], Training Loss: 292820.3368913275, Test Loss: 14469731.0\n",
      "Epoch [64/100], Training Loss: 290986.39183845744, Test Loss: 14403471.0\n",
      "Epoch [65/100], Training Loss: 289192.2936807654, Test Loss: 14398601.0\n",
      "Epoch [66/100], Training Loss: 287509.3987248978, Test Loss: 14426450.0\n",
      "Epoch [67/100], Training Loss: 285893.4537423731, Test Loss: 14446849.0\n",
      "Epoch [68/100], Training Loss: 284227.8285350394, Test Loss: 14444692.0\n",
      "Epoch [69/100], Training Loss: 282622.1721017712, Test Loss: 14448645.0\n",
      "Epoch [70/100], Training Loss: 280998.3149509804, Test Loss: 14457083.0\n",
      "Epoch [71/100], Training Loss: 279456.753565399, Test Loss: 14474437.0\n",
      "Epoch [72/100], Training Loss: 277967.80648362066, Test Loss: 14424614.0\n",
      "Epoch [73/100], Training Loss: 276522.998900391, Test Loss: 14531364.0\n",
      "Epoch [74/100], Training Loss: 275193.3087494817, Test Loss: 14464161.0\n",
      "Epoch [75/100], Training Loss: 273811.6847084, Test Loss: 14517973.0\n",
      "Epoch [76/100], Training Loss: 272492.7442761092, Test Loss: 14483760.0\n",
      "Epoch [77/100], Training Loss: 271209.06398095493, Test Loss: 14507407.0\n",
      "Epoch [78/100], Training Loss: 269981.94561193057, Test Loss: 14538846.0\n",
      "Epoch [79/100], Training Loss: 268780.7361160476, Test Loss: 14540717.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/100], Training Loss: 267587.1467182039, Test Loss: 14522132.0\n",
      "Epoch [81/100], Training Loss: 266377.89331570995, Test Loss: 14549947.0\n",
      "Epoch [82/100], Training Loss: 265233.293791837, Test Loss: 14540036.0\n",
      "Epoch [83/100], Training Loss: 264130.15296413127, Test Loss: 14588701.0\n",
      "Epoch [84/100], Training Loss: 263017.54355858656, Test Loss: 14485881.0\n",
      "Epoch [85/100], Training Loss: 261919.1149443161, Test Loss: 14581993.0\n",
      "Epoch [86/100], Training Loss: 260849.5293562289, Test Loss: 14542297.0\n",
      "Epoch [87/100], Training Loss: 259805.2710110479, Test Loss: 14558483.0\n",
      "Epoch [88/100], Training Loss: 258732.51931165214, Test Loss: 14567808.0\n",
      "Epoch [89/100], Training Loss: 257662.3001155145, Test Loss: 14579419.0\n",
      "Epoch [90/100], Training Loss: 256619.99281736865, Test Loss: 14532409.0\n",
      "Epoch [91/100], Training Loss: 255596.4956237782, Test Loss: 14635217.0\n",
      "Epoch [92/100], Training Loss: 254583.8732265565, Test Loss: 14547578.0\n",
      "Epoch [93/100], Training Loss: 253614.09285587346, Test Loss: 14618531.0\n",
      "Epoch [94/100], Training Loss: 252645.93340145727, Test Loss: 14585350.0\n",
      "Epoch [95/100], Training Loss: 251677.09529944908, Test Loss: 14586518.0\n",
      "Epoch [96/100], Training Loss: 250762.72089923584, Test Loss: 14617161.0\n",
      "Epoch [97/100], Training Loss: 249858.1760448137, Test Loss: 14636593.0\n",
      "Epoch [98/100], Training Loss: 249000.40354910848, Test Loss: 14603039.0\n",
      "Epoch [99/100], Training Loss: 248190.7027168118, Test Loss: 14678115.0\n",
      "Epoch [100/100], Training Loss: 247395.31582844618, Test Loss: 14599072.0\n",
      "Epoch [1/100], Training Loss: 4369480.822700077, Test Loss: 226248576.0\n",
      "Epoch [2/100], Training Loss: 2627484.321189503, Test Loss: 143524240.0\n",
      "Epoch [3/100], Training Loss: 2053425.8528523191, Test Loss: 123772000.0\n",
      "Epoch [4/100], Training Loss: 1772400.5018660033, Test Loss: 107676240.0\n",
      "Epoch [5/100], Training Loss: 1534146.3761625497, Test Loss: 94468880.0\n",
      "Epoch [6/100], Training Loss: 1343314.2054380665, Test Loss: 83999912.0\n",
      "Epoch [7/100], Training Loss: 1189744.2033647296, Test Loss: 75502864.0\n",
      "Epoch [8/100], Training Loss: 1066850.4293584502, Test Loss: 68663616.0\n",
      "Epoch [9/100], Training Loss: 969849.7494816658, Test Loss: 63220584.0\n",
      "Epoch [10/100], Training Loss: 893280.8569397548, Test Loss: 58852668.0\n",
      "Epoch [11/100], Training Loss: 831575.9880338842, Test Loss: 55268112.0\n",
      "Epoch [12/100], Training Loss: 780490.7096735976, Test Loss: 52254112.0\n",
      "Epoch [13/100], Training Loss: 736769.9339494106, Test Loss: 49629668.0\n",
      "Epoch [14/100], Training Loss: 698186.0587642912, Test Loss: 47294712.0\n",
      "Epoch [15/100], Training Loss: 663615.2928144067, Test Loss: 45194120.0\n",
      "Epoch [16/100], Training Loss: 632401.127954505, Test Loss: 43277856.0\n",
      "Epoch [17/100], Training Loss: 604066.8575321367, Test Loss: 41516964.0\n",
      "Epoch [18/100], Training Loss: 578103.0385936853, Test Loss: 39882516.0\n",
      "Epoch [19/100], Training Loss: 554091.3015816598, Test Loss: 38362532.0\n",
      "Epoch [20/100], Training Loss: 531751.6639713288, Test Loss: 36949128.0\n",
      "Epoch [21/100], Training Loss: 511117.7927255494, Test Loss: 35641260.0\n",
      "Epoch [22/100], Training Loss: 491961.0236656596, Test Loss: 34419452.0\n",
      "Epoch [23/100], Training Loss: 474008.850986316, Test Loss: 33274712.0\n",
      "Epoch [24/100], Training Loss: 457354.54712398554, Test Loss: 32214740.0\n",
      "Epoch [25/100], Training Loss: 441936.3366210533, Test Loss: 31223002.0\n",
      "Epoch [26/100], Training Loss: 427708.49138084234, Test Loss: 30312372.0\n",
      "Epoch [27/100], Training Loss: 414525.9792666311, Test Loss: 29461036.0\n",
      "Epoch [28/100], Training Loss: 402229.23659735796, Test Loss: 28667900.0\n",
      "Epoch [29/100], Training Loss: 390770.0122326876, Test Loss: 27925408.0\n",
      "Epoch [30/100], Training Loss: 380031.7005509152, Test Loss: 27222926.0\n",
      "Epoch [31/100], Training Loss: 369755.5979799775, Test Loss: 26549678.0\n",
      "Epoch [32/100], Training Loss: 359996.0917599668, Test Loss: 25925982.0\n",
      "Epoch [33/100], Training Loss: 350953.44935134175, Test Loss: 25348518.0\n",
      "Epoch [34/100], Training Loss: 342527.06708725786, Test Loss: 24807800.0\n",
      "Epoch [35/100], Training Loss: 334480.4267223506, Test Loss: 24287360.0\n",
      "Epoch [36/100], Training Loss: 326625.9540903975, Test Loss: 23776438.0\n",
      "Epoch [37/100], Training Loss: 319008.39396362775, Test Loss: 23294252.0\n",
      "Epoch [38/100], Training Loss: 311903.0074936319, Test Loss: 22858728.0\n",
      "Epoch [39/100], Training Loss: 305297.271222084, Test Loss: 22451942.0\n",
      "Epoch [40/100], Training Loss: 299017.4151264735, Test Loss: 22070458.0\n",
      "Epoch [41/100], Training Loss: 293024.44727800484, Test Loss: 21703356.0\n",
      "Epoch [42/100], Training Loss: 287194.4937503702, Test Loss: 21351726.0\n",
      "Epoch [43/100], Training Loss: 281564.3474172146, Test Loss: 21017714.0\n",
      "Epoch [44/100], Training Loss: 276160.46527160716, Test Loss: 20703264.0\n",
      "Epoch [45/100], Training Loss: 270850.6055328476, Test Loss: 20388896.0\n",
      "Epoch [46/100], Training Loss: 265688.1338635152, Test Loss: 20101768.0\n",
      "Epoch [47/100], Training Loss: 260741.59707659498, Test Loss: 19828130.0\n",
      "Epoch [48/100], Training Loss: 256044.2549315799, Test Loss: 19575080.0\n",
      "Epoch [49/100], Training Loss: 251563.74061074582, Test Loss: 19329562.0\n",
      "Epoch [50/100], Training Loss: 247182.52003732006, Test Loss: 19091162.0\n",
      "Epoch [51/100], Training Loss: 242776.3843226112, Test Loss: 18854344.0\n",
      "Epoch [52/100], Training Loss: 238569.9853681654, Test Loss: 18651594.0\n",
      "Epoch [53/100], Training Loss: 234784.38330075232, Test Loss: 18468234.0\n",
      "Epoch [54/100], Training Loss: 231290.6083910906, Test Loss: 18304512.0\n",
      "Epoch [55/100], Training Loss: 227964.43679284403, Test Loss: 18144808.0\n",
      "Epoch [56/100], Training Loss: 224676.972009952, Test Loss: 17985724.0\n",
      "Epoch [57/100], Training Loss: 221441.44865529294, Test Loss: 17829516.0\n",
      "Epoch [58/100], Training Loss: 218333.1540637403, Test Loss: 17692600.0\n",
      "Epoch [59/100], Training Loss: 215339.66780700194, Test Loss: 17545316.0\n",
      "Epoch [60/100], Training Loss: 212497.25978911202, Test Loss: 17433642.0\n",
      "Epoch [61/100], Training Loss: 209814.61102719035, Test Loss: 17312160.0\n",
      "Epoch [62/100], Training Loss: 207288.72008471063, Test Loss: 17218126.0\n",
      "Epoch [63/100], Training Loss: 204905.85390379716, Test Loss: 17116504.0\n",
      "Epoch [64/100], Training Loss: 202584.48051063326, Test Loss: 17027328.0\n",
      "Epoch [65/100], Training Loss: 200272.42777382856, Test Loss: 16919952.0\n",
      "Epoch [66/100], Training Loss: 197995.31514720691, Test Loss: 16842646.0\n",
      "Epoch [67/100], Training Loss: 195796.76904508026, Test Loss: 16734707.0\n",
      "Epoch [68/100], Training Loss: 193669.7304662046, Test Loss: 16664952.0\n",
      "Epoch [69/100], Training Loss: 191688.2547538653, Test Loss: 16574588.0\n",
      "Epoch [70/100], Training Loss: 189797.69905515076, Test Loss: 16510802.0\n",
      "Epoch [71/100], Training Loss: 187963.7073781174, Test Loss: 16419831.0\n",
      "Epoch [72/100], Training Loss: 186178.5177566495, Test Loss: 16360715.0\n",
      "Epoch [73/100], Training Loss: 184494.0322403886, Test Loss: 16274515.0\n",
      "Epoch [74/100], Training Loss: 182890.06539896925, Test Loss: 16225542.0\n",
      "Epoch [75/100], Training Loss: 181330.75170309815, Test Loss: 16137503.0\n",
      "Epoch [76/100], Training Loss: 179844.27408032698, Test Loss: 16104419.0\n",
      "Epoch [77/100], Training Loss: 178432.50747882234, Test Loss: 16000096.0\n",
      "Epoch [78/100], Training Loss: 177072.2076150702, Test Loss: 15985613.0\n",
      "Epoch [79/100], Training Loss: 175782.15101297316, Test Loss: 15901722.0\n",
      "Epoch [80/100], Training Loss: 174520.3523487945, Test Loss: 15871844.0\n",
      "Epoch [81/100], Training Loss: 173304.21263550737, Test Loss: 15801395.0\n",
      "Epoch [82/100], Training Loss: 172132.9184882412, Test Loss: 15773595.0\n",
      "Epoch [83/100], Training Loss: 171003.33583614716, Test Loss: 15711059.0\n",
      "Epoch [84/100], Training Loss: 169898.0637254902, Test Loss: 15679009.0\n",
      "Epoch [85/100], Training Loss: 168814.66010603638, Test Loss: 15629300.0\n",
      "Epoch [86/100], Training Loss: 167759.30589123868, Test Loss: 15585471.0\n",
      "Epoch [87/100], Training Loss: 166722.5436437415, Test Loss: 15545229.0\n",
      "Epoch [88/100], Training Loss: 165697.01630531368, Test Loss: 15547616.0\n",
      "Epoch [89/100], Training Loss: 164703.03207748357, Test Loss: 15452707.0\n",
      "Epoch [90/100], Training Loss: 163705.4582518808, Test Loss: 15454370.0\n",
      "Epoch [91/100], Training Loss: 162759.94434571412, Test Loss: 15382786.0\n",
      "Epoch [92/100], Training Loss: 161823.99767490078, Test Loss: 15374908.0\n",
      "Epoch [93/100], Training Loss: 160921.45228363248, Test Loss: 15317419.0\n",
      "Epoch [94/100], Training Loss: 160009.0173419821, Test Loss: 15301803.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [95/100], Training Loss: 159124.37324506842, Test Loss: 15252173.0\n",
      "Epoch [96/100], Training Loss: 158242.75260648064, Test Loss: 15226454.0\n",
      "Epoch [97/100], Training Loss: 157393.1401871927, Test Loss: 15193460.0\n",
      "Epoch [98/100], Training Loss: 156559.92195367574, Test Loss: 15158474.0\n",
      "Epoch [99/100], Training Loss: 155750.46949973342, Test Loss: 15120753.0\n",
      "Epoch [100/100], Training Loss: 154960.6896288727, Test Loss: 15145745.0\n",
      "Epoch [1/100], Training Loss: 2316714.064806587, Test Loss: 291625248.0\n",
      "Epoch [2/100], Training Loss: 2052049.0046798175, Test Loss: 225798368.0\n",
      "Epoch [3/100], Training Loss: 1457883.6675552395, Test Loss: 160568880.0\n",
      "Epoch [4/100], Training Loss: 1157734.316924353, Test Loss: 141337808.0\n",
      "Epoch [5/100], Training Loss: 1043900.6321900361, Test Loss: 129533568.0\n",
      "Epoch [6/100], Training Loss: 956716.2127836029, Test Loss: 119402600.0\n",
      "Epoch [7/100], Training Loss: 879268.003317339, Test Loss: 110292416.0\n",
      "Epoch [8/100], Training Loss: 809268.6120490492, Test Loss: 102128136.0\n",
      "Epoch [9/100], Training Loss: 746613.7515550026, Test Loss: 94916968.0\n",
      "Epoch [10/100], Training Loss: 691277.5452876014, Test Loss: 88611800.0\n",
      "Epoch [11/100], Training Loss: 642762.5673834488, Test Loss: 83088240.0\n",
      "Epoch [12/100], Training Loss: 600258.0467981754, Test Loss: 78226000.0\n",
      "Epoch [13/100], Training Loss: 563010.7276820093, Test Loss: 73935376.0\n",
      "Epoch [14/100], Training Loss: 530345.4674486108, Test Loss: 70137208.0\n",
      "Epoch [15/100], Training Loss: 501641.16166103905, Test Loss: 66765192.0\n",
      "Epoch [16/100], Training Loss: 476360.02097032167, Test Loss: 63772664.0\n",
      "Epoch [17/100], Training Loss: 453996.37722883717, Test Loss: 61105512.0\n",
      "Epoch [18/100], Training Loss: 434030.1772406848, Test Loss: 58706944.0\n",
      "Epoch [19/100], Training Loss: 416031.11711391504, Test Loss: 56531792.0\n",
      "Epoch [20/100], Training Loss: 399657.38996504946, Test Loss: 54542448.0\n",
      "Epoch [21/100], Training Loss: 384589.14317872166, Test Loss: 52704140.0\n",
      "Epoch [22/100], Training Loss: 370599.2926366921, Test Loss: 50990344.0\n",
      "Epoch [23/100], Training Loss: 357522.73390202003, Test Loss: 49383300.0\n",
      "Epoch [24/100], Training Loss: 345241.4114092767, Test Loss: 47866696.0\n",
      "Epoch [25/100], Training Loss: 333654.8538001303, Test Loss: 46428360.0\n",
      "Epoch [26/100], Training Loss: 322678.16201646824, Test Loss: 45060220.0\n",
      "Epoch [27/100], Training Loss: 312262.25869320537, Test Loss: 43752788.0\n",
      "Epoch [28/100], Training Loss: 302367.75931520644, Test Loss: 42502076.0\n",
      "Epoch [29/100], Training Loss: 292964.0125584977, Test Loss: 41304344.0\n",
      "Epoch [30/100], Training Loss: 284014.7032166341, Test Loss: 40154660.0\n",
      "Epoch [31/100], Training Loss: 275483.0923523488, Test Loss: 39050544.0\n",
      "Epoch [32/100], Training Loss: 267355.5119957348, Test Loss: 37991664.0\n",
      "Epoch [33/100], Training Loss: 259615.1014750311, Test Loss: 36977884.0\n",
      "Epoch [34/100], Training Loss: 252232.46235412595, Test Loss: 36008028.0\n",
      "Epoch [35/100], Training Loss: 245200.79391031337, Test Loss: 35081612.0\n",
      "Epoch [36/100], Training Loss: 238498.313666252, Test Loss: 34197496.0\n",
      "Epoch [37/100], Training Loss: 232100.70792607073, Test Loss: 33351606.0\n",
      "Epoch [38/100], Training Loss: 225994.79020792607, Test Loss: 32541672.0\n",
      "Epoch [39/100], Training Loss: 220176.7750725668, Test Loss: 31767588.0\n",
      "Epoch [40/100], Training Loss: 214635.54783484392, Test Loss: 31029860.0\n",
      "Epoch [41/100], Training Loss: 209371.61720277235, Test Loss: 30328406.0\n",
      "Epoch [42/100], Training Loss: 204367.46744861087, Test Loss: 29660242.0\n",
      "Epoch [43/100], Training Loss: 199603.0889165334, Test Loss: 29024306.0\n",
      "Epoch [44/100], Training Loss: 195060.38641075767, Test Loss: 28417832.0\n",
      "Epoch [45/100], Training Loss: 190732.46442746284, Test Loss: 27839868.0\n",
      "Epoch [46/100], Training Loss: 186610.6589657011, Test Loss: 27289012.0\n",
      "Epoch [47/100], Training Loss: 182679.31799064035, Test Loss: 26764608.0\n",
      "Epoch [48/100], Training Loss: 178933.96333155618, Test Loss: 26264600.0\n",
      "Epoch [49/100], Training Loss: 175356.6310052722, Test Loss: 25787808.0\n",
      "Epoch [50/100], Training Loss: 171934.19616728867, Test Loss: 25332308.0\n",
      "Epoch [51/100], Training Loss: 168648.14809549198, Test Loss: 24897232.0\n",
      "Epoch [52/100], Training Loss: 165490.82198921865, Test Loss: 24481370.0\n",
      "Epoch [53/100], Training Loss: 162451.65591493394, Test Loss: 24083928.0\n",
      "Epoch [54/100], Training Loss: 159524.3321189503, Test Loss: 23704356.0\n",
      "Epoch [55/100], Training Loss: 156703.01732717257, Test Loss: 23340048.0\n",
      "Epoch [56/100], Training Loss: 153983.72744505657, Test Loss: 22992834.0\n",
      "Epoch [57/100], Training Loss: 151362.95249096618, Test Loss: 22659164.0\n",
      "Epoch [58/100], Training Loss: 148832.1753746816, Test Loss: 22341502.0\n",
      "Epoch [59/100], Training Loss: 146388.50349505362, Test Loss: 22036098.0\n",
      "Epoch [60/100], Training Loss: 144031.8117706297, Test Loss: 21745402.0\n",
      "Epoch [61/100], Training Loss: 141753.23316154256, Test Loss: 21464594.0\n",
      "Epoch [62/100], Training Loss: 139543.74086250816, Test Loss: 21195536.0\n",
      "Epoch [63/100], Training Loss: 137405.35160831705, Test Loss: 20937542.0\n",
      "Epoch [64/100], Training Loss: 135336.39834725432, Test Loss: 20691454.0\n",
      "Epoch [65/100], Training Loss: 133334.23295420888, Test Loss: 20453106.0\n",
      "Epoch [66/100], Training Loss: 131399.7337687341, Test Loss: 20227356.0\n",
      "Epoch [67/100], Training Loss: 129526.27507256679, Test Loss: 20007794.0\n",
      "Epoch [68/100], Training Loss: 127711.95182453646, Test Loss: 19799834.0\n",
      "Epoch [69/100], Training Loss: 125951.82132278894, Test Loss: 19598108.0\n",
      "Epoch [70/100], Training Loss: 124247.45403115929, Test Loss: 19407506.0\n",
      "Epoch [71/100], Training Loss: 122595.48785616965, Test Loss: 19222068.0\n",
      "Epoch [72/100], Training Loss: 120995.83873881878, Test Loss: 19046810.0\n",
      "Epoch [73/100], Training Loss: 119440.44689295658, Test Loss: 18873810.0\n",
      "Epoch [74/100], Training Loss: 117930.2508737634, Test Loss: 18710974.0\n",
      "Epoch [75/100], Training Loss: 116461.45835554766, Test Loss: 18551298.0\n",
      "Epoch [76/100], Training Loss: 115029.04730170013, Test Loss: 18403278.0\n",
      "Epoch [77/100], Training Loss: 113639.83102304366, Test Loss: 18256548.0\n",
      "Epoch [78/100], Training Loss: 112288.24053669807, Test Loss: 18119508.0\n",
      "Epoch [79/100], Training Loss: 110972.64550382087, Test Loss: 17984816.0\n",
      "Epoch [80/100], Training Loss: 109692.58182275931, Test Loss: 17857154.0\n",
      "Epoch [81/100], Training Loss: 108447.28059652864, Test Loss: 17729832.0\n",
      "Epoch [82/100], Training Loss: 107234.03644630057, Test Loss: 17612564.0\n",
      "Epoch [83/100], Training Loss: 106049.78997097329, Test Loss: 17491710.0\n",
      "Epoch [84/100], Training Loss: 104897.01350630887, Test Loss: 17384002.0\n",
      "Epoch [85/100], Training Loss: 103773.93855518039, Test Loss: 17273600.0\n",
      "Epoch [86/100], Training Loss: 102678.76980036727, Test Loss: 17173072.0\n",
      "Epoch [87/100], Training Loss: 101617.32400331734, Test Loss: 17070936.0\n",
      "Epoch [88/100], Training Loss: 100580.1770925893, Test Loss: 16979526.0\n",
      "Epoch [89/100], Training Loss: 99571.5294710029, Test Loss: 16887398.0\n",
      "Epoch [90/100], Training Loss: 98591.35347432025, Test Loss: 16803188.0\n",
      "Epoch [91/100], Training Loss: 97641.42915111664, Test Loss: 16718438.0\n",
      "Epoch [92/100], Training Loss: 96716.23662697707, Test Loss: 16641903.0\n",
      "Epoch [93/100], Training Loss: 95815.15758841301, Test Loss: 16564102.0\n",
      "Epoch [94/100], Training Loss: 94940.162253421, Test Loss: 16496761.0\n",
      "Epoch [95/100], Training Loss: 94091.4732391446, Test Loss: 16423697.0\n",
      "Epoch [96/100], Training Loss: 93263.10519222794, Test Loss: 16362407.0\n",
      "Epoch [97/100], Training Loss: 92459.42380486938, Test Loss: 16297853.0\n",
      "Epoch [98/100], Training Loss: 91672.48468692612, Test Loss: 16241816.0\n",
      "Epoch [99/100], Training Loss: 90908.75813044251, Test Loss: 16183248.0\n",
      "Epoch [100/100], Training Loss: 90166.29025235472, Test Loss: 16126681.0\n",
      "Epoch [1/100], Training Loss: 1497677.1205497305, Test Loss: 298632192.0\n",
      "Epoch [2/100], Training Loss: 1464836.2746282804, Test Loss: 283746016.0\n",
      "Epoch [3/100], Training Loss: 1327450.0652804929, Test Loss: 242521328.0\n",
      "Epoch [4/100], Training Loss: 1084572.8741188317, Test Loss: 191389424.0\n",
      "Epoch [5/100], Training Loss: 871943.2716071323, Test Loss: 160603168.0\n",
      "Epoch [6/100], Training Loss: 764028.1018896985, Test Loss: 146571312.0\n",
      "Epoch [7/100], Training Loss: 706022.5842070967, Test Loss: 137338656.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100], Training Loss: 661755.3692316806, Test Loss: 129662936.0\n",
      "Epoch [9/100], Training Loss: 623107.6353296606, Test Loss: 122869912.0\n",
      "Epoch [10/100], Training Loss: 588130.0877910076, Test Loss: 116747064.0\n",
      "Epoch [11/100], Training Loss: 556136.8653515787, Test Loss: 111206640.0\n",
      "Epoch [12/100], Training Loss: 526837.4053669806, Test Loss: 106201120.0\n",
      "Epoch [13/100], Training Loss: 500061.32693560806, Test Loss: 101685512.0\n",
      "Epoch [14/100], Training Loss: 475505.9906403649, Test Loss: 97546648.0\n",
      "Epoch [15/100], Training Loss: 452580.5409632131, Test Loss: 93736664.0\n",
      "Epoch [16/100], Training Loss: 431623.881997512, Test Loss: 90329960.0\n",
      "Epoch [17/100], Training Loss: 412961.055387714, Test Loss: 87304280.0\n",
      "Epoch [18/100], Training Loss: 396226.7993602275, Test Loss: 84561616.0\n",
      "Epoch [19/100], Training Loss: 381047.9534387773, Test Loss: 82038360.0\n",
      "Epoch [20/100], Training Loss: 367067.0217404182, Test Loss: 79683784.0\n",
      "Epoch [21/100], Training Loss: 354144.9763639595, Test Loss: 77480008.0\n",
      "Epoch [22/100], Training Loss: 342483.6398317635, Test Loss: 75493464.0\n",
      "Epoch [23/100], Training Loss: 332080.7478229963, Test Loss: 73678840.0\n",
      "Epoch [24/100], Training Loss: 322691.13370061014, Test Loss: 72003120.0\n",
      "Epoch [25/100], Training Loss: 314146.67804040044, Test Loss: 70445304.0\n",
      "Epoch [26/100], Training Loss: 306335.62040163495, Test Loss: 68989504.0\n",
      "Epoch [27/100], Training Loss: 299147.5406670221, Test Loss: 67621592.0\n",
      "Epoch [28/100], Training Loss: 292478.8439369706, Test Loss: 66327836.0\n",
      "Epoch [29/100], Training Loss: 286248.79796812986, Test Loss: 65097300.0\n",
      "Epoch [30/100], Training Loss: 280370.7232687637, Test Loss: 63916152.0\n",
      "Epoch [31/100], Training Loss: 274788.2051863041, Test Loss: 62776880.0\n",
      "Epoch [32/100], Training Loss: 269433.22178827156, Test Loss: 61672768.0\n",
      "Epoch [33/100], Training Loss: 264271.9410964643, Test Loss: 60601128.0\n",
      "Epoch [34/100], Training Loss: 259282.81375048132, Test Loss: 59559012.0\n",
      "Epoch [35/100], Training Loss: 254445.55406596174, Test Loss: 58546004.0\n",
      "Epoch [36/100], Training Loss: 249748.17305698714, Test Loss: 57558508.0\n",
      "Epoch [37/100], Training Loss: 245166.5763432261, Test Loss: 56596944.0\n",
      "Epoch [38/100], Training Loss: 240691.4796516794, Test Loss: 55657912.0\n",
      "Epoch [39/100], Training Loss: 236315.224112908, Test Loss: 54739972.0\n",
      "Epoch [40/100], Training Loss: 232040.43737041645, Test Loss: 53844344.0\n",
      "Epoch [41/100], Training Loss: 227863.12500740477, Test Loss: 52968904.0\n",
      "Epoch [42/100], Training Loss: 223777.30374385405, Test Loss: 52116196.0\n",
      "Epoch [43/100], Training Loss: 219784.4451306202, Test Loss: 51286764.0\n",
      "Epoch [44/100], Training Loss: 215886.66095018067, Test Loss: 50479248.0\n",
      "Epoch [45/100], Training Loss: 212088.12749540905, Test Loss: 49691804.0\n",
      "Epoch [46/100], Training Loss: 208388.37608850186, Test Loss: 48921700.0\n",
      "Epoch [47/100], Training Loss: 204774.82224098098, Test Loss: 48167496.0\n",
      "Epoch [48/100], Training Loss: 201228.63546294652, Test Loss: 47434216.0\n",
      "Epoch [49/100], Training Loss: 197770.72175818967, Test Loss: 46722612.0\n",
      "Epoch [50/100], Training Loss: 194404.91433416266, Test Loss: 46029684.0\n",
      "Epoch [51/100], Training Loss: 191133.6482954209, Test Loss: 45353332.0\n",
      "Epoch [52/100], Training Loss: 187951.97265416742, Test Loss: 44694020.0\n",
      "Epoch [53/100], Training Loss: 184862.66005420295, Test Loss: 44053428.0\n",
      "Epoch [54/100], Training Loss: 181869.28383981992, Test Loss: 43427916.0\n",
      "Epoch [55/100], Training Loss: 178969.5641327528, Test Loss: 42819532.0\n",
      "Epoch [56/100], Training Loss: 176160.00956696877, Test Loss: 42228340.0\n",
      "Epoch [57/100], Training Loss: 173438.33807609146, Test Loss: 41654224.0\n",
      "Epoch [58/100], Training Loss: 170799.1057938659, Test Loss: 41091904.0\n",
      "Epoch [59/100], Training Loss: 168244.20690421184, Test Loss: 40545656.0\n",
      "Epoch [60/100], Training Loss: 165767.33359527725, Test Loss: 40014516.0\n",
      "Epoch [61/100], Training Loss: 163362.63473126222, Test Loss: 39497040.0\n",
      "Epoch [62/100], Training Loss: 161024.78376438378, Test Loss: 38992508.0\n",
      "Epoch [63/100], Training Loss: 158751.04015840663, Test Loss: 38500488.0\n",
      "Epoch [64/100], Training Loss: 156530.0133228093, Test Loss: 38019112.0\n",
      "Epoch [65/100], Training Loss: 154367.09803065393, Test Loss: 37549412.0\n",
      "Epoch [66/100], Training Loss: 152265.02975359917, Test Loss: 37093440.0\n",
      "Epoch [67/100], Training Loss: 150218.8545995278, Test Loss: 36650096.0\n",
      "Epoch [68/100], Training Loss: 148229.0279996655, Test Loss: 36218792.0\n",
      "Epoch [69/100], Training Loss: 146297.74587199723, Test Loss: 35799568.0\n",
      "Epoch [70/100], Training Loss: 144414.42462013217, Test Loss: 35390500.0\n",
      "Epoch [71/100], Training Loss: 142575.57730567327, Test Loss: 34990812.0\n",
      "Epoch [72/100], Training Loss: 140780.814983492, Test Loss: 34599604.0\n",
      "Epoch [73/100], Training Loss: 139031.98739360264, Test Loss: 34218056.0\n",
      "Epoch [74/100], Training Loss: 137322.3313288377, Test Loss: 33842984.0\n",
      "Epoch [75/100], Training Loss: 135654.8640093476, Test Loss: 33476980.0\n",
      "Epoch [76/100], Training Loss: 134018.3683243643, Test Loss: 33119346.0\n",
      "Epoch [77/100], Training Loss: 132421.095148114, Test Loss: 32771564.0\n",
      "Epoch [78/100], Training Loss: 130867.04085630664, Test Loss: 32431576.0\n",
      "Epoch [79/100], Training Loss: 129350.25318798686, Test Loss: 32100390.0\n",
      "Epoch [80/100], Training Loss: 127873.5600823411, Test Loss: 31779714.0\n",
      "Epoch [81/100], Training Loss: 126433.76739589813, Test Loss: 31466052.0\n",
      "Epoch [82/100], Training Loss: 125032.09877969314, Test Loss: 31161460.0\n",
      "Epoch [83/100], Training Loss: 123666.89062962799, Test Loss: 30863716.0\n",
      "Epoch [84/100], Training Loss: 122335.47110703231, Test Loss: 30574338.0\n",
      "Epoch [85/100], Training Loss: 121032.66541063177, Test Loss: 30289032.0\n",
      "Epoch [86/100], Training Loss: 119758.1512642727, Test Loss: 30012554.0\n",
      "Epoch [87/100], Training Loss: 118503.88128248845, Test Loss: 29739596.0\n",
      "Epoch [88/100], Training Loss: 117271.03884914993, Test Loss: 29473422.0\n",
      "Epoch [89/100], Training Loss: 116062.64231930498, Test Loss: 29213368.0\n",
      "Epoch [90/100], Training Loss: 114878.70984807254, Test Loss: 28960196.0\n",
      "Epoch [91/100], Training Loss: 113719.16674673079, Test Loss: 28713106.0\n",
      "Epoch [92/100], Training Loss: 112578.62159380369, Test Loss: 28472348.0\n",
      "Epoch [93/100], Training Loss: 111466.11633132293, Test Loss: 28237076.0\n",
      "Epoch [94/100], Training Loss: 110374.44575724927, Test Loss: 28006906.0\n",
      "Epoch [95/100], Training Loss: 109305.08045102483, Test Loss: 27779978.0\n",
      "Epoch [96/100], Training Loss: 108252.52604305506, Test Loss: 27557114.0\n",
      "Epoch [97/100], Training Loss: 107215.49423260619, Test Loss: 27337134.0\n",
      "Epoch [98/100], Training Loss: 106186.51289865455, Test Loss: 27120226.0\n",
      "Epoch [99/100], Training Loss: 105177.69392734436, Test Loss: 26910270.0\n",
      "Epoch [100/100], Training Loss: 104190.78989461155, Test Loss: 26705158.0\n",
      "Epoch [1/100], Training Loss: 1164347.663763995, Test Loss: 299283424.0\n",
      "Epoch [2/100], Training Loss: 1149411.6528641668, Test Loss: 290413248.0\n",
      "Epoch [3/100], Training Loss: 1081595.0562170488, Test Loss: 262979968.0\n",
      "Epoch [4/100], Training Loss: 938675.1922279486, Test Loss: 218813792.0\n",
      "Epoch [5/100], Training Loss: 765577.3714827321, Test Loss: 178139424.0\n",
      "Epoch [6/100], Training Loss: 641842.0797346128, Test Loss: 155803424.0\n",
      "Epoch [7/100], Training Loss: 579330.7351460222, Test Loss: 144552144.0\n",
      "Epoch [8/100], Training Loss: 542380.776968189, Test Loss: 136609824.0\n",
      "Epoch [9/100], Training Loss: 513075.8867365677, Test Loss: 129812328.0\n",
      "Epoch [10/100], Training Loss: 487003.62040163495, Test Loss: 123629512.0\n",
      "Epoch [11/100], Training Loss: 462896.44736686215, Test Loss: 117873440.0\n",
      "Epoch [12/100], Training Loss: 440267.89787334873, Test Loss: 112469912.0\n",
      "Epoch [13/100], Training Loss: 418940.36988330074, Test Loss: 107397032.0\n",
      "Epoch [14/100], Training Loss: 398882.6190391565, Test Loss: 102652976.0\n",
      "Epoch [15/100], Training Loss: 380073.0880871986, Test Loss: 98209680.0\n",
      "Epoch [16/100], Training Loss: 362277.8117410106, Test Loss: 94008360.0\n",
      "Epoch [17/100], Training Loss: 345524.7291037261, Test Loss: 90113472.0\n",
      "Epoch [18/100], Training Loss: 330183.39245305373, Test Loss: 86593952.0\n",
      "Epoch [19/100], Training Loss: 316319.2345240211, Test Loss: 83398424.0\n",
      "Epoch [20/100], Training Loss: 303708.23766364553, Test Loss: 80470088.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/100], Training Loss: 292178.9793258693, Test Loss: 77774480.0\n",
      "Epoch [22/100], Training Loss: 281618.57780937146, Test Loss: 75289296.0\n",
      "Epoch [23/100], Training Loss: 271920.03992654465, Test Loss: 72987832.0\n",
      "Epoch [24/100], Training Loss: 262982.1794917363, Test Loss: 70851936.0\n",
      "Epoch [25/100], Training Loss: 254725.5386529234, Test Loss: 68862920.0\n",
      "Epoch [26/100], Training Loss: 247086.74261003494, Test Loss: 67010616.0\n",
      "Epoch [27/100], Training Loss: 240002.8910609561, Test Loss: 65282992.0\n",
      "Epoch [28/100], Training Loss: 233410.58633967183, Test Loss: 63667020.0\n",
      "Epoch [29/100], Training Loss: 227264.32983827972, Test Loss: 62153416.0\n",
      "Epoch [30/100], Training Loss: 221515.0071678218, Test Loss: 60731632.0\n",
      "Epoch [31/100], Training Loss: 216118.4907292222, Test Loss: 59391896.0\n",
      "Epoch [32/100], Training Loss: 211030.1031929388, Test Loss: 58124184.0\n",
      "Epoch [33/100], Training Loss: 206211.23547183225, Test Loss: 56921936.0\n",
      "Epoch [34/100], Training Loss: 201629.48486464072, Test Loss: 55777240.0\n",
      "Epoch [35/100], Training Loss: 197257.1457852023, Test Loss: 54685400.0\n",
      "Epoch [36/100], Training Loss: 193060.7235353356, Test Loss: 53639636.0\n",
      "Epoch [37/100], Training Loss: 189023.90557431433, Test Loss: 52632276.0\n",
      "Epoch [38/100], Training Loss: 185135.60784313726, Test Loss: 51661892.0\n",
      "Epoch [39/100], Training Loss: 181385.90794384218, Test Loss: 50721336.0\n",
      "Epoch [40/100], Training Loss: 177769.66103903798, Test Loss: 49813960.0\n",
      "Epoch [41/100], Training Loss: 174279.87915407855, Test Loss: 48937548.0\n",
      "Epoch [42/100], Training Loss: 170910.03980806825, Test Loss: 48093744.0\n",
      "Epoch [43/100], Training Loss: 167660.8300456134, Test Loss: 47280232.0\n",
      "Epoch [44/100], Training Loss: 164521.56151886735, Test Loss: 46494104.0\n",
      "Epoch [45/100], Training Loss: 161481.29482850543, Test Loss: 45731880.0\n",
      "Epoch [46/100], Training Loss: 158535.64824358746, Test Loss: 44990828.0\n",
      "Epoch [47/100], Training Loss: 155682.3503346958, Test Loss: 44271448.0\n",
      "Epoch [48/100], Training Loss: 152921.12955393636, Test Loss: 43573040.0\n",
      "Epoch [49/100], Training Loss: 150248.8358509567, Test Loss: 42894920.0\n",
      "Epoch [50/100], Training Loss: 147663.71506427345, Test Loss: 42237160.0\n",
      "Epoch [51/100], Training Loss: 145163.72051418756, Test Loss: 41597660.0\n",
      "Epoch [52/100], Training Loss: 142742.940702565, Test Loss: 40976616.0\n",
      "Epoch [53/100], Training Loss: 140395.40501155145, Test Loss: 40374044.0\n",
      "Epoch [54/100], Training Loss: 138115.70534920917, Test Loss: 39787648.0\n",
      "Epoch [55/100], Training Loss: 135899.67247200996, Test Loss: 39216364.0\n",
      "Epoch [56/100], Training Loss: 133744.560511818, Test Loss: 38660564.0\n",
      "Epoch [57/100], Training Loss: 131649.68106154847, Test Loss: 38120232.0\n",
      "Epoch [58/100], Training Loss: 129611.06314791778, Test Loss: 37594800.0\n",
      "Epoch [59/100], Training Loss: 127633.20336472958, Test Loss: 37083652.0\n",
      "Epoch [60/100], Training Loss: 125716.1105977134, Test Loss: 36585628.0\n",
      "Epoch [61/100], Training Loss: 123853.014750311, Test Loss: 36099932.0\n",
      "Epoch [62/100], Training Loss: 122036.74788223447, Test Loss: 35625056.0\n",
      "Epoch [63/100], Training Loss: 120264.64303062615, Test Loss: 35163140.0\n",
      "Epoch [64/100], Training Loss: 118538.166163142, Test Loss: 34713960.0\n",
      "Epoch [65/100], Training Loss: 116855.70256501393, Test Loss: 34275764.0\n",
      "Epoch [66/100], Training Loss: 115214.71453112968, Test Loss: 33848328.0\n",
      "Epoch [67/100], Training Loss: 113611.84082696523, Test Loss: 33430082.0\n",
      "Epoch [68/100], Training Loss: 112046.60233398495, Test Loss: 33023414.0\n",
      "Epoch [69/100], Training Loss: 110523.2398554588, Test Loss: 32628226.0\n",
      "Epoch [70/100], Training Loss: 109043.38718085422, Test Loss: 32244488.0\n",
      "Epoch [71/100], Training Loss: 107608.63758071205, Test Loss: 31871860.0\n",
      "Epoch [72/100], Training Loss: 106216.03352881939, Test Loss: 31509104.0\n",
      "Epoch [73/100], Training Loss: 104863.51371364256, Test Loss: 31156902.0\n",
      "Epoch [74/100], Training Loss: 103549.90391564481, Test Loss: 30814818.0\n",
      "Epoch [75/100], Training Loss: 102275.38090160536, Test Loss: 30482658.0\n",
      "Epoch [76/100], Training Loss: 101034.14892482673, Test Loss: 30157944.0\n",
      "Epoch [77/100], Training Loss: 99821.121912209, Test Loss: 29841600.0\n",
      "Epoch [78/100], Training Loss: 98638.4077957467, Test Loss: 29533644.0\n",
      "Epoch [79/100], Training Loss: 97482.33493276464, Test Loss: 29233938.0\n",
      "Epoch [80/100], Training Loss: 96353.76517978792, Test Loss: 28939204.0\n",
      "Epoch [81/100], Training Loss: 95251.71790770689, Test Loss: 28651400.0\n",
      "Epoch [82/100], Training Loss: 94173.97778567621, Test Loss: 28370424.0\n",
      "Epoch [83/100], Training Loss: 93119.88436703986, Test Loss: 28097004.0\n",
      "Epoch [84/100], Training Loss: 92089.27782714294, Test Loss: 27830364.0\n",
      "Epoch [85/100], Training Loss: 91079.62442983236, Test Loss: 27569602.0\n",
      "Epoch [86/100], Training Loss: 90088.80735738404, Test Loss: 27313650.0\n",
      "Epoch [87/100], Training Loss: 89117.45779278479, Test Loss: 27062214.0\n",
      "Epoch [88/100], Training Loss: 88165.45157277412, Test Loss: 26816076.0\n",
      "Epoch [89/100], Training Loss: 87231.74213612938, Test Loss: 26574504.0\n",
      "Epoch [90/100], Training Loss: 86313.0738107932, Test Loss: 26336674.0\n",
      "Epoch [91/100], Training Loss: 85406.01617202772, Test Loss: 26102328.0\n",
      "Epoch [92/100], Training Loss: 84509.25045909603, Test Loss: 25868518.0\n",
      "Epoch [93/100], Training Loss: 83622.78739411173, Test Loss: 25638656.0\n",
      "Epoch [94/100], Training Loss: 82749.04259226349, Test Loss: 25412450.0\n",
      "Epoch [95/100], Training Loss: 81888.37557016764, Test Loss: 25191136.0\n",
      "Epoch [96/100], Training Loss: 81040.98998874474, Test Loss: 24975092.0\n",
      "Epoch [97/100], Training Loss: 80208.39582963094, Test Loss: 24765284.0\n",
      "Epoch [98/100], Training Loss: 79394.09241158699, Test Loss: 24560790.0\n",
      "Epoch [99/100], Training Loss: 78598.69966234227, Test Loss: 24361416.0\n",
      "Epoch [100/100], Training Loss: 77823.14015757361, Test Loss: 24163526.0\n",
      "Epoch [1/100], Training Loss: 582414.5811267105, Test Loss: 300015616.0\n",
      "Epoch [2/100], Training Loss: 581782.3266394171, Test Loss: 299266336.0\n",
      "Epoch [3/100], Training Loss: 578714.2941768853, Test Loss: 296541920.0\n",
      "Epoch [4/100], Training Loss: 570252.5850364314, Test Loss: 290199008.0\n",
      "Epoch [5/100], Training Loss: 553335.7919554529, Test Loss: 278856096.0\n",
      "Epoch [6/100], Training Loss: 526005.2309697293, Test Loss: 262006496.0\n",
      "Epoch [7/100], Training Loss: 488530.8766068361, Test Loss: 240532800.0\n",
      "Epoch [8/100], Training Loss: 444128.650198448, Test Loss: 216856832.0\n",
      "Epoch [9/100], Training Loss: 398587.00219181326, Test Loss: 194347856.0\n",
      "Epoch [10/100], Training Loss: 358313.71838161245, Test Loss: 175940256.0\n",
      "Epoch [11/100], Training Loss: 327355.5282270008, Test Loss: 162669616.0\n",
      "Epoch [12/100], Training Loss: 305656.7264972454, Test Loss: 153575120.0\n",
      "Epoch [13/100], Training Loss: 290464.022273562, Test Loss: 147035680.0\n",
      "Epoch [14/100], Training Loss: 278951.6388839524, Test Loss: 141849904.0\n",
      "Epoch [15/100], Training Loss: 269363.5813044251, Test Loss: 137379136.0\n",
      "Epoch [16/100], Training Loss: 260834.53065576684, Test Loss: 133326832.0\n",
      "Epoch [17/100], Training Loss: 252964.4447603815, Test Loss: 129553192.0\n",
      "Epoch [18/100], Training Loss: 245555.65381197797, Test Loss: 125985096.0\n",
      "Epoch [19/100], Training Loss: 238498.80504709436, Test Loss: 122580192.0\n",
      "Epoch [20/100], Training Loss: 231728.39191990995, Test Loss: 119312576.0\n",
      "Epoch [21/100], Training Loss: 225204.22581600616, Test Loss: 116166448.0\n",
      "Epoch [22/100], Training Loss: 218902.23138439667, Test Loss: 113132472.0\n",
      "Epoch [23/100], Training Loss: 212808.86985368165, Test Loss: 110205616.0\n",
      "Epoch [24/100], Training Loss: 206917.8761921687, Test Loss: 107384048.0\n",
      "Epoch [25/100], Training Loss: 201228.723653812, Test Loss: 104667704.0\n",
      "Epoch [26/100], Training Loss: 195742.59534387774, Test Loss: 102056832.0\n",
      "Epoch [27/100], Training Loss: 190460.47698596056, Test Loss: 99551168.0\n",
      "Epoch [28/100], Training Loss: 185383.15644807773, Test Loss: 97149536.0\n",
      "Epoch [29/100], Training Loss: 180512.26135892424, Test Loss: 94851560.0\n",
      "Epoch [30/100], Training Loss: 175849.00231028968, Test Loss: 92656816.0\n",
      "Epoch [31/100], Training Loss: 171393.2859427759, Test Loss: 90564000.0\n",
      "Epoch [32/100], Training Loss: 167143.84621764114, Test Loss: 88571432.0\n",
      "Epoch [33/100], Training Loss: 163097.76908950892, Test Loss: 86675936.0\n",
      "Epoch [34/100], Training Loss: 159249.6603281796, Test Loss: 84873632.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/100], Training Loss: 155592.23861145668, Test Loss: 83159768.0\n",
      "Epoch [36/100], Training Loss: 152117.35892423434, Test Loss: 81529208.0\n",
      "Epoch [37/100], Training Loss: 148815.34766897696, Test Loss: 79976168.0\n",
      "Epoch [38/100], Training Loss: 145675.78555772762, Test Loss: 78494848.0\n",
      "Epoch [39/100], Training Loss: 142687.45453468396, Test Loss: 77079728.0\n",
      "Epoch [40/100], Training Loss: 139839.5445767431, Test Loss: 75725808.0\n",
      "Epoch [41/100], Training Loss: 137121.67975830816, Test Loss: 74428224.0\n",
      "Epoch [42/100], Training Loss: 134524.29642793673, Test Loss: 73182608.0\n",
      "Epoch [43/100], Training Loss: 132038.64297138795, Test Loss: 71985112.0\n",
      "Epoch [44/100], Training Loss: 129656.22297257271, Test Loss: 70832224.0\n",
      "Epoch [45/100], Training Loss: 127369.55891238671, Test Loss: 69721192.0\n",
      "Epoch [46/100], Training Loss: 125172.33955334399, Test Loss: 68649048.0\n",
      "Epoch [47/100], Training Loss: 123058.5536401872, Test Loss: 67613296.0\n",
      "Epoch [48/100], Training Loss: 121022.76974112909, Test Loss: 66611932.0\n",
      "Epoch [49/100], Training Loss: 119058.54582074522, Test Loss: 65642712.0\n",
      "Epoch [50/100], Training Loss: 117160.59712102363, Test Loss: 64703688.0\n",
      "Epoch [51/100], Training Loss: 115326.15698122149, Test Loss: 63793932.0\n",
      "Epoch [52/100], Training Loss: 113553.31769444938, Test Loss: 62912296.0\n",
      "Epoch [53/100], Training Loss: 111837.77205141875, Test Loss: 62057760.0\n",
      "Epoch [54/100], Training Loss: 110175.79385107517, Test Loss: 61229000.0\n",
      "Epoch [55/100], Training Loss: 108562.49499437238, Test Loss: 60423104.0\n",
      "Epoch [56/100], Training Loss: 106993.00657543984, Test Loss: 59637660.0\n",
      "Epoch [57/100], Training Loss: 105465.03595758545, Test Loss: 58872284.0\n",
      "Epoch [58/100], Training Loss: 103977.16758485872, Test Loss: 58126392.0\n",
      "Epoch [59/100], Training Loss: 102526.53468396422, Test Loss: 57398924.0\n",
      "Epoch [60/100], Training Loss: 101110.21918132812, Test Loss: 56688708.0\n",
      "Epoch [61/100], Training Loss: 99726.07570641549, Test Loss: 55995188.0\n",
      "Epoch [62/100], Training Loss: 98372.1347076595, Test Loss: 55317164.0\n",
      "Epoch [63/100], Training Loss: 97046.69344233161, Test Loss: 54653196.0\n",
      "Epoch [64/100], Training Loss: 95748.93406788698, Test Loss: 54002876.0\n",
      "Epoch [65/100], Training Loss: 94478.25105147799, Test Loss: 53365048.0\n",
      "Epoch [66/100], Training Loss: 93233.14495586754, Test Loss: 52739528.0\n",
      "Epoch [67/100], Training Loss: 92012.08553995617, Test Loss: 52126064.0\n",
      "Epoch [68/100], Training Loss: 90814.32379598366, Test Loss: 51523824.0\n",
      "Epoch [69/100], Training Loss: 89639.25348024406, Test Loss: 50932616.0\n",
      "Epoch [70/100], Training Loss: 88485.70203187015, Test Loss: 50351808.0\n",
      "Epoch [71/100], Training Loss: 87353.14365262722, Test Loss: 49780688.0\n",
      "Epoch [72/100], Training Loss: 86241.65013920976, Test Loss: 49219136.0\n",
      "Epoch [73/100], Training Loss: 85150.6455778686, Test Loss: 48666892.0\n",
      "Epoch [74/100], Training Loss: 84079.68876251407, Test Loss: 48124028.0\n",
      "Epoch [75/100], Training Loss: 83028.66737752502, Test Loss: 47590468.0\n",
      "Epoch [76/100], Training Loss: 81997.28926011492, Test Loss: 47065828.0\n",
      "Epoch [77/100], Training Loss: 80985.08559919435, Test Loss: 46550104.0\n",
      "Epoch [78/100], Training Loss: 79991.73793021741, Test Loss: 46042620.0\n",
      "Epoch [79/100], Training Loss: 79017.30383271133, Test Loss: 45543252.0\n",
      "Epoch [80/100], Training Loss: 78061.13358213376, Test Loss: 45052216.0\n",
      "Epoch [81/100], Training Loss: 77122.22415733665, Test Loss: 44569196.0\n",
      "Epoch [82/100], Training Loss: 76200.30152242165, Test Loss: 44093868.0\n",
      "Epoch [83/100], Training Loss: 75294.77530951958, Test Loss: 43625904.0\n",
      "Epoch [84/100], Training Loss: 74405.55002665719, Test Loss: 43165496.0\n",
      "Epoch [85/100], Training Loss: 73532.52437651798, Test Loss: 42712400.0\n",
      "Epoch [86/100], Training Loss: 72675.0736330786, Test Loss: 42266328.0\n",
      "Epoch [87/100], Training Loss: 71833.19246490137, Test Loss: 41827536.0\n",
      "Epoch [88/100], Training Loss: 71006.86404833836, Test Loss: 41395600.0\n",
      "Epoch [89/100], Training Loss: 70195.22409809845, Test Loss: 40970240.0\n",
      "Epoch [90/100], Training Loss: 69397.98116225342, Test Loss: 40551400.0\n",
      "Epoch [91/100], Training Loss: 68614.91297908891, Test Loss: 40139340.0\n",
      "Epoch [92/100], Training Loss: 67845.45062496298, Test Loss: 39733992.0\n",
      "Epoch [93/100], Training Loss: 67089.30146318347, Test Loss: 39335432.0\n",
      "Epoch [94/100], Training Loss: 66346.42639654048, Test Loss: 38942940.0\n",
      "Epoch [95/100], Training Loss: 65616.7231799064, Test Loss: 38556648.0\n",
      "Epoch [96/100], Training Loss: 64899.466145370534, Test Loss: 38176476.0\n",
      "Epoch [97/100], Training Loss: 64193.89100171791, Test Loss: 37802048.0\n",
      "Epoch [98/100], Training Loss: 63500.07025650139, Test Loss: 37433392.0\n",
      "Epoch [99/100], Training Loss: 62818.00829334755, Test Loss: 37070680.0\n",
      "Epoch [100/100], Training Loss: 62147.724305432144, Test Loss: 36713728.0\n",
      "Epoch [1/100], Training Loss: 7171225.707541022, Test Loss: 148839200.0\n",
      "Epoch [2/100], Training Loss: 4076900.6147740064, Test Loss: 117863464.0\n",
      "Epoch [3/100], Training Loss: 3267743.44316095, Test Loss: 96504768.0\n",
      "Epoch [4/100], Training Loss: 2677939.3156211125, Test Loss: 80911144.0\n",
      "Epoch [5/100], Training Loss: 2240166.151353593, Test Loss: 69263920.0\n",
      "Epoch [6/100], Training Loss: 1919907.4876192168, Test Loss: 60741636.0\n",
      "Epoch [7/100], Training Loss: 1690823.645400154, Test Loss: 54627996.0\n",
      "Epoch [8/100], Training Loss: 1523903.9644570819, Test Loss: 49981204.0\n",
      "Epoch [9/100], Training Loss: 1392239.4751791956, Test Loss: 46149344.0\n",
      "Epoch [10/100], Training Loss: 1281000.8744446419, Test Loss: 42824260.0\n",
      "Epoch [11/100], Training Loss: 1183778.5622001067, Test Loss: 39854980.0\n",
      "Epoch [12/100], Training Loss: 1097923.632886085, Test Loss: 37183940.0\n",
      "Epoch [13/100], Training Loss: 1022000.9946241337, Test Loss: 34779456.0\n",
      "Epoch [14/100], Training Loss: 954854.0972691191, Test Loss: 32640180.0\n",
      "Epoch [15/100], Training Loss: 895472.9271222084, Test Loss: 30745206.0\n",
      "Epoch [16/100], Training Loss: 843159.3558734672, Test Loss: 29066812.0\n",
      "Epoch [17/100], Training Loss: 797000.2489040934, Test Loss: 27580576.0\n",
      "Epoch [18/100], Training Loss: 756325.9460636218, Test Loss: 26261666.0\n",
      "Epoch [19/100], Training Loss: 720376.0523369468, Test Loss: 25092644.0\n",
      "Epoch [20/100], Training Loss: 688261.7367898821, Test Loss: 24048346.0\n",
      "Epoch [21/100], Training Loss: 659260.1311681772, Test Loss: 23112252.0\n",
      "Epoch [22/100], Training Loss: 632908.223587169, Test Loss: 22273322.0\n",
      "Epoch [23/100], Training Loss: 608838.9095877021, Test Loss: 21522084.0\n",
      "Epoch [24/100], Training Loss: 586730.4775561282, Test Loss: 20845402.0\n",
      "Epoch [25/100], Training Loss: 566351.824965938, Test Loss: 20249804.0\n",
      "Epoch [26/100], Training Loss: 547629.0086783959, Test Loss: 19707436.0\n",
      "Epoch [27/100], Training Loss: 530375.333014928, Test Loss: 19227534.0\n",
      "Epoch [28/100], Training Loss: 514436.7090367869, Test Loss: 18793092.0\n",
      "Epoch [29/100], Training Loss: 499657.4448492388, Test Loss: 18405356.0\n",
      "Epoch [30/100], Training Loss: 485882.69232421066, Test Loss: 18041662.0\n",
      "Epoch [31/100], Training Loss: 473049.22427581303, Test Loss: 17717132.0\n",
      "Epoch [32/100], Training Loss: 461100.3711198981, Test Loss: 17409636.0\n",
      "Epoch [33/100], Training Loss: 449948.21556039335, Test Loss: 17136426.0\n",
      "Epoch [34/100], Training Loss: 439541.69116906583, Test Loss: 16886154.0\n",
      "Epoch [35/100], Training Loss: 429764.981569516, Test Loss: 16665853.0\n",
      "Epoch [36/100], Training Loss: 420598.53798649367, Test Loss: 16453973.0\n",
      "Epoch [37/100], Training Loss: 411933.882686156, Test Loss: 16283452.0\n",
      "Epoch [38/100], Training Loss: 403840.422272081, Test Loss: 16099451.0\n",
      "Epoch [39/100], Training Loss: 396249.1792769978, Test Loss: 15968063.0\n",
      "Epoch [40/100], Training Loss: 389121.25767875125, Test Loss: 15819214.0\n",
      "Epoch [41/100], Training Loss: 382365.54872341687, Test Loss: 15721667.0\n",
      "Epoch [42/100], Training Loss: 376065.96296131745, Test Loss: 15594956.0\n",
      "Epoch [43/100], Training Loss: 370093.72199514246, Test Loss: 15529434.0\n",
      "Epoch [44/100], Training Loss: 364494.71646377584, Test Loss: 15421744.0\n",
      "Epoch [45/100], Training Loss: 359180.5061015343, Test Loss: 15371282.0\n",
      "Epoch [46/100], Training Loss: 354150.0087080149, Test Loss: 15286889.0\n",
      "Epoch [47/100], Training Loss: 349372.56531751674, Test Loss: 15239695.0\n",
      "Epoch [48/100], Training Loss: 344859.5105518038, Test Loss: 15141878.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/100], Training Loss: 340556.27299552754, Test Loss: 15106504.0\n",
      "Epoch [50/100], Training Loss: 336538.38017593743, Test Loss: 14991028.0\n",
      "Epoch [51/100], Training Loss: 332717.2024354304, Test Loss: 14997023.0\n",
      "Epoch [52/100], Training Loss: 329157.9446678218, Test Loss: 14880689.0\n",
      "Epoch [53/100], Training Loss: 325737.50971876667, Test Loss: 14878422.0\n",
      "Epoch [54/100], Training Loss: 322507.1380620224, Test Loss: 14772693.0\n",
      "Epoch [55/100], Training Loss: 319453.8348846336, Test Loss: 14778451.0\n",
      "Epoch [56/100], Training Loss: 316529.6151368402, Test Loss: 14681131.0\n",
      "Epoch [57/100], Training Loss: 313740.2249644571, Test Loss: 14685215.0\n",
      "Epoch [58/100], Training Loss: 311106.7370009182, Test Loss: 14603254.0\n",
      "Epoch [59/100], Training Loss: 308579.8489166815, Test Loss: 14618826.0\n",
      "Epoch [60/100], Training Loss: 306205.0975949292, Test Loss: 14537862.0\n",
      "Epoch [61/100], Training Loss: 303929.6665037616, Test Loss: 14551963.0\n",
      "Epoch [62/100], Training Loss: 301778.17479710915, Test Loss: 14465611.0\n",
      "Epoch [63/100], Training Loss: 299684.25210665836, Test Loss: 14501630.0\n",
      "Epoch [64/100], Training Loss: 297691.2581933831, Test Loss: 14432908.0\n",
      "Epoch [65/100], Training Loss: 295775.50317294593, Test Loss: 14468528.0\n",
      "Epoch [66/100], Training Loss: 293890.1806579883, Test Loss: 14420864.0\n",
      "Epoch [67/100], Training Loss: 292009.5495897755, Test Loss: 14454947.0\n",
      "Epoch [68/100], Training Loss: 290188.29928247735, Test Loss: 14418933.0\n",
      "Epoch [69/100], Training Loss: 288430.41471921094, Test Loss: 14380928.0\n",
      "Epoch [70/100], Training Loss: 286753.0809934246, Test Loss: 14391199.0\n",
      "Epoch [71/100], Training Loss: 285173.2157455127, Test Loss: 14408147.0\n",
      "Epoch [72/100], Training Loss: 283557.83219670044, Test Loss: 14350333.0\n",
      "Epoch [73/100], Training Loss: 281972.59192657424, Test Loss: 14409518.0\n",
      "Epoch [74/100], Training Loss: 280447.19225756766, Test Loss: 14352356.0\n",
      "Epoch [75/100], Training Loss: 278946.2169969196, Test Loss: 14369000.0\n",
      "Epoch [76/100], Training Loss: 277431.33501791954, Test Loss: 14381992.0\n",
      "Epoch [77/100], Training Loss: 275994.9278293644, Test Loss: 14361758.0\n",
      "Epoch [78/100], Training Loss: 274552.55329956755, Test Loss: 14359276.0\n",
      "Epoch [79/100], Training Loss: 273153.40771799657, Test Loss: 14355082.0\n",
      "Epoch [80/100], Training Loss: 271791.26905248506, Test Loss: 14345948.0\n",
      "Epoch [81/100], Training Loss: 270494.7012913927, Test Loss: 14375022.0\n",
      "Epoch [82/100], Training Loss: 269256.6710021622, Test Loss: 14291425.0\n",
      "Epoch [83/100], Training Loss: 267997.6034595107, Test Loss: 14377515.0\n",
      "Epoch [84/100], Training Loss: 266807.07765016885, Test Loss: 14332606.0\n",
      "Epoch [85/100], Training Loss: 265631.2234427759, Test Loss: 14353532.0\n",
      "Epoch [86/100], Training Loss: 264494.2207067117, Test Loss: 14347706.0\n",
      "Epoch [87/100], Training Loss: 263384.687566643, Test Loss: 14359272.0\n",
      "Epoch [88/100], Training Loss: 262306.0079934542, Test Loss: 14289801.0\n",
      "Epoch [89/100], Training Loss: 261287.88489648126, Test Loss: 14386481.0\n",
      "Epoch [90/100], Training Loss: 260289.08013076833, Test Loss: 14332697.0\n",
      "Epoch [91/100], Training Loss: 259295.53036698062, Test Loss: 14349397.0\n",
      "Epoch [92/100], Training Loss: 258301.73132886086, Test Loss: 14342638.0\n",
      "Epoch [93/100], Training Loss: 257310.97927773828, Test Loss: 14368320.0\n",
      "Epoch [94/100], Training Loss: 256375.06103755702, Test Loss: 14296046.0\n",
      "Epoch [95/100], Training Loss: 255416.8660513299, Test Loss: 14366868.0\n",
      "Epoch [96/100], Training Loss: 254520.17271636752, Test Loss: 14334214.0\n",
      "Epoch [97/100], Training Loss: 253644.07538801018, Test Loss: 14338289.0\n",
      "Epoch [98/100], Training Loss: 252748.10691754043, Test Loss: 14346045.0\n",
      "Epoch [99/100], Training Loss: 251936.7328320301, Test Loss: 14343957.0\n",
      "Epoch [100/100], Training Loss: 251145.12180483976, Test Loss: 14271620.0\n",
      "Epoch [1/100], Training Loss: 4412250.582311475, Test Loss: 235512640.0\n",
      "Epoch [2/100], Training Loss: 2725156.970321663, Test Loss: 146298320.0\n",
      "Epoch [3/100], Training Loss: 2097438.345951069, Test Loss: 126706208.0\n",
      "Epoch [4/100], Training Loss: 1823610.2081630235, Test Loss: 111258808.0\n",
      "Epoch [5/100], Training Loss: 1596129.857354422, Test Loss: 98390880.0\n",
      "Epoch [6/100], Training Loss: 1406934.6811207866, Test Loss: 87855912.0\n",
      "Epoch [7/100], Training Loss: 1250740.5242580415, Test Loss: 79142104.0\n",
      "Epoch [8/100], Training Loss: 1121854.7953320302, Test Loss: 71910104.0\n",
      "Epoch [9/100], Training Loss: 1016135.2079260708, Test Loss: 65924248.0\n",
      "Epoch [10/100], Training Loss: 929869.7003139624, Test Loss: 61021012.0\n",
      "Epoch [11/100], Training Loss: 859800.9075291748, Test Loss: 57023396.0\n",
      "Epoch [12/100], Training Loss: 802301.9365558912, Test Loss: 53678076.0\n",
      "Epoch [13/100], Training Loss: 753589.7565310112, Test Loss: 50785492.0\n",
      "Epoch [14/100], Training Loss: 710895.9284402583, Test Loss: 48206736.0\n",
      "Epoch [15/100], Training Loss: 672575.811563296, Test Loss: 45868872.0\n",
      "Epoch [16/100], Training Loss: 637721.4528760144, Test Loss: 43718024.0\n",
      "Epoch [17/100], Training Loss: 605737.7689710326, Test Loss: 41719760.0\n",
      "Epoch [18/100], Training Loss: 576212.6282803152, Test Loss: 39854008.0\n",
      "Epoch [19/100], Training Loss: 548890.2681713168, Test Loss: 38106800.0\n",
      "Epoch [20/100], Training Loss: 523635.9632130798, Test Loss: 36474980.0\n",
      "Epoch [21/100], Training Loss: 500341.6147147681, Test Loss: 34956924.0\n",
      "Epoch [22/100], Training Loss: 478814.18224631244, Test Loss: 33548932.0\n",
      "Epoch [23/100], Training Loss: 458909.91762928735, Test Loss: 32243670.0\n",
      "Epoch [24/100], Training Loss: 440493.5176529827, Test Loss: 31034928.0\n",
      "Epoch [25/100], Training Loss: 423462.1483324448, Test Loss: 29913666.0\n",
      "Epoch [26/100], Training Loss: 407733.3537705112, Test Loss: 28875402.0\n",
      "Epoch [27/100], Training Loss: 393230.8566435638, Test Loss: 27914508.0\n",
      "Epoch [28/100], Training Loss: 379830.333081571, Test Loss: 27024648.0\n",
      "Epoch [29/100], Training Loss: 367448.04950832296, Test Loss: 26200498.0\n",
      "Epoch [30/100], Training Loss: 355995.3995172087, Test Loss: 25435792.0\n",
      "Epoch [31/100], Training Loss: 345337.69486404833, Test Loss: 24724428.0\n",
      "Epoch [32/100], Training Loss: 335343.9048782655, Test Loss: 24058100.0\n",
      "Epoch [33/100], Training Loss: 325943.76018896984, Test Loss: 23437554.0\n",
      "Epoch [34/100], Training Loss: 317093.0643623008, Test Loss: 22857382.0\n",
      "Epoch [35/100], Training Loss: 308743.25876725314, Test Loss: 22318788.0\n",
      "Epoch [36/100], Training Loss: 300845.58128961554, Test Loss: 21816172.0\n",
      "Epoch [37/100], Training Loss: 293375.6480658729, Test Loss: 21348838.0\n",
      "Epoch [38/100], Training Loss: 286292.92198329483, Test Loss: 20911138.0\n",
      "Epoch [39/100], Training Loss: 279577.2062674012, Test Loss: 20505272.0\n",
      "Epoch [40/100], Training Loss: 273212.6729607251, Test Loss: 20122174.0\n",
      "Epoch [41/100], Training Loss: 267173.8829749422, Test Loss: 19771750.0\n",
      "Epoch [42/100], Training Loss: 261446.98686392987, Test Loss: 19439082.0\n",
      "Epoch [43/100], Training Loss: 256006.86207866832, Test Loss: 19133662.0\n",
      "Epoch [44/100], Training Loss: 250832.70149872638, Test Loss: 18839214.0\n",
      "Epoch [45/100], Training Loss: 245904.32370712634, Test Loss: 18569284.0\n",
      "Epoch [46/100], Training Loss: 241199.61003495054, Test Loss: 18311024.0\n",
      "Epoch [47/100], Training Loss: 236707.18522303182, Test Loss: 18070732.0\n",
      "Epoch [48/100], Training Loss: 232421.16121675257, Test Loss: 17845088.0\n",
      "Epoch [49/100], Training Loss: 228328.31545820745, Test Loss: 17632670.0\n",
      "Epoch [50/100], Training Loss: 224411.58391090576, Test Loss: 17431700.0\n",
      "Epoch [51/100], Training Loss: 220655.2646318346, Test Loss: 17246762.0\n",
      "Epoch [52/100], Training Loss: 217067.45317220545, Test Loss: 17067788.0\n",
      "Epoch [53/100], Training Loss: 213629.7910520704, Test Loss: 16907290.0\n",
      "Epoch [54/100], Training Loss: 210333.38655885315, Test Loss: 16749709.0\n",
      "Epoch [55/100], Training Loss: 207177.99044784077, Test Loss: 16607459.0\n",
      "Epoch [56/100], Training Loss: 204157.2004768675, Test Loss: 16469208.0\n",
      "Epoch [57/100], Training Loss: 201254.16274213613, Test Loss: 16349677.0\n",
      "Epoch [58/100], Training Loss: 198481.0490344174, Test Loss: 16229695.0\n",
      "Epoch [59/100], Training Loss: 195826.55531366626, Test Loss: 16130248.0\n",
      "Epoch [60/100], Training Loss: 193282.59718026183, Test Loss: 16018724.0\n",
      "Epoch [61/100], Training Loss: 190842.52704223685, Test Loss: 15939965.0\n",
      "Epoch [62/100], Training Loss: 188481.4078253658, Test Loss: 15845275.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [63/100], Training Loss: 186221.50958177834, Test Loss: 15775797.0\n",
      "Epoch [64/100], Training Loss: 184030.77202179967, Test Loss: 15696734.0\n",
      "Epoch [65/100], Training Loss: 181935.6735086784, Test Loss: 15635628.0\n",
      "Epoch [66/100], Training Loss: 179909.61022747468, Test Loss: 15567307.0\n",
      "Epoch [67/100], Training Loss: 177973.43288312302, Test Loss: 15514355.0\n",
      "Epoch [68/100], Training Loss: 176099.7759463302, Test Loss: 15452685.0\n",
      "Epoch [69/100], Training Loss: 174300.57959392216, Test Loss: 15408692.0\n",
      "Epoch [70/100], Training Loss: 172568.28948966294, Test Loss: 15344376.0\n",
      "Epoch [71/100], Training Loss: 170891.17914371187, Test Loss: 15311725.0\n",
      "Epoch [72/100], Training Loss: 169288.95169125052, Test Loss: 15242489.0\n",
      "Epoch [73/100], Training Loss: 167732.69347935548, Test Loss: 15212946.0\n",
      "Epoch [74/100], Training Loss: 166253.08365173865, Test Loss: 15153673.0\n",
      "Epoch [75/100], Training Loss: 164815.40547805224, Test Loss: 15118213.0\n",
      "Epoch [76/100], Training Loss: 163451.6892512292, Test Loss: 15069520.0\n",
      "Epoch [77/100], Training Loss: 162131.29961198982, Test Loss: 15020550.0\n",
      "Epoch [78/100], Training Loss: 160861.2305032285, Test Loss: 14993373.0\n",
      "Epoch [79/100], Training Loss: 159630.49580889757, Test Loss: 14951929.0\n",
      "Epoch [80/100], Training Loss: 158430.6634529945, Test Loss: 14918546.0\n",
      "Epoch [81/100], Training Loss: 157278.8754072626, Test Loss: 14889634.0\n",
      "Epoch [82/100], Training Loss: 156173.00253983767, Test Loss: 14852724.0\n",
      "Epoch [83/100], Training Loss: 155100.0518630413, Test Loss: 14824019.0\n",
      "Epoch [84/100], Training Loss: 154062.56851637937, Test Loss: 14788903.0\n",
      "Epoch [85/100], Training Loss: 153061.85719151708, Test Loss: 14771623.0\n",
      "Epoch [86/100], Training Loss: 152096.03895651916, Test Loss: 14729094.0\n",
      "Epoch [87/100], Training Loss: 151151.16722942953, Test Loss: 14728717.0\n",
      "Epoch [88/100], Training Loss: 150238.67943990286, Test Loss: 14666242.0\n",
      "Epoch [89/100], Training Loss: 149347.57814258634, Test Loss: 14685035.0\n",
      "Epoch [90/100], Training Loss: 148486.76307683194, Test Loss: 14644936.0\n",
      "Epoch [91/100], Training Loss: 147657.11127895268, Test Loss: 14604683.0\n",
      "Epoch [92/100], Training Loss: 146850.06242224987, Test Loss: 14600914.0\n",
      "Epoch [93/100], Training Loss: 146076.70692642615, Test Loss: 14591338.0\n",
      "Epoch [94/100], Training Loss: 145312.6377436171, Test Loss: 14550726.0\n",
      "Epoch [95/100], Training Loss: 144582.35702120728, Test Loss: 14559683.0\n",
      "Epoch [96/100], Training Loss: 143859.00456134116, Test Loss: 14519608.0\n",
      "Epoch [97/100], Training Loss: 143156.32857206327, Test Loss: 14521230.0\n",
      "Epoch [98/100], Training Loss: 142474.07740210887, Test Loss: 14496428.0\n",
      "Epoch [99/100], Training Loss: 141814.69628576507, Test Loss: 14500536.0\n",
      "Epoch [100/100], Training Loss: 141170.74367632248, Test Loss: 14436627.0\n",
      "Epoch [1/100], Training Loss: 2314873.079082993, Test Loss: 290517312.0\n",
      "Epoch [2/100], Training Loss: 2021066.012676974, Test Loss: 218874288.0\n",
      "Epoch [3/100], Training Loss: 1411283.923227297, Test Loss: 156941152.0\n",
      "Epoch [4/100], Training Loss: 1137392.6025709377, Test Loss: 139158992.0\n",
      "Epoch [5/100], Training Loss: 1026208.2303181092, Test Loss: 127257224.0\n",
      "Epoch [6/100], Training Loss: 937602.8413008708, Test Loss: 116936144.0\n",
      "Epoch [7/100], Training Loss: 858716.6691546709, Test Loss: 107695728.0\n",
      "Epoch [8/100], Training Loss: 787923.3261062733, Test Loss: 99494512.0\n",
      "Epoch [9/100], Training Loss: 725191.2175818968, Test Loss: 92329064.0\n",
      "Epoch [10/100], Training Loss: 670310.8151175878, Test Loss: 86102792.0\n",
      "Epoch [11/100], Training Loss: 622494.5434512174, Test Loss: 80663768.0\n",
      "Epoch [12/100], Training Loss: 580788.9510100113, Test Loss: 75889400.0\n",
      "Epoch [13/100], Training Loss: 544367.3045435697, Test Loss: 71680704.0\n",
      "Epoch [14/100], Training Loss: 512494.1163438185, Test Loss: 67958864.0\n",
      "Epoch [15/100], Training Loss: 484543.8028552811, Test Loss: 64670532.0\n",
      "Epoch [16/100], Training Loss: 460010.572240981, Test Loss: 61771980.0\n",
      "Epoch [17/100], Training Loss: 438405.8342515254, Test Loss: 59205472.0\n",
      "Epoch [18/100], Training Loss: 419218.88069427165, Test Loss: 56899852.0\n",
      "Epoch [19/100], Training Loss: 401923.3935785795, Test Loss: 54802256.0\n",
      "Epoch [20/100], Training Loss: 386092.84242639656, Test Loss: 52871712.0\n",
      "Epoch [21/100], Training Loss: 371437.7558201528, Test Loss: 51075876.0\n",
      "Epoch [22/100], Training Loss: 357773.25259167113, Test Loss: 49392864.0\n",
      "Epoch [23/100], Training Loss: 344960.5407854985, Test Loss: 47808272.0\n",
      "Epoch [24/100], Training Loss: 332907.08536224155, Test Loss: 46311712.0\n",
      "Epoch [25/100], Training Loss: 321536.8413008708, Test Loss: 44892208.0\n",
      "Epoch [26/100], Training Loss: 310761.3703572063, Test Loss: 43536532.0\n",
      "Epoch [27/100], Training Loss: 300524.56134115276, Test Loss: 42241740.0\n",
      "Epoch [28/100], Training Loss: 290794.02665718854, Test Loss: 41001344.0\n",
      "Epoch [29/100], Training Loss: 281547.07677270303, Test Loss: 39813504.0\n",
      "Epoch [30/100], Training Loss: 272766.10390379716, Test Loss: 38676176.0\n",
      "Epoch [31/100], Training Loss: 264417.9350749363, Test Loss: 37588716.0\n",
      "Epoch [32/100], Training Loss: 256472.23310230437, Test Loss: 36549880.0\n",
      "Epoch [33/100], Training Loss: 248923.37965760322, Test Loss: 35559780.0\n",
      "Epoch [34/100], Training Loss: 241754.6176766779, Test Loss: 34616884.0\n",
      "Epoch [35/100], Training Loss: 234941.26971151, Test Loss: 33717792.0\n",
      "Epoch [36/100], Training Loss: 228459.46910728037, Test Loss: 32859846.0\n",
      "Epoch [37/100], Training Loss: 222300.49736390024, Test Loss: 32042204.0\n",
      "Epoch [38/100], Training Loss: 216451.95820745217, Test Loss: 31264576.0\n",
      "Epoch [39/100], Training Loss: 210905.11584029382, Test Loss: 30526520.0\n",
      "Epoch [40/100], Training Loss: 205641.25413186423, Test Loss: 29826468.0\n",
      "Epoch [41/100], Training Loss: 200638.93522303182, Test Loss: 29161180.0\n",
      "Epoch [42/100], Training Loss: 195880.97349090694, Test Loss: 28527900.0\n",
      "Epoch [43/100], Training Loss: 191358.15813636634, Test Loss: 27925700.0\n",
      "Epoch [44/100], Training Loss: 187060.6786624015, Test Loss: 27352876.0\n",
      "Epoch [45/100], Training Loss: 182975.07093774065, Test Loss: 26808150.0\n",
      "Epoch [46/100], Training Loss: 179092.58841300872, Test Loss: 26290666.0\n",
      "Epoch [47/100], Training Loss: 175395.30324032938, Test Loss: 25798334.0\n",
      "Epoch [48/100], Training Loss: 171862.4424204727, Test Loss: 25330460.0\n",
      "Epoch [49/100], Training Loss: 168479.25490196078, Test Loss: 24884194.0\n",
      "Epoch [50/100], Training Loss: 165231.248948522, Test Loss: 24456758.0\n",
      "Epoch [51/100], Training Loss: 162103.75250281382, Test Loss: 24048082.0\n",
      "Epoch [52/100], Training Loss: 159091.6105977134, Test Loss: 23656748.0\n",
      "Epoch [53/100], Training Loss: 156192.93237959835, Test Loss: 23285660.0\n",
      "Epoch [54/100], Training Loss: 153400.2249274332, Test Loss: 22930352.0\n",
      "Epoch [55/100], Training Loss: 150706.51430602453, Test Loss: 22591832.0\n",
      "Epoch [56/100], Training Loss: 148104.7805224809, Test Loss: 22266530.0\n",
      "Epoch [57/100], Training Loss: 145599.24613470765, Test Loss: 21956236.0\n",
      "Epoch [58/100], Training Loss: 143181.53542444168, Test Loss: 21657518.0\n",
      "Epoch [59/100], Training Loss: 140841.68378650554, Test Loss: 21371688.0\n",
      "Epoch [60/100], Training Loss: 138576.9371186541, Test Loss: 21098046.0\n",
      "Epoch [61/100], Training Loss: 136386.14750311, Test Loss: 20837030.0\n",
      "Epoch [62/100], Training Loss: 134269.66530418815, Test Loss: 20587346.0\n",
      "Epoch [63/100], Training Loss: 132223.74965938038, Test Loss: 20346130.0\n",
      "Epoch [64/100], Training Loss: 130241.17535987204, Test Loss: 20116638.0\n",
      "Epoch [65/100], Training Loss: 128325.01241040223, Test Loss: 19895342.0\n",
      "Epoch [66/100], Training Loss: 126470.74385403708, Test Loss: 19684170.0\n",
      "Epoch [67/100], Training Loss: 124679.58922753391, Test Loss: 19481698.0\n",
      "Epoch [68/100], Training Loss: 122944.7729992299, Test Loss: 19286564.0\n",
      "Epoch [69/100], Training Loss: 121260.94508619158, Test Loss: 19099142.0\n",
      "Epoch [70/100], Training Loss: 119626.8865736627, Test Loss: 18920780.0\n",
      "Epoch [71/100], Training Loss: 118044.8035513299, Test Loss: 18746996.0\n",
      "Epoch [72/100], Training Loss: 116509.69114685149, Test Loss: 18580492.0\n",
      "Epoch [73/100], Training Loss: 115020.01054439903, Test Loss: 18422752.0\n",
      "Epoch [74/100], Training Loss: 113570.95826669036, Test Loss: 18267842.0\n",
      "Epoch [75/100], Training Loss: 112161.4483442924, Test Loss: 18123960.0\n",
      "Epoch [76/100], Training Loss: 110791.62771755228, Test Loss: 17979322.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [77/100], Training Loss: 109456.81520644511, Test Loss: 17847680.0\n",
      "Epoch [78/100], Training Loss: 108162.26075173271, Test Loss: 17713908.0\n",
      "Epoch [79/100], Training Loss: 106899.46730051537, Test Loss: 17590092.0\n",
      "Epoch [80/100], Training Loss: 105669.75311000533, Test Loss: 17467026.0\n",
      "Epoch [81/100], Training Loss: 104474.53918606718, Test Loss: 17353534.0\n",
      "Epoch [82/100], Training Loss: 103314.63636632901, Test Loss: 17242986.0\n",
      "Epoch [83/100], Training Loss: 102186.46952194776, Test Loss: 17139102.0\n",
      "Epoch [84/100], Training Loss: 101093.54351045554, Test Loss: 17037590.0\n",
      "Epoch [85/100], Training Loss: 100027.98781174101, Test Loss: 16942794.0\n",
      "Epoch [86/100], Training Loss: 98993.09851312127, Test Loss: 16848690.0\n",
      "Epoch [87/100], Training Loss: 97986.98108820568, Test Loss: 16763487.0\n",
      "Epoch [88/100], Training Loss: 97005.94730762395, Test Loss: 16675634.0\n",
      "Epoch [89/100], Training Loss: 96056.36727682009, Test Loss: 16600060.0\n",
      "Epoch [90/100], Training Loss: 95131.60031988626, Test Loss: 16519324.0\n",
      "Epoch [91/100], Training Loss: 94235.8448255435, Test Loss: 16452191.0\n",
      "Epoch [92/100], Training Loss: 93361.97778567621, Test Loss: 16378401.0\n",
      "Epoch [93/100], Training Loss: 92514.84101948937, Test Loss: 16317781.0\n",
      "Epoch [94/100], Training Loss: 91690.76082578047, Test Loss: 16247614.0\n",
      "Epoch [95/100], Training Loss: 90885.68584503287, Test Loss: 16194233.0\n",
      "Epoch [96/100], Training Loss: 90109.20641549671, Test Loss: 16128856.0\n",
      "Epoch [97/100], Training Loss: 89348.1209199692, Test Loss: 16076999.0\n",
      "Epoch [98/100], Training Loss: 88612.63688466322, Test Loss: 16017917.0\n",
      "Epoch [99/100], Training Loss: 87896.90388602571, Test Loss: 15969806.0\n",
      "Epoch [100/100], Training Loss: 87199.43856998993, Test Loss: 15914405.0\n",
      "Epoch [1/100], Training Loss: 1497715.1154552456, Test Loss: 298677504.0\n",
      "Epoch [2/100], Training Loss: 1466036.7011432971, Test Loss: 284326880.0\n",
      "Epoch [3/100], Training Loss: 1333274.8936674367, Test Loss: 244334192.0\n",
      "Epoch [4/100], Training Loss: 1095352.0199040342, Test Loss: 193578656.0\n",
      "Epoch [5/100], Training Loss: 880883.5388898762, Test Loss: 161871344.0\n",
      "Epoch [6/100], Training Loss: 769245.5404300693, Test Loss: 147390464.0\n",
      "Epoch [7/100], Training Loss: 710166.3920383863, Test Loss: 138086912.0\n",
      "Epoch [8/100], Training Loss: 665832.3947633434, Test Loss: 130415680.0\n",
      "Epoch [9/100], Training Loss: 627299.534624726, Test Loss: 123637488.0\n",
      "Epoch [10/100], Training Loss: 592449.4074995557, Test Loss: 117524848.0\n",
      "Epoch [11/100], Training Loss: 560544.5269829986, Test Loss: 111983896.0\n",
      "Epoch [12/100], Training Loss: 531232.9821693028, Test Loss: 106930272.0\n",
      "Epoch [13/100], Training Loss: 503826.26479473966, Test Loss: 102199272.0\n",
      "Epoch [14/100], Training Loss: 477879.3358213376, Test Loss: 97814200.0\n",
      "Epoch [15/100], Training Loss: 454042.4735501451, Test Loss: 93920416.0\n",
      "Epoch [16/100], Training Loss: 432897.1553817902, Test Loss: 90503144.0\n",
      "Epoch [17/100], Training Loss: 414098.6370475683, Test Loss: 87450640.0\n",
      "Epoch [18/100], Training Loss: 397245.09306320717, Test Loss: 84692288.0\n",
      "Epoch [19/100], Training Loss: 382021.5763284166, Test Loss: 82178392.0\n",
      "Epoch [20/100], Training Loss: 368256.33540667023, Test Loss: 79879160.0\n",
      "Epoch [21/100], Training Loss: 355784.36692139093, Test Loss: 77768816.0\n",
      "Epoch [22/100], Training Loss: 344488.7943842189, Test Loss: 75830688.0\n",
      "Epoch [23/100], Training Loss: 334258.0799715657, Test Loss: 74047280.0\n",
      "Epoch [24/100], Training Loss: 324971.32219655236, Test Loss: 72395984.0\n",
      "Epoch [25/100], Training Loss: 316514.1001125526, Test Loss: 70861208.0\n",
      "Epoch [26/100], Training Loss: 308771.2250459096, Test Loss: 69423504.0\n",
      "Epoch [27/100], Training Loss: 301647.42947692674, Test Loss: 68074960.0\n",
      "Epoch [28/100], Training Loss: 295059.2057046383, Test Loss: 66802340.0\n",
      "Epoch [29/100], Training Loss: 288923.81526568334, Test Loss: 65595700.0\n",
      "Epoch [30/100], Training Loss: 283167.2162786565, Test Loss: 64442460.0\n",
      "Epoch [31/100], Training Loss: 277731.76851934125, Test Loss: 63337652.0\n",
      "Epoch [32/100], Training Loss: 272554.93547664536, Test Loss: 62273916.0\n",
      "Epoch [33/100], Training Loss: 267599.4334101926, Test Loss: 61244404.0\n",
      "Epoch [34/100], Training Loss: 262832.7253055395, Test Loss: 60246768.0\n",
      "Epoch [35/100], Training Loss: 258225.487811741, Test Loss: 59276724.0\n",
      "Epoch [36/100], Training Loss: 253750.7814591849, Test Loss: 58331012.0\n",
      "Epoch [37/100], Training Loss: 249384.6707096736, Test Loss: 57407408.0\n",
      "Epoch [38/100], Training Loss: 245114.027693857, Test Loss: 56504548.0\n",
      "Epoch [39/100], Training Loss: 240940.50724927432, Test Loss: 55619268.0\n",
      "Epoch [40/100], Training Loss: 236852.04128902315, Test Loss: 54753808.0\n",
      "Epoch [41/100], Training Loss: 232836.91046146557, Test Loss: 53905108.0\n",
      "Epoch [42/100], Training Loss: 228878.07777975238, Test Loss: 53075116.0\n",
      "Epoch [43/100], Training Loss: 224997.8836561815, Test Loss: 52266404.0\n",
      "Epoch [44/100], Training Loss: 221209.08023813754, Test Loss: 51478048.0\n",
      "Epoch [45/100], Training Loss: 217524.6871186541, Test Loss: 50707588.0\n",
      "Epoch [46/100], Training Loss: 213932.74678632783, Test Loss: 49959484.0\n",
      "Epoch [47/100], Training Loss: 210429.88839523724, Test Loss: 49232112.0\n",
      "Epoch [48/100], Training Loss: 207023.1485249689, Test Loss: 48526428.0\n",
      "Epoch [49/100], Training Loss: 203702.1660742847, Test Loss: 47840156.0\n",
      "Epoch [50/100], Training Loss: 200472.26726793437, Test Loss: 47171780.0\n",
      "Epoch [51/100], Training Loss: 197332.1275102186, Test Loss: 46522828.0\n",
      "Epoch [52/100], Training Loss: 194290.95598972216, Test Loss: 45892580.0\n",
      "Epoch [53/100], Training Loss: 191343.84301137374, Test Loss: 45278848.0\n",
      "Epoch [54/100], Training Loss: 188492.21333525857, Test Loss: 44683184.0\n",
      "Epoch [55/100], Training Loss: 185738.42137240092, Test Loss: 44106560.0\n",
      "Epoch [56/100], Training Loss: 183076.42256086724, Test Loss: 43546000.0\n",
      "Epoch [57/100], Training Loss: 180503.83458288905, Test Loss: 43002104.0\n",
      "Epoch [58/100], Training Loss: 178017.6837994639, Test Loss: 42474068.0\n",
      "Epoch [59/100], Training Loss: 175614.31477696818, Test Loss: 41960400.0\n",
      "Epoch [60/100], Training Loss: 173287.99692609295, Test Loss: 41464872.0\n",
      "Epoch [61/100], Training Loss: 171039.35350301376, Test Loss: 40985276.0\n",
      "Epoch [62/100], Training Loss: 168861.93154170184, Test Loss: 40518348.0\n",
      "Epoch [63/100], Training Loss: 166743.97281360146, Test Loss: 40063380.0\n",
      "Epoch [64/100], Training Loss: 164675.50981826833, Test Loss: 39615012.0\n",
      "Epoch [65/100], Training Loss: 162654.10769481037, Test Loss: 39176772.0\n",
      "Epoch [66/100], Training Loss: 160684.17449226964, Test Loss: 38752684.0\n",
      "Epoch [67/100], Training Loss: 158765.95627157003, Test Loss: 38341952.0\n",
      "Epoch [68/100], Training Loss: 156900.96642190794, Test Loss: 37940172.0\n",
      "Epoch [69/100], Training Loss: 155091.9800972675, Test Loss: 37546772.0\n",
      "Epoch [70/100], Training Loss: 153343.14759371727, Test Loss: 37168716.0\n",
      "Epoch [71/100], Training Loss: 151657.75728369495, Test Loss: 36802928.0\n",
      "Epoch [72/100], Training Loss: 150028.32945693383, Test Loss: 36450236.0\n",
      "Epoch [73/100], Training Loss: 148447.01622397685, Test Loss: 36103956.0\n",
      "Epoch [74/100], Training Loss: 146903.5605733702, Test Loss: 35764768.0\n",
      "Epoch [75/100], Training Loss: 145380.32523019594, Test Loss: 35425940.0\n",
      "Epoch [76/100], Training Loss: 143883.90431735382, Test Loss: 35096996.0\n",
      "Epoch [77/100], Training Loss: 142429.6321152942, Test Loss: 34774588.0\n",
      "Epoch [78/100], Training Loss: 141011.49626105162, Test Loss: 34461556.0\n",
      "Epoch [79/100], Training Loss: 139622.62377289002, Test Loss: 34152072.0\n",
      "Epoch [80/100], Training Loss: 138254.2733798816, Test Loss: 33848712.0\n",
      "Epoch [81/100], Training Loss: 136927.448636781, Test Loss: 33555232.0\n",
      "Epoch [82/100], Training Loss: 135647.5499047098, Test Loss: 33273808.0\n",
      "Epoch [83/100], Training Loss: 134412.9875192524, Test Loss: 33003730.0\n",
      "Epoch [84/100], Training Loss: 133213.82819997333, Test Loss: 32740678.0\n",
      "Epoch [85/100], Training Loss: 132046.98753174796, Test Loss: 32480540.0\n",
      "Epoch [86/100], Training Loss: 130905.64160289304, Test Loss: 32227102.0\n",
      "Epoch [87/100], Training Loss: 129788.62899950388, Test Loss: 31979504.0\n",
      "Epoch [88/100], Training Loss: 128698.13748907796, Test Loss: 31740218.0\n",
      "Epoch [89/100], Training Loss: 127633.23911451854, Test Loss: 31507286.0\n",
      "Epoch [90/100], Training Loss: 126594.81428269949, Test Loss: 31283774.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [91/100], Training Loss: 125577.43740281233, Test Loss: 31065924.0\n",
      "Epoch [92/100], Training Loss: 124580.79612989456, Test Loss: 30855026.0\n",
      "Epoch [93/100], Training Loss: 123603.54872665645, Test Loss: 30646868.0\n",
      "Epoch [94/100], Training Loss: 122643.99177375821, Test Loss: 30444818.0\n",
      "Epoch [95/100], Training Loss: 121700.51181709467, Test Loss: 30245032.0\n",
      "Epoch [96/100], Training Loss: 120774.74643089864, Test Loss: 30051834.0\n",
      "Epoch [97/100], Training Loss: 119864.680559875, Test Loss: 29861124.0\n",
      "Epoch [98/100], Training Loss: 118973.14587961318, Test Loss: 29677876.0\n",
      "Epoch [99/100], Training Loss: 118095.4510535143, Test Loss: 29495134.0\n",
      "Epoch [100/100], Training Loss: 117224.05296172472, Test Loss: 29316088.0\n",
      "Epoch [1/100], Training Loss: 1164563.9447900006, Test Loss: 299529952.0\n",
      "Epoch [2/100], Training Loss: 1153484.0296190984, Test Loss: 292794944.0\n",
      "Epoch [3/100], Training Loss: 1100468.8167762572, Test Loss: 270888352.0\n",
      "Epoch [4/100], Training Loss: 981833.5373496831, Test Loss: 232651520.0\n",
      "Epoch [5/100], Training Loss: 821180.5068420117, Test Loss: 191513376.0\n",
      "Epoch [6/100], Training Loss: 683937.1572774125, Test Loss: 163974336.0\n",
      "Epoch [7/100], Training Loss: 605083.8559327055, Test Loss: 149971552.0\n",
      "Epoch [8/100], Training Loss: 562429.9096025117, Test Loss: 141421040.0\n",
      "Epoch [9/100], Training Loss: 532277.0039689592, Test Loss: 134611104.0\n",
      "Epoch [10/100], Training Loss: 506591.3711273029, Test Loss: 128564960.0\n",
      "Epoch [11/100], Training Loss: 483198.6439191991, Test Loss: 122981632.0\n",
      "Epoch [12/100], Training Loss: 461325.1075173272, Test Loss: 117737792.0\n",
      "Epoch [13/100], Training Loss: 440642.72472009953, Test Loss: 112781712.0\n",
      "Epoch [14/100], Training Loss: 421025.9555713524, Test Loss: 108097616.0\n",
      "Epoch [15/100], Training Loss: 402453.89325276937, Test Loss: 103686120.0\n",
      "Epoch [16/100], Training Loss: 384939.60263017594, Test Loss: 99547264.0\n",
      "Epoch [17/100], Training Loss: 368497.14448196196, Test Loss: 95685136.0\n",
      "Epoch [18/100], Training Loss: 353146.9898702683, Test Loss: 92099896.0\n",
      "Epoch [19/100], Training Loss: 338887.7118654108, Test Loss: 88780864.0\n",
      "Epoch [20/100], Training Loss: 325681.95723002194, Test Loss: 85709944.0\n",
      "Epoch [21/100], Training Loss: 313467.60582903854, Test Loss: 82865608.0\n",
      "Epoch [22/100], Training Loss: 302168.7915407855, Test Loss: 80225352.0\n",
      "Epoch [23/100], Training Loss: 291705.5481310349, Test Loss: 77768656.0\n",
      "Epoch [24/100], Training Loss: 281998.1958414786, Test Loss: 75476168.0\n",
      "Epoch [25/100], Training Loss: 272973.57289260114, Test Loss: 73331016.0\n",
      "Epoch [26/100], Training Loss: 264566.4913216042, Test Loss: 71318824.0\n",
      "Epoch [27/100], Training Loss: 256719.74989633315, Test Loss: 69427968.0\n",
      "Epoch [28/100], Training Loss: 249379.21485693977, Test Loss: 67648848.0\n",
      "Epoch [29/100], Training Loss: 242499.27054084474, Test Loss: 65973716.0\n",
      "Epoch [30/100], Training Loss: 236049.03275872284, Test Loss: 64398172.0\n",
      "Epoch [31/100], Training Loss: 229992.4816065399, Test Loss: 62913208.0\n",
      "Epoch [32/100], Training Loss: 224292.0479829394, Test Loss: 61511452.0\n",
      "Epoch [33/100], Training Loss: 218912.42473787098, Test Loss: 60185184.0\n",
      "Epoch [34/100], Training Loss: 213823.61897991825, Test Loss: 58927740.0\n",
      "Epoch [35/100], Training Loss: 208989.8395829631, Test Loss: 57729960.0\n",
      "Epoch [36/100], Training Loss: 204379.84941650377, Test Loss: 56586456.0\n",
      "Epoch [37/100], Training Loss: 199972.63811385582, Test Loss: 55491308.0\n",
      "Epoch [38/100], Training Loss: 195746.8099046265, Test Loss: 54439392.0\n",
      "Epoch [39/100], Training Loss: 191686.58183756887, Test Loss: 53427644.0\n",
      "Epoch [40/100], Training Loss: 187776.26147740064, Test Loss: 52452140.0\n",
      "Epoch [41/100], Training Loss: 184001.87204549494, Test Loss: 51508288.0\n",
      "Epoch [42/100], Training Loss: 180349.08417747763, Test Loss: 50593368.0\n",
      "Epoch [43/100], Training Loss: 176808.49487589597, Test Loss: 49706196.0\n",
      "Epoch [44/100], Training Loss: 173378.15390083526, Test Loss: 48845816.0\n",
      "Epoch [45/100], Training Loss: 170055.19305728332, Test Loss: 48011028.0\n",
      "Epoch [46/100], Training Loss: 166832.57164859903, Test Loss: 47199556.0\n",
      "Epoch [47/100], Training Loss: 163702.57899413543, Test Loss: 46408872.0\n",
      "Epoch [48/100], Training Loss: 160658.89982820922, Test Loss: 45637012.0\n",
      "Epoch [49/100], Training Loss: 157697.10254131863, Test Loss: 44883916.0\n",
      "Epoch [50/100], Training Loss: 154818.48480540252, Test Loss: 44150344.0\n",
      "Epoch [51/100], Training Loss: 152023.67122800782, Test Loss: 43435736.0\n",
      "Epoch [52/100], Training Loss: 149308.04122978495, Test Loss: 42738580.0\n",
      "Epoch [53/100], Training Loss: 146666.08228185534, Test Loss: 42057812.0\n",
      "Epoch [54/100], Training Loss: 144096.6677329542, Test Loss: 41393632.0\n",
      "Epoch [55/100], Training Loss: 141599.35560689532, Test Loss: 40745384.0\n",
      "Epoch [56/100], Training Loss: 139170.17842544874, Test Loss: 40112552.0\n",
      "Epoch [57/100], Training Loss: 136807.4316095018, Test Loss: 39495336.0\n",
      "Epoch [58/100], Training Loss: 134506.11178247735, Test Loss: 38892472.0\n",
      "Epoch [59/100], Training Loss: 132262.93217226467, Test Loss: 38303616.0\n",
      "Epoch [60/100], Training Loss: 130079.71547894082, Test Loss: 37728520.0\n",
      "Epoch [61/100], Training Loss: 127955.40631479178, Test Loss: 37168128.0\n",
      "Epoch [62/100], Training Loss: 125888.81671701913, Test Loss: 36623188.0\n",
      "Epoch [63/100], Training Loss: 123878.2803151472, Test Loss: 36092364.0\n",
      "Epoch [64/100], Training Loss: 121922.55464723654, Test Loss: 35575892.0\n",
      "Epoch [65/100], Training Loss: 120022.12688821752, Test Loss: 35072804.0\n",
      "Epoch [66/100], Training Loss: 118172.75605710562, Test Loss: 34582844.0\n",
      "Epoch [67/100], Training Loss: 116373.02351756413, Test Loss: 34105624.0\n",
      "Epoch [68/100], Training Loss: 114621.3818494165, Test Loss: 33640220.0\n",
      "Epoch [69/100], Training Loss: 112913.84568449737, Test Loss: 33185646.0\n",
      "Epoch [70/100], Training Loss: 111248.87376340265, Test Loss: 32742510.0\n",
      "Epoch [71/100], Training Loss: 109627.09839464487, Test Loss: 32310898.0\n",
      "Epoch [72/100], Training Loss: 108047.89200876726, Test Loss: 31891592.0\n",
      "Epoch [73/100], Training Loss: 106510.45897754873, Test Loss: 31483256.0\n",
      "Epoch [74/100], Training Loss: 105015.97535691013, Test Loss: 31086196.0\n",
      "Epoch [75/100], Training Loss: 103562.76044073218, Test Loss: 30699226.0\n",
      "Epoch [76/100], Training Loss: 102147.51004087436, Test Loss: 30322844.0\n",
      "Epoch [77/100], Training Loss: 100770.332030093, Test Loss: 29956394.0\n",
      "Epoch [78/100], Training Loss: 99428.15940998757, Test Loss: 29599794.0\n",
      "Epoch [79/100], Training Loss: 98122.21130264795, Test Loss: 29252636.0\n",
      "Epoch [80/100], Training Loss: 96850.62217878088, Test Loss: 28915250.0\n",
      "Epoch [81/100], Training Loss: 95611.90634441088, Test Loss: 28587044.0\n",
      "Epoch [82/100], Training Loss: 94405.61287838398, Test Loss: 28267330.0\n",
      "Epoch [83/100], Training Loss: 93230.69829986375, Test Loss: 27956244.0\n",
      "Epoch [84/100], Training Loss: 92085.59504768674, Test Loss: 27653510.0\n",
      "Epoch [85/100], Training Loss: 90968.3540667022, Test Loss: 27358080.0\n",
      "Epoch [86/100], Training Loss: 89878.04774598661, Test Loss: 27071026.0\n",
      "Epoch [87/100], Training Loss: 88814.86665481902, Test Loss: 26792226.0\n",
      "Epoch [88/100], Training Loss: 87779.48125111072, Test Loss: 26521090.0\n",
      "Epoch [89/100], Training Loss: 86769.27895266868, Test Loss: 26258132.0\n",
      "Epoch [90/100], Training Loss: 85783.2969018423, Test Loss: 26001952.0\n",
      "Epoch [91/100], Training Loss: 84819.45459392216, Test Loss: 25752500.0\n",
      "Epoch [92/100], Training Loss: 83877.94567857354, Test Loss: 25509162.0\n",
      "Epoch [93/100], Training Loss: 82956.31816835495, Test Loss: 25271938.0\n",
      "Epoch [94/100], Training Loss: 82053.90699603104, Test Loss: 25040934.0\n",
      "Epoch [95/100], Training Loss: 81170.69142823292, Test Loss: 24815796.0\n",
      "Epoch [96/100], Training Loss: 80305.39440791422, Test Loss: 24596288.0\n",
      "Epoch [97/100], Training Loss: 79458.08909424797, Test Loss: 24382710.0\n",
      "Epoch [98/100], Training Loss: 78629.16817724069, Test Loss: 24174718.0\n",
      "Epoch [99/100], Training Loss: 77816.93341626681, Test Loss: 23971720.0\n",
      "Epoch [100/100], Training Loss: 77020.87761388543, Test Loss: 23773940.0\n",
      "Epoch [1/100], Training Loss: 582417.0662875422, Test Loss: 300018048.0\n",
      "Epoch [2/100], Training Loss: 581803.0126177359, Test Loss: 299291648.0\n",
      "Epoch [3/100], Training Loss: 578809.9671820389, Test Loss: 296621760.0\n",
      "Epoch [4/100], Training Loss: 570496.0435993129, Test Loss: 290382656.0\n",
      "Epoch [5/100], Training Loss: 553846.0167051714, Test Loss: 279212480.0\n",
      "Epoch [6/100], Training Loss: 526910.9839464487, Test Loss: 262592432.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100], Training Loss: 489899.84479592444, Test Loss: 241353328.0\n",
      "Epoch [8/100], Training Loss: 445893.8574728985, Test Loss: 217834096.0\n",
      "Epoch [9/100], Training Loss: 400521.5766838457, Test Loss: 195332064.0\n",
      "Epoch [10/100], Training Loss: 360110.7033943487, Test Loss: 176782160.0\n",
      "Epoch [11/100], Training Loss: 328801.69895148394, Test Loss: 163309968.0\n",
      "Epoch [12/100], Training Loss: 306739.1751673479, Test Loss: 154051920.0\n",
      "Epoch [13/100], Training Loss: 291296.24643089867, Test Loss: 147415856.0\n",
      "Epoch [14/100], Training Loss: 279649.4961198981, Test Loss: 142182448.0\n",
      "Epoch [15/100], Training Loss: 269999.0166459333, Test Loss: 137690416.0\n",
      "Epoch [16/100], Training Loss: 261443.7893489722, Test Loss: 133629552.0\n",
      "Epoch [17/100], Training Loss: 253565.17030981576, Test Loss: 129853688.0\n",
      "Epoch [18/100], Training Loss: 246157.1527753095, Test Loss: 126286856.0\n",
      "Epoch [19/100], Training Loss: 239106.4851608317, Test Loss: 122885296.0\n",
      "Epoch [20/100], Training Loss: 232345.4980155204, Test Loss: 119622224.0\n",
      "Epoch [21/100], Training Loss: 225832.57058231148, Test Loss: 116481216.0\n",
      "Epoch [22/100], Training Loss: 219542.5273384278, Test Loss: 113452464.0\n",
      "Epoch [23/100], Training Loss: 213460.9833540667, Test Loss: 110530552.0\n",
      "Epoch [24/100], Training Loss: 207580.85137136426, Test Loss: 107713280.0\n",
      "Epoch [25/100], Training Loss: 201900.90065754397, Test Loss: 105000312.0\n",
      "Epoch [26/100], Training Loss: 196421.91055032285, Test Loss: 102391768.0\n",
      "Epoch [27/100], Training Loss: 191144.8911794325, Test Loss: 99887480.0\n",
      "Epoch [28/100], Training Loss: 186070.56311829868, Test Loss: 97486224.0\n",
      "Epoch [29/100], Training Loss: 181200.15733665068, Test Loss: 95187448.0\n",
      "Epoch [30/100], Training Loss: 176534.8777916, Test Loss: 92990848.0\n",
      "Epoch [31/100], Training Loss: 172074.51359516615, Test Loss: 90895016.0\n",
      "Epoch [32/100], Training Loss: 167817.99774894852, Test Loss: 88898264.0\n",
      "Epoch [33/100], Training Loss: 163762.284461821, Test Loss: 86997808.0\n",
      "Epoch [34/100], Training Loss: 159902.3576802322, Test Loss: 85189640.0\n",
      "Epoch [35/100], Training Loss: 156231.24577927848, Test Loss: 83469064.0\n",
      "Epoch [36/100], Training Loss: 152740.80706119305, Test Loss: 81831136.0\n",
      "Epoch [37/100], Training Loss: 149421.46602689414, Test Loss: 80270272.0\n",
      "Epoch [38/100], Training Loss: 146263.09484035306, Test Loss: 78780856.0\n",
      "Epoch [39/100], Training Loss: 143255.2671050293, Test Loss: 77357496.0\n",
      "Epoch [40/100], Training Loss: 140387.06806468812, Test Loss: 75994976.0\n",
      "Epoch [41/100], Training Loss: 137648.89970973282, Test Loss: 74688624.0\n",
      "Epoch [42/100], Training Loss: 135031.73508678397, Test Loss: 73434488.0\n",
      "Epoch [43/100], Training Loss: 132526.67685563653, Test Loss: 72228632.0\n",
      "Epoch [44/100], Training Loss: 130125.36247852615, Test Loss: 71067784.0\n",
      "Epoch [45/100], Training Loss: 127821.10988685505, Test Loss: 69949384.0\n",
      "Epoch [46/100], Training Loss: 125607.27468751851, Test Loss: 68870736.0\n",
      "Epoch [47/100], Training Loss: 123478.04087435578, Test Loss: 67829488.0\n",
      "Epoch [48/100], Training Loss: 121427.76920798532, Test Loss: 66823176.0\n",
      "Epoch [49/100], Training Loss: 119451.70760026065, Test Loss: 65850124.0\n",
      "Epoch [50/100], Training Loss: 117545.53521710799, Test Loss: 64908624.0\n",
      "Epoch [51/100], Training Loss: 115705.3172205438, Test Loss: 63998128.0\n",
      "Epoch [52/100], Training Loss: 113928.40661098275, Test Loss: 63117268.0\n",
      "Epoch [53/100], Training Loss: 112209.84586221195, Test Loss: 62262580.0\n",
      "Epoch [54/100], Training Loss: 110544.0978615011, Test Loss: 61432464.0\n",
      "Epoch [55/100], Training Loss: 108928.01883774658, Test Loss: 60625744.0\n",
      "Epoch [56/100], Training Loss: 107358.01540193116, Test Loss: 59840704.0\n",
      "Epoch [57/100], Training Loss: 105830.67958059357, Test Loss: 59075928.0\n",
      "Epoch [58/100], Training Loss: 104342.99330608376, Test Loss: 58330704.0\n",
      "Epoch [59/100], Training Loss: 102892.35151945974, Test Loss: 57603776.0\n",
      "Epoch [60/100], Training Loss: 101476.28588353771, Test Loss: 56893904.0\n",
      "Epoch [61/100], Training Loss: 100092.35531070434, Test Loss: 56199544.0\n",
      "Epoch [62/100], Training Loss: 98738.011136781, Test Loss: 55520124.0\n",
      "Epoch [63/100], Training Loss: 97413.04176292874, Test Loss: 54855112.0\n",
      "Epoch [64/100], Training Loss: 96116.34180439547, Test Loss: 54203580.0\n",
      "Epoch [65/100], Training Loss: 94845.96113974291, Test Loss: 53564608.0\n",
      "Epoch [66/100], Training Loss: 93599.71695989574, Test Loss: 52937648.0\n",
      "Epoch [67/100], Training Loss: 92376.9113204194, Test Loss: 52322136.0\n",
      "Epoch [68/100], Training Loss: 91176.96510870209, Test Loss: 51717896.0\n",
      "Epoch [69/100], Training Loss: 89999.21331674664, Test Loss: 51124280.0\n",
      "Epoch [70/100], Training Loss: 88843.752858243, Test Loss: 50541608.0\n",
      "Epoch [71/100], Training Loss: 87709.92749244712, Test Loss: 49969592.0\n",
      "Epoch [72/100], Training Loss: 86597.40560393341, Test Loss: 49408260.0\n",
      "Epoch [73/100], Training Loss: 85506.29251821575, Test Loss: 48856980.0\n",
      "Epoch [74/100], Training Loss: 84436.24074403175, Test Loss: 48315340.0\n",
      "Epoch [75/100], Training Loss: 83386.11302647948, Test Loss: 47782560.0\n",
      "Epoch [76/100], Training Loss: 82355.58331852379, Test Loss: 47258972.0\n",
      "Epoch [77/100], Training Loss: 81344.321189503, Test Loss: 46744116.0\n",
      "Epoch [78/100], Training Loss: 80351.65772169895, Test Loss: 46237576.0\n",
      "Epoch [79/100], Training Loss: 79377.07872756354, Test Loss: 45738872.0\n",
      "Epoch [80/100], Training Loss: 78420.10366684438, Test Loss: 45248356.0\n",
      "Epoch [81/100], Training Loss: 77480.16290504117, Test Loss: 44765624.0\n",
      "Epoch [82/100], Training Loss: 76557.39778449143, Test Loss: 44290512.0\n",
      "Epoch [83/100], Training Loss: 75651.18239440791, Test Loss: 43822864.0\n",
      "Epoch [84/100], Training Loss: 74761.08583614715, Test Loss: 43362460.0\n",
      "Epoch [85/100], Training Loss: 73886.47817072448, Test Loss: 42909156.0\n",
      "Epoch [86/100], Training Loss: 73026.82495112848, Test Loss: 42462772.0\n",
      "Epoch [87/100], Training Loss: 72182.30045613412, Test Loss: 42023000.0\n",
      "Epoch [88/100], Training Loss: 71352.70066939162, Test Loss: 41589564.0\n",
      "Epoch [89/100], Training Loss: 70537.40631479178, Test Loss: 41162408.0\n",
      "Epoch [90/100], Training Loss: 69736.3577987086, Test Loss: 40742104.0\n",
      "Epoch [91/100], Training Loss: 68949.64397843729, Test Loss: 40328488.0\n",
      "Epoch [92/100], Training Loss: 68176.90160535513, Test Loss: 39921192.0\n",
      "Epoch [93/100], Training Loss: 67417.54125940407, Test Loss: 39520220.0\n",
      "Epoch [94/100], Training Loss: 66671.04626503169, Test Loss: 39125252.0\n",
      "Epoch [95/100], Training Loss: 65937.06877554648, Test Loss: 38736252.0\n",
      "Epoch [96/100], Training Loss: 65215.20561578106, Test Loss: 38353008.0\n",
      "Epoch [97/100], Training Loss: 64505.94147266157, Test Loss: 37975528.0\n",
      "Epoch [98/100], Training Loss: 63808.8118002488, Test Loss: 37604156.0\n",
      "Epoch [99/100], Training Loss: 63123.44422723772, Test Loss: 37238928.0\n",
      "Epoch [100/100], Training Loss: 62449.707718737045, Test Loss: 36879680.0\n",
      "Epoch [1/100], Training Loss: 7304574.887625141, Test Loss: 151305744.0\n",
      "Epoch [2/100], Training Loss: 4150514.7044606362, Test Loss: 120494792.0\n",
      "Epoch [3/100], Training Loss: 3358049.874533499, Test Loss: 99427568.0\n",
      "Epoch [4/100], Training Loss: 2772831.7824181034, Test Loss: 83845304.0\n",
      "Epoch [5/100], Training Loss: 2331700.2864759197, Test Loss: 72017904.0\n",
      "Epoch [6/100], Training Loss: 2002058.2549019607, Test Loss: 63157864.0\n",
      "Epoch [7/100], Training Loss: 1760505.242610035, Test Loss: 56672832.0\n",
      "Epoch [8/100], Training Loss: 1583886.850719744, Test Loss: 51806324.0\n",
      "Epoch [9/100], Training Loss: 1447243.6262958355, Test Loss: 47870504.0\n",
      "Epoch [10/100], Training Loss: 1333481.4869083585, Test Loss: 44498896.0\n",
      "Epoch [11/100], Training Loss: 1234798.71912209, Test Loss: 41508824.0\n",
      "Epoch [12/100], Training Loss: 1147400.8575025175, Test Loss: 38813472.0\n",
      "Epoch [13/100], Training Loss: 1069656.6766038742, Test Loss: 36368260.0\n",
      "Epoch [14/100], Training Loss: 1000504.0999792666, Test Loss: 34160492.0\n",
      "Epoch [15/100], Training Loss: 938966.8938303418, Test Loss: 32187104.0\n",
      "Epoch [16/100], Training Loss: 884309.8596647118, Test Loss: 30429256.0\n",
      "Epoch [17/100], Training Loss: 835674.984524021, Test Loss: 28864602.0\n",
      "Epoch [18/100], Training Loss: 792526.756767964, Test Loss: 27471500.0\n",
      "Epoch [19/100], Training Loss: 754249.6965671465, Test Loss: 26223808.0\n",
      "Epoch [20/100], Training Loss: 720200.6772851135, Test Loss: 25111318.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/100], Training Loss: 689639.4263669214, Test Loss: 24115892.0\n",
      "Epoch [22/100], Training Loss: 661868.2505775724, Test Loss: 23214110.0\n",
      "Epoch [23/100], Training Loss: 636467.8597017357, Test Loss: 22401528.0\n",
      "Epoch [24/100], Training Loss: 613094.9443901428, Test Loss: 21667670.0\n",
      "Epoch [25/100], Training Loss: 591556.0183416266, Test Loss: 21010628.0\n",
      "Epoch [26/100], Training Loss: 571667.277893786, Test Loss: 20414890.0\n",
      "Epoch [27/100], Training Loss: 553299.4624800071, Test Loss: 19885236.0\n",
      "Epoch [28/100], Training Loss: 536305.4976452817, Test Loss: 19403316.0\n",
      "Epoch [29/100], Training Loss: 520550.6455778686, Test Loss: 18974560.0\n",
      "Epoch [30/100], Training Loss: 505906.03015224217, Test Loss: 18576132.0\n",
      "Epoch [31/100], Training Loss: 492256.85722113616, Test Loss: 18224612.0\n",
      "Epoch [32/100], Training Loss: 479496.696648599, Test Loss: 17886686.0\n",
      "Epoch [33/100], Training Loss: 467574.6872075114, Test Loss: 17584356.0\n",
      "Epoch [34/100], Training Loss: 456402.8391460814, Test Loss: 17294786.0\n",
      "Epoch [35/100], Training Loss: 445969.4102985605, Test Loss: 17040512.0\n",
      "Epoch [36/100], Training Loss: 436172.9621023636, Test Loss: 16800514.0\n",
      "Epoch [37/100], Training Loss: 426986.7419065814, Test Loss: 16586791.0\n",
      "Epoch [38/100], Training Loss: 418346.72353533556, Test Loss: 16387918.0\n",
      "Epoch [39/100], Training Loss: 410225.8502014099, Test Loss: 16228135.0\n",
      "Epoch [40/100], Training Loss: 402614.51794176886, Test Loss: 16054293.0\n",
      "Epoch [41/100], Training Loss: 395430.22465345653, Test Loss: 15925789.0\n",
      "Epoch [42/100], Training Loss: 388648.3474542385, Test Loss: 15789336.0\n",
      "Epoch [43/100], Training Loss: 382251.64862863574, Test Loss: 15692056.0\n",
      "Epoch [44/100], Training Loss: 376234.7814851016, Test Loss: 15579936.0\n",
      "Epoch [45/100], Training Loss: 370499.6827498371, Test Loss: 15501426.0\n",
      "Epoch [46/100], Training Loss: 365064.75633848703, Test Loss: 15415281.0\n",
      "Epoch [47/100], Training Loss: 359915.0508189681, Test Loss: 15350219.0\n",
      "Epoch [48/100], Training Loss: 354964.51359516615, Test Loss: 15285231.0\n",
      "Epoch [49/100], Training Loss: 350291.5166163142, Test Loss: 15234809.0\n",
      "Epoch [50/100], Training Loss: 345844.7238463361, Test Loss: 15150494.0\n",
      "Epoch [51/100], Training Loss: 341587.64460043836, Test Loss: 15099427.0\n",
      "Epoch [52/100], Training Loss: 337619.31829423615, Test Loss: 15020642.0\n",
      "Epoch [53/100], Training Loss: 333820.4388143475, Test Loss: 14982614.0\n",
      "Epoch [54/100], Training Loss: 330243.9880264795, Test Loss: 14889526.0\n",
      "Epoch [55/100], Training Loss: 326824.5624222499, Test Loss: 14889725.0\n",
      "Epoch [56/100], Training Loss: 323613.5425626444, Test Loss: 14802621.0\n",
      "Epoch [57/100], Training Loss: 320484.6372178781, Test Loss: 14791561.0\n",
      "Epoch [58/100], Training Loss: 317561.1162549612, Test Loss: 14710269.0\n",
      "Epoch [59/100], Training Loss: 314759.4306579883, Test Loss: 14705603.0\n",
      "Epoch [60/100], Training Loss: 312152.8345180973, Test Loss: 14634491.0\n",
      "Epoch [61/100], Training Loss: 309592.78102230316, Test Loss: 14624315.0\n",
      "Epoch [62/100], Training Loss: 307243.4234198211, Test Loss: 14565329.0\n",
      "Epoch [63/100], Training Loss: 304970.01409128605, Test Loss: 14572248.0\n",
      "Epoch [64/100], Training Loss: 302789.0843996209, Test Loss: 14517695.0\n",
      "Epoch [65/100], Training Loss: 300685.3939673301, Test Loss: 14519878.0\n",
      "Epoch [66/100], Training Loss: 298709.01453557255, Test Loss: 14469694.0\n",
      "Epoch [67/100], Training Loss: 296787.5509152301, Test Loss: 14474026.0\n",
      "Epoch [68/100], Training Loss: 294942.9914326758, Test Loss: 14476244.0\n",
      "Epoch [69/100], Training Loss: 293170.34638795094, Test Loss: 14446959.0\n",
      "Epoch [70/100], Training Loss: 291320.3668325336, Test Loss: 14461389.0\n",
      "Epoch [71/100], Training Loss: 289560.81573958887, Test Loss: 14446762.0\n",
      "Epoch [72/100], Training Loss: 287859.11729903443, Test Loss: 14413565.0\n",
      "Epoch [73/100], Training Loss: 286203.676822315, Test Loss: 14447233.0\n",
      "Epoch [74/100], Training Loss: 284606.71824092173, Test Loss: 14412296.0\n",
      "Epoch [75/100], Training Loss: 283050.0045391268, Test Loss: 14393270.0\n",
      "Epoch [76/100], Training Loss: 281541.11904285883, Test Loss: 14427400.0\n",
      "Epoch [77/100], Training Loss: 280082.77335836145, Test Loss: 14423175.0\n",
      "Epoch [78/100], Training Loss: 278655.0484679521, Test Loss: 14398711.0\n",
      "Epoch [79/100], Training Loss: 277295.97089183104, Test Loss: 14441743.0\n",
      "Epoch [80/100], Training Loss: 275992.73316894734, Test Loss: 14392830.0\n",
      "Epoch [81/100], Training Loss: 274680.2349349861, Test Loss: 14441835.0\n",
      "Epoch [82/100], Training Loss: 273422.42751095904, Test Loss: 14466649.0\n",
      "Epoch [83/100], Training Loss: 272184.1653115929, Test Loss: 14431011.0\n",
      "Epoch [84/100], Training Loss: 270960.7695041763, Test Loss: 14425295.0\n",
      "Epoch [85/100], Training Loss: 269779.95562318584, Test Loss: 14453270.0\n",
      "Epoch [86/100], Training Loss: 268603.0069123571, Test Loss: 14409113.0\n",
      "Epoch [87/100], Training Loss: 267452.713568509, Test Loss: 14465884.0\n",
      "Epoch [88/100], Training Loss: 266319.12434837985, Test Loss: 14464970.0\n",
      "Epoch [89/100], Training Loss: 265200.05924930394, Test Loss: 14434199.0\n",
      "Epoch [90/100], Training Loss: 264117.0521777442, Test Loss: 14447290.0\n",
      "Epoch [91/100], Training Loss: 263075.7796079912, Test Loss: 14480841.0\n",
      "Epoch [92/100], Training Loss: 262066.51747526805, Test Loss: 14433683.0\n",
      "Epoch [93/100], Training Loss: 261033.0002924886, Test Loss: 14484304.0\n",
      "Epoch [94/100], Training Loss: 260035.98482021206, Test Loss: 14497829.0\n",
      "Epoch [95/100], Training Loss: 259100.57172634915, Test Loss: 14467654.0\n",
      "Epoch [96/100], Training Loss: 258036.04508397015, Test Loss: 14463900.0\n",
      "Epoch [97/100], Training Loss: 257069.26068879213, Test Loss: 14502790.0\n",
      "Epoch [98/100], Training Loss: 256113.91061326343, Test Loss: 14438440.0\n",
      "Epoch [99/100], Training Loss: 255194.8802425804, Test Loss: 14509135.0\n",
      "Epoch [100/100], Training Loss: 254276.114685149, Test Loss: 14497115.0\n",
      "Epoch [1/100], Training Loss: 4407039.115455246, Test Loss: 234830048.0\n",
      "Epoch [2/100], Training Loss: 2719986.1906285174, Test Loss: 146182400.0\n",
      "Epoch [3/100], Training Loss: 2095759.4725430957, Test Loss: 126610416.0\n",
      "Epoch [4/100], Training Loss: 1822090.2687044607, Test Loss: 111171504.0\n",
      "Epoch [5/100], Training Loss: 1594802.1655115217, Test Loss: 98318992.0\n",
      "Epoch [6/100], Training Loss: 1405910.416918429, Test Loss: 87803952.0\n",
      "Epoch [7/100], Training Loss: 1250092.6416681476, Test Loss: 79113056.0\n",
      "Epoch [8/100], Training Loss: 1121729.06077839, Test Loss: 71916032.0\n",
      "Epoch [9/100], Training Loss: 1016725.6009715064, Test Loss: 65969196.0\n",
      "Epoch [10/100], Training Loss: 931072.2972572715, Test Loss: 61096168.0\n",
      "Epoch [11/100], Training Loss: 861422.4121793732, Test Loss: 57110532.0\n",
      "Epoch [12/100], Training Loss: 804028.8934897222, Test Loss: 53761316.0\n",
      "Epoch [13/100], Training Loss: 755165.298915941, Test Loss: 50856732.0\n",
      "Epoch [14/100], Training Loss: 712190.3350512411, Test Loss: 48263296.0\n",
      "Epoch [15/100], Training Loss: 673604.7660091227, Test Loss: 45913172.0\n",
      "Epoch [16/100], Training Loss: 638504.3528819382, Test Loss: 43750180.0\n",
      "Epoch [17/100], Training Loss: 606265.7735027546, Test Loss: 41739840.0\n",
      "Epoch [18/100], Training Loss: 576538.5134470707, Test Loss: 39865608.0\n",
      "Epoch [19/100], Training Loss: 549066.6792844026, Test Loss: 38113444.0\n",
      "Epoch [20/100], Training Loss: 523680.4008352586, Test Loss: 36477004.0\n",
      "Epoch [21/100], Training Loss: 500244.6051181802, Test Loss: 34951724.0\n",
      "Epoch [22/100], Training Loss: 478598.5606599135, Test Loss: 33537954.0\n",
      "Epoch [23/100], Training Loss: 458604.4477519104, Test Loss: 32228876.0\n",
      "Epoch [24/100], Training Loss: 440163.1041111309, Test Loss: 31016654.0\n",
      "Epoch [25/100], Training Loss: 423136.0062496298, Test Loss: 29895650.0\n",
      "Epoch [26/100], Training Loss: 407419.8827379895, Test Loss: 28858310.0\n",
      "Epoch [27/100], Training Loss: 392896.6746193946, Test Loss: 27895906.0\n",
      "Epoch [28/100], Training Loss: 379470.3166725905, Test Loss: 27005520.0\n",
      "Epoch [29/100], Training Loss: 367076.4811918725, Test Loss: 26180926.0\n",
      "Epoch [30/100], Training Loss: 355627.35151945974, Test Loss: 25418300.0\n",
      "Epoch [31/100], Training Loss: 344960.3992802559, Test Loss: 24707060.0\n",
      "Epoch [32/100], Training Loss: 334940.73770807416, Test Loss: 24042900.0\n",
      "Epoch [33/100], Training Loss: 325518.042147977, Test Loss: 23423864.0\n",
      "Epoch [34/100], Training Loss: 316644.0802085184, Test Loss: 22842858.0\n",
      "Epoch [35/100], Training Loss: 308269.7785972395, Test Loss: 22302518.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/100], Training Loss: 300345.671287246, Test Loss: 21798642.0\n",
      "Epoch [37/100], Training Loss: 292828.4497660091, Test Loss: 21330346.0\n",
      "Epoch [38/100], Training Loss: 285716.0330549138, Test Loss: 20893018.0\n",
      "Epoch [39/100], Training Loss: 279007.0649250637, Test Loss: 20485976.0\n",
      "Epoch [40/100], Training Loss: 272669.2396777442, Test Loss: 20105906.0\n",
      "Epoch [41/100], Training Loss: 266657.0943368284, Test Loss: 19748026.0\n",
      "Epoch [42/100], Training Loss: 260955.63330075232, Test Loss: 19419374.0\n",
      "Epoch [43/100], Training Loss: 255526.08348142882, Test Loss: 19104624.0\n",
      "Epoch [44/100], Training Loss: 250343.02801966708, Test Loss: 18811018.0\n",
      "Epoch [45/100], Training Loss: 245386.41055032285, Test Loss: 18535024.0\n",
      "Epoch [46/100], Training Loss: 240665.15475978912, Test Loss: 18276556.0\n",
      "Epoch [47/100], Training Loss: 236153.31881997513, Test Loss: 18031670.0\n",
      "Epoch [48/100], Training Loss: 231847.97544576743, Test Loss: 17807344.0\n",
      "Epoch [49/100], Training Loss: 227738.42402701263, Test Loss: 17593868.0\n",
      "Epoch [50/100], Training Loss: 223807.81994550087, Test Loss: 17394278.0\n",
      "Epoch [51/100], Training Loss: 220033.47356495468, Test Loss: 17204976.0\n",
      "Epoch [52/100], Training Loss: 216425.26820093597, Test Loss: 17027052.0\n",
      "Epoch [53/100], Training Loss: 212971.65517445648, Test Loss: 16864426.0\n",
      "Epoch [54/100], Training Loss: 209668.9983709496, Test Loss: 16711234.0\n",
      "Epoch [55/100], Training Loss: 206512.3357620994, Test Loss: 16572074.0\n",
      "Epoch [56/100], Training Loss: 203483.6808245957, Test Loss: 16437022.0\n",
      "Epoch [57/100], Training Loss: 200587.97380190747, Test Loss: 16317265.0\n",
      "Epoch [58/100], Training Loss: 197810.05285528107, Test Loss: 16192863.0\n",
      "Epoch [59/100], Training Loss: 195125.30800900422, Test Loss: 16097578.0\n",
      "Epoch [60/100], Training Loss: 192565.64944316095, Test Loss: 15988354.0\n",
      "Epoch [61/100], Training Loss: 190101.90173864109, Test Loss: 15904208.0\n",
      "Epoch [62/100], Training Loss: 187738.13181979742, Test Loss: 15814724.0\n",
      "Epoch [63/100], Training Loss: 185468.16174989633, Test Loss: 15743578.0\n",
      "Epoch [64/100], Training Loss: 183281.59019015462, Test Loss: 15668218.0\n",
      "Epoch [65/100], Training Loss: 181182.09799478704, Test Loss: 15608840.0\n",
      "Epoch [66/100], Training Loss: 179155.4981043777, Test Loss: 15543182.0\n",
      "Epoch [67/100], Training Loss: 177208.7843433446, Test Loss: 15493178.0\n",
      "Epoch [68/100], Training Loss: 175336.74121793732, Test Loss: 15435427.0\n",
      "Epoch [69/100], Training Loss: 173533.86930572832, Test Loss: 15390869.0\n",
      "Epoch [70/100], Training Loss: 171810.73053284758, Test Loss: 15335799.0\n",
      "Epoch [71/100], Training Loss: 170151.52592411588, Test Loss: 15292891.0\n",
      "Epoch [72/100], Training Loss: 168561.4411468515, Test Loss: 15245174.0\n",
      "Epoch [73/100], Training Loss: 167029.98235442213, Test Loss: 15199738.0\n",
      "Epoch [74/100], Training Loss: 165560.30004887152, Test Loss: 15159825.0\n",
      "Epoch [75/100], Training Loss: 164145.67713701795, Test Loss: 15114868.0\n",
      "Epoch [76/100], Training Loss: 162783.23153249215, Test Loss: 15075643.0\n",
      "Epoch [77/100], Training Loss: 161477.06455482496, Test Loss: 15033640.0\n",
      "Epoch [78/100], Training Loss: 160217.54411764705, Test Loss: 15018688.0\n",
      "Epoch [79/100], Training Loss: 159010.68281648005, Test Loss: 14956478.0\n",
      "Epoch [80/100], Training Loss: 157839.71296872222, Test Loss: 14946358.0\n",
      "Epoch [81/100], Training Loss: 156718.78898613827, Test Loss: 14898225.0\n",
      "Epoch [82/100], Training Loss: 155631.59118239442, Test Loss: 14878339.0\n",
      "Epoch [83/100], Training Loss: 154573.5411039038, Test Loss: 14840027.0\n",
      "Epoch [84/100], Training Loss: 153554.23453883064, Test Loss: 14818201.0\n",
      "Epoch [85/100], Training Loss: 152567.7090071678, Test Loss: 14785069.0\n",
      "Epoch [86/100], Training Loss: 151603.17153900836, Test Loss: 14762861.0\n",
      "Epoch [87/100], Training Loss: 150673.0520629702, Test Loss: 14737490.0\n",
      "Epoch [88/100], Training Loss: 149773.57717996565, Test Loss: 14710545.0\n",
      "Epoch [89/100], Training Loss: 148906.86802470233, Test Loss: 14709131.0\n",
      "Epoch [90/100], Training Loss: 148082.68652627213, Test Loss: 14657788.0\n",
      "Epoch [91/100], Training Loss: 147253.25255464725, Test Loss: 14661474.0\n",
      "Epoch [92/100], Training Loss: 146463.81017860316, Test Loss: 14652667.0\n",
      "Epoch [93/100], Training Loss: 145687.4743202417, Test Loss: 14593034.0\n",
      "Epoch [94/100], Training Loss: 144940.09429239974, Test Loss: 14616089.0\n",
      "Epoch [95/100], Training Loss: 144214.16131301463, Test Loss: 14582182.0\n",
      "Epoch [96/100], Training Loss: 143506.74130679463, Test Loss: 14578330.0\n",
      "Epoch [97/100], Training Loss: 142824.9719284995, Test Loss: 14559180.0\n",
      "Epoch [98/100], Training Loss: 142155.04574669746, Test Loss: 14543026.0\n",
      "Epoch [99/100], Training Loss: 141501.31974557193, Test Loss: 14532080.0\n",
      "Epoch [100/100], Training Loss: 140871.57575824892, Test Loss: 14511380.0\n",
      "Epoch [1/100], Training Loss: 2315985.7316509685, Test Loss: 291116832.0\n",
      "Epoch [2/100], Training Loss: 2035869.7048753037, Test Loss: 222028160.0\n",
      "Epoch [3/100], Training Loss: 1431668.0258278537, Test Loss: 158500848.0\n",
      "Epoch [4/100], Training Loss: 1146586.1380249986, Test Loss: 140195920.0\n",
      "Epoch [5/100], Training Loss: 1034751.5983650257, Test Loss: 128372296.0\n",
      "Epoch [6/100], Training Loss: 946964.2720217997, Test Loss: 118148880.0\n",
      "Epoch [7/100], Training Loss: 868333.4352230318, Test Loss: 108725328.0\n",
      "Epoch [8/100], Training Loss: 793425.5437474083, Test Loss: 99887432.0\n",
      "Epoch [9/100], Training Loss: 727261.933179314, Test Loss: 92533896.0\n",
      "Epoch [10/100], Training Loss: 671529.3054913809, Test Loss: 86266104.0\n",
      "Epoch [11/100], Training Loss: 623618.0515372312, Test Loss: 80833680.0\n",
      "Epoch [12/100], Training Loss: 582252.0941887329, Test Loss: 76112472.0\n",
      "Epoch [13/100], Training Loss: 546626.1413423376, Test Loss: 72009688.0\n",
      "Epoch [14/100], Training Loss: 515972.7993602275, Test Loss: 68444360.0\n",
      "Epoch [15/100], Training Loss: 489614.55067827733, Test Loss: 65347876.0\n",
      "Epoch [16/100], Training Loss: 466862.0964397844, Test Loss: 62648436.0\n",
      "Epoch [17/100], Training Loss: 447095.76269178366, Test Loss: 60279084.0\n",
      "Epoch [18/100], Training Loss: 429717.5098631598, Test Loss: 58170528.0\n",
      "Epoch [19/100], Training Loss: 414183.91031337006, Test Loss: 56258836.0\n",
      "Epoch [20/100], Training Loss: 400084.6010307446, Test Loss: 54517256.0\n",
      "Epoch [21/100], Training Loss: 387164.69942538947, Test Loss: 52912820.0\n",
      "Epoch [22/100], Training Loss: 375258.6693323855, Test Loss: 51433692.0\n",
      "Epoch [23/100], Training Loss: 364218.4004502103, Test Loss: 50057480.0\n",
      "Epoch [24/100], Training Loss: 353905.17267934367, Test Loss: 48774300.0\n",
      "Epoch [25/100], Training Loss: 344219.4040637403, Test Loss: 47568128.0\n",
      "Epoch [26/100], Training Loss: 335165.220662283, Test Loss: 46444964.0\n",
      "Epoch [27/100], Training Loss: 326747.55310704344, Test Loss: 45394092.0\n",
      "Epoch [28/100], Training Loss: 318881.6381138558, Test Loss: 44400860.0\n",
      "Epoch [29/100], Training Loss: 311454.61121971445, Test Loss: 43463820.0\n",
      "Epoch [30/100], Training Loss: 304418.7677270304, Test Loss: 42575924.0\n",
      "Epoch [31/100], Training Loss: 297828.91807357385, Test Loss: 41744268.0\n",
      "Epoch [32/100], Training Loss: 291664.83976067766, Test Loss: 40958076.0\n",
      "Epoch [33/100], Training Loss: 285831.82702446537, Test Loss: 40210560.0\n",
      "Epoch [34/100], Training Loss: 280292.8202713109, Test Loss: 39500144.0\n",
      "Epoch [35/100], Training Loss: 275074.9058705053, Test Loss: 38835148.0\n",
      "Epoch [36/100], Training Loss: 270188.32219655236, Test Loss: 38215720.0\n",
      "Epoch [37/100], Training Loss: 265600.20212072745, Test Loss: 37634936.0\n",
      "Epoch [38/100], Training Loss: 261282.88857295184, Test Loss: 37086468.0\n",
      "Epoch [39/100], Training Loss: 257204.29743498607, Test Loss: 36567076.0\n",
      "Epoch [40/100], Training Loss: 253332.88466323086, Test Loss: 36068648.0\n",
      "Epoch [41/100], Training Loss: 249587.5040578165, Test Loss: 35582328.0\n",
      "Epoch [42/100], Training Loss: 245914.56151886735, Test Loss: 35108644.0\n",
      "Epoch [43/100], Training Loss: 242414.3262839879, Test Loss: 34665020.0\n",
      "Epoch [44/100], Training Loss: 239081.2024761566, Test Loss: 34230960.0\n",
      "Epoch [45/100], Training Loss: 235790.12635507376, Test Loss: 33801252.0\n",
      "Epoch [46/100], Training Loss: 232621.51756412536, Test Loss: 33396724.0\n",
      "Epoch [47/100], Training Loss: 229632.1274213613, Test Loss: 33011256.0\n",
      "Epoch [48/100], Training Loss: 226640.58343700017, Test Loss: 32607342.0\n",
      "Epoch [49/100], Training Loss: 223548.34447011433, Test Loss: 32208876.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/100], Training Loss: 220568.65422664533, Test Loss: 31832726.0\n",
      "Epoch [51/100], Training Loss: 217677.5869320538, Test Loss: 31450790.0\n",
      "Epoch [52/100], Training Loss: 214710.44790000594, Test Loss: 31065418.0\n",
      "Epoch [53/100], Training Loss: 211798.20543806648, Test Loss: 30699840.0\n",
      "Epoch [54/100], Training Loss: 209041.66678514305, Test Loss: 30358758.0\n",
      "Epoch [55/100], Training Loss: 206429.71014750312, Test Loss: 30025666.0\n",
      "Epoch [56/100], Training Loss: 203693.85522184704, Test Loss: 29663646.0\n",
      "Epoch [57/100], Training Loss: 200851.11178247735, Test Loss: 29314766.0\n",
      "Epoch [58/100], Training Loss: 198251.236004976, Test Loss: 29006544.0\n",
      "Epoch [59/100], Training Loss: 195907.06957526214, Test Loss: 28728292.0\n",
      "Epoch [60/100], Training Loss: 193675.9242935845, Test Loss: 28462180.0\n",
      "Epoch [61/100], Training Loss: 191495.79201469108, Test Loss: 28208622.0\n",
      "Epoch [62/100], Training Loss: 189310.7120135063, Test Loss: 27941878.0\n",
      "Epoch [63/100], Training Loss: 187068.69465671465, Test Loss: 27677762.0\n",
      "Epoch [64/100], Training Loss: 184919.57982347018, Test Loss: 27428750.0\n",
      "Epoch [65/100], Training Loss: 182843.6900065162, Test Loss: 27183268.0\n",
      "Epoch [66/100], Training Loss: 180712.1734198211, Test Loss: 26925836.0\n",
      "Epoch [67/100], Training Loss: 178596.72433505123, Test Loss: 26683112.0\n",
      "Epoch [68/100], Training Loss: 176515.40892127244, Test Loss: 26440430.0\n",
      "Epoch [69/100], Training Loss: 174536.7767904745, Test Loss: 26218780.0\n",
      "Epoch [70/100], Training Loss: 172686.78920087672, Test Loss: 26004808.0\n",
      "Epoch [71/100], Training Loss: 170910.57428469879, Test Loss: 25800782.0\n",
      "Epoch [72/100], Training Loss: 169169.45311296725, Test Loss: 25597114.0\n",
      "Epoch [73/100], Training Loss: 167441.00802677567, Test Loss: 25396742.0\n",
      "Epoch [74/100], Training Loss: 165708.2951543155, Test Loss: 25191138.0\n",
      "Epoch [75/100], Training Loss: 163874.41807357385, Test Loss: 24962316.0\n",
      "Epoch [76/100], Training Loss: 161736.59024939282, Test Loss: 24686212.0\n",
      "Epoch [77/100], Training Loss: 159442.72279485813, Test Loss: 24423276.0\n",
      "Epoch [78/100], Training Loss: 157214.6365736627, Test Loss: 24151488.0\n",
      "Epoch [79/100], Training Loss: 154913.70212072745, Test Loss: 23882700.0\n",
      "Epoch [80/100], Training Loss: 152804.4813695871, Test Loss: 23656350.0\n",
      "Epoch [81/100], Training Loss: 150940.892838102, Test Loss: 23452286.0\n",
      "Epoch [82/100], Training Loss: 149177.56939754754, Test Loss: 23251838.0\n",
      "Epoch [83/100], Training Loss: 147365.76165511523, Test Loss: 23039040.0\n",
      "Epoch [84/100], Training Loss: 145510.5422664534, Test Loss: 22829384.0\n",
      "Epoch [85/100], Training Loss: 143707.27291037262, Test Loss: 22630252.0\n",
      "Epoch [86/100], Training Loss: 141919.68695574906, Test Loss: 22424380.0\n",
      "Epoch [87/100], Training Loss: 140013.1664297139, Test Loss: 22198296.0\n",
      "Epoch [88/100], Training Loss: 138077.7528878621, Test Loss: 21988176.0\n",
      "Epoch [89/100], Training Loss: 136238.9706474735, Test Loss: 21785968.0\n",
      "Epoch [90/100], Training Loss: 134388.10248208046, Test Loss: 21574816.0\n",
      "Epoch [91/100], Training Loss: 132486.38623304307, Test Loss: 21362874.0\n",
      "Epoch [92/100], Training Loss: 130598.65372312067, Test Loss: 21153818.0\n",
      "Epoch [93/100], Training Loss: 128709.04025235472, Test Loss: 20945324.0\n",
      "Epoch [94/100], Training Loss: 126870.81621349446, Test Loss: 20750104.0\n",
      "Epoch [95/100], Training Loss: 125152.81899768971, Test Loss: 20570234.0\n",
      "Epoch [96/100], Training Loss: 123524.05944553048, Test Loss: 20401518.0\n",
      "Epoch [97/100], Training Loss: 121959.4446419051, Test Loss: 20235864.0\n",
      "Epoch [98/100], Training Loss: 120433.47658610273, Test Loss: 20074504.0\n",
      "Epoch [99/100], Training Loss: 118937.48179906403, Test Loss: 19912544.0\n",
      "Epoch [100/100], Training Loss: 117466.96624903737, Test Loss: 19754930.0\n",
      "Epoch [1/100], Training Loss: 1497614.4910846513, Test Loss: 298583424.0\n",
      "Epoch [2/100], Training Loss: 1464269.143297198, Test Loss: 283556896.0\n",
      "Epoch [3/100], Training Loss: 1326273.3468396422, Test Loss: 242246336.0\n",
      "Epoch [4/100], Training Loss: 1083368.7174930396, Test Loss: 191181936.0\n",
      "Epoch [5/100], Training Loss: 871186.8626266216, Test Loss: 160472768.0\n",
      "Epoch [6/100], Training Loss: 763445.3764587406, Test Loss: 146441200.0\n",
      "Epoch [7/100], Training Loss: 705372.9933060837, Test Loss: 137190960.0\n",
      "Epoch [8/100], Training Loss: 661014.528286239, Test Loss: 129499800.0\n",
      "Epoch [9/100], Training Loss: 622293.3418636337, Test Loss: 122695584.0\n",
      "Epoch [10/100], Training Loss: 587264.2736804691, Test Loss: 116565936.0\n",
      "Epoch [11/100], Training Loss: 555240.8726971151, Test Loss: 111022552.0\n",
      "Epoch [12/100], Training Loss: 525929.3774065517, Test Loss: 106017192.0\n",
      "Epoch [13/100], Training Loss: 499157.066524495, Test Loss: 101505864.0\n",
      "Epoch [14/100], Training Loss: 474712.5736626977, Test Loss: 97415448.0\n",
      "Epoch [15/100], Training Loss: 452130.3740299745, Test Loss: 93651536.0\n",
      "Epoch [16/100], Training Loss: 431225.33712457796, Test Loss: 90221216.0\n",
      "Epoch [17/100], Training Loss: 412459.5583200047, Test Loss: 87185856.0\n",
      "Epoch [18/100], Training Loss: 395687.79491736274, Test Loss: 84441328.0\n",
      "Epoch [19/100], Training Loss: 380551.5922042533, Test Loss: 81942208.0\n",
      "Epoch [20/100], Training Loss: 366864.31680587644, Test Loss: 79654568.0\n",
      "Epoch [21/100], Training Loss: 354467.4497956282, Test Loss: 77554088.0\n",
      "Epoch [22/100], Training Loss: 343221.36982406257, Test Loss: 75620480.0\n",
      "Epoch [23/100], Training Loss: 333014.5708192643, Test Loss: 73837960.0\n",
      "Epoch [24/100], Training Loss: 323734.05402523547, Test Loss: 72184568.0\n",
      "Epoch [25/100], Training Loss: 315257.4758604348, Test Loss: 70642448.0\n",
      "Epoch [26/100], Training Loss: 307469.6843788875, Test Loss: 69192896.0\n",
      "Epoch [27/100], Training Loss: 300271.29861975, Test Loss: 67826376.0\n",
      "Epoch [28/100], Training Loss: 293576.2707777975, Test Loss: 66532120.0\n",
      "Epoch [29/100], Training Loss: 287307.6981961969, Test Loss: 65298008.0\n",
      "Epoch [30/100], Training Loss: 281391.60551063326, Test Loss: 64115116.0\n",
      "Epoch [31/100], Training Loss: 275776.43501569814, Test Loss: 62975068.0\n",
      "Epoch [32/100], Training Loss: 270405.2901287875, Test Loss: 61871380.0\n",
      "Epoch [33/100], Training Loss: 265240.0150735468, Test Loss: 60802004.0\n",
      "Epoch [34/100], Training Loss: 260253.15586495172, Test Loss: 59762280.0\n",
      "Epoch [35/100], Training Loss: 255415.12777679047, Test Loss: 58750400.0\n",
      "Epoch [36/100], Training Loss: 250703.377447278, Test Loss: 57762896.0\n",
      "Epoch [37/100], Training Loss: 246105.56546561222, Test Loss: 56797592.0\n",
      "Epoch [38/100], Training Loss: 241607.31390320478, Test Loss: 55852860.0\n",
      "Epoch [39/100], Training Loss: 237205.7534061963, Test Loss: 54930196.0\n",
      "Epoch [40/100], Training Loss: 232896.98396125823, Test Loss: 54028400.0\n",
      "Epoch [41/100], Training Loss: 228665.29638350807, Test Loss: 53138388.0\n",
      "Epoch [42/100], Training Loss: 224503.2041200166, Test Loss: 52269364.0\n",
      "Epoch [43/100], Training Loss: 220441.98944079143, Test Loss: 51422528.0\n",
      "Epoch [44/100], Training Loss: 216474.7111989811, Test Loss: 50594300.0\n",
      "Epoch [45/100], Training Loss: 212590.29199988153, Test Loss: 49781668.0\n",
      "Epoch [46/100], Training Loss: 208785.9972602334, Test Loss: 48985964.0\n",
      "Epoch [47/100], Training Loss: 205066.47104733132, Test Loss: 48212372.0\n",
      "Epoch [48/100], Training Loss: 201435.10648065872, Test Loss: 47456728.0\n",
      "Epoch [49/100], Training Loss: 197886.01534269296, Test Loss: 46719184.0\n",
      "Epoch [50/100], Training Loss: 194416.8923197678, Test Loss: 45998052.0\n",
      "Epoch [51/100], Training Loss: 191032.60263017594, Test Loss: 45295172.0\n",
      "Epoch [52/100], Training Loss: 187733.5529071145, Test Loss: 44608876.0\n",
      "Epoch [53/100], Training Loss: 184522.42765535216, Test Loss: 43936152.0\n",
      "Epoch [54/100], Training Loss: 181394.97449795628, Test Loss: 43278228.0\n",
      "Epoch [55/100], Training Loss: 178341.9295731888, Test Loss: 42636128.0\n",
      "Epoch [56/100], Training Loss: 175369.01756412536, Test Loss: 42008904.0\n",
      "Epoch [57/100], Training Loss: 172476.24092915113, Test Loss: 41396904.0\n",
      "Epoch [58/100], Training Loss: 169661.8093159469, Test Loss: 40800756.0\n",
      "Epoch [59/100], Training Loss: 166924.94707992714, Test Loss: 40218444.0\n",
      "Epoch [60/100], Training Loss: 164261.89048338367, Test Loss: 39649480.0\n",
      "Epoch [61/100], Training Loss: 161672.80464631095, Test Loss: 39093892.0\n",
      "Epoch [62/100], Training Loss: 159158.50818227592, Test Loss: 38550744.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [63/100], Training Loss: 156714.91305359945, Test Loss: 38022600.0\n",
      "Epoch [64/100], Training Loss: 154335.69308736525, Test Loss: 37506660.0\n",
      "Epoch [65/100], Training Loss: 152020.70306090242, Test Loss: 37004064.0\n",
      "Epoch [66/100], Training Loss: 149765.9658066021, Test Loss: 36513304.0\n",
      "Epoch [67/100], Training Loss: 147571.46967282004, Test Loss: 36034764.0\n",
      "Epoch [68/100], Training Loss: 145438.25777073245, Test Loss: 35568172.0\n",
      "Epoch [69/100], Training Loss: 143363.91066229113, Test Loss: 35113792.0\n",
      "Epoch [70/100], Training Loss: 141342.8275764031, Test Loss: 34671068.0\n",
      "Epoch [71/100], Training Loss: 139367.56717782895, Test Loss: 34238340.0\n",
      "Epoch [72/100], Training Loss: 137438.23192264567, Test Loss: 33815356.0\n",
      "Epoch [73/100], Training Loss: 135549.85883575308, Test Loss: 33401378.0\n",
      "Epoch [74/100], Training Loss: 133696.29751800053, Test Loss: 32994068.0\n",
      "Epoch [75/100], Training Loss: 131874.72160031757, Test Loss: 32593680.0\n",
      "Epoch [76/100], Training Loss: 130086.21107402553, Test Loss: 32200930.0\n",
      "Epoch [77/100], Training Loss: 128332.94321104811, Test Loss: 31816098.0\n",
      "Epoch [78/100], Training Loss: 126615.27977876384, Test Loss: 31437620.0\n",
      "Epoch [79/100], Training Loss: 124928.83084764305, Test Loss: 31066948.0\n",
      "Epoch [80/100], Training Loss: 123278.62749726024, Test Loss: 30708268.0\n",
      "Epoch [81/100], Training Loss: 121667.6211205923, Test Loss: 30357300.0\n",
      "Epoch [82/100], Training Loss: 120093.89452245683, Test Loss: 30013714.0\n",
      "Epoch [83/100], Training Loss: 118556.39017515995, Test Loss: 29674098.0\n",
      "Epoch [84/100], Training Loss: 117051.77069125422, Test Loss: 29342604.0\n",
      "Epoch [85/100], Training Loss: 115579.15980868839, Test Loss: 29019176.0\n",
      "Epoch [86/100], Training Loss: 114137.81607650613, Test Loss: 28701696.0\n",
      "Epoch [87/100], Training Loss: 112726.45847703224, Test Loss: 28392576.0\n",
      "Epoch [88/100], Training Loss: 111351.62752201995, Test Loss: 28091858.0\n",
      "Epoch [89/100], Training Loss: 110013.39174196396, Test Loss: 27800398.0\n",
      "Epoch [90/100], Training Loss: 108708.71097359828, Test Loss: 27515292.0\n",
      "Epoch [91/100], Training Loss: 107436.19719655234, Test Loss: 27238674.0\n",
      "Epoch [92/100], Training Loss: 106195.39375583126, Test Loss: 26969500.0\n",
      "Epoch [93/100], Training Loss: 104980.07091136115, Test Loss: 26703692.0\n",
      "Epoch [94/100], Training Loss: 103792.98741651117, Test Loss: 26448212.0\n",
      "Epoch [95/100], Training Loss: 102628.98659805217, Test Loss: 26195900.0\n",
      "Epoch [96/100], Training Loss: 101487.39683135811, Test Loss: 25952818.0\n",
      "Epoch [97/100], Training Loss: 100370.22034966272, Test Loss: 25716516.0\n",
      "Epoch [98/100], Training Loss: 99277.31441505982, Test Loss: 25484928.0\n",
      "Epoch [99/100], Training Loss: 98203.0553284758, Test Loss: 25260500.0\n",
      "Epoch [100/100], Training Loss: 97151.0880066717, Test Loss: 25039012.0\n",
      "Epoch [1/100], Training Loss: 1164595.8225223625, Test Loss: 299566752.0\n",
      "Epoch [2/100], Training Loss: 1154161.4529944907, Test Loss: 293184544.0\n",
      "Epoch [3/100], Training Loss: 1103394.4586221196, Test Loss: 272079520.0\n",
      "Epoch [4/100], Training Loss: 988256.2179965642, Test Loss: 234716640.0\n",
      "Epoch [5/100], Training Loss: 829705.693264617, Test Loss: 193644672.0\n",
      "Epoch [6/100], Training Loss: 690837.4447011433, Test Loss: 165335168.0\n",
      "Epoch [7/100], Training Loss: 609186.9187844322, Test Loss: 150784976.0\n",
      "Epoch [8/100], Training Loss: 565268.2563829157, Test Loss: 142076720.0\n",
      "Epoch [9/100], Training Loss: 534829.5020437178, Test Loss: 135242848.0\n",
      "Epoch [10/100], Training Loss: 509140.67484153784, Test Loss: 129207624.0\n",
      "Epoch [11/100], Training Loss: 485824.41419347195, Test Loss: 123645920.0\n",
      "Epoch [12/100], Training Loss: 464051.2469640424, Test Loss: 118425760.0\n",
      "Epoch [13/100], Training Loss: 443466.98584207095, Test Loss: 113489992.0\n",
      "Epoch [14/100], Training Loss: 423918.20247615664, Test Loss: 108808328.0\n",
      "Epoch [15/100], Training Loss: 405201.50488715124, Test Loss: 104295776.0\n",
      "Epoch [16/100], Training Loss: 387080.26894141344, Test Loss: 99980040.0\n",
      "Epoch [17/100], Training Loss: 369823.7865055388, Test Loss: 95909568.0\n",
      "Epoch [18/100], Training Loss: 353659.85853918607, Test Loss: 92157984.0\n",
      "Epoch [19/100], Training Loss: 338798.05438066466, Test Loss: 88727528.0\n",
      "Epoch [20/100], Training Loss: 325260.7805224809, Test Loss: 85608096.0\n",
      "Epoch [21/100], Training Loss: 312911.9156448078, Test Loss: 82741792.0\n",
      "Epoch [22/100], Training Loss: 301580.10982761683, Test Loss: 80094336.0\n",
      "Epoch [23/100], Training Loss: 291145.50607191515, Test Loss: 77642752.0\n",
      "Epoch [24/100], Training Loss: 281520.93525265093, Test Loss: 75365864.0\n",
      "Epoch [25/100], Training Loss: 272626.5234287068, Test Loss: 73248200.0\n",
      "Epoch [26/100], Training Loss: 264382.2290148688, Test Loss: 71271776.0\n",
      "Epoch [27/100], Training Loss: 256734.08648776732, Test Loss: 69423336.0\n",
      "Epoch [28/100], Training Loss: 249631.62277116286, Test Loss: 67695936.0\n",
      "Epoch [29/100], Training Loss: 243025.79468040992, Test Loss: 66080792.0\n",
      "Epoch [30/100], Training Loss: 236861.1846454594, Test Loss: 64569880.0\n",
      "Epoch [31/100], Training Loss: 231109.83816124639, Test Loss: 63153468.0\n",
      "Epoch [32/100], Training Loss: 225725.17220543808, Test Loss: 61819968.0\n",
      "Epoch [33/100], Training Loss: 220676.45163201232, Test Loss: 60564596.0\n",
      "Epoch [34/100], Training Loss: 215921.31804987855, Test Loss: 59375956.0\n",
      "Epoch [35/100], Training Loss: 211409.20146910727, Test Loss: 58246948.0\n",
      "Epoch [36/100], Training Loss: 207118.35886499615, Test Loss: 57171768.0\n",
      "Epoch [37/100], Training Loss: 203027.74622356496, Test Loss: 56144308.0\n",
      "Epoch [38/100], Training Loss: 199109.03382501035, Test Loss: 55161996.0\n",
      "Epoch [39/100], Training Loss: 195352.97328357326, Test Loss: 54219652.0\n",
      "Epoch [40/100], Training Loss: 191752.62650316925, Test Loss: 53312024.0\n",
      "Epoch [41/100], Training Loss: 188287.17374563118, Test Loss: 52441180.0\n",
      "Epoch [42/100], Training Loss: 184946.94958829452, Test Loss: 51602304.0\n",
      "Epoch [43/100], Training Loss: 181731.7734731355, Test Loss: 50795540.0\n",
      "Epoch [44/100], Training Loss: 178632.53918606718, Test Loss: 50016108.0\n",
      "Epoch [45/100], Training Loss: 175639.89206800546, Test Loss: 49263796.0\n",
      "Epoch [46/100], Training Loss: 172745.34826135891, Test Loss: 48536556.0\n",
      "Epoch [47/100], Training Loss: 169941.040341212, Test Loss: 47834644.0\n",
      "Epoch [48/100], Training Loss: 167225.0241099461, Test Loss: 47155024.0\n",
      "Epoch [49/100], Training Loss: 164598.2419287957, Test Loss: 46496808.0\n",
      "Epoch [50/100], Training Loss: 162060.36087909483, Test Loss: 45858716.0\n",
      "Epoch [51/100], Training Loss: 159608.6971150998, Test Loss: 45239620.0\n",
      "Epoch [52/100], Training Loss: 157238.41407499555, Test Loss: 44640044.0\n",
      "Epoch [53/100], Training Loss: 154948.1550855992, Test Loss: 44058316.0\n",
      "Epoch [54/100], Training Loss: 152730.76772703038, Test Loss: 43493724.0\n",
      "Epoch [55/100], Training Loss: 150582.30483976068, Test Loss: 42943572.0\n",
      "Epoch [56/100], Training Loss: 148493.61388543333, Test Loss: 42408476.0\n",
      "Epoch [57/100], Training Loss: 146464.16930276642, Test Loss: 41887064.0\n",
      "Epoch [58/100], Training Loss: 144491.20614892483, Test Loss: 41381104.0\n",
      "Epoch [59/100], Training Loss: 142573.66868076535, Test Loss: 40888092.0\n",
      "Epoch [60/100], Training Loss: 140712.71470884426, Test Loss: 40407160.0\n",
      "Epoch [61/100], Training Loss: 138907.09418873288, Test Loss: 39941740.0\n",
      "Epoch [62/100], Training Loss: 137162.7377525028, Test Loss: 39492808.0\n",
      "Epoch [63/100], Training Loss: 135476.67116876962, Test Loss: 39058468.0\n",
      "Epoch [64/100], Training Loss: 133840.20164682186, Test Loss: 38635788.0\n",
      "Epoch [65/100], Training Loss: 132247.7916000237, Test Loss: 38223768.0\n",
      "Epoch [66/100], Training Loss: 130698.71334636574, Test Loss: 37822580.0\n",
      "Epoch [67/100], Training Loss: 129193.81422901487, Test Loss: 37432604.0\n",
      "Epoch [68/100], Training Loss: 127735.73532373675, Test Loss: 37053920.0\n",
      "Epoch [69/100], Training Loss: 126321.53231443635, Test Loss: 36686048.0\n",
      "Epoch [70/100], Training Loss: 124947.99869675968, Test Loss: 36329028.0\n",
      "Epoch [71/100], Training Loss: 123610.14430424738, Test Loss: 35982184.0\n",
      "Epoch [72/100], Training Loss: 122301.68728155915, Test Loss: 35643464.0\n",
      "Epoch [73/100], Training Loss: 121019.05888276761, Test Loss: 35312344.0\n",
      "Epoch [74/100], Training Loss: 119755.32705408447, Test Loss: 34986840.0\n",
      "Epoch [75/100], Training Loss: 118506.2731473254, Test Loss: 34665112.0\n",
      "Epoch [76/100], Training Loss: 117263.84011610686, Test Loss: 34345180.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [77/100], Training Loss: 116031.16924352823, Test Loss: 34029112.0\n",
      "Epoch [78/100], Training Loss: 114823.15093892542, Test Loss: 33721284.0\n",
      "Epoch [79/100], Training Loss: 113647.99235827262, Test Loss: 33420716.0\n",
      "Epoch [80/100], Training Loss: 112510.72626029263, Test Loss: 33131782.0\n",
      "Epoch [81/100], Training Loss: 111417.50044428647, Test Loss: 32853410.0\n",
      "Epoch [82/100], Training Loss: 110363.80546176175, Test Loss: 32583634.0\n",
      "Epoch [83/100], Training Loss: 109344.01569812215, Test Loss: 32321204.0\n",
      "Epoch [84/100], Training Loss: 108349.38386351519, Test Loss: 32064746.0\n",
      "Epoch [85/100], Training Loss: 107374.35880575795, Test Loss: 31811634.0\n",
      "Epoch [86/100], Training Loss: 106415.35151945974, Test Loss: 31561414.0\n",
      "Epoch [87/100], Training Loss: 105466.9869675967, Test Loss: 31313054.0\n",
      "Epoch [88/100], Training Loss: 104523.30590604822, Test Loss: 31066506.0\n",
      "Epoch [89/100], Training Loss: 103594.82862389668, Test Loss: 30826502.0\n",
      "Epoch [90/100], Training Loss: 102696.10265979504, Test Loss: 30595680.0\n",
      "Epoch [91/100], Training Loss: 101826.10295598602, Test Loss: 30372472.0\n",
      "Epoch [92/100], Training Loss: 100978.79728689059, Test Loss: 30153946.0\n",
      "Epoch [93/100], Training Loss: 100150.58580652805, Test Loss: 29939440.0\n",
      "Epoch [94/100], Training Loss: 99339.05461761744, Test Loss: 29729508.0\n",
      "Epoch [95/100], Training Loss: 98539.08992358272, Test Loss: 29522796.0\n",
      "Epoch [96/100], Training Loss: 97746.8671287246, Test Loss: 29319352.0\n",
      "Epoch [97/100], Training Loss: 96956.87026834903, Test Loss: 29114970.0\n",
      "Epoch [98/100], Training Loss: 96167.31568035069, Test Loss: 28912628.0\n",
      "Epoch [99/100], Training Loss: 95385.89348972218, Test Loss: 28712174.0\n",
      "Epoch [100/100], Training Loss: 94621.53231443635, Test Loss: 28521062.0\n",
      "Epoch [1/100], Training Loss: 582422.6460517742, Test Loss: 300027232.0\n",
      "Epoch [2/100], Training Loss: 581855.8540370831, Test Loss: 299350400.0\n",
      "Epoch [3/100], Training Loss: 579063.958770215, Test Loss: 296862272.0\n",
      "Epoch [4/100], Training Loss: 571318.6346780404, Test Loss: 291049408.0\n",
      "Epoch [5/100], Training Loss: 555783.9303358806, Test Loss: 280611008.0\n",
      "Epoch [6/100], Training Loss: 530533.9522540134, Test Loss: 264977520.0\n",
      "Epoch [7/100], Training Loss: 495531.22966648894, Test Loss: 244770560.0\n",
      "Epoch [8/100], Training Loss: 453312.64640720334, Test Loss: 221991568.0\n",
      "Epoch [9/100], Training Loss: 408843.3140216812, Test Loss: 199629152.0\n",
      "Epoch [10/100], Training Loss: 368072.2767608554, Test Loss: 180581424.0\n",
      "Epoch [11/100], Training Loss: 335443.94384218944, Test Loss: 166309824.0\n",
      "Epoch [12/100], Training Loss: 311905.6079616136, Test Loss: 156367792.0\n",
      "Epoch [13/100], Training Loss: 295415.27824181033, Test Loss: 149322368.0\n",
      "Epoch [14/100], Training Loss: 283216.7762573307, Test Loss: 143897392.0\n",
      "Epoch [15/100], Training Loss: 273338.668562289, Test Loss: 139334560.0\n",
      "Epoch [16/100], Training Loss: 264720.52840471535, Test Loss: 135260000.0\n",
      "Epoch [17/100], Training Loss: 256855.08962739175, Test Loss: 131496672.0\n",
      "Epoch [18/100], Training Loss: 249495.7900598306, Test Loss: 127954696.0\n",
      "Epoch [19/100], Training Loss: 242510.69059889816, Test Loss: 124583488.0\n",
      "Epoch [20/100], Training Loss: 235822.08352585748, Test Loss: 121352288.0\n",
      "Epoch [21/100], Training Loss: 229381.8541555595, Test Loss: 118241928.0\n",
      "Epoch [22/100], Training Loss: 223160.23126592027, Test Loss: 115240592.0\n",
      "Epoch [23/100], Training Loss: 217139.36093833303, Test Loss: 112341408.0\n",
      "Epoch [24/100], Training Loss: 211309.3082163379, Test Loss: 109540720.0\n",
      "Epoch [25/100], Training Loss: 205666.2619513062, Test Loss: 106837552.0\n",
      "Epoch [26/100], Training Loss: 200210.24109946092, Test Loss: 104231840.0\n",
      "Epoch [27/100], Training Loss: 194941.60606599136, Test Loss: 101722584.0\n",
      "Epoch [28/100], Training Loss: 189853.79017830698, Test Loss: 99301912.0\n",
      "Epoch [29/100], Training Loss: 184921.50156981222, Test Loss: 96950800.0\n",
      "Epoch [30/100], Training Loss: 180110.11812096438, Test Loss: 94668224.0\n",
      "Epoch [31/100], Training Loss: 175454.55079675376, Test Loss: 92474400.0\n",
      "Epoch [32/100], Training Loss: 170992.36917244238, Test Loss: 90381256.0\n",
      "Epoch [33/100], Training Loss: 166750.31597654166, Test Loss: 88396136.0\n",
      "Epoch [34/100], Training Loss: 162732.15567798115, Test Loss: 86521712.0\n",
      "Epoch [35/100], Training Loss: 158939.6955156685, Test Loss: 84751408.0\n",
      "Epoch [36/100], Training Loss: 155353.80889757717, Test Loss: 83073512.0\n",
      "Epoch [37/100], Training Loss: 151958.59818731117, Test Loss: 81480160.0\n",
      "Epoch [38/100], Training Loss: 148737.60417036904, Test Loss: 79963280.0\n",
      "Epoch [39/100], Training Loss: 145676.9115573722, Test Loss: 78516544.0\n",
      "Epoch [40/100], Training Loss: 142765.1198388721, Test Loss: 77134648.0\n",
      "Epoch [41/100], Training Loss: 139991.97109175997, Test Loss: 75813248.0\n",
      "Epoch [42/100], Training Loss: 137348.94022865943, Test Loss: 74547784.0\n",
      "Epoch [43/100], Training Loss: 134826.75884130088, Test Loss: 73335528.0\n",
      "Epoch [44/100], Training Loss: 132417.0883241514, Test Loss: 72173008.0\n",
      "Epoch [45/100], Training Loss: 130112.23316154256, Test Loss: 71057376.0\n",
      "Epoch [46/100], Training Loss: 127905.46270955513, Test Loss: 69984192.0\n",
      "Epoch [47/100], Training Loss: 125787.64149043303, Test Loss: 68948792.0\n",
      "Epoch [48/100], Training Loss: 123753.37764350453, Test Loss: 67951112.0\n",
      "Epoch [49/100], Training Loss: 121799.35714708845, Test Loss: 66989724.0\n",
      "Epoch [50/100], Training Loss: 119921.33688762515, Test Loss: 66061584.0\n",
      "Epoch [51/100], Training Loss: 118112.72057342574, Test Loss: 65164024.0\n",
      "Epoch [52/100], Training Loss: 116369.46578994136, Test Loss: 64295740.0\n",
      "Epoch [53/100], Training Loss: 114687.6865114626, Test Loss: 63455304.0\n",
      "Epoch [54/100], Training Loss: 113062.06291096499, Test Loss: 62638980.0\n",
      "Epoch [55/100], Training Loss: 111488.32723179906, Test Loss: 61846356.0\n",
      "Epoch [56/100], Training Loss: 109962.83632486228, Test Loss: 61076500.0\n",
      "Epoch [57/100], Training Loss: 108481.9316391209, Test Loss: 60327916.0\n",
      "Epoch [58/100], Training Loss: 107042.79130383271, Test Loss: 59599356.0\n",
      "Epoch [59/100], Training Loss: 105641.33001599432, Test Loss: 58889568.0\n",
      "Epoch [60/100], Training Loss: 104273.29423612345, Test Loss: 58197176.0\n",
      "Epoch [61/100], Training Loss: 102936.85539956164, Test Loss: 57520756.0\n",
      "Epoch [62/100], Training Loss: 101631.31011196019, Test Loss: 56860656.0\n",
      "Epoch [63/100], Training Loss: 100355.11296724128, Test Loss: 56215856.0\n",
      "Epoch [64/100], Training Loss: 99106.26100349505, Test Loss: 55584804.0\n",
      "Epoch [65/100], Training Loss: 97882.50518334222, Test Loss: 54967332.0\n",
      "Epoch [66/100], Training Loss: 96684.06374029974, Test Loss: 54363952.0\n",
      "Epoch [67/100], Training Loss: 95511.00408743558, Test Loss: 53772660.0\n",
      "Epoch [68/100], Training Loss: 94361.84811326343, Test Loss: 53192936.0\n",
      "Epoch [69/100], Training Loss: 93235.79562822108, Test Loss: 52623932.0\n",
      "Epoch [70/100], Training Loss: 92130.34358154138, Test Loss: 52064788.0\n",
      "Epoch [71/100], Training Loss: 91046.00699010723, Test Loss: 51515724.0\n",
      "Epoch [72/100], Training Loss: 89982.06599135122, Test Loss: 50977180.0\n",
      "Epoch [73/100], Training Loss: 88938.09359635093, Test Loss: 50449008.0\n",
      "Epoch [74/100], Training Loss: 87913.70973283573, Test Loss: 49930892.0\n",
      "Epoch [75/100], Training Loss: 86909.03548367988, Test Loss: 49421172.0\n",
      "Epoch [76/100], Training Loss: 85923.57099697886, Test Loss: 48921424.0\n",
      "Epoch [77/100], Training Loss: 84955.5738404123, Test Loss: 48430568.0\n",
      "Epoch [78/100], Training Loss: 84004.9593033588, Test Loss: 47949376.0\n",
      "Epoch [79/100], Training Loss: 83071.98222854096, Test Loss: 47477392.0\n",
      "Epoch [80/100], Training Loss: 82156.85208222261, Test Loss: 47014008.0\n",
      "Epoch [81/100], Training Loss: 81259.42337539245, Test Loss: 46558004.0\n",
      "Epoch [82/100], Training Loss: 80378.3689354896, Test Loss: 46109708.0\n",
      "Epoch [83/100], Training Loss: 79513.60665837332, Test Loss: 45669660.0\n",
      "Epoch [84/100], Training Loss: 78665.48604940466, Test Loss: 45237652.0\n",
      "Epoch [85/100], Training Loss: 77834.23316154256, Test Loss: 44813884.0\n",
      "Epoch [86/100], Training Loss: 77019.28831230378, Test Loss: 44398604.0\n",
      "Epoch [87/100], Training Loss: 76219.85249689, Test Loss: 43990604.0\n",
      "Epoch [88/100], Training Loss: 75436.81855340324, Test Loss: 43590144.0\n",
      "Epoch [89/100], Training Loss: 74669.60665837332, Test Loss: 43198652.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/100], Training Loss: 73917.74835614004, Test Loss: 42814344.0\n",
      "Epoch [91/100], Training Loss: 73180.51714945797, Test Loss: 42436872.0\n",
      "Epoch [92/100], Training Loss: 72457.09531425864, Test Loss: 42065156.0\n",
      "Epoch [93/100], Training Loss: 71746.66820685979, Test Loss: 41699700.0\n",
      "Epoch [94/100], Training Loss: 71049.10372608258, Test Loss: 41339328.0\n",
      "Epoch [95/100], Training Loss: 70363.06332563236, Test Loss: 40984100.0\n",
      "Epoch [96/100], Training Loss: 69687.69053965998, Test Loss: 40634000.0\n",
      "Epoch [97/100], Training Loss: 69022.0939517801, Test Loss: 40288608.0\n",
      "Epoch [98/100], Training Loss: 68365.56353296606, Test Loss: 39947648.0\n",
      "Epoch [99/100], Training Loss: 67718.28197381672, Test Loss: 39609984.0\n",
      "Epoch [100/100], Training Loss: 67079.20561578106, Test Loss: 39276140.0\n",
      "Epoch [1/100], Training Loss: 7081936.26971151, Test Loss: 147213936.0\n",
      "Epoch [2/100], Training Loss: 4024182.2657425506, Test Loss: 115950928.0\n",
      "Epoch [3/100], Training Loss: 3202896.755879391, Test Loss: 94409416.0\n",
      "Epoch [4/100], Training Loss: 2610051.823944079, Test Loss: 78811280.0\n",
      "Epoch [5/100], Training Loss: 2175307.9289141637, Test Loss: 67315608.0\n",
      "Epoch [6/100], Training Loss: 1862804.3471358332, Test Loss: 59074968.0\n",
      "Epoch [7/100], Training Loss: 1642528.8306676145, Test Loss: 53186968.0\n",
      "Epoch [8/100], Training Loss: 1480826.4764231977, Test Loss: 48641080.0\n",
      "Epoch [9/100], Training Loss: 1351222.5423553106, Test Loss: 44852392.0\n",
      "Epoch [10/100], Training Loss: 1241261.8673064392, Test Loss: 41548344.0\n",
      "Epoch [11/100], Training Loss: 1145324.047434986, Test Loss: 38603940.0\n",
      "Epoch [12/100], Training Loss: 1060954.477134056, Test Loss: 35962404.0\n",
      "Epoch [13/100], Training Loss: 986746.4550382086, Test Loss: 33604644.0\n",
      "Epoch [14/100], Training Loss: 921492.5454060779, Test Loss: 31521264.0\n",
      "Epoch [15/100], Training Loss: 864132.1042147977, Test Loss: 29687850.0\n",
      "Epoch [16/100], Training Loss: 813754.9252562053, Test Loss: 28075288.0\n",
      "Epoch [17/100], Training Loss: 769554.2949766009, Test Loss: 26647992.0\n",
      "Epoch [18/100], Training Loss: 730713.2426544636, Test Loss: 25388334.0\n",
      "Epoch [19/100], Training Loss: 696325.9480036728, Test Loss: 24274510.0\n",
      "Epoch [20/100], Training Loss: 665495.9184882413, Test Loss: 23280350.0\n",
      "Epoch [21/100], Training Loss: 637581.5125881168, Test Loss: 22397098.0\n",
      "Epoch [22/100], Training Loss: 612137.0523665659, Test Loss: 21603458.0\n",
      "Epoch [23/100], Training Loss: 588859.2737989456, Test Loss: 20897078.0\n",
      "Epoch [24/100], Training Loss: 567531.0575128843, Test Loss: 20264772.0\n",
      "Epoch [25/100], Training Loss: 547993.9024791186, Test Loss: 19705992.0\n",
      "Epoch [26/100], Training Loss: 530039.8074166223, Test Loss: 19200440.0\n",
      "Epoch [27/100], Training Loss: 513467.0474646052, Test Loss: 18750602.0\n",
      "Epoch [28/100], Training Loss: 498122.4788741781, Test Loss: 18337658.0\n",
      "Epoch [29/100], Training Loss: 483950.29582074523, Test Loss: 17975188.0\n",
      "Epoch [30/100], Training Loss: 470788.7369083585, Test Loss: 17626980.0\n",
      "Epoch [31/100], Training Loss: 458514.13017593743, Test Loss: 17323320.0\n",
      "Epoch [32/100], Training Loss: 447079.60225253244, Test Loss: 17043054.0\n",
      "Epoch [33/100], Training Loss: 436406.94237604405, Test Loss: 16791186.0\n",
      "Epoch [34/100], Training Loss: 426467.22803003376, Test Loss: 16568443.0\n",
      "Epoch [35/100], Training Loss: 417167.1299908181, Test Loss: 16365594.0\n",
      "Epoch [36/100], Training Loss: 408467.93882175226, Test Loss: 16175858.0\n",
      "Epoch [37/100], Training Loss: 400301.2445500859, Test Loss: 16033345.0\n",
      "Epoch [38/100], Training Loss: 392709.0956178544, Test Loss: 15877892.0\n",
      "Epoch [39/100], Training Loss: 385525.1854747941, Test Loss: 15759382.0\n",
      "Epoch [40/100], Training Loss: 378796.2541688881, Test Loss: 15646487.0\n",
      "Epoch [41/100], Training Loss: 372463.0526997808, Test Loss: 15545778.0\n",
      "Epoch [42/100], Training Loss: 366469.7526509093, Test Loss: 15461729.0\n",
      "Epoch [43/100], Training Loss: 360816.4593107636, Test Loss: 15381397.0\n",
      "Epoch [44/100], Training Loss: 355488.0265016883, Test Loss: 15313478.0\n",
      "Epoch [45/100], Training Loss: 350460.200032581, Test Loss: 15243243.0\n",
      "Epoch [46/100], Training Loss: 345701.60947218764, Test Loss: 15184669.0\n",
      "Epoch [47/100], Training Loss: 341246.88539630355, Test Loss: 15118351.0\n",
      "Epoch [48/100], Training Loss: 337009.43839227536, Test Loss: 15043610.0\n",
      "Epoch [49/100], Training Loss: 333067.7054973047, Test Loss: 15020330.0\n",
      "Epoch [50/100], Training Loss: 329352.68106895324, Test Loss: 14933500.0\n",
      "Epoch [51/100], Training Loss: 325820.45999940764, Test Loss: 14917327.0\n",
      "Epoch [52/100], Training Loss: 322478.69270185416, Test Loss: 14839609.0\n",
      "Epoch [53/100], Training Loss: 319335.8559030863, Test Loss: 14824036.0\n",
      "Epoch [54/100], Training Loss: 316365.77451350633, Test Loss: 14754852.0\n",
      "Epoch [55/100], Training Loss: 313524.5608191162, Test Loss: 14727696.0\n",
      "Epoch [56/100], Training Loss: 310870.46075099224, Test Loss: 14671767.0\n",
      "Epoch [57/100], Training Loss: 308279.9294917363, Test Loss: 14646601.0\n",
      "Epoch [58/100], Training Loss: 305825.9554861975, Test Loss: 14598596.0\n",
      "Epoch [59/100], Training Loss: 303459.7684193768, Test Loss: 14577237.0\n",
      "Epoch [60/100], Training Loss: 301186.04856051184, Test Loss: 14544804.0\n",
      "Epoch [61/100], Training Loss: 299018.42809963867, Test Loss: 14484642.0\n",
      "Epoch [62/100], Training Loss: 296965.70867395296, Test Loss: 14534392.0\n",
      "Epoch [63/100], Training Loss: 295064.98359842424, Test Loss: 14456138.0\n",
      "Epoch [64/100], Training Loss: 293182.62223431666, Test Loss: 14493573.0\n",
      "Epoch [65/100], Training Loss: 291412.17612256383, Test Loss: 14426699.0\n",
      "Epoch [66/100], Training Loss: 289617.6953971921, Test Loss: 14490195.0\n",
      "Epoch [67/100], Training Loss: 287848.25379864936, Test Loss: 14446746.0\n",
      "Epoch [68/100], Training Loss: 286113.96842604113, Test Loss: 14438066.0\n",
      "Epoch [69/100], Training Loss: 284448.58731710206, Test Loss: 14467591.0\n",
      "Epoch [70/100], Training Loss: 282812.9011906877, Test Loss: 14441882.0\n",
      "Epoch [71/100], Training Loss: 281202.9206874593, Test Loss: 14442291.0\n",
      "Epoch [72/100], Training Loss: 279690.53715715895, Test Loss: 14436587.0\n",
      "Epoch [73/100], Training Loss: 278190.86146777443, Test Loss: 14435917.0\n",
      "Epoch [74/100], Training Loss: 276734.65603341034, Test Loss: 14446729.0\n",
      "Epoch [75/100], Training Loss: 275371.89463375986, Test Loss: 14401551.0\n",
      "Epoch [76/100], Training Loss: 273965.91832163377, Test Loss: 14485341.0\n",
      "Epoch [77/100], Training Loss: 272692.93086532195, Test Loss: 14423992.0\n",
      "Epoch [78/100], Training Loss: 271420.05618002487, Test Loss: 14453578.0\n",
      "Epoch [79/100], Training Loss: 270222.51057772053, Test Loss: 14473999.0\n",
      "Epoch [80/100], Training Loss: 269077.5256612464, Test Loss: 14448450.0\n",
      "Epoch [81/100], Training Loss: 267903.1340708489, Test Loss: 14463608.0\n",
      "Epoch [82/100], Training Loss: 266774.2584710621, Test Loss: 14460405.0\n",
      "Epoch [83/100], Training Loss: 265654.07901264733, Test Loss: 14441331.0\n",
      "Epoch [84/100], Training Loss: 264525.2817442687, Test Loss: 14507897.0\n",
      "Epoch [85/100], Training Loss: 263494.2501443931, Test Loss: 14433189.0\n",
      "Epoch [86/100], Training Loss: 262420.6752488004, Test Loss: 14498047.0\n",
      "Epoch [87/100], Training Loss: 261443.61766557075, Test Loss: 14468841.0\n",
      "Epoch [88/100], Training Loss: 260397.35083822047, Test Loss: 14479397.0\n",
      "Epoch [89/100], Training Loss: 259447.83659513656, Test Loss: 14521872.0\n",
      "Epoch [90/100], Training Loss: 258497.3407973461, Test Loss: 14492539.0\n",
      "Epoch [91/100], Training Loss: 257589.79670931818, Test Loss: 14506975.0\n",
      "Epoch [92/100], Training Loss: 256658.57971980333, Test Loss: 14502890.0\n",
      "Epoch [93/100], Training Loss: 255785.1083836858, Test Loss: 14519471.0\n",
      "Epoch [94/100], Training Loss: 254891.74278034476, Test Loss: 14560024.0\n",
      "Epoch [95/100], Training Loss: 254069.65187192702, Test Loss: 14479601.0\n",
      "Epoch [96/100], Training Loss: 253163.96938495943, Test Loss: 14547426.0\n",
      "Epoch [97/100], Training Loss: 252342.83861664002, Test Loss: 14527102.0\n",
      "Epoch [98/100], Training Loss: 251448.85800604228, Test Loss: 14527237.0\n",
      "Epoch [99/100], Training Loss: 250680.6159365559, Test Loss: 14551420.0\n",
      "Epoch [100/100], Training Loss: 249888.70059534386, Test Loss: 14563009.0\n",
      "Epoch [1/100], Training Loss: 4397305.314140158, Test Loss: 232448624.0\n",
      "Epoch [2/100], Training Loss: 2692013.387121616, Test Loss: 145302240.0\n",
      "Epoch [3/100], Training Loss: 2081300.7961613648, Test Loss: 125607840.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100], Training Loss: 1804753.383804277, Test Loss: 110014536.0\n",
      "Epoch [5/100], Training Loss: 1575735.1248148805, Test Loss: 97116096.0\n",
      "Epoch [6/100], Training Loss: 1386412.3458325928, Test Loss: 86599832.0\n",
      "Epoch [7/100], Training Loss: 1230684.3140809194, Test Loss: 77917880.0\n",
      "Epoch [8/100], Training Loss: 1102751.6744861086, Test Loss: 70752336.0\n",
      "Epoch [9/100], Training Loss: 998755.575854511, Test Loss: 64886936.0\n",
      "Epoch [10/100], Training Loss: 914826.6202831586, Test Loss: 60129688.0\n",
      "Epoch [11/100], Training Loss: 846943.1633197085, Test Loss: 56228960.0\n",
      "Epoch [12/100], Training Loss: 790696.8164208281, Test Loss: 52936620.0\n",
      "Epoch [13/100], Training Loss: 742579.0206148925, Test Loss: 50067524.0\n",
      "Epoch [14/100], Training Loss: 700131.1239855458, Test Loss: 47503552.0\n",
      "Epoch [15/100], Training Loss: 661945.8090160536, Test Loss: 45171784.0\n",
      "Epoch [16/100], Training Loss: 627160.8737041644, Test Loss: 43022924.0\n",
      "Epoch [17/100], Training Loss: 595243.2791600024, Test Loss: 41028328.0\n",
      "Epoch [18/100], Training Loss: 565849.0270422369, Test Loss: 39166500.0\n",
      "Epoch [19/100], Training Loss: 538751.3209821694, Test Loss: 37429640.0\n",
      "Epoch [20/100], Training Loss: 513744.29213316744, Test Loss: 35809460.0\n",
      "Epoch [21/100], Training Loss: 490663.2024169184, Test Loss: 34305200.0\n",
      "Epoch [22/100], Training Loss: 469398.42174634204, Test Loss: 32914164.0\n",
      "Epoch [23/100], Training Loss: 449790.7661572182, Test Loss: 31626828.0\n",
      "Epoch [24/100], Training Loss: 431686.56406610983, Test Loss: 30437654.0\n",
      "Epoch [25/100], Training Loss: 415007.9737870979, Test Loss: 29338922.0\n",
      "Epoch [26/100], Training Loss: 399645.8030922339, Test Loss: 28324524.0\n",
      "Epoch [27/100], Training Loss: 385469.76316568925, Test Loss: 27384970.0\n",
      "Epoch [28/100], Training Loss: 372421.222765239, Test Loss: 26516594.0\n",
      "Epoch [29/100], Training Loss: 360364.2626621646, Test Loss: 25713948.0\n",
      "Epoch [30/100], Training Loss: 349168.9200284343, Test Loss: 24969082.0\n",
      "Epoch [31/100], Training Loss: 338737.5954475446, Test Loss: 24275098.0\n",
      "Epoch [32/100], Training Loss: 328974.7268822937, Test Loss: 23628394.0\n",
      "Epoch [33/100], Training Loss: 319790.4486108643, Test Loss: 23025216.0\n",
      "Epoch [34/100], Training Loss: 311138.0711006457, Test Loss: 22462912.0\n",
      "Epoch [35/100], Training Loss: 302982.225919673, Test Loss: 21941012.0\n",
      "Epoch [36/100], Training Loss: 295290.7047123986, Test Loss: 21454426.0\n",
      "Epoch [37/100], Training Loss: 288005.4976304721, Test Loss: 21004300.0\n",
      "Epoch [38/100], Training Loss: 281104.1958859072, Test Loss: 20583452.0\n",
      "Epoch [39/100], Training Loss: 274568.38442627806, Test Loss: 20194732.0\n",
      "Epoch [40/100], Training Loss: 268366.2429802737, Test Loss: 19828124.0\n",
      "Epoch [41/100], Training Loss: 262483.5308038623, Test Loss: 19490046.0\n",
      "Epoch [42/100], Training Loss: 256891.75167347907, Test Loss: 19169206.0\n",
      "Epoch [43/100], Training Loss: 251579.12885788755, Test Loss: 18870474.0\n",
      "Epoch [44/100], Training Loss: 246517.59775783424, Test Loss: 18589530.0\n",
      "Epoch [45/100], Training Loss: 241697.93583022332, Test Loss: 18325242.0\n",
      "Epoch [46/100], Training Loss: 237100.88621823353, Test Loss: 18079516.0\n",
      "Epoch [47/100], Training Loss: 232711.08518452698, Test Loss: 17849296.0\n",
      "Epoch [48/100], Training Loss: 228530.76232154493, Test Loss: 17632312.0\n",
      "Epoch [49/100], Training Loss: 224544.5739885078, Test Loss: 17430100.0\n",
      "Epoch [50/100], Training Loss: 220710.74619394584, Test Loss: 17237246.0\n",
      "Epoch [51/100], Training Loss: 217038.53189976898, Test Loss: 17059586.0\n",
      "Epoch [52/100], Training Loss: 213520.97089923584, Test Loss: 16887430.0\n",
      "Epoch [53/100], Training Loss: 210151.6069841834, Test Loss: 16732640.0\n",
      "Epoch [54/100], Training Loss: 206938.66209051595, Test Loss: 16580391.0\n",
      "Epoch [55/100], Training Loss: 203863.7466530419, Test Loss: 16447597.0\n",
      "Epoch [56/100], Training Loss: 200921.93772584564, Test Loss: 16315778.0\n",
      "Epoch [57/100], Training Loss: 198091.85714708845, Test Loss: 16195956.0\n",
      "Epoch [58/100], Training Loss: 195379.70585273384, Test Loss: 16088667.0\n",
      "Epoch [59/100], Training Loss: 192782.70816302352, Test Loss: 15982033.0\n",
      "Epoch [60/100], Training Loss: 190276.40203483205, Test Loss: 15887761.0\n",
      "Epoch [61/100], Training Loss: 187894.06412534803, Test Loss: 15802661.0\n",
      "Epoch [62/100], Training Loss: 185597.34107872756, Test Loss: 15719141.0\n",
      "Epoch [63/100], Training Loss: 183399.12408921274, Test Loss: 15647544.0\n",
      "Epoch [64/100], Training Loss: 181281.31582844618, Test Loss: 15579925.0\n",
      "Epoch [65/100], Training Loss: 179257.06076358035, Test Loss: 15519401.0\n",
      "Epoch [66/100], Training Loss: 177308.04950832296, Test Loss: 15460540.0\n",
      "Epoch [67/100], Training Loss: 175425.67867721108, Test Loss: 15408991.0\n",
      "Epoch [68/100], Training Loss: 173615.5465538179, Test Loss: 15353989.0\n",
      "Epoch [69/100], Training Loss: 171885.28030033765, Test Loss: 15311354.0\n",
      "Epoch [70/100], Training Loss: 170209.20648954445, Test Loss: 15258798.0\n",
      "Epoch [71/100], Training Loss: 168622.5172827439, Test Loss: 15215019.0\n",
      "Epoch [72/100], Training Loss: 167070.7983309638, Test Loss: 15164289.0\n",
      "Epoch [73/100], Training Loss: 165603.30735738404, Test Loss: 15122192.0\n",
      "Epoch [74/100], Training Loss: 164181.7281633197, Test Loss: 15079182.0\n",
      "Epoch [75/100], Training Loss: 162826.04462117172, Test Loss: 15030115.0\n",
      "Epoch [76/100], Training Loss: 161513.07387743617, Test Loss: 15003633.0\n",
      "Epoch [77/100], Training Loss: 160247.80646140632, Test Loss: 14946953.0\n",
      "Epoch [78/100], Training Loss: 159037.78814939872, Test Loss: 14929720.0\n",
      "Epoch [79/100], Training Loss: 157873.01392838103, Test Loss: 14878700.0\n",
      "Epoch [80/100], Training Loss: 156739.31860523665, Test Loss: 14854877.0\n",
      "Epoch [81/100], Training Loss: 155669.71952935253, Test Loss: 14812545.0\n",
      "Epoch [82/100], Training Loss: 154608.20588975772, Test Loss: 14786178.0\n",
      "Epoch [83/100], Training Loss: 153603.83417747763, Test Loss: 14750299.0\n",
      "Epoch [84/100], Training Loss: 152618.76878591315, Test Loss: 14722566.0\n",
      "Epoch [85/100], Training Loss: 151683.43679284403, Test Loss: 14696732.0\n",
      "Epoch [86/100], Training Loss: 150769.90553729044, Test Loss: 14667919.0\n",
      "Epoch [87/100], Training Loss: 149885.25601267698, Test Loss: 14648159.0\n",
      "Epoch [88/100], Training Loss: 149025.18340145727, Test Loss: 14610705.0\n",
      "Epoch [89/100], Training Loss: 148194.306143001, Test Loss: 14617553.0\n",
      "Epoch [90/100], Training Loss: 147382.83980510634, Test Loss: 14577459.0\n",
      "Epoch [91/100], Training Loss: 146604.8382426989, Test Loss: 14538407.0\n",
      "Epoch [92/100], Training Loss: 145829.13102008175, Test Loss: 14551059.0\n",
      "Epoch [93/100], Training Loss: 145099.7263047213, Test Loss: 14525134.0\n",
      "Epoch [94/100], Training Loss: 144375.84295213554, Test Loss: 14502348.0\n",
      "Epoch [95/100], Training Loss: 143682.67546353888, Test Loss: 14491811.0\n",
      "Epoch [96/100], Training Loss: 142998.71952194776, Test Loss: 14465046.0\n",
      "Epoch [97/100], Training Loss: 142351.63488537408, Test Loss: 14462644.0\n",
      "Epoch [98/100], Training Loss: 141702.3357028612, Test Loss: 14427341.0\n",
      "Epoch [99/100], Training Loss: 141073.0821485694, Test Loss: 14431397.0\n",
      "Epoch [100/100], Training Loss: 140451.48523487945, Test Loss: 14416367.0\n",
      "Epoch [1/100], Training Loss: 2318201.936852082, Test Loss: 292394432.0\n",
      "Epoch [2/100], Training Loss: 2070497.8885137136, Test Loss: 229808768.0\n",
      "Epoch [3/100], Training Loss: 1485792.7596706357, Test Loss: 162809824.0\n",
      "Epoch [4/100], Training Loss: 1169262.8780285527, Test Loss: 142479456.0\n",
      "Epoch [5/100], Training Loss: 1052861.892778864, Test Loss: 130676456.0\n",
      "Epoch [6/100], Training Loss: 966195.1066879925, Test Loss: 120630008.0\n",
      "Epoch [7/100], Training Loss: 889445.5278715716, Test Loss: 111588976.0\n",
      "Epoch [8/100], Training Loss: 819891.0204371779, Test Loss: 103451040.0\n",
      "Epoch [9/100], Training Loss: 757330.1998696759, Test Loss: 96221656.0\n",
      "Epoch [10/100], Training Loss: 701785.476808246, Test Loss: 89873816.0\n",
      "Epoch [11/100], Training Loss: 652886.7319471595, Test Loss: 84299640.0\n",
      "Epoch [12/100], Training Loss: 609889.4076180321, Test Loss: 79379464.0\n",
      "Epoch [13/100], Training Loss: 572038.5387121616, Test Loss: 75019120.0\n",
      "Epoch [14/100], Training Loss: 538679.3739707363, Test Loss: 71143112.0\n",
      "Epoch [15/100], Training Loss: 509267.4116462295, Test Loss: 67695312.0\n",
      "Epoch [16/100], Training Loss: 483295.50654582074, Test Loss: 64625872.0\n",
      "Epoch [17/100], Training Loss: 460293.74800071085, Test Loss: 61894536.0\n",
      "Epoch [18/100], Training Loss: 439857.53794206504, Test Loss: 59456372.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/100], Training Loss: 421580.20354244416, Test Loss: 57261004.0\n",
      "Epoch [20/100], Training Loss: 405075.8011966116, Test Loss: 55260580.0\n",
      "Epoch [21/100], Training Loss: 389969.92488596647, Test Loss: 53419076.0\n",
      "Epoch [22/100], Training Loss: 376009.30294413836, Test Loss: 51707680.0\n",
      "Epoch [23/100], Training Loss: 362983.767904745, Test Loss: 50104684.0\n",
      "Epoch [24/100], Training Loss: 350746.10816894734, Test Loss: 48590948.0\n",
      "Epoch [25/100], Training Loss: 339192.40370831115, Test Loss: 47154956.0\n",
      "Epoch [26/100], Training Loss: 328256.1808542148, Test Loss: 45790108.0\n",
      "Epoch [27/100], Training Loss: 317880.3920383864, Test Loss: 44487060.0\n",
      "Epoch [28/100], Training Loss: 308015.0971506427, Test Loss: 43238708.0\n",
      "Epoch [29/100], Training Loss: 298607.42313843966, Test Loss: 42042120.0\n",
      "Epoch [30/100], Training Loss: 289629.4912031278, Test Loss: 40891916.0\n",
      "Epoch [31/100], Training Loss: 281051.9825839701, Test Loss: 39785712.0\n",
      "Epoch [32/100], Training Loss: 272854.878383982, Test Loss: 38723452.0\n",
      "Epoch [33/100], Training Loss: 265031.4263965405, Test Loss: 37702892.0\n",
      "Epoch [34/100], Training Loss: 257562.09649902256, Test Loss: 36724580.0\n",
      "Epoch [35/100], Training Loss: 250432.63864699958, Test Loss: 35787704.0\n",
      "Epoch [36/100], Training Loss: 243633.01403945265, Test Loss: 34891972.0\n",
      "Epoch [37/100], Training Loss: 237145.20206148925, Test Loss: 34036552.0\n",
      "Epoch [38/100], Training Loss: 230938.4281440673, Test Loss: 33216046.0\n",
      "Epoch [39/100], Training Loss: 225020.81443634856, Test Loss: 32430570.0\n",
      "Epoch [40/100], Training Loss: 219379.106865707, Test Loss: 31679278.0\n",
      "Epoch [41/100], Training Loss: 213997.4396066584, Test Loss: 30961576.0\n",
      "Epoch [42/100], Training Loss: 208874.5611634382, Test Loss: 30278486.0\n",
      "Epoch [43/100], Training Loss: 203997.15938036845, Test Loss: 29627202.0\n",
      "Epoch [44/100], Training Loss: 199343.88516675553, Test Loss: 29005576.0\n",
      "Epoch [45/100], Training Loss: 194898.70363130147, Test Loss: 28411192.0\n",
      "Epoch [46/100], Training Loss: 190657.76443931047, Test Loss: 27844252.0\n",
      "Epoch [47/100], Training Loss: 186615.58225223623, Test Loss: 27302922.0\n",
      "Epoch [48/100], Training Loss: 182761.4430128547, Test Loss: 26785938.0\n",
      "Epoch [49/100], Training Loss: 179083.19258337776, Test Loss: 26293336.0\n",
      "Epoch [50/100], Training Loss: 175568.95062496298, Test Loss: 25824360.0\n",
      "Epoch [51/100], Training Loss: 172204.70058053432, Test Loss: 25376476.0\n",
      "Epoch [52/100], Training Loss: 168976.592115396, Test Loss: 24948286.0\n",
      "Epoch [53/100], Training Loss: 165872.40459688407, Test Loss: 24537592.0\n",
      "Epoch [54/100], Training Loss: 162884.59676559447, Test Loss: 24145108.0\n",
      "Epoch [55/100], Training Loss: 160003.04155559506, Test Loss: 23768822.0\n",
      "Epoch [56/100], Training Loss: 157229.2452165156, Test Loss: 23411424.0\n",
      "Epoch [57/100], Training Loss: 154553.2784195249, Test Loss: 23068624.0\n",
      "Epoch [58/100], Training Loss: 151970.64072033647, Test Loss: 22740334.0\n",
      "Epoch [59/100], Training Loss: 149474.6050885611, Test Loss: 22424886.0\n",
      "Epoch [60/100], Training Loss: 147063.2767312363, Test Loss: 22123736.0\n",
      "Epoch [61/100], Training Loss: 144734.3394941058, Test Loss: 21835406.0\n",
      "Epoch [62/100], Training Loss: 142480.7620697826, Test Loss: 21558216.0\n",
      "Epoch [63/100], Training Loss: 140296.6220899236, Test Loss: 21292206.0\n",
      "Epoch [64/100], Training Loss: 138179.89538534448, Test Loss: 21037276.0\n",
      "Epoch [65/100], Training Loss: 136126.55076713464, Test Loss: 20792118.0\n",
      "Epoch [66/100], Training Loss: 134140.44336828386, Test Loss: 20556156.0\n",
      "Epoch [67/100], Training Loss: 132213.24586813577, Test Loss: 20328054.0\n",
      "Epoch [68/100], Training Loss: 130344.93412712518, Test Loss: 20110244.0\n",
      "Epoch [69/100], Training Loss: 128536.43024702328, Test Loss: 19900068.0\n",
      "Epoch [70/100], Training Loss: 126785.61825425034, Test Loss: 19700970.0\n",
      "Epoch [71/100], Training Loss: 125088.59642497482, Test Loss: 19507510.0\n",
      "Epoch [72/100], Training Loss: 123444.76162549612, Test Loss: 19323794.0\n",
      "Epoch [73/100], Training Loss: 121845.30931224454, Test Loss: 19146644.0\n",
      "Epoch [74/100], Training Loss: 120292.66056513239, Test Loss: 18976398.0\n",
      "Epoch [75/100], Training Loss: 118784.67001362478, Test Loss: 18811016.0\n",
      "Epoch [76/100], Training Loss: 117316.77129613174, Test Loss: 18652938.0\n",
      "Epoch [77/100], Training Loss: 115885.41678514305, Test Loss: 18499740.0\n",
      "Epoch [78/100], Training Loss: 114494.49263965405, Test Loss: 18355758.0\n",
      "Epoch [79/100], Training Loss: 113139.7343759256, Test Loss: 18212674.0\n",
      "Epoch [80/100], Training Loss: 111825.15668503051, Test Loss: 18080630.0\n",
      "Epoch [81/100], Training Loss: 110548.3848261359, Test Loss: 17949442.0\n",
      "Epoch [82/100], Training Loss: 109305.90658136367, Test Loss: 17824662.0\n",
      "Epoch [83/100], Training Loss: 108097.60671761152, Test Loss: 17704132.0\n",
      "Epoch [84/100], Training Loss: 106923.53409158225, Test Loss: 17585066.0\n",
      "Epoch [85/100], Training Loss: 105778.46596765594, Test Loss: 17472028.0\n",
      "Epoch [86/100], Training Loss: 104661.57900894497, Test Loss: 17361782.0\n",
      "Epoch [87/100], Training Loss: 103573.72461643268, Test Loss: 17255956.0\n",
      "Epoch [88/100], Training Loss: 102517.65264202358, Test Loss: 17155580.0\n",
      "Epoch [89/100], Training Loss: 101489.39938392275, Test Loss: 17059032.0\n",
      "Epoch [90/100], Training Loss: 100489.98554587999, Test Loss: 16968334.0\n",
      "Epoch [91/100], Training Loss: 99512.24434275221, Test Loss: 16878548.0\n",
      "Epoch [92/100], Training Loss: 98561.43403826788, Test Loss: 16795146.0\n",
      "Epoch [93/100], Training Loss: 97630.1024376518, Test Loss: 16712336.0\n",
      "Epoch [94/100], Training Loss: 96728.62779160003, Test Loss: 16636232.0\n",
      "Epoch [95/100], Training Loss: 95850.72799300989, Test Loss: 16562916.0\n",
      "Epoch [96/100], Training Loss: 94994.27877495409, Test Loss: 16491055.0\n",
      "Epoch [97/100], Training Loss: 94160.44517504887, Test Loss: 16425467.0\n",
      "Epoch [98/100], Training Loss: 93350.23713050174, Test Loss: 16355641.0\n",
      "Epoch [99/100], Training Loss: 92558.3790504117, Test Loss: 16300005.0\n",
      "Epoch [100/100], Training Loss: 91788.97076594988, Test Loss: 16233922.0\n",
      "Epoch [1/100], Training Loss: 1497586.2083999764, Test Loss: 298560928.0\n",
      "Epoch [2/100], Training Loss: 1463600.1478585391, Test Loss: 283239776.0\n",
      "Epoch [3/100], Training Loss: 1323099.1206682068, Test Loss: 241279824.0\n",
      "Epoch [4/100], Training Loss: 1077616.075824892, Test Loss: 190047696.0\n",
      "Epoch [5/100], Training Loss: 866565.8003672768, Test Loss: 159857952.0\n",
      "Epoch [6/100], Training Loss: 760944.8802796043, Test Loss: 146097424.0\n",
      "Epoch [7/100], Training Loss: 703640.6347965167, Test Loss: 136924448.0\n",
      "Epoch [8/100], Training Loss: 659519.0988685504, Test Loss: 129265016.0\n",
      "Epoch [9/100], Training Loss: 620906.1328120372, Test Loss: 122479256.0\n",
      "Epoch [10/100], Training Loss: 585940.2362419288, Test Loss: 116362112.0\n",
      "Epoch [11/100], Training Loss: 553949.2392630768, Test Loss: 110819480.0\n",
      "Epoch [12/100], Training Loss: 524406.145370535, Test Loss: 105698056.0\n",
      "Epoch [13/100], Training Loss: 496677.413897281, Test Loss: 100998968.0\n",
      "Epoch [14/100], Training Loss: 471404.90231621347, Test Loss: 96841104.0\n",
      "Epoch [15/100], Training Loss: 448892.53243291273, Test Loss: 93170296.0\n",
      "Epoch [16/100], Training Loss: 428716.4213020556, Test Loss: 89880312.0\n",
      "Epoch [17/100], Training Loss: 410520.4298323559, Test Loss: 86906120.0\n",
      "Epoch [18/100], Training Loss: 394054.23861145665, Test Loss: 84201424.0\n",
      "Epoch [19/100], Training Loss: 379146.93394941057, Test Loss: 81732696.0\n",
      "Epoch [20/100], Training Loss: 365648.7777975238, Test Loss: 79471536.0\n",
      "Epoch [21/100], Training Loss: 353417.17445648956, Test Loss: 77394832.0\n",
      "Epoch [22/100], Training Loss: 342329.52455423254, Test Loss: 75481776.0\n",
      "Epoch [23/100], Training Loss: 332267.6295243173, Test Loss: 73718632.0\n",
      "Epoch [24/100], Training Loss: 323122.61181209644, Test Loss: 72083360.0\n",
      "Epoch [25/100], Training Loss: 314779.47503110004, Test Loss: 70560088.0\n",
      "Epoch [26/100], Training Loss: 307125.38149398734, Test Loss: 69132232.0\n",
      "Epoch [27/100], Training Loss: 300063.05053018185, Test Loss: 67786240.0\n",
      "Epoch [28/100], Training Loss: 293501.844973639, Test Loss: 66510020.0\n",
      "Epoch [29/100], Training Loss: 287365.1582252236, Test Loss: 65294196.0\n",
      "Epoch [30/100], Training Loss: 281569.68416414905, Test Loss: 64126580.0\n",
      "Epoch [31/100], Training Loss: 276055.2604037083, Test Loss: 62999752.0\n",
      "Epoch [32/100], Training Loss: 270778.1860866136, Test Loss: 61910508.0\n",
      "Epoch [33/100], Training Loss: 265694.56362645136, Test Loss: 60853224.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/100], Training Loss: 260777.2811056069, Test Loss: 59824552.0\n",
      "Epoch [35/100], Training Loss: 256000.55789052782, Test Loss: 58821720.0\n",
      "Epoch [36/100], Training Loss: 251346.59618802203, Test Loss: 57841756.0\n",
      "Epoch [37/100], Training Loss: 246800.07909039748, Test Loss: 56884812.0\n",
      "Epoch [38/100], Training Loss: 242356.18838487056, Test Loss: 55950272.0\n",
      "Epoch [39/100], Training Loss: 237997.9434719507, Test Loss: 55036780.0\n",
      "Epoch [40/100], Training Loss: 233725.49700847105, Test Loss: 54142276.0\n",
      "Epoch [41/100], Training Loss: 229543.6696581956, Test Loss: 53266932.0\n",
      "Epoch [42/100], Training Loss: 225451.41477104436, Test Loss: 52409672.0\n",
      "Epoch [43/100], Training Loss: 221446.2723624193, Test Loss: 51574324.0\n",
      "Epoch [44/100], Training Loss: 217540.51229192584, Test Loss: 50763180.0\n",
      "Epoch [45/100], Training Loss: 213733.1393430484, Test Loss: 49975440.0\n",
      "Epoch [46/100], Training Loss: 210023.8663734376, Test Loss: 49207568.0\n",
      "Epoch [47/100], Training Loss: 206409.99266927314, Test Loss: 48457368.0\n",
      "Epoch [48/100], Training Loss: 202895.5451543155, Test Loss: 47723592.0\n",
      "Epoch [49/100], Training Loss: 199476.48502754577, Test Loss: 47010540.0\n",
      "Epoch [50/100], Training Loss: 196148.20021770036, Test Loss: 46319792.0\n",
      "Epoch [51/100], Training Loss: 192909.997963687, Test Loss: 45648116.0\n",
      "Epoch [52/100], Training Loss: 189761.4067738878, Test Loss: 44992244.0\n",
      "Epoch [53/100], Training Loss: 186699.263402642, Test Loss: 44350968.0\n",
      "Epoch [54/100], Training Loss: 183713.86778774954, Test Loss: 43724904.0\n",
      "Epoch [55/100], Training Loss: 180801.08631005272, Test Loss: 43110768.0\n",
      "Epoch [56/100], Training Loss: 177955.11956119305, Test Loss: 42508372.0\n",
      "Epoch [57/100], Training Loss: 175175.19002687934, Test Loss: 41917656.0\n",
      "Epoch [58/100], Training Loss: 172463.9875784906, Test Loss: 41339780.0\n",
      "Epoch [59/100], Training Loss: 169823.02196811503, Test Loss: 40774732.0\n",
      "Epoch [60/100], Training Loss: 167247.51202998194, Test Loss: 40222448.0\n",
      "Epoch [61/100], Training Loss: 164730.5485984613, Test Loss: 39680300.0\n",
      "Epoch [62/100], Training Loss: 162272.32479377702, Test Loss: 39148720.0\n",
      "Epoch [63/100], Training Loss: 159876.4007163194, Test Loss: 38630756.0\n",
      "Epoch [64/100], Training Loss: 157540.8305692698, Test Loss: 38126784.0\n",
      "Epoch [65/100], Training Loss: 155264.91314569634, Test Loss: 37631504.0\n",
      "Epoch [66/100], Training Loss: 153042.0512153549, Test Loss: 37147896.0\n",
      "Epoch [67/100], Training Loss: 150873.59991204517, Test Loss: 36676512.0\n",
      "Epoch [68/100], Training Loss: 148764.67586213848, Test Loss: 36218784.0\n",
      "Epoch [69/100], Training Loss: 146716.71387646516, Test Loss: 35772296.0\n",
      "Epoch [70/100], Training Loss: 144720.64487069906, Test Loss: 35336204.0\n",
      "Epoch [71/100], Training Loss: 142774.1672939899, Test Loss: 34910620.0\n",
      "Epoch [72/100], Training Loss: 140882.80325690334, Test Loss: 34496160.0\n",
      "Epoch [73/100], Training Loss: 139046.3294023236, Test Loss: 34093728.0\n",
      "Epoch [74/100], Training Loss: 137267.34822144257, Test Loss: 33702220.0\n",
      "Epoch [75/100], Training Loss: 135540.07446646286, Test Loss: 33320760.0\n",
      "Epoch [76/100], Training Loss: 133862.28840011978, Test Loss: 32948380.0\n",
      "Epoch [77/100], Training Loss: 132225.72267684454, Test Loss: 32587550.0\n",
      "Epoch [78/100], Training Loss: 130633.19799858199, Test Loss: 32237292.0\n",
      "Epoch [79/100], Training Loss: 129078.29134317058, Test Loss: 31895174.0\n",
      "Epoch [80/100], Training Loss: 127557.28810797827, Test Loss: 31559860.0\n",
      "Epoch [81/100], Training Loss: 126065.25399742129, Test Loss: 31231286.0\n",
      "Epoch [82/100], Training Loss: 124607.20354244417, Test Loss: 30913182.0\n",
      "Epoch [83/100], Training Loss: 123186.93228102231, Test Loss: 30602200.0\n",
      "Epoch [84/100], Training Loss: 121808.88043464176, Test Loss: 30304028.0\n",
      "Epoch [85/100], Training Loss: 120471.57676529826, Test Loss: 30012550.0\n",
      "Epoch [86/100], Training Loss: 119167.86783819456, Test Loss: 29731596.0\n",
      "Epoch [87/100], Training Loss: 117895.1662774532, Test Loss: 29456732.0\n",
      "Epoch [88/100], Training Loss: 116653.02774198803, Test Loss: 29190630.0\n",
      "Epoch [89/100], Training Loss: 115439.02694504916, Test Loss: 28930492.0\n",
      "Epoch [90/100], Training Loss: 114251.78337054233, Test Loss: 28677800.0\n",
      "Epoch [91/100], Training Loss: 113090.57924034417, Test Loss: 28430304.0\n",
      "Epoch [92/100], Training Loss: 111955.681145315, Test Loss: 28190166.0\n",
      "Epoch [93/100], Training Loss: 110847.27407153383, Test Loss: 27954738.0\n",
      "Epoch [94/100], Training Loss: 109761.57205725001, Test Loss: 27722378.0\n",
      "Epoch [95/100], Training Loss: 108697.45950328773, Test Loss: 27497514.0\n",
      "Epoch [96/100], Training Loss: 107654.2326816206, Test Loss: 27277618.0\n",
      "Epoch [97/100], Training Loss: 106629.73992256456, Test Loss: 27062124.0\n",
      "Epoch [98/100], Training Loss: 105625.56057938658, Test Loss: 26850620.0\n",
      "Epoch [99/100], Training Loss: 104637.71401233265, Test Loss: 26641604.0\n",
      "Epoch [100/100], Training Loss: 103655.7879554862, Test Loss: 26431630.0\n",
      "Epoch [1/100], Training Loss: 1164513.918369765, Test Loss: 299463616.0\n",
      "Epoch [2/100], Training Loss: 1152058.1984479593, Test Loss: 291892608.0\n",
      "Epoch [3/100], Training Loss: 1092778.7851430601, Test Loss: 267541424.0\n",
      "Epoch [4/100], Training Loss: 962853.9356673183, Test Loss: 226373088.0\n",
      "Epoch [5/100], Training Loss: 795029.3176944493, Test Loss: 184976960.0\n",
      "Epoch [6/100], Training Loss: 662795.2322729696, Test Loss: 159774016.0\n",
      "Epoch [7/100], Training Loss: 591941.7472898525, Test Loss: 147232880.0\n",
      "Epoch [8/100], Training Loss: 552446.7010248208, Test Loss: 139037648.0\n",
      "Epoch [9/100], Training Loss: 522824.4691665186, Test Loss: 132246072.0\n",
      "Epoch [10/100], Training Loss: 496971.1732717256, Test Loss: 126134336.0\n",
      "Epoch [11/100], Training Loss: 473224.88288608496, Test Loss: 120463464.0\n",
      "Epoch [12/100], Training Loss: 450963.3346365737, Test Loss: 115135168.0\n",
      "Epoch [13/100], Training Loss: 429934.55624666787, Test Loss: 110112672.0\n",
      "Epoch [14/100], Training Loss: 410062.26834903145, Test Loss: 105388312.0\n",
      "Epoch [15/100], Training Loss: 391282.2159824655, Test Loss: 100911040.0\n",
      "Epoch [16/100], Training Loss: 373205.1854747941, Test Loss: 96574840.0\n",
      "Epoch [17/100], Training Loss: 355752.93430483976, Test Loss: 92482648.0\n",
      "Epoch [18/100], Training Loss: 339614.5365795865, Test Loss: 88789184.0\n",
      "Epoch [19/100], Training Loss: 325129.2785972395, Test Loss: 85481296.0\n",
      "Epoch [20/100], Training Loss: 312075.6129376222, Test Loss: 82464392.0\n",
      "Epoch [21/100], Training Loss: 300184.90492269414, Test Loss: 79693000.0\n",
      "Epoch [22/100], Training Loss: 289301.575262129, Test Loss: 77139872.0\n",
      "Epoch [23/100], Training Loss: 279318.5240210888, Test Loss: 74781016.0\n",
      "Epoch [24/100], Training Loss: 270138.32983827975, Test Loss: 72597432.0\n",
      "Epoch [25/100], Training Loss: 261679.76529826433, Test Loss: 70567872.0\n",
      "Epoch [26/100], Training Loss: 253877.25549434274, Test Loss: 68680872.0\n",
      "Epoch [27/100], Training Loss: 246671.68757775013, Test Loss: 66928684.0\n",
      "Epoch [28/100], Training Loss: 239997.45133582133, Test Loss: 65298596.0\n",
      "Epoch [29/100], Training Loss: 233798.53302529472, Test Loss: 63774844.0\n",
      "Epoch [30/100], Training Loss: 228026.22569752976, Test Loss: 62345760.0\n",
      "Epoch [31/100], Training Loss: 222627.70606006752, Test Loss: 61000904.0\n",
      "Epoch [32/100], Training Loss: 217539.56554706476, Test Loss: 59730120.0\n",
      "Epoch [33/100], Training Loss: 212719.66483028256, Test Loss: 58526652.0\n",
      "Epoch [34/100], Training Loss: 208149.4871156922, Test Loss: 57382808.0\n",
      "Epoch [35/100], Training Loss: 203797.2193590427, Test Loss: 56291944.0\n",
      "Epoch [36/100], Training Loss: 199633.56400687163, Test Loss: 55250832.0\n",
      "Epoch [37/100], Training Loss: 195655.0348912979, Test Loss: 54253844.0\n",
      "Epoch [38/100], Training Loss: 191849.01321011788, Test Loss: 53297568.0\n",
      "Epoch [39/100], Training Loss: 188194.81618387537, Test Loss: 52383440.0\n",
      "Epoch [40/100], Training Loss: 184687.2250459096, Test Loss: 51506644.0\n",
      "Epoch [41/100], Training Loss: 181316.93679284403, Test Loss: 50662108.0\n",
      "Epoch [42/100], Training Loss: 178074.21728570582, Test Loss: 49848616.0\n",
      "Epoch [43/100], Training Loss: 174949.39968011374, Test Loss: 49066656.0\n",
      "Epoch [44/100], Training Loss: 171936.1272436467, Test Loss: 48315796.0\n",
      "Epoch [45/100], Training Loss: 169028.77163675136, Test Loss: 47589780.0\n",
      "Epoch [46/100], Training Loss: 166230.96617498965, Test Loss: 46889984.0\n",
      "Epoch [47/100], Training Loss: 163535.00408743558, Test Loss: 46212840.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/100], Training Loss: 160933.87891712575, Test Loss: 45557944.0\n",
      "Epoch [49/100], Training Loss: 158424.0496416089, Test Loss: 44925432.0\n",
      "Epoch [50/100], Training Loss: 155997.86576624608, Test Loss: 44311244.0\n",
      "Epoch [51/100], Training Loss: 153642.31408091937, Test Loss: 43713824.0\n",
      "Epoch [52/100], Training Loss: 151352.18944375333, Test Loss: 43129804.0\n",
      "Epoch [53/100], Training Loss: 149120.34263373024, Test Loss: 42561448.0\n",
      "Epoch [54/100], Training Loss: 146947.15561874295, Test Loss: 42006708.0\n",
      "Epoch [55/100], Training Loss: 144835.24945204667, Test Loss: 41469156.0\n",
      "Epoch [56/100], Training Loss: 142783.977726438, Test Loss: 40946048.0\n",
      "Epoch [57/100], Training Loss: 140790.03471358333, Test Loss: 40435064.0\n",
      "Epoch [58/100], Training Loss: 138848.39630353652, Test Loss: 39936256.0\n",
      "Epoch [59/100], Training Loss: 136959.3178129258, Test Loss: 39449756.0\n",
      "Epoch [60/100], Training Loss: 135123.03192938806, Test Loss: 38976336.0\n",
      "Epoch [61/100], Training Loss: 133336.10437770275, Test Loss: 38515656.0\n",
      "Epoch [62/100], Training Loss: 131599.06593211304, Test Loss: 38068336.0\n",
      "Epoch [63/100], Training Loss: 129911.75256205202, Test Loss: 37632540.0\n",
      "Epoch [64/100], Training Loss: 128268.82033054913, Test Loss: 37208728.0\n",
      "Epoch [65/100], Training Loss: 126666.59303358805, Test Loss: 36795992.0\n",
      "Epoch [66/100], Training Loss: 125099.7694449381, Test Loss: 36391556.0\n",
      "Epoch [67/100], Training Loss: 123568.89378591315, Test Loss: 35995508.0\n",
      "Epoch [68/100], Training Loss: 122079.30377347313, Test Loss: 35609732.0\n",
      "Epoch [69/100], Training Loss: 120633.64143119483, Test Loss: 35235312.0\n",
      "Epoch [70/100], Training Loss: 119239.25425034061, Test Loss: 34873932.0\n",
      "Epoch [71/100], Training Loss: 117894.77329542089, Test Loss: 34524004.0\n",
      "Epoch [72/100], Training Loss: 116592.22368343108, Test Loss: 34183756.0\n",
      "Epoch [73/100], Training Loss: 115326.11664000948, Test Loss: 33852544.0\n",
      "Epoch [74/100], Training Loss: 114093.72454238492, Test Loss: 33530878.0\n",
      "Epoch [75/100], Training Loss: 112895.48936674367, Test Loss: 33218652.0\n",
      "Epoch [76/100], Training Loss: 111724.9078253658, Test Loss: 32914040.0\n",
      "Epoch [77/100], Training Loss: 110578.38048693798, Test Loss: 32616262.0\n",
      "Epoch [78/100], Training Loss: 109455.78105562467, Test Loss: 32324844.0\n",
      "Epoch [79/100], Training Loss: 108355.68147621586, Test Loss: 32039124.0\n",
      "Epoch [80/100], Training Loss: 107279.31479177774, Test Loss: 31760102.0\n",
      "Epoch [81/100], Training Loss: 106229.11527753095, Test Loss: 31487854.0\n",
      "Epoch [82/100], Training Loss: 105204.88531485101, Test Loss: 31222058.0\n",
      "Epoch [83/100], Training Loss: 104206.89070552692, Test Loss: 30962790.0\n",
      "Epoch [84/100], Training Loss: 103235.7972276524, Test Loss: 30710222.0\n",
      "Epoch [85/100], Training Loss: 102289.36810615484, Test Loss: 30464784.0\n",
      "Epoch [86/100], Training Loss: 101365.55482495113, Test Loss: 30225704.0\n",
      "Epoch [87/100], Training Loss: 100463.54730170013, Test Loss: 29991488.0\n",
      "Epoch [88/100], Training Loss: 99580.55257389965, Test Loss: 29762914.0\n",
      "Epoch [89/100], Training Loss: 98715.26047035128, Test Loss: 29537050.0\n",
      "Epoch [90/100], Training Loss: 97866.30673538297, Test Loss: 29317336.0\n",
      "Epoch [91/100], Training Loss: 97033.15194597477, Test Loss: 29100840.0\n",
      "Epoch [92/100], Training Loss: 96214.4962383745, Test Loss: 28892356.0\n",
      "Epoch [93/100], Training Loss: 95406.20579349564, Test Loss: 28685306.0\n",
      "Epoch [94/100], Training Loss: 94603.6661927611, Test Loss: 28482688.0\n",
      "Epoch [95/100], Training Loss: 93798.11024228422, Test Loss: 28279014.0\n",
      "Epoch [96/100], Training Loss: 92989.38119779633, Test Loss: 28078506.0\n",
      "Epoch [97/100], Training Loss: 92194.68864403767, Test Loss: 27882838.0\n",
      "Epoch [98/100], Training Loss: 91422.40560393341, Test Loss: 27695078.0\n",
      "Epoch [99/100], Training Loss: 90673.47876310645, Test Loss: 27510770.0\n",
      "Epoch [100/100], Training Loss: 89943.87785083822, Test Loss: 27330934.0\n",
      "Epoch [1/100], Training Loss: 582414.4806587287, Test Loss: 300018944.0\n",
      "Epoch [2/100], Training Loss: 581817.3387832474, Test Loss: 299313088.0\n",
      "Epoch [3/100], Training Loss: 578926.6844381257, Test Loss: 296743424.0\n",
      "Epoch [4/100], Training Loss: 570942.7318286832, Test Loss: 290759328.0\n",
      "Epoch [5/100], Training Loss: 554976.0180084119, Test Loss: 280045344.0\n",
      "Epoch [6/100], Training Loss: 529105.6747822997, Test Loss: 264055520.0\n",
      "Epoch [7/100], Training Loss: 493387.78034476633, Test Loss: 243485760.0\n",
      "Epoch [8/100], Training Loss: 450546.53255138913, Test Loss: 220451232.0\n",
      "Epoch [9/100], Training Loss: 405767.35691013566, Test Loss: 198041648.0\n",
      "Epoch [10/100], Training Loss: 365121.27527990047, Test Loss: 179165344.0\n",
      "Epoch [11/100], Training Loss: 332947.4078549849, Test Loss: 165169344.0\n",
      "Epoch [12/100], Training Loss: 309918.01054439903, Test Loss: 155462992.0\n",
      "Epoch [13/100], Training Loss: 293785.78093714826, Test Loss: 148555904.0\n",
      "Epoch [14/100], Training Loss: 281768.4829097802, Test Loss: 143191248.0\n",
      "Epoch [15/100], Training Loss: 271955.16711095313, Test Loss: 138645440.0\n",
      "Epoch [16/100], Training Loss: 263342.9687814703, Test Loss: 134567712.0\n",
      "Epoch [17/100], Training Loss: 255457.0790829927, Test Loss: 130792408.0\n",
      "Epoch [18/100], Training Loss: 248065.8785616966, Test Loss: 127234720.0\n",
      "Epoch [19/100], Training Loss: 241044.23861145668, Test Loss: 123846592.0\n",
      "Epoch [20/100], Training Loss: 234318.08210414075, Test Loss: 120598656.0\n",
      "Epoch [21/100], Training Loss: 227841.65629998222, Test Loss: 117472704.0\n",
      "Epoch [22/100], Training Loss: 221586.82352941178, Test Loss: 114457712.0\n",
      "Epoch [23/100], Training Loss: 215537.00989277885, Test Loss: 111547320.0\n",
      "Epoch [24/100], Training Loss: 209683.32113026478, Test Loss: 108738440.0\n",
      "Epoch [25/100], Training Loss: 204022.9429536165, Test Loss: 106030304.0\n",
      "Epoch [26/100], Training Loss: 198556.29737574788, Test Loss: 103423016.0\n",
      "Epoch [27/100], Training Loss: 193284.29690184232, Test Loss: 100916656.0\n",
      "Epoch [28/100], Training Loss: 188207.43273502754, Test Loss: 98510416.0\n",
      "Epoch [29/100], Training Loss: 183327.169480481, Test Loss: 96203280.0\n",
      "Epoch [30/100], Training Loss: 178644.83620638587, Test Loss: 93995224.0\n",
      "Epoch [31/100], Training Loss: 174161.0568094307, Test Loss: 91885424.0\n",
      "Epoch [32/100], Training Loss: 169875.3007523251, Test Loss: 89872584.0\n",
      "Epoch [33/100], Training Loss: 165785.64990225696, Test Loss: 87954424.0\n",
      "Epoch [34/100], Training Loss: 161888.6400094781, Test Loss: 86127856.0\n",
      "Epoch [35/100], Training Loss: 158178.3875362834, Test Loss: 84388664.0\n",
      "Epoch [36/100], Training Loss: 154647.77418399384, Test Loss: 82732624.0\n",
      "Epoch [37/100], Training Loss: 151288.8155914934, Test Loss: 81154640.0\n",
      "Epoch [38/100], Training Loss: 148092.31609501806, Test Loss: 79649376.0\n",
      "Epoch [39/100], Training Loss: 145048.6826609798, Test Loss: 78211640.0\n",
      "Epoch [40/100], Training Loss: 142147.58391090576, Test Loss: 76836208.0\n",
      "Epoch [41/100], Training Loss: 139379.0076417274, Test Loss: 75518296.0\n",
      "Epoch [42/100], Training Loss: 136733.763402642, Test Loss: 74254032.0\n",
      "Epoch [43/100], Training Loss: 134203.22540133877, Test Loss: 73039432.0\n",
      "Epoch [44/100], Training Loss: 131778.9093063207, Test Loss: 71870824.0\n",
      "Epoch [45/100], Training Loss: 129452.78952668681, Test Loss: 70744584.0\n",
      "Epoch [46/100], Training Loss: 127218.04632426989, Test Loss: 69658112.0\n",
      "Epoch [47/100], Training Loss: 125068.44784076772, Test Loss: 68608832.0\n",
      "Epoch [48/100], Training Loss: 122999.16142408625, Test Loss: 67595072.0\n",
      "Epoch [49/100], Training Loss: 121005.80700195486, Test Loss: 66615116.0\n",
      "Epoch [50/100], Training Loss: 119083.64599253598, Test Loss: 65666900.0\n",
      "Epoch [51/100], Training Loss: 117226.76523902612, Test Loss: 64747900.0\n",
      "Epoch [52/100], Training Loss: 115431.35288193828, Test Loss: 63857292.0\n",
      "Epoch [53/100], Training Loss: 113694.20555654286, Test Loss: 62993316.0\n",
      "Epoch [54/100], Training Loss: 112010.24038860257, Test Loss: 62153856.0\n",
      "Epoch [55/100], Training Loss: 110375.92974349861, Test Loss: 61337208.0\n",
      "Epoch [56/100], Training Loss: 108788.03601682365, Test Loss: 60543224.0\n",
      "Epoch [57/100], Training Loss: 107243.5976541674, Test Loss: 59770828.0\n",
      "Epoch [58/100], Training Loss: 105740.66026894141, Test Loss: 59018436.0\n",
      "Epoch [59/100], Training Loss: 104276.03222557905, Test Loss: 58285092.0\n",
      "Epoch [60/100], Training Loss: 102846.1384989041, Test Loss: 57568312.0\n",
      "Epoch [61/100], Training Loss: 101448.98975179196, Test Loss: 56867532.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [62/100], Training Loss: 100082.66998400568, Test Loss: 56182028.0\n",
      "Epoch [63/100], Training Loss: 98745.92204253303, Test Loss: 55511648.0\n",
      "Epoch [64/100], Training Loss: 97437.9132752799, Test Loss: 54855292.0\n",
      "Epoch [65/100], Training Loss: 96156.9685445175, Test Loss: 54212368.0\n",
      "Epoch [66/100], Training Loss: 94902.18328298087, Test Loss: 53582888.0\n",
      "Epoch [67/100], Training Loss: 93671.96540489307, Test Loss: 52965160.0\n",
      "Epoch [68/100], Training Loss: 92465.03418043954, Test Loss: 52358432.0\n",
      "Epoch [69/100], Training Loss: 91280.24157336651, Test Loss: 51762220.0\n",
      "Epoch [70/100], Training Loss: 90117.44233161543, Test Loss: 51176636.0\n",
      "Epoch [71/100], Training Loss: 88976.42319767787, Test Loss: 50601672.0\n",
      "Epoch [72/100], Training Loss: 87856.43575617558, Test Loss: 50036796.0\n",
      "Epoch [73/100], Training Loss: 86757.51981517683, Test Loss: 49482096.0\n",
      "Epoch [74/100], Training Loss: 85679.24139565192, Test Loss: 48937044.0\n",
      "Epoch [75/100], Training Loss: 84620.76962265269, Test Loss: 48401092.0\n",
      "Epoch [76/100], Training Loss: 83581.90024287661, Test Loss: 47873892.0\n",
      "Epoch [77/100], Training Loss: 82562.00497600852, Test Loss: 47355476.0\n",
      "Epoch [78/100], Training Loss: 81560.97020318701, Test Loss: 46845800.0\n",
      "Epoch [79/100], Training Loss: 80578.79900479829, Test Loss: 46344408.0\n",
      "Epoch [80/100], Training Loss: 79614.64154967123, Test Loss: 45851208.0\n",
      "Epoch [81/100], Training Loss: 78667.84977193295, Test Loss: 45366188.0\n",
      "Epoch [82/100], Training Loss: 77737.59706178543, Test Loss: 44888648.0\n",
      "Epoch [83/100], Training Loss: 76823.72004028197, Test Loss: 44418804.0\n",
      "Epoch [84/100], Training Loss: 75925.89811030152, Test Loss: 43956296.0\n",
      "Epoch [85/100], Training Loss: 75043.59729873823, Test Loss: 43501068.0\n",
      "Epoch [86/100], Training Loss: 74176.57792784787, Test Loss: 43052648.0\n",
      "Epoch [87/100], Training Loss: 73324.32936437415, Test Loss: 42610752.0\n",
      "Epoch [88/100], Training Loss: 72486.69273147326, Test Loss: 42175340.0\n",
      "Epoch [89/100], Training Loss: 71663.7453942302, Test Loss: 41746536.0\n",
      "Epoch [90/100], Training Loss: 70855.16876962266, Test Loss: 41324284.0\n",
      "Epoch [91/100], Training Loss: 70060.52165156092, Test Loss: 40908564.0\n",
      "Epoch [92/100], Training Loss: 69279.5905455838, Test Loss: 40499160.0\n",
      "Epoch [93/100], Training Loss: 68512.188140513, Test Loss: 40095520.0\n",
      "Epoch [94/100], Training Loss: 67758.11871334637, Test Loss: 39697956.0\n",
      "Epoch [95/100], Training Loss: 67016.83229666488, Test Loss: 39306556.0\n",
      "Epoch [96/100], Training Loss: 66287.79787927255, Test Loss: 38920860.0\n",
      "Epoch [97/100], Training Loss: 65570.94022865944, Test Loss: 38540852.0\n",
      "Epoch [98/100], Training Loss: 64866.01338783248, Test Loss: 38166540.0\n",
      "Epoch [99/100], Training Loss: 64173.38960962028, Test Loss: 37797968.0\n",
      "Epoch [100/100], Training Loss: 63492.85895385344, Test Loss: 37435252.0\n",
      "Epoch [1/100], Training Loss: 7241462.430898643, Test Loss: 150110256.0\n",
      "Epoch [2/100], Training Loss: 4113373.738404123, Test Loss: 119145224.0\n",
      "Epoch [3/100], Training Loss: 3311707.0551507613, Test Loss: 97930632.0\n",
      "Epoch [4/100], Training Loss: 2724326.150879687, Test Loss: 82343720.0\n",
      "Epoch [5/100], Training Loss: 2284664.800130324, Test Loss: 70596656.0\n",
      "Epoch [6/100], Training Loss: 1959789.199247675, Test Loss: 61925676.0\n",
      "Epoch [7/100], Training Loss: 1725547.4744683371, Test Loss: 55648980.0\n",
      "Epoch [8/100], Training Loss: 1553974.0356021563, Test Loss: 50877828.0\n",
      "Epoch [9/100], Training Loss: 1419275.9548012558, Test Loss: 46984644.0\n",
      "Epoch [10/100], Training Loss: 1306397.2538060541, Test Loss: 43632100.0\n",
      "Epoch [11/100], Training Loss: 1208253.902168118, Test Loss: 40654572.0\n",
      "Epoch [12/100], Training Loss: 1121621.1738048694, Test Loss: 37973128.0\n",
      "Epoch [13/100], Training Loss: 1044722.3080238138, Test Loss: 35543432.0\n",
      "Epoch [14/100], Training Loss: 976550.9915437474, Test Loss: 33369170.0\n",
      "Epoch [15/100], Training Loss: 916090.4620283159, Test Loss: 31431354.0\n",
      "Epoch [16/100], Training Loss: 862579.1196315384, Test Loss: 29713862.0\n",
      "Epoch [17/100], Training Loss: 815183.7413364138, Test Loss: 28187722.0\n",
      "Epoch [18/100], Training Loss: 773287.9290030211, Test Loss: 26832454.0\n",
      "Epoch [19/100], Training Loss: 736211.5755138914, Test Loss: 25625002.0\n",
      "Epoch [20/100], Training Loss: 703189.7018985843, Test Loss: 24548806.0\n",
      "Epoch [21/100], Training Loss: 673469.4970232806, Test Loss: 23585974.0\n",
      "Epoch [22/100], Training Loss: 646474.791229785, Test Loss: 22719154.0\n",
      "Epoch [23/100], Training Loss: 621778.2910816895, Test Loss: 21942088.0\n",
      "Epoch [24/100], Training Loss: 599122.7091478585, Test Loss: 21242132.0\n",
      "Epoch [25/100], Training Loss: 578225.3007967537, Test Loss: 20616810.0\n",
      "Epoch [26/100], Training Loss: 558939.9494920324, Test Loss: 20055568.0\n",
      "Epoch [27/100], Training Loss: 541126.1275028138, Test Loss: 19553312.0\n",
      "Epoch [28/100], Training Loss: 524671.8174204727, Test Loss: 19098086.0\n",
      "Epoch [29/100], Training Loss: 509374.3575913749, Test Loss: 18689324.0\n",
      "Epoch [30/100], Training Loss: 495163.5332918666, Test Loss: 18317328.0\n",
      "Epoch [31/100], Training Loss: 481941.69844795926, Test Loss: 17979502.0\n",
      "Epoch [32/100], Training Loss: 469641.2172931106, Test Loss: 17658804.0\n",
      "Epoch [33/100], Training Loss: 458114.7875422072, Test Loss: 17368914.0\n",
      "Epoch [34/100], Training Loss: 447315.45775576093, Test Loss: 17103106.0\n",
      "Epoch [35/100], Training Loss: 437182.864388958, Test Loss: 16863796.0\n",
      "Epoch [36/100], Training Loss: 427702.46212457796, Test Loss: 16647627.0\n",
      "Epoch [37/100], Training Loss: 418750.49100319884, Test Loss: 16462104.0\n",
      "Epoch [38/100], Training Loss: 410365.4024939281, Test Loss: 16278602.0\n",
      "Epoch [39/100], Training Loss: 402426.8403974883, Test Loss: 16136539.0\n",
      "Epoch [40/100], Training Loss: 394966.1014824359, Test Loss: 15980339.0\n",
      "Epoch [41/100], Training Loss: 387937.39055002667, Test Loss: 15867801.0\n",
      "Epoch [42/100], Training Loss: 381320.49674930394, Test Loss: 15753969.0\n",
      "Epoch [43/100], Training Loss: 375046.0575128843, Test Loss: 15662949.0\n",
      "Epoch [44/100], Training Loss: 369132.80840886204, Test Loss: 15575378.0\n",
      "Epoch [45/100], Training Loss: 363516.63568508974, Test Loss: 15502014.0\n",
      "Epoch [46/100], Training Loss: 358201.09516616317, Test Loss: 15440008.0\n",
      "Epoch [47/100], Training Loss: 353169.30889757717, Test Loss: 15387202.0\n",
      "Epoch [48/100], Training Loss: 348384.4589849535, Test Loss: 15317438.0\n",
      "Epoch [49/100], Training Loss: 343874.46156181506, Test Loss: 15263985.0\n",
      "Epoch [50/100], Training Loss: 339616.54156299983, Test Loss: 15196736.0\n",
      "Epoch [51/100], Training Loss: 335544.71220603044, Test Loss: 15152681.0\n",
      "Epoch [52/100], Training Loss: 331714.33145252056, Test Loss: 15070840.0\n",
      "Epoch [53/100], Training Loss: 328038.91925833776, Test Loss: 15064240.0\n",
      "Epoch [54/100], Training Loss: 324588.8264913216, Test Loss: 14983789.0\n",
      "Epoch [55/100], Training Loss: 321298.405618743, Test Loss: 14969355.0\n",
      "Epoch [56/100], Training Loss: 318169.9137417807, Test Loss: 14900259.0\n",
      "Epoch [57/100], Training Loss: 315205.2674567561, Test Loss: 14899463.0\n",
      "Epoch [58/100], Training Loss: 312356.1392875126, Test Loss: 14830067.0\n",
      "Epoch [59/100], Training Loss: 309643.78557253716, Test Loss: 14834337.0\n",
      "Epoch [60/100], Training Loss: 307046.6548856703, Test Loss: 14762793.0\n",
      "Epoch [61/100], Training Loss: 304519.7515994313, Test Loss: 14773739.0\n",
      "Epoch [62/100], Training Loss: 302149.4758382205, Test Loss: 14709632.0\n",
      "Epoch [63/100], Training Loss: 299865.6895585273, Test Loss: 14717806.0\n",
      "Epoch [64/100], Training Loss: 297702.12863574433, Test Loss: 14656900.0\n",
      "Epoch [65/100], Training Loss: 295602.74211761745, Test Loss: 14672500.0\n",
      "Epoch [66/100], Training Loss: 293619.78866773297, Test Loss: 14614848.0\n",
      "Epoch [67/100], Training Loss: 291689.6600986316, Test Loss: 14616927.0\n",
      "Epoch [68/100], Training Loss: 289892.3217115396, Test Loss: 14613850.0\n",
      "Epoch [69/100], Training Loss: 288124.62699558673, Test Loss: 14555862.0\n",
      "Epoch [70/100], Training Loss: 286391.14275664947, Test Loss: 14550628.0\n",
      "Epoch [71/100], Training Loss: 284731.7238759552, Test Loss: 14515854.0\n",
      "Epoch [72/100], Training Loss: 283112.714486701, Test Loss: 14498421.0\n",
      "Epoch [73/100], Training Loss: 281590.6399613471, Test Loss: 14498245.0\n",
      "Epoch [74/100], Training Loss: 280090.71868520824, Test Loss: 14433630.0\n",
      "Epoch [75/100], Training Loss: 278668.86729903443, Test Loss: 14482224.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [76/100], Training Loss: 277265.2623289497, Test Loss: 14439585.0\n",
      "Epoch [77/100], Training Loss: 275934.32171894435, Test Loss: 14434776.0\n",
      "Epoch [78/100], Training Loss: 274635.5761099757, Test Loss: 14460085.0\n",
      "Epoch [79/100], Training Loss: 273387.2457718737, Test Loss: 14415905.0\n",
      "Epoch [80/100], Training Loss: 272100.63297123986, Test Loss: 14426461.0\n",
      "Epoch [81/100], Training Loss: 270904.16203498014, Test Loss: 14410663.0\n",
      "Epoch [82/100], Training Loss: 269692.1703616492, Test Loss: 14397124.0\n",
      "Epoch [83/100], Training Loss: 268551.5641808838, Test Loss: 14435915.0\n",
      "Epoch [84/100], Training Loss: 267428.66338635155, Test Loss: 14356948.0\n",
      "Epoch [85/100], Training Loss: 266307.9513691428, Test Loss: 14402176.0\n",
      "Epoch [86/100], Training Loss: 265198.4664193472, Test Loss: 14371017.0\n",
      "Epoch [87/100], Training Loss: 264099.87256753154, Test Loss: 14357699.0\n",
      "Epoch [88/100], Training Loss: 263044.8053617973, Test Loss: 14369088.0\n",
      "Epoch [89/100], Training Loss: 262028.17908077128, Test Loss: 14355034.0\n",
      "Epoch [90/100], Training Loss: 261000.20284639535, Test Loss: 14293612.0\n",
      "Epoch [91/100], Training Loss: 260052.65617039867, Test Loss: 14354837.0\n",
      "Epoch [92/100], Training Loss: 259068.38169391625, Test Loss: 14304306.0\n",
      "Epoch [93/100], Training Loss: 258186.328827528, Test Loss: 14313669.0\n",
      "Epoch [94/100], Training Loss: 257276.66689621468, Test Loss: 14306211.0\n",
      "Epoch [95/100], Training Loss: 256439.58858331852, Test Loss: 14283178.0\n",
      "Epoch [96/100], Training Loss: 255544.82344795924, Test Loss: 14246711.0\n",
      "Epoch [97/100], Training Loss: 254706.3993135774, Test Loss: 14321663.0\n",
      "Epoch [98/100], Training Loss: 253841.2149013684, Test Loss: 14270249.0\n",
      "Epoch [99/100], Training Loss: 253033.947814851, Test Loss: 14278791.0\n",
      "Epoch [100/100], Training Loss: 252204.27236612167, Test Loss: 14267225.0\n",
      "Epoch [1/100], Training Loss: 4422571.106687993, Test Loss: 238262320.0\n",
      "Epoch [2/100], Training Loss: 2760639.510929447, Test Loss: 147368864.0\n",
      "Epoch [3/100], Training Loss: 2114039.285942776, Test Loss: 127815024.0\n",
      "Epoch [4/100], Training Loss: 1842629.1999289142, Test Loss: 112517952.0\n",
      "Epoch [5/100], Training Loss: 1616849.2862982051, Test Loss: 99694248.0\n",
      "Epoch [6/100], Training Loss: 1427986.2150346544, Test Loss: 89148808.0\n",
      "Epoch [7/100], Training Loss: 1271510.6241336414, Test Loss: 80411256.0\n",
      "Epoch [8/100], Training Loss: 1141981.8932527695, Test Loss: 73142256.0\n",
      "Epoch [9/100], Training Loss: 1035477.8366802915, Test Loss: 67117984.0\n",
      "Epoch [10/100], Training Loss: 948413.0182453647, Test Loss: 62151720.0\n",
      "Epoch [11/100], Training Loss: 877100.41360109, Test Loss: 58055864.0\n",
      "Epoch [12/100], Training Loss: 818158.3066169065, Test Loss: 54626672.0\n",
      "Epoch [13/100], Training Loss: 768164.4847461643, Test Loss: 51670232.0\n",
      "Epoch [14/100], Training Loss: 724579.2000473905, Test Loss: 49055520.0\n",
      "Epoch [15/100], Training Loss: 685668.9356080801, Test Loss: 46691596.0\n",
      "Epoch [16/100], Training Loss: 650360.1087020912, Test Loss: 44522244.0\n",
      "Epoch [17/100], Training Loss: 617961.0142467864, Test Loss: 42507288.0\n",
      "Epoch [18/100], Training Loss: 588031.6320715598, Test Loss: 40623768.0\n",
      "Epoch [19/100], Training Loss: 560337.3033884248, Test Loss: 38860896.0\n",
      "Epoch [20/100], Training Loss: 534675.4279959718, Test Loss: 37209720.0\n",
      "Epoch [21/100], Training Loss: 510922.9550678277, Test Loss: 35664264.0\n",
      "Epoch [22/100], Training Loss: 488961.20389787335, Test Loss: 34226988.0\n",
      "Epoch [23/100], Training Loss: 468662.0201409869, Test Loss: 32895870.0\n",
      "Epoch [24/100], Training Loss: 449879.6997511996, Test Loss: 31660402.0\n",
      "Epoch [25/100], Training Loss: 432508.95133582136, Test Loss: 30516274.0\n",
      "Epoch [26/100], Training Loss: 416442.9126236597, Test Loss: 29456322.0\n",
      "Epoch [27/100], Training Loss: 401588.678810497, Test Loss: 28474926.0\n",
      "Epoch [28/100], Training Loss: 387858.33787986496, Test Loss: 27561742.0\n",
      "Epoch [29/100], Training Loss: 375152.7264972454, Test Loss: 26714094.0\n",
      "Epoch [30/100], Training Loss: 363409.5641845862, Test Loss: 25929172.0\n",
      "Epoch [31/100], Training Loss: 352510.4392216101, Test Loss: 25202694.0\n",
      "Epoch [32/100], Training Loss: 342321.13201232156, Test Loss: 24522310.0\n",
      "Epoch [33/100], Training Loss: 332746.23482021206, Test Loss: 23889140.0\n",
      "Epoch [34/100], Training Loss: 323732.97957763163, Test Loss: 23294088.0\n",
      "Epoch [35/100], Training Loss: 315234.2388928381, Test Loss: 22738988.0\n",
      "Epoch [36/100], Training Loss: 307189.1533380724, Test Loss: 22219854.0\n",
      "Epoch [37/100], Training Loss: 299565.20121734496, Test Loss: 21737252.0\n",
      "Epoch [38/100], Training Loss: 292339.2441798472, Test Loss: 21285388.0\n",
      "Epoch [39/100], Training Loss: 285479.9025235472, Test Loss: 20865454.0\n",
      "Epoch [40/100], Training Loss: 278984.06292577455, Test Loss: 20471656.0\n",
      "Epoch [41/100], Training Loss: 272809.9404211836, Test Loss: 20104128.0\n",
      "Epoch [42/100], Training Loss: 266940.7807594337, Test Loss: 19761318.0\n",
      "Epoch [43/100], Training Loss: 261352.33927196256, Test Loss: 19438106.0\n",
      "Epoch [44/100], Training Loss: 256027.52754576152, Test Loss: 19138484.0\n",
      "Epoch [45/100], Training Loss: 250970.88112374858, Test Loss: 18854480.0\n",
      "Epoch [46/100], Training Loss: 246135.6921983295, Test Loss: 18588738.0\n",
      "Epoch [47/100], Training Loss: 241511.829927137, Test Loss: 18334988.0\n",
      "Epoch [48/100], Training Loss: 237083.94083585095, Test Loss: 18099076.0\n",
      "Epoch [49/100], Training Loss: 232858.72427581303, Test Loss: 17875586.0\n",
      "Epoch [50/100], Training Loss: 228819.34275220663, Test Loss: 17668080.0\n",
      "Epoch [51/100], Training Loss: 224964.4621319827, Test Loss: 17469550.0\n",
      "Epoch [52/100], Training Loss: 221264.6365144245, Test Loss: 17289066.0\n",
      "Epoch [53/100], Training Loss: 217699.58367395296, Test Loss: 17110764.0\n",
      "Epoch [54/100], Training Loss: 214279.14402286595, Test Loss: 16948414.0\n",
      "Epoch [55/100], Training Loss: 211008.7381671702, Test Loss: 16795690.0\n",
      "Epoch [56/100], Training Loss: 207869.03998578285, Test Loss: 16652582.0\n",
      "Epoch [57/100], Training Loss: 204863.4379035602, Test Loss: 16517737.0\n",
      "Epoch [58/100], Training Loss: 201974.86742491557, Test Loss: 16397017.0\n",
      "Epoch [59/100], Training Loss: 199216.1256738345, Test Loss: 16273856.0\n",
      "Epoch [60/100], Training Loss: 196544.6995438659, Test Loss: 16174917.0\n",
      "Epoch [61/100], Training Loss: 193991.32974942244, Test Loss: 16068110.0\n",
      "Epoch [62/100], Training Loss: 191520.31388839523, Test Loss: 15979720.0\n",
      "Epoch [63/100], Training Loss: 189164.2805224809, Test Loss: 15888844.0\n",
      "Epoch [64/100], Training Loss: 186893.90858065282, Test Loss: 15814855.0\n",
      "Epoch [65/100], Training Loss: 184729.26581659855, Test Loss: 15735310.0\n",
      "Epoch [66/100], Training Loss: 182627.72255790533, Test Loss: 15676482.0\n",
      "Epoch [67/100], Training Loss: 180632.71272436468, Test Loss: 15604161.0\n",
      "Epoch [68/100], Training Loss: 178698.9694775191, Test Loss: 15555473.0\n",
      "Epoch [69/100], Training Loss: 176843.0502784195, Test Loss: 15490075.0\n",
      "Epoch [70/100], Training Loss: 175054.05983057877, Test Loss: 15447352.0\n",
      "Epoch [71/100], Training Loss: 173325.84332977905, Test Loss: 15387250.0\n",
      "Epoch [72/100], Training Loss: 171661.85339286772, Test Loss: 15349862.0\n",
      "Epoch [73/100], Training Loss: 170069.29949351342, Test Loss: 15289091.0\n",
      "Epoch [74/100], Training Loss: 168532.76035927967, Test Loss: 15257252.0\n",
      "Epoch [75/100], Training Loss: 167062.55088561104, Test Loss: 15191921.0\n",
      "Epoch [76/100], Training Loss: 165631.15285676203, Test Loss: 15166152.0\n",
      "Epoch [77/100], Training Loss: 164271.35091226824, Test Loss: 15100334.0\n",
      "Epoch [78/100], Training Loss: 162947.39263076833, Test Loss: 15081739.0\n",
      "Epoch [79/100], Training Loss: 161692.59892038387, Test Loss: 15013796.0\n",
      "Epoch [80/100], Training Loss: 160455.79047449795, Test Loss: 15013645.0\n",
      "Epoch [81/100], Training Loss: 159283.1412090516, Test Loss: 14940357.0\n",
      "Epoch [82/100], Training Loss: 158124.1091093537, Test Loss: 14939518.0\n",
      "Epoch [83/100], Training Loss: 157021.76732717257, Test Loss: 14874871.0\n",
      "Epoch [84/100], Training Loss: 155948.67507108583, Test Loss: 14873166.0\n",
      "Epoch [85/100], Training Loss: 154929.26905248503, Test Loss: 14816869.0\n",
      "Epoch [86/100], Training Loss: 153921.97077335467, Test Loss: 14807993.0\n",
      "Epoch [87/100], Training Loss: 152978.66918429002, Test Loss: 14764316.0\n",
      "Epoch [88/100], Training Loss: 152041.07495112848, Test Loss: 14751909.0\n",
      "Epoch [89/100], Training Loss: 151160.75131804988, Test Loss: 14716833.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/100], Training Loss: 150274.68759996447, Test Loss: 14694779.0\n",
      "Epoch [91/100], Training Loss: 149452.63467063563, Test Loss: 14679161.0\n",
      "Epoch [92/100], Training Loss: 148622.40207185593, Test Loss: 14648072.0\n",
      "Epoch [93/100], Training Loss: 147836.30674278774, Test Loss: 14628241.0\n",
      "Epoch [94/100], Training Loss: 147062.15029471004, Test Loss: 14657908.0\n",
      "Epoch [95/100], Training Loss: 146335.67054676855, Test Loss: 14591157.0\n",
      "Epoch [96/100], Training Loss: 145592.34591404538, Test Loss: 14604380.0\n",
      "Epoch [97/100], Training Loss: 144891.07497334282, Test Loss: 14569408.0\n",
      "Epoch [98/100], Training Loss: 144177.6816983591, Test Loss: 14565889.0\n",
      "Epoch [99/100], Training Loss: 143507.56283691723, Test Loss: 14540591.0\n",
      "Epoch [100/100], Training Loss: 142828.0585347432, Test Loss: 14537743.0\n",
      "Epoch [1/100], Training Loss: 2317874.5808897577, Test Loss: 292116096.0\n",
      "Epoch [2/100], Training Loss: 2058828.8745927373, Test Loss: 226846048.0\n",
      "Epoch [3/100], Training Loss: 1463388.8328890468, Test Loss: 160907776.0\n",
      "Epoch [4/100], Training Loss: 1159391.6739529648, Test Loss: 141499040.0\n",
      "Epoch [5/100], Training Loss: 1045074.1418162431, Test Loss: 129675824.0\n",
      "Epoch [6/100], Training Loss: 957806.4941650376, Test Loss: 119536128.0\n",
      "Epoch [7/100], Training Loss: 880300.3376577217, Test Loss: 110417056.0\n",
      "Epoch [8/100], Training Loss: 810228.8272021799, Test Loss: 102242208.0\n",
      "Epoch [9/100], Training Loss: 747492.9972158048, Test Loss: 95018984.0\n",
      "Epoch [10/100], Training Loss: 692050.3843374208, Test Loss: 88698464.0\n",
      "Epoch [11/100], Training Loss: 643389.6648302826, Test Loss: 83154808.0\n",
      "Epoch [12/100], Training Loss: 600673.5268052841, Test Loss: 78264472.0\n",
      "Epoch [13/100], Training Loss: 563122.2780640958, Test Loss: 73933392.0\n",
      "Epoch [14/100], Training Loss: 530078.4536461111, Test Loss: 70088256.0\n",
      "Epoch [15/100], Training Loss: 500991.69480481016, Test Loss: 66681724.0\n",
      "Epoch [16/100], Training Loss: 475447.60369646345, Test Loss: 63672508.0\n",
      "Epoch [17/100], Training Loss: 452971.03572063264, Test Loss: 61006348.0\n",
      "Epoch [18/100], Training Loss: 433052.0503524673, Test Loss: 58623048.0\n",
      "Epoch [19/100], Training Loss: 415222.30365499674, Test Loss: 56471080.0\n",
      "Epoch [20/100], Training Loss: 399027.79432498076, Test Loss: 54500944.0\n",
      "Epoch [21/100], Training Loss: 384138.01711983886, Test Loss: 52678724.0\n",
      "Epoch [22/100], Training Loss: 370306.9646347965, Test Loss: 50978424.0\n",
      "Epoch [23/100], Training Loss: 357356.1342929921, Test Loss: 49381208.0\n",
      "Epoch [24/100], Training Loss: 345178.00876725314, Test Loss: 47873076.0\n",
      "Epoch [25/100], Training Loss: 333673.48628635745, Test Loss: 46442136.0\n",
      "Epoch [26/100], Training Loss: 322769.77809371485, Test Loss: 45079276.0\n",
      "Epoch [27/100], Training Loss: 312413.12582193, Test Loss: 43776576.0\n",
      "Epoch [28/100], Training Loss: 302541.5917303477, Test Loss: 42527504.0\n",
      "Epoch [29/100], Training Loss: 293139.62401516497, Test Loss: 41330088.0\n",
      "Epoch [30/100], Training Loss: 284188.52982643206, Test Loss: 40179904.0\n",
      "Epoch [31/100], Training Loss: 275655.5867543392, Test Loss: 39078208.0\n",
      "Epoch [32/100], Training Loss: 267511.25786387065, Test Loss: 38020112.0\n",
      "Epoch [33/100], Training Loss: 259750.41982110066, Test Loss: 37006112.0\n",
      "Epoch [34/100], Training Loss: 252359.67276820092, Test Loss: 36037516.0\n",
      "Epoch [35/100], Training Loss: 245318.9134529945, Test Loss: 35112208.0\n",
      "Epoch [36/100], Training Loss: 238616.65955808305, Test Loss: 34229476.0\n",
      "Epoch [37/100], Training Loss: 232220.24805994905, Test Loss: 33384420.0\n",
      "Epoch [38/100], Training Loss: 226123.7986789882, Test Loss: 32575488.0\n",
      "Epoch [39/100], Training Loss: 220321.92153900836, Test Loss: 31803964.0\n",
      "Epoch [40/100], Training Loss: 214794.99188436705, Test Loss: 31068252.0\n",
      "Epoch [41/100], Training Loss: 209526.228481725, Test Loss: 30366806.0\n",
      "Epoch [42/100], Training Loss: 204509.11569219833, Test Loss: 29699548.0\n",
      "Epoch [43/100], Training Loss: 199730.0076417274, Test Loss: 29064072.0\n",
      "Epoch [44/100], Training Loss: 195174.82136721758, Test Loss: 28458272.0\n",
      "Epoch [45/100], Training Loss: 190834.47437947989, Test Loss: 27879022.0\n",
      "Epoch [46/100], Training Loss: 186703.87145311298, Test Loss: 27326008.0\n",
      "Epoch [47/100], Training Loss: 182770.95139505953, Test Loss: 26799450.0\n",
      "Epoch [48/100], Training Loss: 179023.23378354363, Test Loss: 26298826.0\n",
      "Epoch [49/100], Training Loss: 175447.56190391566, Test Loss: 25821026.0\n",
      "Epoch [50/100], Training Loss: 172020.9576150702, Test Loss: 25365354.0\n",
      "Epoch [51/100], Training Loss: 168730.9186659558, Test Loss: 24930166.0\n",
      "Epoch [52/100], Training Loss: 165565.53640187194, Test Loss: 24513016.0\n",
      "Epoch [53/100], Training Loss: 162518.76156625792, Test Loss: 24114194.0\n",
      "Epoch [54/100], Training Loss: 159581.46463479652, Test Loss: 23730842.0\n",
      "Epoch [55/100], Training Loss: 156753.77222913335, Test Loss: 23366842.0\n",
      "Epoch [56/100], Training Loss: 154026.20917007286, Test Loss: 23017636.0\n",
      "Epoch [57/100], Training Loss: 151392.0975060719, Test Loss: 22683212.0\n",
      "Epoch [58/100], Training Loss: 148848.20143948818, Test Loss: 22362696.0\n",
      "Epoch [59/100], Training Loss: 146396.65443397904, Test Loss: 22056996.0\n",
      "Epoch [60/100], Training Loss: 144028.25422072152, Test Loss: 21762576.0\n",
      "Epoch [61/100], Training Loss: 141737.7304069664, Test Loss: 21480812.0\n",
      "Epoch [62/100], Training Loss: 139520.35323736747, Test Loss: 21209282.0\n",
      "Epoch [63/100], Training Loss: 137374.97375747882, Test Loss: 20950662.0\n",
      "Epoch [64/100], Training Loss: 135298.00002961908, Test Loss: 20701892.0\n",
      "Epoch [65/100], Training Loss: 133285.39005390677, Test Loss: 20464566.0\n",
      "Epoch [66/100], Training Loss: 131337.39876192168, Test Loss: 20234362.0\n",
      "Epoch [67/100], Training Loss: 129449.61498133997, Test Loss: 20015542.0\n",
      "Epoch [68/100], Training Loss: 127618.9426870446, Test Loss: 19803010.0\n",
      "Epoch [69/100], Training Loss: 125846.13888395237, Test Loss: 19599914.0\n",
      "Epoch [70/100], Training Loss: 124127.44911438896, Test Loss: 19404292.0\n",
      "Epoch [71/100], Training Loss: 122460.92958059357, Test Loss: 19217138.0\n",
      "Epoch [72/100], Training Loss: 120844.1053255139, Test Loss: 19036686.0\n",
      "Epoch [73/100], Training Loss: 119276.82344055446, Test Loss: 18862786.0\n",
      "Epoch [74/100], Training Loss: 117752.25159943131, Test Loss: 18696524.0\n",
      "Epoch [75/100], Training Loss: 116271.50108109709, Test Loss: 18538530.0\n",
      "Epoch [76/100], Training Loss: 114832.79559860198, Test Loss: 18384750.0\n",
      "Epoch [77/100], Training Loss: 113434.57573603459, Test Loss: 18240848.0\n",
      "Epoch [78/100], Training Loss: 112079.6891623719, Test Loss: 18097818.0\n",
      "Epoch [79/100], Training Loss: 110758.64913216041, Test Loss: 17966794.0\n",
      "Epoch [80/100], Training Loss: 109476.68535631776, Test Loss: 17833474.0\n",
      "Epoch [81/100], Training Loss: 108231.45449025532, Test Loss: 17712916.0\n",
      "Epoch [82/100], Training Loss: 107020.15375273976, Test Loss: 17591014.0\n",
      "Epoch [83/100], Training Loss: 105842.28170724484, Test Loss: 17478202.0\n",
      "Epoch [84/100], Training Loss: 104692.68366802915, Test Loss: 17366026.0\n",
      "Epoch [85/100], Training Loss: 103575.05595047686, Test Loss: 17261766.0\n",
      "Epoch [86/100], Training Loss: 102489.19252413957, Test Loss: 17159308.0\n",
      "Epoch [87/100], Training Loss: 101435.08081570997, Test Loss: 17063226.0\n",
      "Epoch [88/100], Training Loss: 100405.90903974883, Test Loss: 16968830.0\n",
      "Epoch [89/100], Training Loss: 99408.25633848706, Test Loss: 16882444.0\n",
      "Epoch [90/100], Training Loss: 98433.95063977252, Test Loss: 16793786.0\n",
      "Epoch [91/100], Training Loss: 97490.39535572537, Test Loss: 16715443.0\n",
      "Epoch [92/100], Training Loss: 96571.94935134174, Test Loss: 16633304.0\n",
      "Epoch [93/100], Training Loss: 95682.72839286772, Test Loss: 16563537.0\n",
      "Epoch [94/100], Training Loss: 94812.85445175049, Test Loss: 16489815.0\n",
      "Epoch [95/100], Training Loss: 93966.34706178543, Test Loss: 16424620.0\n",
      "Epoch [96/100], Training Loss: 93144.79102245127, Test Loss: 16358370.0\n",
      "Epoch [97/100], Training Loss: 92345.74053669807, Test Loss: 16295559.0\n",
      "Epoch [98/100], Training Loss: 91567.64612582194, Test Loss: 16239872.0\n",
      "Epoch [99/100], Training Loss: 90807.74637166044, Test Loss: 16179070.0\n",
      "Epoch [100/100], Training Loss: 90072.67792192406, Test Loss: 16128181.0\n",
      "Epoch [1/100], Training Loss: 1497714.4197618624, Test Loss: 298657984.0\n",
      "Epoch [2/100], Training Loss: 1465525.1541970263, Test Loss: 284092640.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100], Training Loss: 1331123.5888869143, Test Loss: 243691328.0\n",
      "Epoch [4/100], Training Loss: 1091657.2188851372, Test Loss: 192838736.0\n",
      "Epoch [5/100], Training Loss: 877876.0580534328, Test Loss: 161439024.0\n",
      "Epoch [6/100], Training Loss: 767435.9023754516, Test Loss: 147096736.0\n",
      "Epoch [7/100], Training Loss: 708649.085480718, Test Loss: 137804800.0\n",
      "Epoch [8/100], Training Loss: 664276.5653693501, Test Loss: 130121456.0\n",
      "Epoch [9/100], Training Loss: 625654.2595817783, Test Loss: 123329720.0\n",
      "Epoch [10/100], Training Loss: 590720.9437829512, Test Loss: 117207248.0\n",
      "Epoch [11/100], Training Loss: 558757.9107872756, Test Loss: 111662712.0\n",
      "Epoch [12/100], Training Loss: 529461.7292814407, Test Loss: 106647936.0\n",
      "Epoch [13/100], Training Loss: 502662.8806350335, Test Loss: 102122152.0\n",
      "Epoch [14/100], Training Loss: 478207.58604348084, Test Loss: 98042048.0\n",
      "Epoch [15/100], Training Loss: 455943.20952550205, Test Loss: 94365152.0\n",
      "Epoch [16/100], Training Loss: 435622.96451632015, Test Loss: 91005080.0\n",
      "Epoch [17/100], Training Loss: 416763.27918962145, Test Loss: 87886528.0\n",
      "Epoch [18/100], Training Loss: 399340.53243291273, Test Loss: 85031184.0\n",
      "Epoch [19/100], Training Loss: 383728.10449617915, Test Loss: 82479592.0\n",
      "Epoch [20/100], Training Loss: 369752.926248445, Test Loss: 80153352.0\n",
      "Epoch [21/100], Training Loss: 357103.65404893074, Test Loss: 78019232.0\n",
      "Epoch [22/100], Training Loss: 345643.60571056214, Test Loss: 76058600.0\n",
      "Epoch [23/100], Training Loss: 335249.19270185416, Test Loss: 74251768.0\n",
      "Epoch [24/100], Training Loss: 325799.7343759256, Test Loss: 72577728.0\n",
      "Epoch [25/100], Training Loss: 317175.5976541674, Test Loss: 71018168.0\n",
      "Epoch [26/100], Training Loss: 309257.29553936375, Test Loss: 69553672.0\n",
      "Epoch [27/100], Training Loss: 301939.0650435401, Test Loss: 68171648.0\n",
      "Epoch [28/100], Training Loss: 295131.63177536876, Test Loss: 66860500.0\n",
      "Epoch [29/100], Training Loss: 288757.3001599431, Test Loss: 65611884.0\n",
      "Epoch [30/100], Training Loss: 282748.9847609739, Test Loss: 64416480.0\n",
      "Epoch [31/100], Training Loss: 277046.8278389906, Test Loss: 63265956.0\n",
      "Epoch [32/100], Training Loss: 271596.37682527694, Test Loss: 62154560.0\n",
      "Epoch [33/100], Training Loss: 266365.23749774677, Test Loss: 61076828.0\n",
      "Epoch [34/100], Training Loss: 261319.13268384204, Test Loss: 60028924.0\n",
      "Epoch [35/100], Training Loss: 256424.07983457734, Test Loss: 59005464.0\n",
      "Epoch [36/100], Training Loss: 251657.31723535334, Test Loss: 58007060.0\n",
      "Epoch [37/100], Training Loss: 247008.34350749364, Test Loss: 57031824.0\n",
      "Epoch [38/100], Training Loss: 242466.16505242581, Test Loss: 56080052.0\n",
      "Epoch [39/100], Training Loss: 238015.23387240092, Test Loss: 55148660.0\n",
      "Epoch [40/100], Training Loss: 233658.60184526982, Test Loss: 54238012.0\n",
      "Epoch [41/100], Training Loss: 229395.93544517504, Test Loss: 53346872.0\n",
      "Epoch [42/100], Training Loss: 225217.4541200166, Test Loss: 52474388.0\n",
      "Epoch [43/100], Training Loss: 221125.74299508322, Test Loss: 51620700.0\n",
      "Epoch [44/100], Training Loss: 217107.44847757835, Test Loss: 50780660.0\n",
      "Epoch [45/100], Training Loss: 213167.28152953024, Test Loss: 49960656.0\n",
      "Epoch [46/100], Training Loss: 209323.0257982347, Test Loss: 49160900.0\n",
      "Epoch [47/100], Training Loss: 205566.90846217642, Test Loss: 48377800.0\n",
      "Epoch [48/100], Training Loss: 201897.5167644097, Test Loss: 47613652.0\n",
      "Epoch [49/100], Training Loss: 198315.89414134235, Test Loss: 46867780.0\n",
      "Epoch [50/100], Training Loss: 194818.5968100231, Test Loss: 46139576.0\n",
      "Epoch [51/100], Training Loss: 191410.80201854155, Test Loss: 45428744.0\n",
      "Epoch [52/100], Training Loss: 188090.19451602394, Test Loss: 44734484.0\n",
      "Epoch [53/100], Training Loss: 184856.73177684972, Test Loss: 44059516.0\n",
      "Epoch [54/100], Training Loss: 181708.7699928914, Test Loss: 43399020.0\n",
      "Epoch [55/100], Training Loss: 178640.79523206563, Test Loss: 42752116.0\n",
      "Epoch [56/100], Training Loss: 175649.2097772644, Test Loss: 42119536.0\n",
      "Epoch [57/100], Training Loss: 172734.0611004976, Test Loss: 41499652.0\n",
      "Epoch [58/100], Training Loss: 169894.1752302885, Test Loss: 40893756.0\n",
      "Epoch [59/100], Training Loss: 167127.01626088502, Test Loss: 40302688.0\n",
      "Epoch [60/100], Training Loss: 164436.35376310645, Test Loss: 39724880.0\n",
      "Epoch [61/100], Training Loss: 161817.67501740123, Test Loss: 39160188.0\n",
      "Epoch [62/100], Training Loss: 159263.46391283098, Test Loss: 38607860.0\n",
      "Epoch [63/100], Training Loss: 156774.04249322464, Test Loss: 38066696.0\n",
      "Epoch [64/100], Training Loss: 154347.0434401102, Test Loss: 37539204.0\n",
      "Epoch [65/100], Training Loss: 151976.54183975328, Test Loss: 37023012.0\n",
      "Epoch [66/100], Training Loss: 149659.50843727787, Test Loss: 36516772.0\n",
      "Epoch [67/100], Training Loss: 147392.37285516076, Test Loss: 36020344.0\n",
      "Epoch [68/100], Training Loss: 145176.06962050067, Test Loss: 35533916.0\n",
      "Epoch [69/100], Training Loss: 143008.12643895595, Test Loss: 35058160.0\n",
      "Epoch [70/100], Training Loss: 140888.31462126545, Test Loss: 34592708.0\n",
      "Epoch [71/100], Training Loss: 138820.06952863518, Test Loss: 34139360.0\n",
      "Epoch [72/100], Training Loss: 136800.82508464943, Test Loss: 33696760.0\n",
      "Epoch [73/100], Training Loss: 134832.48426399345, Test Loss: 33266608.0\n",
      "Epoch [74/100], Training Loss: 132912.27783437417, Test Loss: 32846008.0\n",
      "Epoch [75/100], Training Loss: 131041.35998699305, Test Loss: 32436548.0\n",
      "Epoch [76/100], Training Loss: 129218.50517512755, Test Loss: 32036530.0\n",
      "Epoch [77/100], Training Loss: 127437.88142202217, Test Loss: 31645082.0\n",
      "Epoch [78/100], Training Loss: 125699.15103657589, Test Loss: 31263670.0\n",
      "Epoch [79/100], Training Loss: 123997.44505252299, Test Loss: 30891420.0\n",
      "Epoch [80/100], Training Loss: 122327.8183904982, Test Loss: 30524498.0\n",
      "Epoch [81/100], Training Loss: 120688.17092927145, Test Loss: 30164586.0\n",
      "Epoch [82/100], Training Loss: 119084.15719688556, Test Loss: 29813488.0\n",
      "Epoch [83/100], Training Loss: 117513.10881640231, Test Loss: 29467740.0\n",
      "Epoch [84/100], Training Loss: 115973.03778772177, Test Loss: 29128866.0\n",
      "Epoch [85/100], Training Loss: 114463.76072118802, Test Loss: 28795796.0\n",
      "Epoch [86/100], Training Loss: 112983.38918801093, Test Loss: 28469678.0\n",
      "Epoch [87/100], Training Loss: 111538.81885121402, Test Loss: 28151296.0\n",
      "Epoch [88/100], Training Loss: 110127.13993381057, Test Loss: 27842312.0\n",
      "Epoch [89/100], Training Loss: 108746.01149753235, Test Loss: 27542538.0\n",
      "Epoch [90/100], Training Loss: 107397.60480255168, Test Loss: 27250158.0\n",
      "Epoch [91/100], Training Loss: 106079.24156411053, Test Loss: 26964944.0\n",
      "Epoch [92/100], Training Loss: 104793.62839624615, Test Loss: 26687726.0\n",
      "Epoch [93/100], Training Loss: 103539.53286053847, Test Loss: 26416318.0\n",
      "Epoch [94/100], Training Loss: 102311.71918919569, Test Loss: 26151774.0\n",
      "Epoch [95/100], Training Loss: 101113.99037471862, Test Loss: 25899276.0\n",
      "Epoch [96/100], Training Loss: 99945.80481361257, Test Loss: 25649464.0\n",
      "Epoch [97/100], Training Loss: 98806.27971142667, Test Loss: 25410264.0\n",
      "Epoch [98/100], Training Loss: 97690.86216729422, Test Loss: 25176430.0\n",
      "Epoch [99/100], Training Loss: 96604.55349926508, Test Loss: 24951302.0\n",
      "Epoch [100/100], Training Loss: 95547.92972521807, Test Loss: 24729786.0\n",
      "Epoch [1/100], Training Loss: 1164526.4057816481, Test Loss: 299492224.0\n",
      "Epoch [2/100], Training Loss: 1152760.7084888336, Test Loss: 292332064.0\n",
      "Epoch [3/100], Training Loss: 1096384.2142053195, Test Loss: 269070240.0\n",
      "Epoch [4/100], Training Loss: 971296.2469048043, Test Loss: 229102992.0\n",
      "Epoch [5/100], Training Loss: 806152.5990166459, Test Loss: 187693424.0\n",
      "Epoch [6/100], Training Loss: 671460.4499733428, Test Loss: 161482688.0\n",
      "Epoch [7/100], Training Loss: 597359.0822818553, Test Loss: 148389904.0\n",
      "Epoch [8/100], Training Loss: 556740.3426337303, Test Loss: 140084784.0\n",
      "Epoch [9/100], Training Loss: 527001.5375866359, Test Loss: 133304944.0\n",
      "Epoch [10/100], Training Loss: 501277.87785083824, Test Loss: 127232672.0\n",
      "Epoch [11/100], Training Loss: 477718.18517860316, Test Loss: 121605944.0\n",
      "Epoch [12/100], Training Loss: 455640.75611634384, Test Loss: 116315864.0\n",
      "Epoch [13/100], Training Loss: 434746.18707422545, Test Loss: 111304760.0\n",
      "Epoch [14/100], Training Loss: 414718.5197559386, Test Loss: 106460520.0\n",
      "Epoch [15/100], Training Loss: 395313.4646051774, Test Loss: 101848240.0\n",
      "Epoch [16/100], Training Loss: 377065.4003909721, Test Loss: 97583648.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/100], Training Loss: 360284.9577631657, Test Loss: 93697832.0\n",
      "Epoch [18/100], Training Loss: 344905.35821337596, Test Loss: 90134368.0\n",
      "Epoch [19/100], Training Loss: 330783.8412416326, Test Loss: 86860376.0\n",
      "Epoch [20/100], Training Loss: 317796.52485042356, Test Loss: 83844216.0\n",
      "Epoch [21/100], Training Loss: 305845.2064451158, Test Loss: 81058256.0\n",
      "Epoch [22/100], Training Loss: 294840.36395948107, Test Loss: 78479896.0\n",
      "Epoch [23/100], Training Loss: 284695.89029085956, Test Loss: 76089528.0\n",
      "Epoch [24/100], Training Loss: 275333.26023339847, Test Loss: 73868616.0\n",
      "Epoch [25/100], Training Loss: 266673.055387714, Test Loss: 71799960.0\n",
      "Epoch [26/100], Training Loss: 258643.5186304129, Test Loss: 69867632.0\n",
      "Epoch [27/100], Training Loss: 251187.05834962384, Test Loss: 68059248.0\n",
      "Epoch [28/100], Training Loss: 244251.13891357149, Test Loss: 66369244.0\n",
      "Epoch [29/100], Training Loss: 237785.217937326, Test Loss: 64786064.0\n",
      "Epoch [30/100], Training Loss: 231739.76778626858, Test Loss: 63298480.0\n",
      "Epoch [31/100], Training Loss: 226075.2417510811, Test Loss: 61897908.0\n",
      "Epoch [32/100], Training Loss: 220748.73751555002, Test Loss: 60576948.0\n",
      "Epoch [33/100], Training Loss: 215733.5032284817, Test Loss: 59330872.0\n",
      "Epoch [34/100], Training Loss: 210995.4627095551, Test Loss: 58149448.0\n",
      "Epoch [35/100], Training Loss: 206499.446715242, Test Loss: 57025560.0\n",
      "Epoch [36/100], Training Loss: 202215.91007641726, Test Loss: 55955400.0\n",
      "Epoch [37/100], Training Loss: 198126.01883774658, Test Loss: 54932632.0\n",
      "Epoch [38/100], Training Loss: 194199.28475801196, Test Loss: 53952504.0\n",
      "Epoch [39/100], Training Loss: 190425.49884485517, Test Loss: 53009716.0\n",
      "Epoch [40/100], Training Loss: 186795.5272791896, Test Loss: 52099644.0\n",
      "Epoch [41/100], Training Loss: 183297.11711391504, Test Loss: 51218028.0\n",
      "Epoch [42/100], Training Loss: 179920.4667969907, Test Loss: 50369548.0\n",
      "Epoch [43/100], Training Loss: 176656.84805402523, Test Loss: 49548540.0\n",
      "Epoch [44/100], Training Loss: 173493.22054380664, Test Loss: 48754196.0\n",
      "Epoch [45/100], Training Loss: 170430.05236656597, Test Loss: 47985164.0\n",
      "Epoch [46/100], Training Loss: 167460.91759966826, Test Loss: 47239880.0\n",
      "Epoch [47/100], Training Loss: 164581.29660565133, Test Loss: 46516808.0\n",
      "Epoch [48/100], Training Loss: 161790.68680765358, Test Loss: 45813532.0\n",
      "Epoch [49/100], Training Loss: 159081.74456489543, Test Loss: 45130216.0\n",
      "Epoch [50/100], Training Loss: 156450.99922990345, Test Loss: 44468784.0\n",
      "Epoch [51/100], Training Loss: 153904.34630649842, Test Loss: 43826452.0\n",
      "Epoch [52/100], Training Loss: 151435.02754576152, Test Loss: 43204332.0\n",
      "Epoch [53/100], Training Loss: 149044.02831585807, Test Loss: 42600084.0\n",
      "Epoch [54/100], Training Loss: 146731.5941591138, Test Loss: 42012820.0\n",
      "Epoch [55/100], Training Loss: 144487.46034002726, Test Loss: 41441180.0\n",
      "Epoch [56/100], Training Loss: 142311.0661690658, Test Loss: 40884976.0\n",
      "Epoch [57/100], Training Loss: 140202.7249570523, Test Loss: 40343144.0\n",
      "Epoch [58/100], Training Loss: 138161.8858479948, Test Loss: 39816988.0\n",
      "Epoch [59/100], Training Loss: 136187.4721876666, Test Loss: 39304964.0\n",
      "Epoch [60/100], Training Loss: 134273.16106865706, Test Loss: 38809100.0\n",
      "Epoch [61/100], Training Loss: 132419.0183046028, Test Loss: 38328788.0\n",
      "Epoch [62/100], Training Loss: 130620.15336769148, Test Loss: 37862528.0\n",
      "Epoch [63/100], Training Loss: 128871.59291511166, Test Loss: 37409936.0\n",
      "Epoch [64/100], Training Loss: 127171.53035957586, Test Loss: 36969924.0\n",
      "Epoch [65/100], Training Loss: 125518.57052307327, Test Loss: 36540020.0\n",
      "Epoch [66/100], Training Loss: 123907.34227830105, Test Loss: 36120868.0\n",
      "Epoch [67/100], Training Loss: 122334.12890231621, Test Loss: 35711424.0\n",
      "Epoch [68/100], Training Loss: 120796.62318583022, Test Loss: 35310632.0\n",
      "Epoch [69/100], Training Loss: 119296.37314140158, Test Loss: 34920756.0\n",
      "Epoch [70/100], Training Loss: 117834.45862211955, Test Loss: 34540668.0\n",
      "Epoch [71/100], Training Loss: 116412.14323795984, Test Loss: 34170196.0\n",
      "Epoch [72/100], Training Loss: 115029.97565310112, Test Loss: 33809620.0\n",
      "Epoch [73/100], Training Loss: 113687.1107161898, Test Loss: 33458786.0\n",
      "Epoch [74/100], Training Loss: 112385.52526509092, Test Loss: 33117786.0\n",
      "Epoch [75/100], Training Loss: 111119.66151294355, Test Loss: 32785114.0\n",
      "Epoch [76/100], Training Loss: 109883.71654522837, Test Loss: 32459906.0\n",
      "Epoch [77/100], Training Loss: 108674.50139209762, Test Loss: 32142152.0\n",
      "Epoch [78/100], Training Loss: 107491.59475149577, Test Loss: 31832214.0\n",
      "Epoch [79/100], Training Loss: 106332.21313903204, Test Loss: 31529272.0\n",
      "Epoch [80/100], Training Loss: 105195.35074936319, Test Loss: 31233070.0\n",
      "Epoch [81/100], Training Loss: 104080.07327764943, Test Loss: 30943146.0\n",
      "Epoch [82/100], Training Loss: 102986.4059001244, Test Loss: 30658764.0\n",
      "Epoch [83/100], Training Loss: 101912.88851371364, Test Loss: 30380204.0\n",
      "Epoch [84/100], Training Loss: 100862.16136484806, Test Loss: 30107740.0\n",
      "Epoch [85/100], Training Loss: 99836.30898643445, Test Loss: 29841078.0\n",
      "Epoch [86/100], Training Loss: 98830.80522480895, Test Loss: 29579688.0\n",
      "Epoch [87/100], Training Loss: 97838.84775783426, Test Loss: 29321600.0\n",
      "Epoch [88/100], Training Loss: 96855.25496119898, Test Loss: 29065982.0\n",
      "Epoch [89/100], Training Loss: 95880.19465671465, Test Loss: 28814076.0\n",
      "Epoch [90/100], Training Loss: 94916.12517030981, Test Loss: 28565956.0\n",
      "Epoch [91/100], Training Loss: 93967.68905870506, Test Loss: 28321162.0\n",
      "Epoch [92/100], Training Loss: 93037.52242165749, Test Loss: 28081770.0\n",
      "Epoch [93/100], Training Loss: 92125.57087850246, Test Loss: 27846860.0\n",
      "Epoch [94/100], Training Loss: 91228.28529115574, Test Loss: 27617694.0\n",
      "Epoch [95/100], Training Loss: 90347.50832296665, Test Loss: 27391750.0\n",
      "Epoch [96/100], Training Loss: 89491.32148569397, Test Loss: 27174620.0\n",
      "Epoch [97/100], Training Loss: 88661.69818138736, Test Loss: 26963678.0\n",
      "Epoch [98/100], Training Loss: 87857.18766660742, Test Loss: 26761000.0\n",
      "Epoch [99/100], Training Loss: 87070.9761862449, Test Loss: 26562414.0\n",
      "Epoch [100/100], Training Loss: 86300.74172146199, Test Loss: 26369070.0\n",
      "Epoch [1/100], Training Loss: 582424.9454416208, Test Loss: 300042016.0\n",
      "Epoch [2/100], Training Loss: 581998.0712043125, Test Loss: 299531648.0\n",
      "Epoch [3/100], Training Loss: 579878.6631123748, Test Loss: 297632736.0\n",
      "Epoch [4/100], Training Loss: 573927.7104436941, Test Loss: 293141728.0\n",
      "Epoch [5/100], Training Loss: 561835.1794324981, Test Loss: 284956416.0\n",
      "Epoch [6/100], Training Loss: 541820.2845802974, Test Loss: 272422016.0\n",
      "Epoch [7/100], Training Loss: 513288.4724838576, Test Loss: 255649648.0\n",
      "Epoch [8/100], Training Loss: 477368.1331674664, Test Loss: 235722784.0\n",
      "Epoch [9/100], Training Loss: 437077.9693146141, Test Loss: 214628032.0\n",
      "Epoch [10/100], Training Loss: 396801.72217285703, Test Loss: 194773840.0\n",
      "Epoch [11/100], Training Loss: 360988.0831704283, Test Loss: 178162944.0\n",
      "Epoch [12/100], Training Loss: 332462.6986552929, Test Loss: 165593680.0\n",
      "Epoch [13/100], Training Loss: 311456.54688703275, Test Loss: 156566752.0\n",
      "Epoch [14/100], Training Loss: 296266.6363367099, Test Loss: 149976208.0\n",
      "Epoch [15/100], Training Loss: 284780.97316509683, Test Loss: 144834080.0\n",
      "Epoch [16/100], Training Loss: 275428.3452402109, Test Loss: 140510448.0\n",
      "Epoch [17/100], Training Loss: 267291.46804099286, Test Loss: 136665168.0\n",
      "Epoch [18/100], Training Loss: 259892.6603874178, Test Loss: 133125008.0\n",
      "Epoch [19/100], Training Loss: 252986.7415437474, Test Loss: 129799024.0\n",
      "Epoch [20/100], Training Loss: 246440.05971210237, Test Loss: 126635520.0\n",
      "Epoch [21/100], Training Loss: 240173.41863633672, Test Loss: 123602840.0\n",
      "Epoch [22/100], Training Loss: 234136.82886084946, Test Loss: 120680640.0\n",
      "Epoch [23/100], Training Loss: 228297.970499378, Test Loss: 117855744.0\n",
      "Epoch [24/100], Training Loss: 222636.00995201705, Test Loss: 115119672.0\n",
      "Epoch [25/100], Training Loss: 217138.05817190924, Test Loss: 112467352.0\n",
      "Epoch [26/100], Training Loss: 211795.9663527042, Test Loss: 109895000.0\n",
      "Epoch [27/100], Training Loss: 206603.19317575972, Test Loss: 107397360.0\n",
      "Epoch [28/100], Training Loss: 201536.10899828208, Test Loss: 104952264.0\n",
      "Epoch [29/100], Training Loss: 196550.67069486403, Test Loss: 102550896.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/100], Training Loss: 191656.0421775961, Test Loss: 100216040.0\n",
      "Epoch [31/100], Training Loss: 186905.94372371305, Test Loss: 97961064.0\n",
      "Epoch [32/100], Training Loss: 182325.9911142705, Test Loss: 95797688.0\n",
      "Epoch [33/100], Training Loss: 177947.33771695988, Test Loss: 93736296.0\n",
      "Epoch [34/100], Training Loss: 173770.31337006102, Test Loss: 91778032.0\n",
      "Epoch [35/100], Training Loss: 169796.3912090516, Test Loss: 89915608.0\n",
      "Epoch [36/100], Training Loss: 166010.36099757123, Test Loss: 88139368.0\n",
      "Epoch [37/100], Training Loss: 162402.92304958237, Test Loss: 86446568.0\n",
      "Epoch [38/100], Training Loss: 158963.51543155027, Test Loss: 84831048.0\n",
      "Epoch [39/100], Training Loss: 155681.018660032, Test Loss: 83288504.0\n",
      "Epoch [40/100], Training Loss: 152549.92855873468, Test Loss: 81814704.0\n",
      "Epoch [41/100], Training Loss: 149563.81043777027, Test Loss: 80406152.0\n",
      "Epoch [42/100], Training Loss: 146714.36313014632, Test Loss: 79058056.0\n",
      "Epoch [43/100], Training Loss: 143993.66080208519, Test Loss: 77766608.0\n",
      "Epoch [44/100], Training Loss: 141394.41905100408, Test Loss: 76528712.0\n",
      "Epoch [45/100], Training Loss: 138908.0459688407, Test Loss: 75339304.0\n",
      "Epoch [46/100], Training Loss: 136527.2934067887, Test Loss: 74195920.0\n",
      "Epoch [47/100], Training Loss: 134245.62028315858, Test Loss: 73096560.0\n",
      "Epoch [48/100], Training Loss: 132056.9572892601, Test Loss: 72037968.0\n",
      "Epoch [49/100], Training Loss: 129955.68888099046, Test Loss: 71018024.0\n",
      "Epoch [50/100], Training Loss: 127934.72851134412, Test Loss: 70033056.0\n",
      "Epoch [51/100], Training Loss: 125990.74557194479, Test Loss: 69080992.0\n",
      "Epoch [52/100], Training Loss: 124118.65031692435, Test Loss: 68161136.0\n",
      "Epoch [53/100], Training Loss: 122315.64267519697, Test Loss: 67272616.0\n",
      "Epoch [54/100], Training Loss: 120578.27379894556, Test Loss: 66413540.0\n",
      "Epoch [55/100], Training Loss: 118900.94591552633, Test Loss: 65581376.0\n",
      "Epoch [56/100], Training Loss: 117280.99662342278, Test Loss: 64774252.0\n",
      "Epoch [57/100], Training Loss: 115714.60340027249, Test Loss: 63991392.0\n",
      "Epoch [58/100], Training Loss: 114198.26550559801, Test Loss: 63230540.0\n",
      "Epoch [59/100], Training Loss: 112727.62703631302, Test Loss: 62490216.0\n",
      "Epoch [60/100], Training Loss: 111299.58983472543, Test Loss: 61768352.0\n",
      "Epoch [61/100], Training Loss: 109910.85077898229, Test Loss: 61065616.0\n",
      "Epoch [62/100], Training Loss: 108559.5573721936, Test Loss: 60380612.0\n",
      "Epoch [63/100], Training Loss: 107243.66719981044, Test Loss: 59712984.0\n",
      "Epoch [64/100], Training Loss: 105959.27148865588, Test Loss: 59061408.0\n",
      "Epoch [65/100], Training Loss: 104702.95847402405, Test Loss: 58424244.0\n",
      "Epoch [66/100], Training Loss: 103474.71346484213, Test Loss: 57800896.0\n",
      "Epoch [67/100], Training Loss: 102273.66293466027, Test Loss: 57191820.0\n",
      "Epoch [68/100], Training Loss: 101097.96244298323, Test Loss: 56596336.0\n",
      "Epoch [69/100], Training Loss: 99947.3986138262, Test Loss: 56013936.0\n",
      "Epoch [70/100], Training Loss: 98820.88324151412, Test Loss: 55444532.0\n",
      "Epoch [71/100], Training Loss: 97717.20063977252, Test Loss: 54886416.0\n",
      "Epoch [72/100], Training Loss: 96635.00266571886, Test Loss: 54338784.0\n",
      "Epoch [73/100], Training Loss: 95573.81659854273, Test Loss: 53801608.0\n",
      "Epoch [74/100], Training Loss: 94533.41318642261, Test Loss: 53274832.0\n",
      "Epoch [75/100], Training Loss: 93513.43439369705, Test Loss: 52757856.0\n",
      "Epoch [76/100], Training Loss: 92513.37965760322, Test Loss: 52250612.0\n",
      "Epoch [77/100], Training Loss: 91531.96848527931, Test Loss: 51752732.0\n",
      "Epoch [78/100], Training Loss: 90568.18837746579, Test Loss: 51262592.0\n",
      "Epoch [79/100], Training Loss: 89619.82299626799, Test Loss: 50780240.0\n",
      "Epoch [80/100], Training Loss: 88686.36490729223, Test Loss: 50305300.0\n",
      "Epoch [81/100], Training Loss: 87768.39784372963, Test Loss: 49839416.0\n",
      "Epoch [82/100], Training Loss: 86865.95320182454, Test Loss: 49382208.0\n",
      "Epoch [83/100], Training Loss: 85978.0202594633, Test Loss: 48932632.0\n",
      "Epoch [84/100], Training Loss: 85104.9965049464, Test Loss: 48490324.0\n",
      "Epoch [85/100], Training Loss: 84247.5405485457, Test Loss: 48055212.0\n",
      "Epoch [86/100], Training Loss: 83405.20253539483, Test Loss: 47627424.0\n",
      "Epoch [87/100], Training Loss: 82577.76079616137, Test Loss: 47206592.0\n",
      "Epoch [88/100], Training Loss: 81764.6827794562, Test Loss: 46791880.0\n",
      "Epoch [89/100], Training Loss: 80965.46484213021, Test Loss: 46384308.0\n",
      "Epoch [90/100], Training Loss: 80178.91854747942, Test Loss: 45984620.0\n",
      "Epoch [91/100], Training Loss: 79405.06356258516, Test Loss: 45591604.0\n",
      "Epoch [92/100], Training Loss: 78643.31982702446, Test Loss: 45204476.0\n",
      "Epoch [93/100], Training Loss: 77893.4463598128, Test Loss: 44823124.0\n",
      "Epoch [94/100], Training Loss: 77155.00539067591, Test Loss: 44448192.0\n",
      "Epoch [95/100], Training Loss: 76428.63704756828, Test Loss: 44079244.0\n",
      "Epoch [96/100], Training Loss: 75714.34843907351, Test Loss: 43716196.0\n",
      "Epoch [97/100], Training Loss: 75012.10366684438, Test Loss: 43357856.0\n",
      "Epoch [98/100], Training Loss: 74322.10805047094, Test Loss: 43005064.0\n",
      "Epoch [99/100], Training Loss: 73644.26443931047, Test Loss: 42657944.0\n",
      "Epoch [100/100], Training Loss: 72978.58942005805, Test Loss: 42316816.0\n",
      "Epoch [1/100], Training Loss: 7211457.548664179, Test Loss: 149534016.0\n",
      "Epoch [2/100], Training Loss: 4095995.715538179, Test Loss: 118493080.0\n",
      "Epoch [3/100], Training Loss: 3282962.3156211125, Test Loss: 96648816.0\n",
      "Epoch [4/100], Training Loss: 2680652.4788815826, Test Loss: 81001696.0\n",
      "Epoch [5/100], Training Loss: 2245854.076831941, Test Loss: 69547984.0\n",
      "Epoch [6/100], Training Loss: 1935637.8439665895, Test Loss: 61359532.0\n",
      "Epoch [7/100], Training Loss: 1717680.3942005804, Test Loss: 55512052.0\n",
      "Epoch [8/100], Training Loss: 1559772.6752562053, Test Loss: 51100248.0\n",
      "Epoch [9/100], Training Loss: 1435614.0131508796, Test Loss: 47497176.0\n",
      "Epoch [10/100], Training Loss: 1331834.084295954, Test Loss: 44449532.0\n",
      "Epoch [11/100], Training Loss: 1242676.0079675375, Test Loss: 41775940.0\n",
      "Epoch [12/100], Training Loss: 1164997.988567028, Test Loss: 39418120.0\n",
      "Epoch [13/100], Training Loss: 1097121.8228185533, Test Loss: 37321896.0\n",
      "Epoch [14/100], Training Loss: 1037578.8556661336, Test Loss: 35479940.0\n",
      "Epoch [15/100], Training Loss: 984460.0578460991, Test Loss: 33826112.0\n",
      "Epoch [16/100], Training Loss: 936644.4416799953, Test Loss: 32321156.0\n",
      "Epoch [17/100], Training Loss: 893023.0643919199, Test Loss: 30957430.0\n",
      "Epoch [18/100], Training Loss: 854128.3391238671, Test Loss: 29748596.0\n",
      "Epoch [19/100], Training Loss: 819490.0981873112, Test Loss: 28661644.0\n",
      "Epoch [20/100], Training Loss: 788305.7464753273, Test Loss: 27676530.0\n",
      "Epoch [21/100], Training Loss: 759704.7684675079, Test Loss: 26768558.0\n",
      "Epoch [22/100], Training Loss: 733738.598720455, Test Loss: 25960672.0\n",
      "Epoch [23/100], Training Loss: 709812.604007464, Test Loss: 25207016.0\n",
      "Epoch [24/100], Training Loss: 686869.4785853919, Test Loss: 24496332.0\n",
      "Epoch [25/100], Training Loss: 665617.1853266987, Test Loss: 23836354.0\n",
      "Epoch [26/100], Training Loss: 645073.0633848705, Test Loss: 23162320.0\n",
      "Epoch [27/100], Training Loss: 624434.4657899414, Test Loss: 22530512.0\n",
      "Epoch [28/100], Training Loss: 605184.390735146, Test Loss: 21968158.0\n",
      "Epoch [29/100], Training Loss: 587972.7379005983, Test Loss: 21481750.0\n",
      "Epoch [30/100], Training Loss: 572451.0990314555, Test Loss: 21045624.0\n",
      "Epoch [31/100], Training Loss: 558023.9763935786, Test Loss: 20642660.0\n",
      "Epoch [32/100], Training Loss: 544302.6882737989, Test Loss: 20253992.0\n",
      "Epoch [33/100], Training Loss: 530870.1555224808, Test Loss: 19874584.0\n",
      "Epoch [34/100], Training Loss: 518079.49686778034, Test Loss: 19541832.0\n",
      "Epoch [35/100], Training Loss: 506030.2072004028, Test Loss: 19219004.0\n",
      "Epoch [36/100], Training Loss: 493506.94646347966, Test Loss: 18865414.0\n",
      "Epoch [37/100], Training Loss: 481625.1437340797, Test Loss: 18576474.0\n",
      "Epoch [38/100], Training Loss: 471230.98319116165, Test Loss: 18318446.0\n",
      "Epoch [39/100], Training Loss: 461624.12117173156, Test Loss: 18092830.0\n",
      "Epoch [40/100], Training Loss: 452649.31318494165, Test Loss: 17891528.0\n",
      "Epoch [41/100], Training Loss: 444252.0689458563, Test Loss: 17699260.0\n",
      "Epoch [42/100], Training Loss: 436543.7291037261, Test Loss: 17551270.0\n",
      "Epoch [43/100], Training Loss: 429096.50232509925, Test Loss: 17380716.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/100], Training Loss: 421767.8888839524, Test Loss: 17236922.0\n",
      "Epoch [45/100], Training Loss: 414514.95633404417, Test Loss: 17079316.0\n",
      "Epoch [46/100], Training Loss: 407842.4129716841, Test Loss: 16963992.0\n",
      "Epoch [47/100], Training Loss: 401686.59543273505, Test Loss: 16835930.0\n",
      "Epoch [48/100], Training Loss: 395984.29132604704, Test Loss: 16764219.0\n",
      "Epoch [49/100], Training Loss: 390778.1975445767, Test Loss: 16641376.0\n",
      "Epoch [50/100], Training Loss: 385887.3183830934, Test Loss: 16587975.0\n",
      "Epoch [51/100], Training Loss: 381219.3790207926, Test Loss: 16440713.0\n",
      "Epoch [52/100], Training Loss: 376774.13967626326, Test Loss: 16414433.0\n",
      "Epoch [53/100], Training Loss: 372649.30778686097, Test Loss: 16291906.0\n",
      "Epoch [54/100], Training Loss: 368702.27878235886, Test Loss: 16254972.0\n",
      "Epoch [55/100], Training Loss: 364945.4543495646, Test Loss: 16141848.0\n",
      "Epoch [56/100], Training Loss: 361278.74118091346, Test Loss: 16097740.0\n",
      "Epoch [57/100], Training Loss: 357789.3852852319, Test Loss: 16028855.0\n",
      "Epoch [58/100], Training Loss: 354439.06396984775, Test Loss: 15941548.0\n",
      "Epoch [59/100], Training Loss: 351169.10617706296, Test Loss: 15902311.0\n",
      "Epoch [60/100], Training Loss: 347965.0200669392, Test Loss: 15841420.0\n",
      "Epoch [61/100], Training Loss: 344868.46506427345, Test Loss: 15773028.0\n",
      "Epoch [62/100], Training Loss: 341797.75831556186, Test Loss: 15727587.0\n",
      "Epoch [63/100], Training Loss: 338820.77984864643, Test Loss: 15640075.0\n",
      "Epoch [64/100], Training Loss: 335931.1574106984, Test Loss: 15658858.0\n",
      "Epoch [65/100], Training Loss: 333233.1826091464, Test Loss: 15548860.0\n",
      "Epoch [66/100], Training Loss: 330585.1345299449, Test Loss: 15565282.0\n",
      "Epoch [67/100], Training Loss: 328024.362648836, Test Loss: 15467357.0\n",
      "Epoch [68/100], Training Loss: 325533.82127836026, Test Loss: 15499230.0\n",
      "Epoch [69/100], Training Loss: 323051.9317650021, Test Loss: 15423713.0\n",
      "Epoch [70/100], Training Loss: 320596.75231028965, Test Loss: 15429797.0\n",
      "Epoch [71/100], Training Loss: 318177.73674545344, Test Loss: 15363094.0\n",
      "Epoch [72/100], Training Loss: 315766.9317872164, Test Loss: 15331470.0\n",
      "Epoch [73/100], Training Loss: 313418.6154626503, Test Loss: 15321667.0\n",
      "Epoch [74/100], Training Loss: 311094.89941354183, Test Loss: 15289672.0\n",
      "Epoch [75/100], Training Loss: 308883.5257908299, Test Loss: 15254452.0\n",
      "Epoch [76/100], Training Loss: 306728.142016172, Test Loss: 15248611.0\n",
      "Epoch [77/100], Training Loss: 304648.9157929033, Test Loss: 15173746.0\n",
      "Epoch [78/100], Training Loss: 302626.89483739116, Test Loss: 15237844.0\n",
      "Epoch [79/100], Training Loss: 300704.1537009063, Test Loss: 15154606.0\n",
      "Epoch [80/100], Training Loss: 298775.52881197794, Test Loss: 15178318.0\n",
      "Epoch [81/100], Training Loss: 296937.1492284225, Test Loss: 15116583.0\n",
      "Epoch [82/100], Training Loss: 295134.4299804514, Test Loss: 15135821.0\n",
      "Epoch [83/100], Training Loss: 293365.200624963, Test Loss: 15119916.0\n",
      "Epoch [84/100], Training Loss: 291624.1208088976, Test Loss: 15067299.0\n",
      "Epoch [85/100], Training Loss: 289898.17777382856, Test Loss: 15064993.0\n",
      "Epoch [86/100], Training Loss: 288190.86629938986, Test Loss: 15049566.0\n",
      "Epoch [87/100], Training Loss: 286554.7709851312, Test Loss: 15016637.0\n",
      "Epoch [88/100], Training Loss: 284952.37081630237, Test Loss: 15048684.0\n",
      "Epoch [89/100], Training Loss: 283423.2592781826, Test Loss: 14967840.0\n",
      "Epoch [90/100], Training Loss: 281902.07922368345, Test Loss: 15019229.0\n",
      "Epoch [91/100], Training Loss: 280437.0768763699, Test Loss: 14975288.0\n",
      "Epoch [92/100], Training Loss: 279000.21629346604, Test Loss: 14955221.0\n",
      "Epoch [93/100], Training Loss: 277599.11524791183, Test Loss: 14974554.0\n",
      "Epoch [94/100], Training Loss: 276256.61402612407, Test Loss: 14966711.0\n",
      "Epoch [95/100], Training Loss: 274962.04255523963, Test Loss: 14923782.0\n",
      "Epoch [96/100], Training Loss: 273671.3700165867, Test Loss: 14986269.0\n",
      "Epoch [97/100], Training Loss: 272469.66876221786, Test Loss: 14908507.0\n",
      "Epoch [98/100], Training Loss: 271224.64812511107, Test Loss: 14960129.0\n",
      "Epoch [99/100], Training Loss: 270071.50777501334, Test Loss: 14904173.0\n",
      "Epoch [100/100], Training Loss: 268859.03093714826, Test Loss: 14915091.0\n",
      "Epoch [1/100], Training Loss: 4385101.251821575, Test Loss: 229905568.0\n",
      "Epoch [2/100], Training Loss: 2666488.810615485, Test Loss: 144673088.0\n",
      "Epoch [3/100], Training Loss: 2071889.6113974291, Test Loss: 125000192.0\n",
      "Epoch [4/100], Training Loss: 1794406.8631005273, Test Loss: 109340264.0\n",
      "Epoch [5/100], Training Loss: 1563660.2460754695, Test Loss: 96167528.0\n",
      "Epoch [6/100], Training Loss: 1368337.808542148, Test Loss: 85452256.0\n",
      "Epoch [7/100], Training Loss: 1212240.5492565606, Test Loss: 76847744.0\n",
      "Epoch [8/100], Training Loss: 1087364.7500740478, Test Loss: 69905096.0\n",
      "Epoch [9/100], Training Loss: 988149.8966293467, Test Loss: 64312228.0\n",
      "Epoch [10/100], Training Loss: 908923.4624726024, Test Loss: 59789884.0\n",
      "Epoch [11/100], Training Loss: 844821.6425567205, Test Loss: 56081612.0\n",
      "Epoch [12/100], Training Loss: 791977.9191398614, Test Loss: 52978188.0\n",
      "Epoch [13/100], Training Loss: 747104.8635151946, Test Loss: 50293204.0\n",
      "Epoch [14/100], Training Loss: 707665.3421005865, Test Loss: 47897120.0\n",
      "Epoch [15/100], Training Loss: 672242.0850068124, Test Loss: 45733088.0\n",
      "Epoch [16/100], Training Loss: 639953.4496179136, Test Loss: 43749900.0\n",
      "Epoch [17/100], Training Loss: 610246.6897991825, Test Loss: 41907472.0\n",
      "Epoch [18/100], Training Loss: 582834.6455482495, Test Loss: 40192672.0\n",
      "Epoch [19/100], Training Loss: 557595.1109531426, Test Loss: 38604172.0\n",
      "Epoch [20/100], Training Loss: 534441.490669984, Test Loss: 37138024.0\n",
      "Epoch [21/100], Training Loss: 513256.0710562171, Test Loss: 35786476.0\n",
      "Epoch [22/100], Training Loss: 493772.77379894553, Test Loss: 34542252.0\n",
      "Epoch [23/100], Training Loss: 475788.8407381079, Test Loss: 33395428.0\n",
      "Epoch [24/100], Training Loss: 459173.5645696345, Test Loss: 32330570.0\n",
      "Epoch [25/100], Training Loss: 443745.6058586577, Test Loss: 31334358.0\n",
      "Epoch [26/100], Training Loss: 429272.6320715597, Test Loss: 30390732.0\n",
      "Epoch [27/100], Training Loss: 415403.0979503584, Test Loss: 29485758.0\n",
      "Epoch [28/100], Training Loss: 402226.787482969, Test Loss: 28639080.0\n",
      "Epoch [29/100], Training Loss: 390096.04537645873, Test Loss: 27858370.0\n",
      "Epoch [30/100], Training Loss: 378803.70931816834, Test Loss: 27122604.0\n",
      "Epoch [31/100], Training Loss: 367992.9228422487, Test Loss: 26415468.0\n",
      "Epoch [32/100], Training Loss: 357695.1967596706, Test Loss: 25749224.0\n",
      "Epoch [33/100], Training Loss: 348089.17741839937, Test Loss: 25120458.0\n",
      "Epoch [34/100], Training Loss: 339053.2410106036, Test Loss: 24529450.0\n",
      "Epoch [35/100], Training Loss: 330398.59990521887, Test Loss: 23962354.0\n",
      "Epoch [36/100], Training Loss: 322176.328120372, Test Loss: 23436870.0\n",
      "Epoch [37/100], Training Loss: 314510.8227593152, Test Loss: 22948210.0\n",
      "Epoch [38/100], Training Loss: 307316.6620312778, Test Loss: 22499392.0\n",
      "Epoch [39/100], Training Loss: 300514.3390646289, Test Loss: 22077530.0\n",
      "Epoch [40/100], Training Loss: 294002.85428884544, Test Loss: 21675852.0\n",
      "Epoch [41/100], Training Loss: 287776.40778093715, Test Loss: 21294830.0\n",
      "Epoch [42/100], Training Loss: 281848.4191102423, Test Loss: 20934720.0\n",
      "Epoch [43/100], Training Loss: 276127.5644511581, Test Loss: 20584326.0\n",
      "Epoch [44/100], Training Loss: 270569.630486938, Test Loss: 20258730.0\n",
      "Epoch [45/100], Training Loss: 265284.51018896984, Test Loss: 19948996.0\n",
      "Epoch [46/100], Training Loss: 260206.14655529885, Test Loss: 19651342.0\n",
      "Epoch [47/100], Training Loss: 255268.4371186541, Test Loss: 19371730.0\n",
      "Epoch [48/100], Training Loss: 250601.0175789349, Test Loss: 19112558.0\n",
      "Epoch [49/100], Training Loss: 246184.06979740536, Test Loss: 18868834.0\n",
      "Epoch [50/100], Training Loss: 241950.06206682068, Test Loss: 18641616.0\n",
      "Epoch [51/100], Training Loss: 237965.38291570405, Test Loss: 18434436.0\n",
      "Epoch [52/100], Training Loss: 234241.4623096973, Test Loss: 18250096.0\n",
      "Epoch [53/100], Training Loss: 230739.4922397962, Test Loss: 18075430.0\n",
      "Epoch [54/100], Training Loss: 227386.80944553047, Test Loss: 17914414.0\n",
      "Epoch [55/100], Training Loss: 224152.73779693147, Test Loss: 17757718.0\n",
      "Epoch [56/100], Training Loss: 221000.85557727623, Test Loss: 17611870.0\n",
      "Epoch [57/100], Training Loss: 218001.73476097386, Test Loss: 17469122.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [58/100], Training Loss: 215115.666918429, Test Loss: 17345298.0\n",
      "Epoch [59/100], Training Loss: 212382.87894674487, Test Loss: 17221754.0\n",
      "Epoch [60/100], Training Loss: 209767.44878857888, Test Loss: 17113444.0\n",
      "Epoch [61/100], Training Loss: 207290.5829630946, Test Loss: 17018600.0\n",
      "Epoch [62/100], Training Loss: 204918.635611042, Test Loss: 16925100.0\n",
      "Epoch [63/100], Training Loss: 202665.54314021682, Test Loss: 16835420.0\n",
      "Epoch [64/100], Training Loss: 200480.6629494698, Test Loss: 16762546.0\n",
      "Epoch [65/100], Training Loss: 198400.42743320894, Test Loss: 16681303.0\n",
      "Epoch [66/100], Training Loss: 196376.189651087, Test Loss: 16618057.0\n",
      "Epoch [67/100], Training Loss: 194431.56529530243, Test Loss: 16540733.0\n",
      "Epoch [68/100], Training Loss: 192525.86469995853, Test Loss: 16482943.0\n",
      "Epoch [69/100], Training Loss: 190676.48326520942, Test Loss: 16408836.0\n",
      "Epoch [70/100], Training Loss: 188851.0172235057, Test Loss: 16346665.0\n",
      "Epoch [71/100], Training Loss: 187090.8038179018, Test Loss: 16273992.0\n",
      "Epoch [72/100], Training Loss: 185401.81812392632, Test Loss: 16222828.0\n",
      "Epoch [73/100], Training Loss: 183820.8830786091, Test Loss: 16160231.0\n",
      "Epoch [74/100], Training Loss: 182289.65846217642, Test Loss: 16116270.0\n",
      "Epoch [75/100], Training Loss: 180844.27450980392, Test Loss: 16048470.0\n",
      "Epoch [76/100], Training Loss: 179447.10720632665, Test Loss: 16027942.0\n",
      "Epoch [77/100], Training Loss: 178116.18101711984, Test Loss: 15940105.0\n",
      "Epoch [78/100], Training Loss: 176806.58989396362, Test Loss: 15930579.0\n",
      "Epoch [79/100], Training Loss: 175560.8243883656, Test Loss: 15853140.0\n",
      "Epoch [80/100], Training Loss: 174321.61945382383, Test Loss: 15834818.0\n",
      "Epoch [81/100], Training Loss: 173147.10071974408, Test Loss: 15764167.0\n",
      "Epoch [82/100], Training Loss: 171960.09715064274, Test Loss: 15740077.0\n",
      "Epoch [83/100], Training Loss: 170847.37577750132, Test Loss: 15682223.0\n",
      "Epoch [84/100], Training Loss: 169740.5515076121, Test Loss: 15648950.0\n",
      "Epoch [85/100], Training Loss: 168671.20980688347, Test Loss: 15606743.0\n",
      "Epoch [86/100], Training Loss: 167614.26753450625, Test Loss: 15560422.0\n",
      "Epoch [87/100], Training Loss: 166580.79387328948, Test Loss: 15519972.0\n",
      "Epoch [88/100], Training Loss: 165543.26433564362, Test Loss: 15521684.0\n",
      "Epoch [89/100], Training Loss: 164565.0696419051, Test Loss: 15430994.0\n",
      "Epoch [90/100], Training Loss: 163589.44567857354, Test Loss: 15434862.0\n",
      "Epoch [91/100], Training Loss: 162696.05195189858, Test Loss: 15364520.0\n",
      "Epoch [92/100], Training Loss: 161801.86465552988, Test Loss: 15363274.0\n",
      "Epoch [93/100], Training Loss: 160969.27653130738, Test Loss: 15304088.0\n",
      "Epoch [94/100], Training Loss: 160131.76007049344, Test Loss: 15297796.0\n",
      "Epoch [95/100], Training Loss: 159341.22410550324, Test Loss: 15251111.0\n",
      "Epoch [96/100], Training Loss: 158537.7111397429, Test Loss: 15239888.0\n",
      "Epoch [97/100], Training Loss: 157776.30956400686, Test Loss: 15204504.0\n",
      "Epoch [98/100], Training Loss: 156997.4121349446, Test Loss: 15183409.0\n",
      "Epoch [99/100], Training Loss: 156255.82773532375, Test Loss: 15151321.0\n",
      "Epoch [100/100], Training Loss: 155512.67128724602, Test Loss: 15176552.0\n",
      "Epoch [1/100], Training Loss: 2315597.8664771044, Test Loss: 290978272.0\n",
      "Epoch [2/100], Training Loss: 2035253.4901960783, Test Loss: 222117552.0\n",
      "Epoch [3/100], Training Loss: 1432950.1496356851, Test Loss: 158618064.0\n",
      "Epoch [4/100], Training Loss: 1147033.1525383568, Test Loss: 140221056.0\n",
      "Epoch [5/100], Training Loss: 1034891.7246608613, Test Loss: 128384992.0\n",
      "Epoch [6/100], Training Loss: 947075.3396125822, Test Loss: 118165152.0\n",
      "Epoch [7/100], Training Loss: 868934.8029145193, Test Loss: 108991712.0\n",
      "Epoch [8/100], Training Loss: 798542.9851312126, Test Loss: 100809032.0\n",
      "Epoch [9/100], Training Loss: 735844.9947278005, Test Loss: 93620600.0\n",
      "Epoch [10/100], Training Loss: 680733.7219359042, Test Loss: 87355992.0\n",
      "Epoch [11/100], Training Loss: 632569.3893726675, Test Loss: 81875856.0\n",
      "Epoch [12/100], Training Loss: 590478.1823351698, Test Loss: 77059632.0\n",
      "Epoch [13/100], Training Loss: 553638.5574314318, Test Loss: 72809240.0\n",
      "Epoch [14/100], Training Loss: 521337.46934423316, Test Loss: 69040992.0\n",
      "Epoch [15/100], Training Loss: 492932.9756531011, Test Loss: 65697596.0\n",
      "Epoch [16/100], Training Loss: 467854.391327528, Test Loss: 62725628.0\n",
      "Epoch [17/100], Training Loss: 445665.9859013092, Test Loss: 60091684.0\n",
      "Epoch [18/100], Training Loss: 425970.9751791955, Test Loss: 57738276.0\n",
      "Epoch [19/100], Training Loss: 408331.8136366329, Test Loss: 55607120.0\n",
      "Epoch [20/100], Training Loss: 392288.98240625556, Test Loss: 53655352.0\n",
      "Epoch [21/100], Training Loss: 377502.4115277531, Test Loss: 51851444.0\n",
      "Epoch [22/100], Training Loss: 363744.94597476453, Test Loss: 50161988.0\n",
      "Epoch [23/100], Training Loss: 350866.1423493869, Test Loss: 48572812.0\n",
      "Epoch [24/100], Training Loss: 338762.897340205, Test Loss: 47072476.0\n",
      "Epoch [25/100], Training Loss: 327343.0817487116, Test Loss: 45648548.0\n",
      "Epoch [26/100], Training Loss: 316533.2429950832, Test Loss: 44290684.0\n",
      "Epoch [27/100], Training Loss: 306262.9121497542, Test Loss: 42991116.0\n",
      "Epoch [28/100], Training Loss: 296484.18731117825, Test Loss: 41746376.0\n",
      "Epoch [29/100], Training Loss: 287172.98442035425, Test Loss: 40553404.0\n",
      "Epoch [30/100], Training Loss: 278319.74059593625, Test Loss: 39412468.0\n",
      "Epoch [31/100], Training Loss: 269901.6363367099, Test Loss: 38319532.0\n",
      "Epoch [32/100], Training Loss: 261889.4521059179, Test Loss: 37272384.0\n",
      "Epoch [33/100], Training Loss: 254249.7249570523, Test Loss: 36270456.0\n",
      "Epoch [34/100], Training Loss: 246965.57295183933, Test Loss: 35312352.0\n",
      "Epoch [35/100], Training Loss: 240041.48972217285, Test Loss: 34399060.0\n",
      "Epoch [36/100], Training Loss: 233449.52967833658, Test Loss: 33527354.0\n",
      "Epoch [37/100], Training Loss: 227157.94322018838, Test Loss: 32694332.0\n",
      "Epoch [38/100], Training Loss: 221174.19092470827, Test Loss: 31899260.0\n",
      "Epoch [39/100], Training Loss: 215486.16148332445, Test Loss: 31141846.0\n",
      "Epoch [40/100], Training Loss: 210085.3782358865, Test Loss: 30421312.0\n",
      "Epoch [41/100], Training Loss: 204957.88472246905, Test Loss: 29737870.0\n",
      "Epoch [42/100], Training Loss: 200081.07991232746, Test Loss: 29086798.0\n",
      "Epoch [43/100], Training Loss: 195441.8432557313, Test Loss: 28466504.0\n",
      "Epoch [44/100], Training Loss: 191026.1698359102, Test Loss: 27875930.0\n",
      "Epoch [45/100], Training Loss: 186823.30128546886, Test Loss: 27312994.0\n",
      "Epoch [46/100], Training Loss: 182816.96202831584, Test Loss: 26776610.0\n",
      "Epoch [47/100], Training Loss: 178996.14809549198, Test Loss: 26266316.0\n",
      "Epoch [48/100], Training Loss: 175350.22957763166, Test Loss: 25780754.0\n",
      "Epoch [49/100], Training Loss: 171861.74865233104, Test Loss: 25317502.0\n",
      "Epoch [50/100], Training Loss: 168518.13553699426, Test Loss: 24875676.0\n",
      "Epoch [51/100], Training Loss: 165310.0407558794, Test Loss: 24453288.0\n",
      "Epoch [52/100], Training Loss: 162228.47891120194, Test Loss: 24050922.0\n",
      "Epoch [53/100], Training Loss: 159261.8987915408, Test Loss: 23665902.0\n",
      "Epoch [54/100], Training Loss: 156408.43380131508, Test Loss: 23299974.0\n",
      "Epoch [55/100], Training Loss: 153661.95687459272, Test Loss: 22948946.0\n",
      "Epoch [56/100], Training Loss: 151012.98927788637, Test Loss: 22614408.0\n",
      "Epoch [57/100], Training Loss: 148456.23162134943, Test Loss: 22293304.0\n",
      "Epoch [58/100], Training Loss: 145991.3355547657, Test Loss: 21987428.0\n",
      "Epoch [59/100], Training Loss: 143610.54321426456, Test Loss: 21693112.0\n",
      "Epoch [60/100], Training Loss: 141307.73490906938, Test Loss: 21412080.0\n",
      "Epoch [61/100], Training Loss: 139082.16293466027, Test Loss: 21143252.0\n",
      "Epoch [62/100], Training Loss: 136932.80937148273, Test Loss: 20887082.0\n",
      "Epoch [63/100], Training Loss: 134853.44308690244, Test Loss: 20641382.0\n",
      "Epoch [64/100], Training Loss: 132840.06659854273, Test Loss: 20405082.0\n",
      "Epoch [65/100], Training Loss: 130886.45752621291, Test Loss: 20179044.0\n",
      "Epoch [66/100], Training Loss: 128997.1383211895, Test Loss: 19962380.0\n",
      "Epoch [67/100], Training Loss: 127168.43005449914, Test Loss: 19753812.0\n",
      "Epoch [68/100], Training Loss: 125393.81850897458, Test Loss: 19556172.0\n",
      "Epoch [69/100], Training Loss: 123675.66515609265, Test Loss: 19362588.0\n",
      "Epoch [70/100], Training Loss: 122007.75356910136, Test Loss: 19180206.0\n",
      "Epoch [71/100], Training Loss: 120386.06114862864, Test Loss: 19001038.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [72/100], Training Loss: 118807.9795035839, Test Loss: 18830998.0\n",
      "Epoch [73/100], Training Loss: 117273.28859368521, Test Loss: 18663266.0\n",
      "Epoch [74/100], Training Loss: 115784.08813162727, Test Loss: 18506378.0\n",
      "Epoch [75/100], Training Loss: 114335.42767016172, Test Loss: 18352866.0\n",
      "Epoch [76/100], Training Loss: 112926.57058231147, Test Loss: 18205948.0\n",
      "Epoch [77/100], Training Loss: 111557.34032344056, Test Loss: 18066424.0\n",
      "Epoch [78/100], Training Loss: 110227.2839879154, Test Loss: 17931024.0\n",
      "Epoch [79/100], Training Loss: 108938.11708429595, Test Loss: 17802600.0\n",
      "Epoch [80/100], Training Loss: 107686.3542147977, Test Loss: 17677188.0\n",
      "Epoch [81/100], Training Loss: 106464.9086991292, Test Loss: 17556034.0\n",
      "Epoch [82/100], Training Loss: 105275.27147384634, Test Loss: 17439454.0\n",
      "Epoch [83/100], Training Loss: 104116.1208311119, Test Loss: 17327216.0\n",
      "Epoch [84/100], Training Loss: 102991.90717374563, Test Loss: 17220474.0\n",
      "Epoch [85/100], Training Loss: 101897.3302825662, Test Loss: 17116424.0\n",
      "Epoch [86/100], Training Loss: 100834.30180380309, Test Loss: 17018092.0\n",
      "Epoch [87/100], Training Loss: 99798.95302410994, Test Loss: 16923418.0\n",
      "Epoch [88/100], Training Loss: 98795.18996208755, Test Loss: 16833444.0\n",
      "Epoch [89/100], Training Loss: 97814.5645103963, Test Loss: 16746969.0\n",
      "Epoch [90/100], Training Loss: 96865.47334281144, Test Loss: 16662936.0\n",
      "Epoch [91/100], Training Loss: 95940.5478348439, Test Loss: 16584534.0\n",
      "Epoch [92/100], Training Loss: 95041.7666163142, Test Loss: 16507202.0\n",
      "Epoch [93/100], Training Loss: 94169.95805935668, Test Loss: 16436100.0\n",
      "Epoch [94/100], Training Loss: 93324.29152597595, Test Loss: 16366195.0\n",
      "Epoch [95/100], Training Loss: 92503.93097269119, Test Loss: 16304413.0\n",
      "Epoch [96/100], Training Loss: 91705.74191398614, Test Loss: 16236915.0\n",
      "Epoch [97/100], Training Loss: 90929.08263728452, Test Loss: 16184023.0\n",
      "Epoch [98/100], Training Loss: 90178.59390735146, Test Loss: 16122167.0\n",
      "Epoch [99/100], Training Loss: 89447.7636395948, Test Loss: 16069039.0\n",
      "Epoch [100/100], Training Loss: 88739.20235768023, Test Loss: 16012889.0\n",
      "Epoch [1/100], Training Loss: 1497415.5332030093, Test Loss: 298384896.0\n",
      "Epoch [2/100], Training Loss: 1458834.4444049522, Test Loss: 280949760.0\n",
      "Epoch [3/100], Training Loss: 1300843.6812985013, Test Loss: 234536304.0\n",
      "Epoch [4/100], Training Loss: 1039455.1464960607, Test Loss: 182725552.0\n",
      "Epoch [5/100], Training Loss: 837742.9282625436, Test Loss: 155806912.0\n",
      "Epoch [6/100], Training Loss: 743626.9680706119, Test Loss: 143202800.0\n",
      "Epoch [7/100], Training Loss: 688761.5193412713, Test Loss: 134144072.0\n",
      "Epoch [8/100], Training Loss: 644489.3328594278, Test Loss: 126427248.0\n",
      "Epoch [9/100], Training Loss: 605344.4594514543, Test Loss: 119574272.0\n",
      "Epoch [10/100], Training Loss: 569902.4820804455, Test Loss: 113420896.0\n",
      "Epoch [11/100], Training Loss: 537635.8225223625, Test Loss: 107893184.0\n",
      "Epoch [12/100], Training Loss: 508294.0304484331, Test Loss: 102939712.0\n",
      "Epoch [13/100], Training Loss: 481687.34174515726, Test Loss: 98507632.0\n",
      "Epoch [14/100], Training Loss: 457628.77602037793, Test Loss: 94546536.0\n",
      "Epoch [15/100], Training Loss: 435926.6000829335, Test Loss: 90999456.0\n",
      "Epoch [16/100], Training Loss: 416360.9001836384, Test Loss: 87805656.0\n",
      "Epoch [17/100], Training Loss: 398697.5759729874, Test Loss: 84911768.0\n",
      "Epoch [18/100], Training Loss: 382730.72554943425, Test Loss: 82276192.0\n",
      "Epoch [19/100], Training Loss: 368284.88478170725, Test Loss: 79864944.0\n",
      "Epoch [20/100], Training Loss: 355201.81588768435, Test Loss: 77650832.0\n",
      "Epoch [21/100], Training Loss: 343340.8078905278, Test Loss: 75611496.0\n",
      "Epoch [22/100], Training Loss: 332571.71636751376, Test Loss: 73725808.0\n",
      "Epoch [23/100], Training Loss: 322768.05603933416, Test Loss: 71974480.0\n",
      "Epoch [24/100], Training Loss: 313807.70001777145, Test Loss: 70339616.0\n",
      "Epoch [25/100], Training Loss: 305575.72797820036, Test Loss: 68806752.0\n",
      "Epoch [26/100], Training Loss: 297974.71269474557, Test Loss: 67365064.0\n",
      "Epoch [27/100], Training Loss: 290915.8622711925, Test Loss: 65999580.0\n",
      "Epoch [28/100], Training Loss: 284301.9488922457, Test Loss: 64697632.0\n",
      "Epoch [29/100], Training Loss: 278062.7444834429, Test Loss: 63450096.0\n",
      "Epoch [30/100], Training Loss: 272139.5398534225, Test Loss: 62249436.0\n",
      "Epoch [31/100], Training Loss: 266475.02097800124, Test Loss: 61088216.0\n",
      "Epoch [32/100], Training Loss: 261022.45819356822, Test Loss: 59961488.0\n",
      "Epoch [33/100], Training Loss: 255749.44700773057, Test Loss: 58865576.0\n",
      "Epoch [34/100], Training Loss: 250628.83991617794, Test Loss: 57797268.0\n",
      "Epoch [35/100], Training Loss: 245638.3417451573, Test Loss: 56754232.0\n",
      "Epoch [36/100], Training Loss: 240766.99286179728, Test Loss: 55735648.0\n",
      "Epoch [37/100], Training Loss: 236004.53539482257, Test Loss: 54739772.0\n",
      "Epoch [38/100], Training Loss: 231339.75025176234, Test Loss: 53765304.0\n",
      "Epoch [39/100], Training Loss: 226771.19252413957, Test Loss: 52812368.0\n",
      "Epoch [40/100], Training Loss: 222298.10648065872, Test Loss: 51879652.0\n",
      "Epoch [41/100], Training Loss: 217917.36434452934, Test Loss: 50967176.0\n",
      "Epoch [42/100], Training Loss: 213632.7176115159, Test Loss: 50074892.0\n",
      "Epoch [43/100], Training Loss: 209446.09395178012, Test Loss: 49203244.0\n",
      "Epoch [44/100], Training Loss: 205355.15194597477, Test Loss: 48351088.0\n",
      "Epoch [45/100], Training Loss: 201355.34755050056, Test Loss: 47517804.0\n",
      "Epoch [46/100], Training Loss: 197444.35822818553, Test Loss: 46701796.0\n",
      "Epoch [47/100], Training Loss: 193619.197174338, Test Loss: 45902780.0\n",
      "Epoch [48/100], Training Loss: 189881.57625436882, Test Loss: 45120492.0\n",
      "Epoch [49/100], Training Loss: 186230.08282980867, Test Loss: 44354300.0\n",
      "Epoch [50/100], Training Loss: 182667.10993868846, Test Loss: 43603624.0\n",
      "Epoch [51/100], Training Loss: 179190.91649635686, Test Loss: 42867952.0\n",
      "Epoch [52/100], Training Loss: 175800.02245868137, Test Loss: 42147612.0\n",
      "Epoch [53/100], Training Loss: 172493.21641934718, Test Loss: 41442160.0\n",
      "Epoch [54/100], Training Loss: 169269.74629020793, Test Loss: 40751528.0\n",
      "Epoch [55/100], Training Loss: 166130.81385877615, Test Loss: 40074728.0\n",
      "Epoch [56/100], Training Loss: 163071.8312970203, Test Loss: 39412388.0\n",
      "Epoch [57/100], Training Loss: 160090.2053751259, Test Loss: 38765132.0\n",
      "Epoch [58/100], Training Loss: 157187.1515016883, Test Loss: 38133832.0\n",
      "Epoch [59/100], Training Loss: 154359.8853740892, Test Loss: 37516540.0\n",
      "Epoch [60/100], Training Loss: 151606.44957903857, Test Loss: 36914172.0\n",
      "Epoch [61/100], Training Loss: 148923.48426763076, Test Loss: 36326112.0\n",
      "Epoch [62/100], Training Loss: 146310.03218855517, Test Loss: 35752968.0\n",
      "Epoch [63/100], Training Loss: 143764.46739538905, Test Loss: 35192988.0\n",
      "Epoch [64/100], Training Loss: 141286.8309471447, Test Loss: 34647144.0\n",
      "Epoch [65/100], Training Loss: 138876.43521909803, Test Loss: 34114608.0\n",
      "Epoch [66/100], Training Loss: 136532.61607169302, Test Loss: 33595084.0\n",
      "Epoch [67/100], Training Loss: 134256.06785087523, Test Loss: 33089182.0\n",
      "Epoch [68/100], Training Loss: 132046.4973772057, Test Loss: 32596452.0\n",
      "Epoch [69/100], Training Loss: 129898.83499842416, Test Loss: 32117700.0\n",
      "Epoch [70/100], Training Loss: 127809.21366258721, Test Loss: 31651896.0\n",
      "Epoch [71/100], Training Loss: 125777.18609706368, Test Loss: 31198794.0\n",
      "Epoch [72/100], Training Loss: 123801.68394337251, Test Loss: 30758688.0\n",
      "Epoch [73/100], Training Loss: 121882.05721432148, Test Loss: 30330224.0\n",
      "Epoch [74/100], Training Loss: 120016.34746106477, Test Loss: 29913276.0\n",
      "Epoch [75/100], Training Loss: 118199.80469209906, Test Loss: 29506204.0\n",
      "Epoch [76/100], Training Loss: 116429.56886642851, Test Loss: 29110250.0\n",
      "Epoch [77/100], Training Loss: 114705.46804365392, Test Loss: 28725772.0\n",
      "Epoch [78/100], Training Loss: 113025.860630225, Test Loss: 28351718.0\n",
      "Epoch [79/100], Training Loss: 111390.13685342434, Test Loss: 27988736.0\n",
      "Epoch [80/100], Training Loss: 109798.33781461036, Test Loss: 27637006.0\n",
      "Epoch [81/100], Training Loss: 108250.35864250582, Test Loss: 27296052.0\n",
      "Epoch [82/100], Training Loss: 106743.37843604681, Test Loss: 26966006.0\n",
      "Epoch [83/100], Training Loss: 105277.28203803, Test Loss: 26646544.0\n",
      "Epoch [84/100], Training Loss: 103850.27497723032, Test Loss: 26336774.0\n",
      "Epoch [85/100], Training Loss: 102460.26959303359, Test Loss: 26036996.0\n",
      "Epoch [86/100], Training Loss: 101106.60420762433, Test Loss: 25746690.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [87/100], Training Loss: 99786.52629805698, Test Loss: 25464864.0\n",
      "Epoch [88/100], Training Loss: 98500.39809271519, Test Loss: 25191738.0\n",
      "Epoch [89/100], Training Loss: 97245.70658349253, Test Loss: 24926874.0\n",
      "Epoch [90/100], Training Loss: 96022.33982616366, Test Loss: 24670206.0\n",
      "Epoch [91/100], Training Loss: 94829.34902752169, Test Loss: 24420114.0\n",
      "Epoch [92/100], Training Loss: 93664.95711108272, Test Loss: 24176994.0\n",
      "Epoch [93/100], Training Loss: 92528.76910802085, Test Loss: 23939790.0\n",
      "Epoch [94/100], Training Loss: 91417.8993994265, Test Loss: 23710486.0\n",
      "Epoch [95/100], Training Loss: 90331.93592926218, Test Loss: 23484332.0\n",
      "Epoch [96/100], Training Loss: 89272.19765657396, Test Loss: 23269282.0\n",
      "Epoch [97/100], Training Loss: 88236.33627997083, Test Loss: 23055510.0\n",
      "Epoch [98/100], Training Loss: 87222.36480339398, Test Loss: 22850224.0\n",
      "Epoch [99/100], Training Loss: 86230.2615160443, Test Loss: 22648298.0\n",
      "Epoch [100/100], Training Loss: 85259.36997771163, Test Loss: 22452836.0\n",
      "Epoch [1/100], Training Loss: 1164478.6853859369, Test Loss: 299412928.0\n",
      "Epoch [2/100], Training Loss: 1151171.6196907766, Test Loss: 291354016.0\n",
      "Epoch [3/100], Training Loss: 1088312.1511758782, Test Loss: 265617056.0\n",
      "Epoch [4/100], Training Loss: 952167.1412830993, Test Loss: 222903632.0\n",
      "Epoch [5/100], Training Loss: 781110.1555595048, Test Loss: 181642576.0\n",
      "Epoch [6/100], Training Loss: 652480.8599016645, Test Loss: 157806768.0\n",
      "Epoch [7/100], Training Loss: 585851.4891297909, Test Loss: 145963488.0\n",
      "Epoch [8/100], Training Loss: 547809.5060719152, Test Loss: 137928912.0\n",
      "Epoch [9/100], Training Loss: 518448.0132693561, Test Loss: 131153856.0\n",
      "Epoch [10/100], Training Loss: 492550.75741958414, Test Loss: 125019760.0\n",
      "Epoch [11/100], Training Loss: 468668.4710621409, Test Loss: 119314520.0\n",
      "Epoch [12/100], Training Loss: 446248.6499614952, Test Loss: 113952104.0\n",
      "Epoch [13/100], Training Loss: 425076.5724779338, Test Loss: 108903136.0\n",
      "Epoch [14/100], Training Loss: 405100.65872874827, Test Loss: 104165560.0\n",
      "Epoch [15/100], Training Loss: 386331.55310704344, Test Loss: 99739336.0\n",
      "Epoch [16/100], Training Loss: 368784.74474261, Test Loss: 95628232.0\n",
      "Epoch [17/100], Training Loss: 352481.198981103, Test Loss: 91831744.0\n",
      "Epoch [18/100], Training Loss: 337414.96972928144, Test Loss: 88335592.0\n",
      "Epoch [19/100], Training Loss: 323535.0396303536, Test Loss: 85116648.0\n",
      "Epoch [20/100], Training Loss: 310763.58023813757, Test Loss: 82148936.0\n",
      "Epoch [21/100], Training Loss: 299008.38433742075, Test Loss: 79406112.0\n",
      "Epoch [22/100], Training Loss: 288168.2928736449, Test Loss: 76862424.0\n",
      "Epoch [23/100], Training Loss: 278148.2344647829, Test Loss: 74496056.0\n",
      "Epoch [24/100], Training Loss: 268865.3486167881, Test Loss: 72288472.0\n",
      "Epoch [25/100], Training Loss: 260244.21444227238, Test Loss: 70222232.0\n",
      "Epoch [26/100], Training Loss: 252217.94668562288, Test Loss: 68285328.0\n",
      "Epoch [27/100], Training Loss: 244728.04881227415, Test Loss: 66467632.0\n",
      "Epoch [28/100], Training Loss: 237721.25573129553, Test Loss: 64759184.0\n",
      "Epoch [29/100], Training Loss: 231155.5499081808, Test Loss: 63153964.0\n",
      "Epoch [30/100], Training Loss: 224993.03477282153, Test Loss: 61643536.0\n",
      "Epoch [31/100], Training Loss: 219200.92530063385, Test Loss: 60221592.0\n",
      "Epoch [32/100], Training Loss: 213741.40382678751, Test Loss: 58877448.0\n",
      "Epoch [33/100], Training Loss: 208573.18002488004, Test Loss: 57603124.0\n",
      "Epoch [34/100], Training Loss: 203671.28902316213, Test Loss: 56393060.0\n",
      "Epoch [35/100], Training Loss: 199006.06859783188, Test Loss: 55238348.0\n",
      "Epoch [36/100], Training Loss: 194551.92950654583, Test Loss: 54133792.0\n",
      "Epoch [37/100], Training Loss: 190287.29542088739, Test Loss: 53074416.0\n",
      "Epoch [38/100], Training Loss: 186192.2531840531, Test Loss: 52055352.0\n",
      "Epoch [39/100], Training Loss: 182249.47586043482, Test Loss: 51071220.0\n",
      "Epoch [40/100], Training Loss: 178442.0640957289, Test Loss: 50119576.0\n",
      "Epoch [41/100], Training Loss: 174763.72738581838, Test Loss: 49198344.0\n",
      "Epoch [42/100], Training Loss: 171209.54765712933, Test Loss: 48305792.0\n",
      "Epoch [43/100], Training Loss: 167770.12665126473, Test Loss: 47439488.0\n",
      "Epoch [44/100], Training Loss: 164438.81310348914, Test Loss: 46597408.0\n",
      "Epoch [45/100], Training Loss: 161210.5442805521, Test Loss: 45778576.0\n",
      "Epoch [46/100], Training Loss: 158081.29080030805, Test Loss: 44981744.0\n",
      "Epoch [47/100], Training Loss: 155046.42023576802, Test Loss: 44206600.0\n",
      "Epoch [48/100], Training Loss: 152101.33712457793, Test Loss: 43452128.0\n",
      "Epoch [49/100], Training Loss: 149242.4243824418, Test Loss: 42717224.0\n",
      "Epoch [50/100], Training Loss: 146465.31508796872, Test Loss: 42000548.0\n",
      "Epoch [51/100], Training Loss: 143766.37130501747, Test Loss: 41301084.0\n",
      "Epoch [52/100], Training Loss: 141142.15615188674, Test Loss: 40618672.0\n",
      "Epoch [53/100], Training Loss: 138591.88128665363, Test Loss: 39953816.0\n",
      "Epoch [54/100], Training Loss: 136115.21355369943, Test Loss: 39305048.0\n",
      "Epoch [55/100], Training Loss: 133709.11746934423, Test Loss: 38672808.0\n",
      "Epoch [56/100], Training Loss: 131368.76660150464, Test Loss: 38055936.0\n",
      "Epoch [57/100], Training Loss: 129091.48237663646, Test Loss: 37454176.0\n",
      "Epoch [58/100], Training Loss: 126878.24595699307, Test Loss: 36868668.0\n",
      "Epoch [59/100], Training Loss: 124725.82015283455, Test Loss: 36299288.0\n",
      "Epoch [60/100], Training Loss: 122632.57597298738, Test Loss: 35746052.0\n",
      "Epoch [61/100], Training Loss: 120599.47686748415, Test Loss: 35208528.0\n",
      "Epoch [62/100], Training Loss: 118625.34447011433, Test Loss: 34685524.0\n",
      "Epoch [63/100], Training Loss: 116706.71559741722, Test Loss: 34176708.0\n",
      "Epoch [64/100], Training Loss: 114841.70345358687, Test Loss: 33681304.0\n",
      "Epoch [65/100], Training Loss: 113028.25590901013, Test Loss: 33199260.0\n",
      "Epoch [66/100], Training Loss: 111265.73870031396, Test Loss: 32730474.0\n",
      "Epoch [67/100], Training Loss: 109550.88815828446, Test Loss: 32274098.0\n",
      "Epoch [68/100], Training Loss: 107883.97316509686, Test Loss: 31830304.0\n",
      "Epoch [69/100], Training Loss: 106265.35566613352, Test Loss: 31398908.0\n",
      "Epoch [70/100], Training Loss: 104694.14045376459, Test Loss: 30979792.0\n",
      "Epoch [71/100], Training Loss: 103167.77264380072, Test Loss: 30572680.0\n",
      "Epoch [72/100], Training Loss: 101686.06326639417, Test Loss: 30177846.0\n",
      "Epoch [73/100], Training Loss: 100248.5092707778, Test Loss: 29794402.0\n",
      "Epoch [74/100], Training Loss: 98852.70220958474, Test Loss: 29421518.0\n",
      "Epoch [75/100], Training Loss: 97495.51940050945, Test Loss: 29059564.0\n",
      "Epoch [76/100], Training Loss: 96177.00580534329, Test Loss: 28708176.0\n",
      "Epoch [77/100], Training Loss: 94895.95160239322, Test Loss: 28366658.0\n",
      "Epoch [78/100], Training Loss: 93648.79402878977, Test Loss: 28034722.0\n",
      "Epoch [79/100], Training Loss: 92435.19483442925, Test Loss: 27712272.0\n",
      "Epoch [80/100], Training Loss: 91254.889224572, Test Loss: 27399238.0\n",
      "Epoch [81/100], Training Loss: 90106.63047212843, Test Loss: 27094742.0\n",
      "Epoch [82/100], Training Loss: 88988.9186067176, Test Loss: 26798976.0\n",
      "Epoch [83/100], Training Loss: 87900.81067472306, Test Loss: 26511718.0\n",
      "Epoch [84/100], Training Loss: 86841.58391090576, Test Loss: 26232752.0\n",
      "Epoch [85/100], Training Loss: 85810.3411527753, Test Loss: 25961346.0\n",
      "Epoch [86/100], Training Loss: 84804.47183223742, Test Loss: 25697836.0\n",
      "Epoch [87/100], Training Loss: 83824.0773058468, Test Loss: 25441884.0\n",
      "Epoch [88/100], Training Loss: 82867.40098335406, Test Loss: 25193356.0\n",
      "Epoch [89/100], Training Loss: 81932.87731769445, Test Loss: 24951358.0\n",
      "Epoch [90/100], Training Loss: 81019.6180913453, Test Loss: 24716558.0\n",
      "Epoch [91/100], Training Loss: 80126.67282743914, Test Loss: 24487656.0\n",
      "Epoch [92/100], Training Loss: 79253.166163142, Test Loss: 24265372.0\n",
      "Epoch [93/100], Training Loss: 78398.77548723418, Test Loss: 24049064.0\n",
      "Epoch [94/100], Training Loss: 77564.38244179847, Test Loss: 23838710.0\n",
      "Epoch [95/100], Training Loss: 76748.61524791185, Test Loss: 23633718.0\n",
      "Epoch [96/100], Training Loss: 75950.19797405366, Test Loss: 23433880.0\n",
      "Epoch [97/100], Training Loss: 75168.59824654937, Test Loss: 23238648.0\n",
      "Epoch [98/100], Training Loss: 74403.0313962443, Test Loss: 23048864.0\n",
      "Epoch [99/100], Training Loss: 73651.96984775783, Test Loss: 22862984.0\n",
      "Epoch [100/100], Training Loss: 72914.8450921154, Test Loss: 22682468.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Training Loss: 582429.7887565902, Test Loss: 300042112.0\n",
      "Epoch [2/100], Training Loss: 581968.7497186186, Test Loss: 299486336.0\n",
      "Epoch [3/100], Training Loss: 579652.2841063918, Test Loss: 297409472.0\n",
      "Epoch [4/100], Training Loss: 573152.1725016291, Test Loss: 292511456.0\n",
      "Epoch [5/100], Training Loss: 559997.015342693, Test Loss: 283629024.0\n",
      "Epoch [6/100], Training Loss: 538355.3448255435, Test Loss: 270125952.0\n",
      "Epoch [7/100], Training Loss: 507778.09087139386, Test Loss: 252251392.0\n",
      "Epoch [8/100], Training Loss: 469784.5502043718, Test Loss: 231349152.0\n",
      "Epoch [9/100], Training Loss: 427964.620105444, Test Loss: 209712560.0\n",
      "Epoch [10/100], Training Loss: 387222.5659617321, Test Loss: 189947440.0\n",
      "Epoch [11/100], Training Loss: 352143.60618446773, Test Loss: 173975136.0\n",
      "Epoch [12/100], Training Loss: 325109.89443753334, Test Loss: 162248016.0\n",
      "Epoch [13/100], Training Loss: 305630.29962679936, Test Loss: 153924384.0\n",
      "Epoch [14/100], Training Loss: 291541.14140157576, Test Loss: 147777600.0\n",
      "Epoch [15/100], Training Loss: 280682.82068597834, Test Loss: 142866080.0\n",
      "Epoch [16/100], Training Loss: 271627.1457852023, Test Loss: 138643536.0\n",
      "Epoch [17/100], Training Loss: 263602.0970321663, Test Loss: 134831680.0\n",
      "Epoch [18/100], Training Loss: 256221.20964397845, Test Loss: 131291184.0\n",
      "Epoch [19/100], Training Loss: 249286.10911675848, Test Loss: 127947808.0\n",
      "Epoch [20/100], Training Loss: 242686.06930869023, Test Loss: 124758296.0\n",
      "Epoch [21/100], Training Loss: 236353.9979859013, Test Loss: 121695728.0\n",
      "Epoch [22/100], Training Loss: 230247.36164919139, Test Loss: 118742848.0\n",
      "Epoch [23/100], Training Loss: 224339.0348912979, Test Loss: 115888728.0\n",
      "Epoch [24/100], Training Loss: 218612.1864818435, Test Loss: 113126720.0\n",
      "Epoch [25/100], Training Loss: 213056.93596350928, Test Loss: 110453032.0\n",
      "Epoch [26/100], Training Loss: 207668.4521059179, Test Loss: 107866184.0\n",
      "Epoch [27/100], Training Loss: 202446.08020851846, Test Loss: 105365944.0\n",
      "Epoch [28/100], Training Loss: 197390.5372904449, Test Loss: 102952488.0\n",
      "Epoch [29/100], Training Loss: 192502.85812451868, Test Loss: 100626096.0\n",
      "Epoch [30/100], Training Loss: 187783.64551863042, Test Loss: 98386040.0\n",
      "Epoch [31/100], Training Loss: 183234.02310289675, Test Loss: 96231552.0\n",
      "Epoch [32/100], Training Loss: 178855.6903027072, Test Loss: 94162824.0\n",
      "Epoch [33/100], Training Loss: 174649.2878383982, Test Loss: 92179424.0\n",
      "Epoch [34/100], Training Loss: 170614.51643859962, Test Loss: 90280200.0\n",
      "Epoch [35/100], Training Loss: 166750.44416799952, Test Loss: 88463776.0\n",
      "Epoch [36/100], Training Loss: 163054.686333748, Test Loss: 86727912.0\n",
      "Epoch [37/100], Training Loss: 159523.398850779, Test Loss: 85069560.0\n",
      "Epoch [38/100], Training Loss: 156150.99721580476, Test Loss: 83485144.0\n",
      "Epoch [39/100], Training Loss: 152931.33345180974, Test Loss: 81971064.0\n",
      "Epoch [40/100], Training Loss: 149857.88140513003, Test Loss: 80522888.0\n",
      "Epoch [41/100], Training Loss: 146922.60648065872, Test Loss: 79136408.0\n",
      "Epoch [42/100], Training Loss: 144117.18405307742, Test Loss: 77807256.0\n",
      "Epoch [43/100], Training Loss: 141433.25158462176, Test Loss: 76531232.0\n",
      "Epoch [44/100], Training Loss: 138862.47331319234, Test Loss: 75304600.0\n",
      "Epoch [45/100], Training Loss: 136397.37361530715, Test Loss: 74123936.0\n",
      "Epoch [46/100], Training Loss: 134030.81855340322, Test Loss: 72986000.0\n",
      "Epoch [47/100], Training Loss: 131756.36016823648, Test Loss: 71887880.0\n",
      "Epoch [48/100], Training Loss: 129567.81517682601, Test Loss: 70827120.0\n",
      "Epoch [49/100], Training Loss: 127459.63983176352, Test Loss: 69801080.0\n",
      "Epoch [50/100], Training Loss: 125426.61690658136, Test Loss: 68807768.0\n",
      "Epoch [51/100], Training Loss: 123464.269415319, Test Loss: 67845520.0\n",
      "Epoch [52/100], Training Loss: 121567.99431313311, Test Loss: 66912612.0\n",
      "Epoch [53/100], Training Loss: 119733.98732302588, Test Loss: 66008088.0\n",
      "Epoch [54/100], Training Loss: 117959.3358213376, Test Loss: 65130572.0\n",
      "Epoch [55/100], Training Loss: 116240.98027368047, Test Loss: 64278676.0\n",
      "Epoch [56/100], Training Loss: 114575.17445648955, Test Loss: 63450524.0\n",
      "Epoch [57/100], Training Loss: 112958.13423375392, Test Loss: 62645116.0\n",
      "Epoch [58/100], Training Loss: 111387.74243232036, Test Loss: 61862380.0\n",
      "Epoch [59/100], Training Loss: 109861.60286712872, Test Loss: 61100916.0\n",
      "Epoch [60/100], Training Loss: 108376.65967655944, Test Loss: 60359052.0\n",
      "Epoch [61/100], Training Loss: 106929.18962146793, Test Loss: 59635300.0\n",
      "Epoch [62/100], Training Loss: 105517.65215330846, Test Loss: 58928920.0\n",
      "Epoch [63/100], Training Loss: 104140.62519992891, Test Loss: 58239308.0\n",
      "Epoch [64/100], Training Loss: 102796.79734612879, Test Loss: 57566400.0\n",
      "Epoch [65/100], Training Loss: 101483.50050352467, Test Loss: 56908236.0\n",
      "Epoch [66/100], Training Loss: 100197.72252828623, Test Loss: 56263444.0\n",
      "Epoch [67/100], Training Loss: 98938.23008115633, Test Loss: 55631468.0\n",
      "Epoch [68/100], Training Loss: 97703.7453942302, Test Loss: 55012176.0\n",
      "Epoch [69/100], Training Loss: 96493.34849831171, Test Loss: 54405052.0\n",
      "Epoch [70/100], Training Loss: 95306.3634855755, Test Loss: 53809708.0\n",
      "Epoch [71/100], Training Loss: 94141.53249215095, Test Loss: 53225100.0\n",
      "Epoch [72/100], Training Loss: 92997.8173094011, Test Loss: 52650208.0\n",
      "Epoch [73/100], Training Loss: 91875.00894496772, Test Loss: 52085072.0\n",
      "Epoch [74/100], Training Loss: 90772.51395059534, Test Loss: 51529692.0\n",
      "Epoch [75/100], Training Loss: 89689.45797049938, Test Loss: 50983852.0\n",
      "Epoch [76/100], Training Loss: 88625.56495468278, Test Loss: 50447024.0\n",
      "Epoch [77/100], Training Loss: 87580.60778389906, Test Loss: 49919332.0\n",
      "Epoch [78/100], Training Loss: 86554.19678928974, Test Loss: 49400228.0\n",
      "Epoch [79/100], Training Loss: 85545.94988448551, Test Loss: 48889588.0\n",
      "Epoch [80/100], Training Loss: 84555.29743498607, Test Loss: 48386824.0\n",
      "Epoch [81/100], Training Loss: 83581.75712339317, Test Loss: 47891824.0\n",
      "Epoch [82/100], Training Loss: 82625.01439488181, Test Loss: 47404276.0\n",
      "Epoch [83/100], Training Loss: 81684.72389076477, Test Loss: 46924272.0\n",
      "Epoch [84/100], Training Loss: 80760.4741425271, Test Loss: 46451376.0\n",
      "Epoch [85/100], Training Loss: 79851.99111427048, Test Loss: 45985892.0\n",
      "Epoch [86/100], Training Loss: 78958.96724127718, Test Loss: 45527716.0\n",
      "Epoch [87/100], Training Loss: 78080.94307209288, Test Loss: 45076404.0\n",
      "Epoch [88/100], Training Loss: 77217.66471180617, Test Loss: 44631884.0\n",
      "Epoch [89/100], Training Loss: 76368.79876784551, Test Loss: 44193788.0\n",
      "Epoch [90/100], Training Loss: 75534.22640838813, Test Loss: 43762196.0\n",
      "Epoch [91/100], Training Loss: 74713.56305906049, Test Loss: 43337144.0\n",
      "Epoch [92/100], Training Loss: 73906.18340145727, Test Loss: 42918248.0\n",
      "Epoch [93/100], Training Loss: 73111.8117410106, Test Loss: 42505472.0\n",
      "Epoch [94/100], Training Loss: 72330.23695278716, Test Loss: 42098364.0\n",
      "Epoch [95/100], Training Loss: 71561.00053314377, Test Loss: 41696748.0\n",
      "Epoch [96/100], Training Loss: 70803.77596113975, Test Loss: 41300400.0\n",
      "Epoch [97/100], Training Loss: 70058.50873763402, Test Loss: 40909372.0\n",
      "Epoch [98/100], Training Loss: 69325.15336769148, Test Loss: 40523708.0\n",
      "Epoch [99/100], Training Loss: 68603.31686511463, Test Loss: 40143464.0\n",
      "Epoch [100/100], Training Loss: 67893.25383567324, Test Loss: 39768616.0\n",
      "Epoch [1/100], Training Loss: 7081469.714590368, Test Loss: 147221248.0\n",
      "Epoch [2/100], Training Loss: 4024458.763343404, Test Loss: 115975424.0\n",
      "Epoch [3/100], Training Loss: 3203946.261003495, Test Loss: 94456056.0\n",
      "Epoch [4/100], Training Loss: 2611858.4670339436, Test Loss: 78883936.0\n",
      "Epoch [5/100], Training Loss: 2178634.57194479, Test Loss: 67451880.0\n",
      "Epoch [6/100], Training Loss: 1867798.735945738, Test Loss: 59215888.0\n",
      "Epoch [7/100], Training Loss: 1647031.9596884071, Test Loss: 53296548.0\n",
      "Epoch [8/100], Training Loss: 1484273.9605473608, Test Loss: 48725704.0\n",
      "Epoch [9/100], Training Loss: 1354092.452757538, Test Loss: 44927452.0\n",
      "Epoch [10/100], Training Loss: 1243739.0898051064, Test Loss: 41616868.0\n",
      "Epoch [11/100], Training Loss: 1147507.2460754695, Test Loss: 38663228.0\n",
      "Epoch [12/100], Training Loss: 1062893.3656625792, Test Loss: 36014772.0\n",
      "Epoch [13/100], Training Loss: 988507.6476067768, Test Loss: 33654248.0\n",
      "Epoch [14/100], Training Loss: 923049.8715419703, Test Loss: 31570884.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/100], Training Loss: 865616.1510870209, Test Loss: 29737030.0\n",
      "Epoch [16/100], Training Loss: 815178.6901694213, Test Loss: 28123666.0\n",
      "Epoch [17/100], Training Loss: 770911.9449825247, Test Loss: 26698592.0\n",
      "Epoch [18/100], Training Loss: 732042.4392660387, Test Loss: 25438708.0\n",
      "Epoch [19/100], Training Loss: 697654.927166637, Test Loss: 24320878.0\n",
      "Epoch [20/100], Training Loss: 666797.1818612642, Test Loss: 23319190.0\n",
      "Epoch [21/100], Training Loss: 638794.3804128902, Test Loss: 22428224.0\n",
      "Epoch [22/100], Training Loss: 613309.5205556543, Test Loss: 21636352.0\n",
      "Epoch [23/100], Training Loss: 590003.5982095255, Test Loss: 20930452.0\n",
      "Epoch [24/100], Training Loss: 568678.9584666193, Test Loss: 20305618.0\n",
      "Epoch [25/100], Training Loss: 549070.3203527635, Test Loss: 19751208.0\n",
      "Epoch [26/100], Training Loss: 531040.7124207689, Test Loss: 19251314.0\n",
      "Epoch [27/100], Training Loss: 514460.1674960014, Test Loss: 18803378.0\n",
      "Epoch [28/100], Training Loss: 499113.01030744624, Test Loss: 18400734.0\n",
      "Epoch [29/100], Training Loss: 484866.70689680707, Test Loss: 18033346.0\n",
      "Epoch [30/100], Training Loss: 471659.4100986316, Test Loss: 17697660.0\n",
      "Epoch [31/100], Training Loss: 459326.3298753036, Test Loss: 17388322.0\n",
      "Epoch [32/100], Training Loss: 447799.37586635863, Test Loss: 17109962.0\n",
      "Epoch [33/100], Training Loss: 437072.81028227, Test Loss: 16863430.0\n",
      "Epoch [34/100], Training Loss: 427008.69807031576, Test Loss: 16640128.0\n",
      "Epoch [35/100], Training Loss: 417604.5697085481, Test Loss: 16436436.0\n",
      "Epoch [36/100], Training Loss: 408771.75467241276, Test Loss: 16248173.0\n",
      "Epoch [37/100], Training Loss: 400564.3570138025, Test Loss: 16102960.0\n",
      "Epoch [38/100], Training Loss: 392790.01820093597, Test Loss: 15949206.0\n",
      "Epoch [39/100], Training Loss: 385567.5205556543, Test Loss: 15830882.0\n",
      "Epoch [40/100], Training Loss: 378750.1409795036, Test Loss: 15707226.0\n",
      "Epoch [41/100], Training Loss: 372338.47240240505, Test Loss: 15621725.0\n",
      "Epoch [42/100], Training Loss: 366243.3060319294, Test Loss: 15512096.0\n",
      "Epoch [43/100], Training Loss: 360524.15781055624, Test Loss: 15456235.0\n",
      "Epoch [44/100], Training Loss: 355058.9914326758, Test Loss: 15368171.0\n",
      "Epoch [45/100], Training Loss: 349916.92252384336, Test Loss: 15325338.0\n",
      "Epoch [46/100], Training Loss: 345069.23945560097, Test Loss: 15243936.0\n",
      "Epoch [47/100], Training Loss: 340503.4408950892, Test Loss: 15189292.0\n",
      "Epoch [48/100], Training Loss: 336165.1558371838, Test Loss: 15125906.0\n",
      "Epoch [49/100], Training Loss: 332094.7982939399, Test Loss: 15057685.0\n",
      "Epoch [50/100], Training Loss: 328202.07986049406, Test Loss: 15017359.0\n",
      "Epoch [51/100], Training Loss: 324569.41539304546, Test Loss: 14973464.0\n",
      "Epoch [52/100], Training Loss: 321088.0836665482, Test Loss: 14918730.0\n",
      "Epoch [53/100], Training Loss: 317839.8765698122, Test Loss: 14880796.0\n",
      "Epoch [54/100], Training Loss: 314683.77365085005, Test Loss: 14840253.0\n",
      "Epoch [55/100], Training Loss: 311754.50811933534, Test Loss: 14810195.0\n",
      "Epoch [56/100], Training Loss: 308885.1793732599, Test Loss: 14770267.0\n",
      "Epoch [57/100], Training Loss: 306266.0100593863, Test Loss: 14742452.0\n",
      "Epoch [58/100], Training Loss: 303695.3315117588, Test Loss: 14713465.0\n",
      "Epoch [59/100], Training Loss: 301298.3140068716, Test Loss: 14689712.0\n",
      "Epoch [60/100], Training Loss: 298934.2217063563, Test Loss: 14658063.0\n",
      "Epoch [61/100], Training Loss: 296759.0051833422, Test Loss: 14631162.0\n",
      "Epoch [62/100], Training Loss: 294617.31142630766, Test Loss: 14613143.0\n",
      "Epoch [63/100], Training Loss: 292629.15504117054, Test Loss: 14595697.0\n",
      "Epoch [64/100], Training Loss: 290668.65035394824, Test Loss: 14565611.0\n",
      "Epoch [65/100], Training Loss: 288825.75135507376, Test Loss: 14551906.0\n",
      "Epoch [66/100], Training Loss: 286978.42725549435, Test Loss: 14520203.0\n",
      "Epoch [67/100], Training Loss: 285283.6456741307, Test Loss: 14558286.0\n",
      "Epoch [68/100], Training Loss: 283587.550148836, Test Loss: 14443704.0\n",
      "Epoch [69/100], Training Loss: 281933.65530404006, Test Loss: 14500912.0\n",
      "Epoch [70/100], Training Loss: 280287.0938407085, Test Loss: 14447072.0\n",
      "Epoch [71/100], Training Loss: 278751.2059823174, Test Loss: 14444474.0\n",
      "Epoch [72/100], Training Loss: 277228.49169554526, Test Loss: 14443842.0\n",
      "Epoch [73/100], Training Loss: 275783.9365003554, Test Loss: 14448694.0\n",
      "Epoch [74/100], Training Loss: 274338.2060378532, Test Loss: 14389724.0\n",
      "Epoch [75/100], Training Loss: 272960.9955497305, Test Loss: 14467078.0\n",
      "Epoch [76/100], Training Loss: 271599.3432483265, Test Loss: 14383261.0\n",
      "Epoch [77/100], Training Loss: 270328.28127036314, Test Loss: 14434490.0\n",
      "Epoch [78/100], Training Loss: 269027.94203542446, Test Loss: 14384497.0\n",
      "Epoch [79/100], Training Loss: 267815.3210673242, Test Loss: 14374226.0\n",
      "Epoch [80/100], Training Loss: 266565.190817339, Test Loss: 14393002.0\n",
      "Epoch [81/100], Training Loss: 265426.4070441621, Test Loss: 14392162.0\n",
      "Epoch [82/100], Training Loss: 264247.3345625259, Test Loss: 14360506.0\n",
      "Epoch [83/100], Training Loss: 263215.9033047509, Test Loss: 14398838.0\n",
      "Epoch [84/100], Training Loss: 262090.11595506783, Test Loss: 14349199.0\n",
      "Epoch [85/100], Training Loss: 261084.4529278479, Test Loss: 14356475.0\n",
      "Epoch [86/100], Training Loss: 260029.17270526034, Test Loss: 14377793.0\n",
      "Epoch [87/100], Training Loss: 259060.78502828625, Test Loss: 14390801.0\n",
      "Epoch [88/100], Training Loss: 258020.9971454594, Test Loss: 14328529.0\n",
      "Epoch [89/100], Training Loss: 257118.6111789882, Test Loss: 14384566.0\n",
      "Epoch [90/100], Training Loss: 256148.93314599255, Test Loss: 14331612.0\n",
      "Epoch [91/100], Training Loss: 255265.89169406434, Test Loss: 14318036.0\n",
      "Epoch [92/100], Training Loss: 254317.2392482673, Test Loss: 14320724.0\n",
      "Epoch [93/100], Training Loss: 253462.88777323617, Test Loss: 14352108.0\n",
      "Epoch [94/100], Training Loss: 252561.29510618447, Test Loss: 14279605.0\n",
      "Epoch [95/100], Training Loss: 251765.28082237428, Test Loss: 14360554.0\n",
      "Epoch [96/100], Training Loss: 250916.66400835259, Test Loss: 14298916.0\n",
      "Epoch [97/100], Training Loss: 250131.30470647474, Test Loss: 14280438.0\n",
      "Epoch [98/100], Training Loss: 249300.10241913988, Test Loss: 14276761.0\n",
      "Epoch [99/100], Training Loss: 248543.66313458918, Test Loss: 14326646.0\n",
      "Epoch [100/100], Training Loss: 247741.0544250933, Test Loss: 14238495.0\n",
      "Epoch [1/100], Training Loss: 4399491.315917304, Test Loss: 232975072.0\n",
      "Epoch [2/100], Training Loss: 2698356.3804277, Test Loss: 145545840.0\n",
      "Epoch [3/100], Training Loss: 2085304.2304365856, Test Loss: 125890624.0\n",
      "Epoch [4/100], Training Loss: 1809449.3149694924, Test Loss: 110323448.0\n",
      "Epoch [5/100], Training Loss: 1576987.5146022155, Test Loss: 96966424.0\n",
      "Epoch [6/100], Training Loss: 1382870.2978496535, Test Loss: 86382888.0\n",
      "Epoch [7/100], Training Loss: 1227294.0414667379, Test Loss: 77762056.0\n",
      "Epoch [8/100], Training Loss: 1101492.7806409572, Test Loss: 70752824.0\n",
      "Epoch [9/100], Training Loss: 1000812.3428114448, Test Loss: 65072880.0\n",
      "Epoch [10/100], Training Loss: 920250.8089568154, Test Loss: 60490000.0\n",
      "Epoch [11/100], Training Loss: 855539.5092707778, Test Loss: 56769296.0\n",
      "Epoch [12/100], Training Loss: 802753.3082163379, Test Loss: 53674100.0\n",
      "Epoch [13/100], Training Loss: 758205.8689651086, Test Loss: 51008132.0\n",
      "Epoch [14/100], Training Loss: 719288.923523488, Test Loss: 48646108.0\n",
      "Epoch [15/100], Training Loss: 684414.0517149458, Test Loss: 46518992.0\n",
      "Epoch [16/100], Training Loss: 652730.4819027309, Test Loss: 44576444.0\n",
      "Epoch [17/100], Training Loss: 623709.1060363722, Test Loss: 42777100.0\n",
      "Epoch [18/100], Training Loss: 596890.937977608, Test Loss: 41104916.0\n",
      "Epoch [19/100], Training Loss: 572211.2076298797, Test Loss: 39555100.0\n",
      "Epoch [20/100], Training Loss: 549607.230555062, Test Loss: 38130932.0\n",
      "Epoch [21/100], Training Loss: 528714.3502458385, Test Loss: 36802200.0\n",
      "Epoch [22/100], Training Loss: 509242.5866358628, Test Loss: 35564408.0\n",
      "Epoch [23/100], Training Loss: 491234.85184526985, Test Loss: 34425080.0\n",
      "Epoch [24/100], Training Loss: 474713.75330252945, Test Loss: 33375540.0\n",
      "Epoch [25/100], Training Loss: 459556.44781114865, Test Loss: 32404890.0\n",
      "Epoch [26/100], Training Loss: 445344.2769681891, Test Loss: 31479588.0\n",
      "Epoch [27/100], Training Loss: 431632.05144837394, Test Loss: 30586732.0\n",
      "Epoch [28/100], Training Loss: 418463.74483146734, Test Loss: 29742098.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/100], Training Loss: 406278.39769563416, Test Loss: 28969632.0\n",
      "Epoch [30/100], Training Loss: 395214.7228540963, Test Loss: 28264764.0\n",
      "Epoch [31/100], Training Loss: 385000.74871156923, Test Loss: 27602074.0\n",
      "Epoch [32/100], Training Loss: 375323.5758248919, Test Loss: 26971070.0\n",
      "Epoch [33/100], Training Loss: 366153.2300811563, Test Loss: 26382002.0\n",
      "Epoch [34/100], Training Loss: 357591.4237604407, Test Loss: 25835508.0\n",
      "Epoch [35/100], Training Loss: 349486.8965997275, Test Loss: 25318330.0\n",
      "Epoch [36/100], Training Loss: 341777.2206326639, Test Loss: 24826008.0\n",
      "Epoch [37/100], Training Loss: 334368.0760026065, Test Loss: 24345066.0\n",
      "Epoch [38/100], Training Loss: 327093.27243646706, Test Loss: 23874008.0\n",
      "Epoch [39/100], Training Loss: 319914.15588531486, Test Loss: 23413260.0\n",
      "Epoch [40/100], Training Loss: 312854.45441620756, Test Loss: 22964774.0\n",
      "Epoch [41/100], Training Loss: 306078.6307979385, Test Loss: 22545270.0\n",
      "Epoch [42/100], Training Loss: 299670.84230792016, Test Loss: 22149880.0\n",
      "Epoch [43/100], Training Loss: 293575.5585865766, Test Loss: 21779746.0\n",
      "Epoch [44/100], Training Loss: 287690.5186304129, Test Loss: 21425842.0\n",
      "Epoch [45/100], Training Loss: 281988.8325780463, Test Loss: 21086490.0\n",
      "Epoch [46/100], Training Loss: 276528.81720573426, Test Loss: 20772630.0\n",
      "Epoch [47/100], Training Loss: 271324.66767371603, Test Loss: 20467330.0\n",
      "Epoch [48/100], Training Loss: 266252.07369231683, Test Loss: 20175838.0\n",
      "Epoch [49/100], Training Loss: 261330.924145489, Test Loss: 19910396.0\n",
      "Epoch [50/100], Training Loss: 256747.7538208637, Test Loss: 19658532.0\n",
      "Epoch [51/100], Training Loss: 252225.48639002428, Test Loss: 19402274.0\n",
      "Epoch [52/100], Training Loss: 247813.45305372906, Test Loss: 19170612.0\n",
      "Epoch [53/100], Training Loss: 243739.23314673302, Test Loss: 18960450.0\n",
      "Epoch [54/100], Training Loss: 239867.1197203957, Test Loss: 18760236.0\n",
      "Epoch [55/100], Training Loss: 236205.03373615307, Test Loss: 18575454.0\n",
      "Epoch [56/100], Training Loss: 232750.6310645104, Test Loss: 18403616.0\n",
      "Epoch [57/100], Training Loss: 229391.2479562822, Test Loss: 18238190.0\n",
      "Epoch [58/100], Training Loss: 226008.52724957053, Test Loss: 18062188.0\n",
      "Epoch [59/100], Training Loss: 222642.73216930276, Test Loss: 17904008.0\n",
      "Epoch [60/100], Training Loss: 219481.8081570997, Test Loss: 17747324.0\n",
      "Epoch [61/100], Training Loss: 216552.88315265684, Test Loss: 17626494.0\n",
      "Epoch [62/100], Training Loss: 213807.2658314081, Test Loss: 17491986.0\n",
      "Epoch [63/100], Training Loss: 211192.82595817783, Test Loss: 17387376.0\n",
      "Epoch [64/100], Training Loss: 208618.19790000594, Test Loss: 17267264.0\n",
      "Epoch [65/100], Training Loss: 206086.39748830046, Test Loss: 17166772.0\n",
      "Epoch [66/100], Training Loss: 203650.13349327646, Test Loss: 17052552.0\n",
      "Epoch [67/100], Training Loss: 201317.1018008412, Test Loss: 16971818.0\n",
      "Epoch [68/100], Training Loss: 199070.13482613588, Test Loss: 16861618.0\n",
      "Epoch [69/100], Training Loss: 196940.0756471773, Test Loss: 16794266.0\n",
      "Epoch [70/100], Training Loss: 194909.30423256915, Test Loss: 16699139.0\n",
      "Epoch [71/100], Training Loss: 192975.67032462533, Test Loss: 16637707.0\n",
      "Epoch [72/100], Training Loss: 191112.30289970973, Test Loss: 16548795.0\n",
      "Epoch [73/100], Training Loss: 189313.0975653101, Test Loss: 16490414.0\n",
      "Epoch [74/100], Training Loss: 187576.57078964516, Test Loss: 16403392.0\n",
      "Epoch [75/100], Training Loss: 185932.7859427759, Test Loss: 16363760.0\n",
      "Epoch [76/100], Training Loss: 184362.09318168354, Test Loss: 16259926.0\n",
      "Epoch [77/100], Training Loss: 182881.7676381731, Test Loss: 16256685.0\n",
      "Epoch [78/100], Training Loss: 181459.04872341687, Test Loss: 16142824.0\n",
      "Epoch [79/100], Training Loss: 180072.46453112966, Test Loss: 16135798.0\n",
      "Epoch [80/100], Training Loss: 178734.55152242165, Test Loss: 16037576.0\n",
      "Epoch [81/100], Training Loss: 177432.84718026183, Test Loss: 16020079.0\n",
      "Epoch [82/100], Training Loss: 176139.8488537409, Test Loss: 15932437.0\n",
      "Epoch [83/100], Training Loss: 174886.0907973461, Test Loss: 15911692.0\n",
      "Epoch [84/100], Training Loss: 173653.15530774242, Test Loss: 15832348.0\n",
      "Epoch [85/100], Training Loss: 172426.11096795215, Test Loss: 15800757.0\n",
      "Epoch [86/100], Training Loss: 171213.42596706355, Test Loss: 15736518.0\n",
      "Epoch [87/100], Training Loss: 170050.1699543866, Test Loss: 15694766.0\n",
      "Epoch [88/100], Training Loss: 168934.14782892007, Test Loss: 15636021.0\n",
      "Epoch [89/100], Training Loss: 167870.2073781174, Test Loss: 15654327.0\n",
      "Epoch [90/100], Training Loss: 166851.3763995024, Test Loss: 15548523.0\n",
      "Epoch [91/100], Training Loss: 165840.6227267342, Test Loss: 15565221.0\n",
      "Epoch [92/100], Training Loss: 164875.90390083526, Test Loss: 15479336.0\n",
      "Epoch [93/100], Training Loss: 163918.464027605, Test Loss: 15483908.0\n",
      "Epoch [94/100], Training Loss: 162982.33075647178, Test Loss: 15410371.0\n",
      "Epoch [95/100], Training Loss: 162061.90111664002, Test Loss: 15407877.0\n",
      "Epoch [96/100], Training Loss: 161148.8973328002, Test Loss: 15343414.0\n",
      "Epoch [97/100], Training Loss: 160241.5027323618, Test Loss: 15328317.0\n",
      "Epoch [98/100], Training Loss: 159365.98905574315, Test Loss: 15282251.0\n",
      "Epoch [99/100], Training Loss: 158506.39094247972, Test Loss: 15273759.0\n",
      "Epoch [100/100], Training Loss: 157659.68430483976, Test Loss: 15186969.0\n",
      "Epoch [1/100], Training Loss: 2318032.8956815354, Test Loss: 292323136.0\n",
      "Epoch [2/100], Training Loss: 2070695.0374977787, Test Loss: 230046048.0\n",
      "Epoch [3/100], Training Loss: 1488538.565013921, Test Loss: 163130832.0\n",
      "Epoch [4/100], Training Loss: 1171147.658314081, Test Loss: 142677776.0\n",
      "Epoch [5/100], Training Loss: 1054397.3913867662, Test Loss: 130868112.0\n",
      "Epoch [6/100], Training Loss: 967764.1668147622, Test Loss: 120829576.0\n",
      "Epoch [7/100], Training Loss: 891097.78449144, Test Loss: 111797536.0\n",
      "Epoch [8/100], Training Loss: 821614.6391801434, Test Loss: 103665496.0\n",
      "Epoch [9/100], Training Loss: 759098.6776849712, Test Loss: 96437968.0\n",
      "Epoch [10/100], Training Loss: 703577.510218589, Test Loss: 90091784.0\n",
      "Epoch [11/100], Training Loss: 654707.8882767608, Test Loss: 84521800.0\n",
      "Epoch [12/100], Training Loss: 611764.2473787098, Test Loss: 79609488.0\n",
      "Epoch [13/100], Training Loss: 574001.9550974468, Test Loss: 75262352.0\n",
      "Epoch [14/100], Training Loss: 540744.5772169895, Test Loss: 71397592.0\n",
      "Epoch [15/100], Training Loss: 511380.2575676796, Test Loss: 67948864.0\n",
      "Epoch [16/100], Training Loss: 485372.6387062378, Test Loss: 64871952.0\n",
      "Epoch [17/100], Training Loss: 462295.0068123926, Test Loss: 62129184.0\n",
      "Epoch [18/100], Training Loss: 441766.83620638587, Test Loss: 59680992.0\n",
      "Epoch [19/100], Training Loss: 423411.808542148, Test Loss: 57475580.0\n",
      "Epoch [20/100], Training Loss: 406827.95711154555, Test Loss: 55468324.0\n",
      "Epoch [21/100], Training Loss: 391668.5316628162, Test Loss: 53618228.0\n",
      "Epoch [22/100], Training Loss: 377631.02108879806, Test Loss: 51897276.0\n",
      "Epoch [23/100], Training Loss: 364514.82820922934, Test Loss: 50282592.0\n",
      "Epoch [24/100], Training Loss: 352183.7507849061, Test Loss: 48758408.0\n",
      "Epoch [25/100], Training Loss: 340559.87068301643, Test Loss: 47313936.0\n",
      "Epoch [26/100], Training Loss: 329559.0653989693, Test Loss: 45940820.0\n",
      "Epoch [27/100], Training Loss: 319107.955452876, Test Loss: 44630164.0\n",
      "Epoch [28/100], Training Loss: 309168.81843492686, Test Loss: 43375344.0\n",
      "Epoch [29/100], Training Loss: 299690.7703927492, Test Loss: 42171984.0\n",
      "Epoch [30/100], Training Loss: 290645.7382264084, Test Loss: 41016576.0\n",
      "Epoch [31/100], Training Loss: 282013.71553817904, Test Loss: 39905592.0\n",
      "Epoch [32/100], Training Loss: 273774.45115810673, Test Loss: 38837956.0\n",
      "Epoch [33/100], Training Loss: 265907.0233990877, Test Loss: 37813084.0\n",
      "Epoch [34/100], Training Loss: 258393.48818197974, Test Loss: 36829400.0\n",
      "Epoch [35/100], Training Loss: 251222.070138025, Test Loss: 35886452.0\n",
      "Epoch [36/100], Training Loss: 244379.71678218115, Test Loss: 34985040.0\n",
      "Epoch [37/100], Training Loss: 237855.20425330254, Test Loss: 34124684.0\n",
      "Epoch [38/100], Training Loss: 231623.49979266632, Test Loss: 33300164.0\n",
      "Epoch [39/100], Training Loss: 225673.61545524554, Test Loss: 32510660.0\n",
      "Epoch [40/100], Training Loss: 220006.36058290384, Test Loss: 31755246.0\n",
      "Epoch [41/100], Training Loss: 214614.8060837628, Test Loss: 31035290.0\n",
      "Epoch [42/100], Training Loss: 209481.7304662046, Test Loss: 30349828.0\n",
      "Epoch [43/100], Training Loss: 204595.81111900954, Test Loss: 29697682.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/100], Training Loss: 199935.1325454653, Test Loss: 29075270.0\n",
      "Epoch [45/100], Training Loss: 195489.2139387477, Test Loss: 28480632.0\n",
      "Epoch [46/100], Training Loss: 191247.67238315265, Test Loss: 27913640.0\n",
      "Epoch [47/100], Training Loss: 187198.73745631182, Test Loss: 27372678.0\n",
      "Epoch [48/100], Training Loss: 183336.64089805106, Test Loss: 26855628.0\n",
      "Epoch [49/100], Training Loss: 179654.88448551626, Test Loss: 26362534.0\n",
      "Epoch [50/100], Training Loss: 176138.03980806825, Test Loss: 25892954.0\n",
      "Epoch [51/100], Training Loss: 172771.55660209703, Test Loss: 25444666.0\n",
      "Epoch [52/100], Training Loss: 169540.72264676265, Test Loss: 25016196.0\n",
      "Epoch [53/100], Training Loss: 166430.42950654583, Test Loss: 24605846.0\n",
      "Epoch [54/100], Training Loss: 163431.53059652864, Test Loss: 24212058.0\n",
      "Epoch [55/100], Training Loss: 160544.82865351578, Test Loss: 23836358.0\n",
      "Epoch [56/100], Training Loss: 157760.91564480777, Test Loss: 23475614.0\n",
      "Epoch [57/100], Training Loss: 155074.7988270837, Test Loss: 23132340.0\n",
      "Epoch [58/100], Training Loss: 152482.30030803863, Test Loss: 22801962.0\n",
      "Epoch [59/100], Training Loss: 149980.50986315976, Test Loss: 22486200.0\n",
      "Epoch [60/100], Training Loss: 147560.90622593448, Test Loss: 22183098.0\n",
      "Epoch [61/100], Training Loss: 145222.70152834547, Test Loss: 21892722.0\n",
      "Epoch [62/100], Training Loss: 142962.95468277947, Test Loss: 21613874.0\n",
      "Epoch [63/100], Training Loss: 140776.47026242522, Test Loss: 21346576.0\n",
      "Epoch [64/100], Training Loss: 138659.72999229902, Test Loss: 21090036.0\n",
      "Epoch [65/100], Training Loss: 136607.84833540666, Test Loss: 20844326.0\n",
      "Epoch [66/100], Training Loss: 134618.10171198388, Test Loss: 20606190.0\n",
      "Epoch [67/100], Training Loss: 132689.3433149695, Test Loss: 20378976.0\n",
      "Epoch [68/100], Training Loss: 130820.83546590841, Test Loss: 20156928.0\n",
      "Epoch [69/100], Training Loss: 129006.11825425034, Test Loss: 19947324.0\n",
      "Epoch [70/100], Training Loss: 127250.43297198034, Test Loss: 19743682.0\n",
      "Epoch [71/100], Training Loss: 125549.7359753569, Test Loss: 19551604.0\n",
      "Epoch [72/100], Training Loss: 123897.65899532018, Test Loss: 19363638.0\n",
      "Epoch [73/100], Training Loss: 122295.54742017653, Test Loss: 19185124.0\n",
      "Epoch [74/100], Training Loss: 120733.08290385641, Test Loss: 19010986.0\n",
      "Epoch [75/100], Training Loss: 119211.31342929922, Test Loss: 18846696.0\n",
      "Epoch [76/100], Training Loss: 117729.6254368817, Test Loss: 18683766.0\n",
      "Epoch [77/100], Training Loss: 116289.50860434808, Test Loss: 18531508.0\n",
      "Epoch [78/100], Training Loss: 114885.25910787276, Test Loss: 18381922.0\n",
      "Epoch [79/100], Training Loss: 113522.62662164564, Test Loss: 18238468.0\n",
      "Epoch [80/100], Training Loss: 112198.74561637343, Test Loss: 18100700.0\n",
      "Epoch [81/100], Training Loss: 110908.89819915882, Test Loss: 17969070.0\n",
      "Epoch [82/100], Training Loss: 109655.51849712695, Test Loss: 17840716.0\n",
      "Epoch [83/100], Training Loss: 108434.0913304899, Test Loss: 17718546.0\n",
      "Epoch [84/100], Training Loss: 107247.07798708607, Test Loss: 17600360.0\n",
      "Epoch [85/100], Training Loss: 106091.86684734316, Test Loss: 17486542.0\n",
      "Epoch [86/100], Training Loss: 104969.2250459096, Test Loss: 17376522.0\n",
      "Epoch [87/100], Training Loss: 103871.10818375689, Test Loss: 17271220.0\n",
      "Epoch [88/100], Training Loss: 102800.64348972218, Test Loss: 17167404.0\n",
      "Epoch [89/100], Training Loss: 101762.67362715479, Test Loss: 17070326.0\n",
      "Epoch [90/100], Training Loss: 100743.94821100646, Test Loss: 16973886.0\n",
      "Epoch [91/100], Training Loss: 99756.52132575084, Test Loss: 16884282.0\n",
      "Epoch [92/100], Training Loss: 98791.13952076298, Test Loss: 16797124.0\n",
      "Epoch [93/100], Training Loss: 97853.94391623719, Test Loss: 16713898.0\n",
      "Epoch [94/100], Training Loss: 96941.45059534388, Test Loss: 16634975.0\n",
      "Epoch [95/100], Training Loss: 96052.49826728275, Test Loss: 16558120.0\n",
      "Epoch [96/100], Training Loss: 95185.45717078372, Test Loss: 16486692.0\n",
      "Epoch [97/100], Training Loss: 94338.37962798412, Test Loss: 16415697.0\n",
      "Epoch [98/100], Training Loss: 93515.37857650613, Test Loss: 16352179.0\n",
      "Epoch [99/100], Training Loss: 92714.80709081216, Test Loss: 16284233.0\n",
      "Epoch [100/100], Training Loss: 91934.80180380309, Test Loss: 16228502.0\n",
      "Epoch [1/100], Training Loss: 1497688.9776672, Test Loss: 298623808.0\n",
      "Epoch [2/100], Training Loss: 1464150.3569693738, Test Loss: 283369216.0\n",
      "Epoch [3/100], Training Loss: 1323512.2791303832, Test Loss: 241266528.0\n",
      "Epoch [4/100], Training Loss: 1077132.5537586636, Test Loss: 189866144.0\n",
      "Epoch [5/100], Training Loss: 865738.441324566, Test Loss: 159687696.0\n",
      "Epoch [6/100], Training Loss: 760212.6433268171, Test Loss: 145927904.0\n",
      "Epoch [7/100], Training Loss: 702823.2870090634, Test Loss: 136727744.0\n",
      "Epoch [8/100], Training Loss: 658547.8258397015, Test Loss: 129040120.0\n",
      "Epoch [9/100], Training Loss: 619789.731177063, Test Loss: 122231056.0\n",
      "Epoch [10/100], Training Loss: 584705.6039334163, Test Loss: 116097424.0\n",
      "Epoch [11/100], Training Loss: 552639.8910017179, Test Loss: 110554032.0\n",
      "Epoch [12/100], Training Loss: 523309.06344410876, Test Loss: 105553128.0\n",
      "Epoch [13/100], Training Loss: 496541.1546709318, Test Loss: 101050464.0\n",
      "Epoch [14/100], Training Loss: 472174.9922397962, Test Loss: 97001912.0\n",
      "Epoch [15/100], Training Loss: 450055.6234820212, Test Loss: 93363656.0\n",
      "Epoch [16/100], Training Loss: 430015.35169717437, Test Loss: 90085304.0\n",
      "Epoch [17/100], Training Loss: 411856.9001836384, Test Loss: 87115000.0\n",
      "Epoch [18/100], Training Loss: 395384.4589775487, Test Loss: 84409216.0\n",
      "Epoch [19/100], Training Loss: 380426.3881286654, Test Loss: 81932600.0\n",
      "Epoch [20/100], Training Loss: 366832.664060186, Test Loss: 79657936.0\n",
      "Epoch [21/100], Training Loss: 354468.31799064035, Test Loss: 77562240.0\n",
      "Epoch [22/100], Training Loss: 343211.9545050649, Test Loss: 75624544.0\n",
      "Epoch [23/100], Training Loss: 332946.4112315621, Test Loss: 73826008.0\n",
      "Epoch [24/100], Training Loss: 323559.8888691428, Test Loss: 72149112.0\n",
      "Epoch [25/100], Training Loss: 314946.5814821397, Test Loss: 70578888.0\n",
      "Epoch [26/100], Training Loss: 307005.46958118596, Test Loss: 69101224.0\n",
      "Epoch [27/100], Training Loss: 299638.49351341743, Test Loss: 67703016.0\n",
      "Epoch [28/100], Training Loss: 292762.8727563533, Test Loss: 66374800.0\n",
      "Epoch [29/100], Training Loss: 286303.63518156507, Test Loss: 65106548.0\n",
      "Epoch [30/100], Training Loss: 280193.33239292697, Test Loss: 63890512.0\n",
      "Epoch [31/100], Training Loss: 274381.659002725, Test Loss: 62719284.0\n",
      "Epoch [32/100], Training Loss: 268821.1855437511, Test Loss: 61586328.0\n",
      "Epoch [33/100], Training Loss: 263471.6178276659, Test Loss: 60486692.0\n",
      "Epoch [34/100], Training Loss: 258303.49047745985, Test Loss: 59417924.0\n",
      "Epoch [35/100], Training Loss: 253292.71969966235, Test Loss: 58375956.0\n",
      "Epoch [36/100], Training Loss: 248420.21800396897, Test Loss: 57359140.0\n",
      "Epoch [37/100], Training Loss: 243668.5143430484, Test Loss: 56365524.0\n",
      "Epoch [38/100], Training Loss: 239023.39611101238, Test Loss: 55392940.0\n",
      "Epoch [39/100], Training Loss: 234476.06689473373, Test Loss: 54440880.0\n",
      "Epoch [40/100], Training Loss: 230021.03997097327, Test Loss: 53508644.0\n",
      "Epoch [41/100], Training Loss: 225653.735708785, Test Loss: 52596204.0\n",
      "Epoch [42/100], Training Loss: 221373.33062318584, Test Loss: 51702156.0\n",
      "Epoch [43/100], Training Loss: 217180.9968159469, Test Loss: 50826608.0\n",
      "Epoch [44/100], Training Loss: 213076.4934097506, Test Loss: 49970056.0\n",
      "Epoch [45/100], Training Loss: 209061.5767282744, Test Loss: 49133016.0\n",
      "Epoch [46/100], Training Loss: 205133.19133937563, Test Loss: 48313992.0\n",
      "Epoch [47/100], Training Loss: 201290.137417807, Test Loss: 47511500.0\n",
      "Epoch [48/100], Training Loss: 197530.64582963096, Test Loss: 46725872.0\n",
      "Epoch [49/100], Training Loss: 193854.19792962502, Test Loss: 45956796.0\n",
      "Epoch [50/100], Training Loss: 190258.76446892956, Test Loss: 45203008.0\n",
      "Epoch [51/100], Training Loss: 186742.4403027072, Test Loss: 44464080.0\n",
      "Epoch [52/100], Training Loss: 183306.58235590308, Test Loss: 43739676.0\n",
      "Epoch [53/100], Training Loss: 179950.1451113678, Test Loss: 43028568.0\n",
      "Epoch [54/100], Training Loss: 176673.63918014336, Test Loss: 42331892.0\n",
      "Epoch [55/100], Training Loss: 173474.8720528997, Test Loss: 41648844.0\n",
      "Epoch [56/100], Training Loss: 170351.7127132575, Test Loss: 40979200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [57/100], Training Loss: 167304.67310511819, Test Loss: 40322812.0\n",
      "Epoch [58/100], Training Loss: 164333.20092115397, Test Loss: 39679012.0\n",
      "Epoch [59/100], Training Loss: 161433.86558853148, Test Loss: 39048460.0\n",
      "Epoch [60/100], Training Loss: 158606.11555150762, Test Loss: 38431048.0\n",
      "Epoch [61/100], Training Loss: 155849.18331074878, Test Loss: 37828012.0\n",
      "Epoch [62/100], Training Loss: 153161.22149717138, Test Loss: 37238928.0\n",
      "Epoch [63/100], Training Loss: 150542.213736042, Test Loss: 36663608.0\n",
      "Epoch [64/100], Training Loss: 147990.49905774242, Test Loss: 36101932.0\n",
      "Epoch [65/100], Training Loss: 145503.6092523584, Test Loss: 35553244.0\n",
      "Epoch [66/100], Training Loss: 143078.15113607753, Test Loss: 35016028.0\n",
      "Epoch [67/100], Training Loss: 140711.68997527732, Test Loss: 34491328.0\n",
      "Epoch [68/100], Training Loss: 138407.9371802063, Test Loss: 33979004.0\n",
      "Epoch [69/100], Training Loss: 136164.7643138921, Test Loss: 33479422.0\n",
      "Epoch [70/100], Training Loss: 133980.15710658202, Test Loss: 32992024.0\n",
      "Epoch [71/100], Training Loss: 131851.9942242758, Test Loss: 32518274.0\n",
      "Epoch [72/100], Training Loss: 129781.476056401, Test Loss: 32057216.0\n",
      "Epoch [73/100], Training Loss: 127766.33070158045, Test Loss: 31608676.0\n",
      "Epoch [74/100], Training Loss: 125805.19261438887, Test Loss: 31172140.0\n",
      "Epoch [75/100], Training Loss: 123897.51228789082, Test Loss: 30747418.0\n",
      "Epoch [76/100], Training Loss: 122041.14232563954, Test Loss: 30333940.0\n",
      "Epoch [77/100], Training Loss: 120234.4854899103, Test Loss: 29931412.0\n",
      "Epoch [78/100], Training Loss: 118476.86913576562, Test Loss: 29538602.0\n",
      "Epoch [79/100], Training Loss: 116763.7577975169, Test Loss: 29155978.0\n",
      "Epoch [80/100], Training Loss: 115095.93533711162, Test Loss: 28783556.0\n",
      "Epoch [81/100], Training Loss: 113470.8305167422, Test Loss: 28421550.0\n",
      "Epoch [82/100], Training Loss: 111886.12907505572, Test Loss: 28070254.0\n",
      "Epoch [83/100], Training Loss: 110344.07128738485, Test Loss: 27729116.0\n",
      "Epoch [84/100], Training Loss: 108840.40852592689, Test Loss: 27397570.0\n",
      "Epoch [85/100], Training Loss: 107375.19632475583, Test Loss: 27076156.0\n",
      "Epoch [86/100], Training Loss: 105948.61865433957, Test Loss: 26764636.0\n",
      "Epoch [87/100], Training Loss: 104560.71192302922, Test Loss: 26462906.0\n",
      "Epoch [88/100], Training Loss: 103208.15720359613, Test Loss: 26169374.0\n",
      "Epoch [89/100], Training Loss: 101888.42473925937, Test Loss: 25885396.0\n",
      "Epoch [90/100], Training Loss: 100603.38440591493, Test Loss: 25610716.0\n",
      "Epoch [91/100], Training Loss: 99351.08404650568, Test Loss: 25344722.0\n",
      "Epoch [92/100], Training Loss: 98130.12720454024, Test Loss: 25086624.0\n",
      "Epoch [93/100], Training Loss: 96939.22324516097, Test Loss: 24836692.0\n",
      "Epoch [94/100], Training Loss: 95777.91758185053, Test Loss: 24593834.0\n",
      "Epoch [95/100], Training Loss: 94643.02741988034, Test Loss: 24356628.0\n",
      "Epoch [96/100], Training Loss: 93534.04476487063, Test Loss: 24125146.0\n",
      "Epoch [97/100], Training Loss: 92450.65732646911, Test Loss: 23900270.0\n",
      "Epoch [98/100], Training Loss: 91390.93682338872, Test Loss: 23681096.0\n",
      "Epoch [99/100], Training Loss: 90353.04074731762, Test Loss: 23466772.0\n",
      "Epoch [100/100], Training Loss: 89335.81036511093, Test Loss: 23258776.0\n",
      "Epoch [1/100], Training Loss: 1164371.430128547, Test Loss: 299353856.0\n",
      "Epoch [2/100], Training Loss: 1151166.0948995913, Test Loss: 291531456.0\n",
      "Epoch [3/100], Training Loss: 1090912.9762454832, Test Loss: 266967984.0\n",
      "Epoch [4/100], Training Loss: 960521.9989337125, Test Loss: 225801696.0\n",
      "Epoch [5/100], Training Loss: 793188.9234050115, Test Loss: 184641072.0\n",
      "Epoch [6/100], Training Loss: 661953.7399443161, Test Loss: 159670928.0\n",
      "Epoch [7/100], Training Loss: 591736.0954919732, Test Loss: 147227856.0\n",
      "Epoch [8/100], Training Loss: 552486.822107695, Test Loss: 139071024.0\n",
      "Epoch [9/100], Training Loss: 522980.85160831705, Test Loss: 132301640.0\n",
      "Epoch [10/100], Training Loss: 497204.05829038564, Test Loss: 126205768.0\n",
      "Epoch [11/100], Training Loss: 473514.9427166637, Test Loss: 120546944.0\n",
      "Epoch [12/100], Training Loss: 451297.32764646644, Test Loss: 115227824.0\n",
      "Epoch [13/100], Training Loss: 430301.2954208874, Test Loss: 110211760.0\n",
      "Epoch [14/100], Training Loss: 410450.845566021, Test Loss: 105492800.0\n",
      "Epoch [15/100], Training Loss: 391751.26260292635, Test Loss: 101072232.0\n",
      "Epoch [16/100], Training Loss: 374215.4907884604, Test Loss: 96950960.0\n",
      "Epoch [17/100], Training Loss: 357861.36982406257, Test Loss: 93132304.0\n",
      "Epoch [18/100], Training Loss: 342698.95337953913, Test Loss: 89607448.0\n",
      "Epoch [19/100], Training Loss: 328695.301937089, Test Loss: 86357040.0\n",
      "Epoch [20/100], Training Loss: 315783.5737219359, Test Loss: 83357264.0\n",
      "Epoch [21/100], Training Loss: 303882.88608494756, Test Loss: 80583456.0\n",
      "Epoch [22/100], Training Loss: 292907.8032107103, Test Loss: 78012808.0\n",
      "Epoch [23/100], Training Loss: 282770.59463301935, Test Loss: 75624272.0\n",
      "Epoch [24/100], Training Loss: 273388.7930809786, Test Loss: 73398848.0\n",
      "Epoch [25/100], Training Loss: 264685.9363781767, Test Loss: 71319952.0\n",
      "Epoch [26/100], Training Loss: 256597.3430483976, Test Loss: 69373880.0\n",
      "Epoch [27/100], Training Loss: 249063.95509744683, Test Loss: 67550392.0\n",
      "Epoch [28/100], Training Loss: 242037.39991706653, Test Loss: 65840840.0\n",
      "Epoch [29/100], Training Loss: 235466.41656299983, Test Loss: 64233136.0\n",
      "Epoch [30/100], Training Loss: 229299.36022747468, Test Loss: 62716176.0\n",
      "Epoch [31/100], Training Loss: 223489.75356910136, Test Loss: 61281268.0\n",
      "Epoch [32/100], Training Loss: 217997.51164030566, Test Loss: 59922108.0\n",
      "Epoch [33/100], Training Loss: 212789.65357502518, Test Loss: 58632036.0\n",
      "Epoch [34/100], Training Loss: 207835.8395829631, Test Loss: 57403092.0\n",
      "Epoch [35/100], Training Loss: 203109.4627095551, Test Loss: 56229524.0\n",
      "Epoch [36/100], Training Loss: 198591.55275161425, Test Loss: 55106120.0\n",
      "Epoch [37/100], Training Loss: 194257.12611812097, Test Loss: 54027692.0\n",
      "Epoch [38/100], Training Loss: 190087.12836917245, Test Loss: 52990132.0\n",
      "Epoch [39/100], Training Loss: 186071.90818079497, Test Loss: 51990560.0\n",
      "Epoch [40/100], Training Loss: 182199.99312836918, Test Loss: 51025320.0\n",
      "Epoch [41/100], Training Loss: 178460.51679402878, Test Loss: 50091948.0\n",
      "Epoch [42/100], Training Loss: 174845.9850719744, Test Loss: 49187848.0\n",
      "Epoch [43/100], Training Loss: 171349.27990047983, Test Loss: 48310996.0\n",
      "Epoch [44/100], Training Loss: 167961.84609916474, Test Loss: 47459276.0\n",
      "Epoch [45/100], Training Loss: 164677.95995497896, Test Loss: 46631440.0\n",
      "Epoch [46/100], Training Loss: 161492.9187844322, Test Loss: 45825516.0\n",
      "Epoch [47/100], Training Loss: 158400.4553047805, Test Loss: 45041348.0\n",
      "Epoch [48/100], Training Loss: 155397.06297020317, Test Loss: 44277232.0\n",
      "Epoch [49/100], Training Loss: 152479.2857058231, Test Loss: 43532496.0\n",
      "Epoch [50/100], Training Loss: 149644.83111190094, Test Loss: 42806216.0\n",
      "Epoch [51/100], Training Loss: 146889.7709851312, Test Loss: 42097792.0\n",
      "Epoch [52/100], Training Loss: 144211.24068479356, Test Loss: 41406772.0\n",
      "Epoch [53/100], Training Loss: 141608.71150998163, Test Loss: 40733536.0\n",
      "Epoch [54/100], Training Loss: 139082.6731236301, Test Loss: 40077112.0\n",
      "Epoch [55/100], Training Loss: 136627.50855991943, Test Loss: 39436596.0\n",
      "Epoch [56/100], Training Loss: 134238.70386825426, Test Loss: 38811168.0\n",
      "Epoch [57/100], Training Loss: 131915.94798886322, Test Loss: 38200864.0\n",
      "Epoch [58/100], Training Loss: 129658.91392690006, Test Loss: 37606536.0\n",
      "Epoch [59/100], Training Loss: 127464.53201824536, Test Loss: 37027940.0\n",
      "Epoch [60/100], Training Loss: 125331.87400035543, Test Loss: 36465408.0\n",
      "Epoch [61/100], Training Loss: 123259.51839346011, Test Loss: 35917792.0\n",
      "Epoch [62/100], Training Loss: 121246.22563829157, Test Loss: 35385504.0\n",
      "Epoch [63/100], Training Loss: 119290.20502339909, Test Loss: 34867600.0\n",
      "Epoch [64/100], Training Loss: 117389.53095195783, Test Loss: 34363216.0\n",
      "Epoch [65/100], Training Loss: 115540.67490077602, Test Loss: 33871904.0\n",
      "Epoch [66/100], Training Loss: 113741.48652331023, Test Loss: 33393174.0\n",
      "Epoch [67/100], Training Loss: 111991.47941472662, Test Loss: 32927206.0\n",
      "Epoch [68/100], Training Loss: 110289.27095551211, Test Loss: 32474050.0\n",
      "Epoch [69/100], Training Loss: 108634.52550204372, Test Loss: 32032680.0\n",
      "Epoch [70/100], Training Loss: 107026.63396718205, Test Loss: 31603124.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [71/100], Training Loss: 105464.49700847107, Test Loss: 31185658.0\n",
      "Epoch [72/100], Training Loss: 103945.8338960962, Test Loss: 30779788.0\n",
      "Epoch [73/100], Training Loss: 102469.7361530715, Test Loss: 30385456.0\n",
      "Epoch [74/100], Training Loss: 101035.76109235235, Test Loss: 30002532.0\n",
      "Epoch [75/100], Training Loss: 99641.12031277768, Test Loss: 29630268.0\n",
      "Epoch [76/100], Training Loss: 98283.1826905989, Test Loss: 29268058.0\n",
      "Epoch [77/100], Training Loss: 96961.45246134707, Test Loss: 28915692.0\n",
      "Epoch [78/100], Training Loss: 95674.55666133523, Test Loss: 28573532.0\n",
      "Epoch [79/100], Training Loss: 94420.88466323086, Test Loss: 28241104.0\n",
      "Epoch [80/100], Training Loss: 93201.19110242285, Test Loss: 27918206.0\n",
      "Epoch [81/100], Training Loss: 92014.77276227712, Test Loss: 27604114.0\n",
      "Epoch [82/100], Training Loss: 90860.46756708725, Test Loss: 27299376.0\n",
      "Epoch [83/100], Training Loss: 89737.13873585688, Test Loss: 27003150.0\n",
      "Epoch [84/100], Training Loss: 88644.10645103962, Test Loss: 26714984.0\n",
      "Epoch [85/100], Training Loss: 87580.00094781115, Test Loss: 26435280.0\n",
      "Epoch [86/100], Training Loss: 86543.42414548901, Test Loss: 26163428.0\n",
      "Epoch [87/100], Training Loss: 85532.43978437297, Test Loss: 25899584.0\n",
      "Epoch [88/100], Training Loss: 84546.88033884249, Test Loss: 25643030.0\n",
      "Epoch [89/100], Training Loss: 83586.21533084533, Test Loss: 25393622.0\n",
      "Epoch [90/100], Training Loss: 82647.9668266098, Test Loss: 25150784.0\n",
      "Epoch [91/100], Training Loss: 81730.67484153782, Test Loss: 24914414.0\n",
      "Epoch [92/100], Training Loss: 80833.52099994077, Test Loss: 24684268.0\n",
      "Epoch [93/100], Training Loss: 79955.77163675138, Test Loss: 24460166.0\n",
      "Epoch [94/100], Training Loss: 79097.4667969907, Test Loss: 24242028.0\n",
      "Epoch [95/100], Training Loss: 78257.76991884367, Test Loss: 24029648.0\n",
      "Epoch [96/100], Training Loss: 77435.30294413838, Test Loss: 23822620.0\n",
      "Epoch [97/100], Training Loss: 76630.029500622, Test Loss: 23620710.0\n",
      "Epoch [98/100], Training Loss: 75842.19151709022, Test Loss: 23423254.0\n",
      "Epoch [99/100], Training Loss: 75069.25573129553, Test Loss: 23230420.0\n",
      "Epoch [100/100], Training Loss: 74312.40163497423, Test Loss: 23042812.0\n",
      "Epoch [1/100], Training Loss: 582429.9707363308, Test Loss: 300041344.0\n",
      "Epoch [2/100], Training Loss: 581960.9956756117, Test Loss: 299474432.0\n",
      "Epoch [3/100], Training Loss: 579579.3083348143, Test Loss: 297328064.0\n",
      "Epoch [4/100], Training Loss: 572817.5022806706, Test Loss: 292211552.0\n",
      "Epoch [5/100], Training Loss: 559036.512054973, Test Loss: 282892224.0\n",
      "Epoch [6/100], Training Loss: 536326.8706830164, Test Loss: 268731264.0\n",
      "Epoch [7/100], Training Loss: 504327.4479000059, Test Loss: 250077056.0\n",
      "Epoch [8/100], Training Loss: 464854.3636040519, Test Loss: 228476608.0\n",
      "Epoch [9/100], Training Loss: 421946.89982820925, Test Loss: 206464608.0\n",
      "Epoch [10/100], Training Loss: 380911.46922575677, Test Loss: 186791632.0\n",
      "Epoch [11/100], Training Loss: 346404.74616432673, Test Loss: 171291632.0\n",
      "Epoch [12/100], Training Loss: 320428.11160476273, Test Loss: 160141856.0\n",
      "Epoch [13/100], Training Loss: 301958.46075469465, Test Loss: 152268800.0\n",
      "Epoch [14/100], Training Loss: 288553.32503998576, Test Loss: 146390272.0\n",
      "Epoch [15/100], Training Loss: 278063.97677862685, Test Loss: 141611504.0\n",
      "Epoch [16/100], Training Loss: 269174.4522243943, Test Loss: 137444720.0\n",
      "Epoch [17/100], Training Loss: 261207.70191339374, Test Loss: 133649704.0\n",
      "Epoch [18/100], Training Loss: 253831.42183519935, Test Loss: 130106712.0\n",
      "Epoch [19/100], Training Loss: 246873.56436230082, Test Loss: 126750840.0\n",
      "Epoch [20/100], Training Loss: 240236.2604111131, Test Loss: 123543576.0\n",
      "Epoch [21/100], Training Loss: 233859.46424974824, Test Loss: 120460832.0\n",
      "Epoch [22/100], Training Loss: 227704.95160239324, Test Loss: 117487040.0\n",
      "Epoch [23/100], Training Loss: 221747.50974468337, Test Loss: 114611232.0\n",
      "Epoch [24/100], Training Loss: 215966.4652567976, Test Loss: 111821704.0\n",
      "Epoch [25/100], Training Loss: 210333.2416326047, Test Loss: 109098312.0\n",
      "Epoch [26/100], Training Loss: 204805.41674071443, Test Loss: 106430528.0\n",
      "Epoch [27/100], Training Loss: 199389.97950358392, Test Loss: 103835080.0\n",
      "Epoch [28/100], Training Loss: 194133.9650494639, Test Loss: 101337160.0\n",
      "Epoch [29/100], Training Loss: 189076.14643682246, Test Loss: 98939568.0\n",
      "Epoch [30/100], Training Loss: 184225.08950891535, Test Loss: 96650552.0\n",
      "Epoch [31/100], Training Loss: 179595.0017179077, Test Loss: 94475784.0\n",
      "Epoch [32/100], Training Loss: 175182.9095432735, Test Loss: 92406104.0\n",
      "Epoch [33/100], Training Loss: 170978.34038267875, Test Loss: 90434016.0\n",
      "Epoch [34/100], Training Loss: 166973.98424263965, Test Loss: 88555424.0\n",
      "Epoch [35/100], Training Loss: 163159.29956756116, Test Loss: 86765440.0\n",
      "Epoch [36/100], Training Loss: 159525.41413423375, Test Loss: 85059800.0\n",
      "Epoch [37/100], Training Loss: 156064.08388128664, Test Loss: 83433632.0\n",
      "Epoch [38/100], Training Loss: 152765.99609027902, Test Loss: 81881616.0\n",
      "Epoch [39/100], Training Loss: 149623.60547360938, Test Loss: 80400480.0\n",
      "Epoch [40/100], Training Loss: 146629.06060067532, Test Loss: 78984600.0\n",
      "Epoch [41/100], Training Loss: 143773.05325513892, Test Loss: 77629808.0\n",
      "Epoch [42/100], Training Loss: 141045.8785616966, Test Loss: 76330824.0\n",
      "Epoch [43/100], Training Loss: 138439.7493039512, Test Loss: 75084224.0\n",
      "Epoch [44/100], Training Loss: 135947.65618150582, Test Loss: 73887376.0\n",
      "Epoch [45/100], Training Loss: 133562.61690658136, Test Loss: 72738120.0\n",
      "Epoch [46/100], Training Loss: 131276.77744209467, Test Loss: 71632800.0\n",
      "Epoch [47/100], Training Loss: 129084.24761566258, Test Loss: 70569008.0\n",
      "Epoch [48/100], Training Loss: 126978.6088501866, Test Loss: 69543400.0\n",
      "Epoch [49/100], Training Loss: 124954.69249452047, Test Loss: 68552936.0\n",
      "Epoch [50/100], Training Loss: 123007.16663704757, Test Loss: 67596912.0\n",
      "Epoch [51/100], Training Loss: 121132.57650613115, Test Loss: 66674332.0\n",
      "Epoch [52/100], Training Loss: 119328.83241514128, Test Loss: 65783308.0\n",
      "Epoch [53/100], Training Loss: 117589.52573899651, Test Loss: 64920484.0\n",
      "Epoch [54/100], Training Loss: 115910.627332504, Test Loss: 64084304.0\n",
      "Epoch [55/100], Training Loss: 114288.40163497423, Test Loss: 63273784.0\n",
      "Epoch [56/100], Training Loss: 112718.83111190096, Test Loss: 62487372.0\n",
      "Epoch [57/100], Training Loss: 111199.19258337776, Test Loss: 61723048.0\n",
      "Epoch [58/100], Training Loss: 109725.04685741366, Test Loss: 60979416.0\n",
      "Epoch [59/100], Training Loss: 108293.4629465079, Test Loss: 60256344.0\n",
      "Epoch [60/100], Training Loss: 106901.00467981755, Test Loss: 59552088.0\n",
      "Epoch [61/100], Training Loss: 105544.09454416207, Test Loss: 58865000.0\n",
      "Epoch [62/100], Training Loss: 104219.68580060423, Test Loss: 58195216.0\n",
      "Epoch [63/100], Training Loss: 102926.3451217345, Test Loss: 57542104.0\n",
      "Epoch [64/100], Training Loss: 101665.39091286062, Test Loss: 56905036.0\n",
      "Epoch [65/100], Training Loss: 100434.10378532078, Test Loss: 56283172.0\n",
      "Epoch [66/100], Training Loss: 99230.30282566199, Test Loss: 55674772.0\n",
      "Epoch [67/100], Training Loss: 98051.80190746994, Test Loss: 55078948.0\n",
      "Epoch [68/100], Training Loss: 96898.29559860198, Test Loss: 54495488.0\n",
      "Epoch [69/100], Training Loss: 95768.64830282566, Test Loss: 53924612.0\n",
      "Epoch [70/100], Training Loss: 94661.46590841775, Test Loss: 53364704.0\n",
      "Epoch [71/100], Training Loss: 93575.57917185001, Test Loss: 52815716.0\n",
      "Epoch [72/100], Training Loss: 92509.3451809727, Test Loss: 52276164.0\n",
      "Epoch [73/100], Training Loss: 91462.22937029797, Test Loss: 51745180.0\n",
      "Epoch [74/100], Training Loss: 90433.2897340205, Test Loss: 51223416.0\n",
      "Epoch [75/100], Training Loss: 89422.9797997749, Test Loss: 50711372.0\n",
      "Epoch [76/100], Training Loss: 88430.96119898111, Test Loss: 50208376.0\n",
      "Epoch [77/100], Training Loss: 87456.11598838931, Test Loss: 49713976.0\n",
      "Epoch [78/100], Training Loss: 86498.18754813104, Test Loss: 49226448.0\n",
      "Epoch [79/100], Training Loss: 85557.29826432084, Test Loss: 48747600.0\n",
      "Epoch [80/100], Training Loss: 84634.21432379598, Test Loss: 48278584.0\n",
      "Epoch [81/100], Training Loss: 83728.65079082992, Test Loss: 47819324.0\n",
      "Epoch [82/100], Training Loss: 82839.96718203898, Test Loss: 47368224.0\n",
      "Epoch [83/100], Training Loss: 81968.17641135004, Test Loss: 46924988.0\n",
      "Epoch [84/100], Training Loss: 81113.52230318109, Test Loss: 46489476.0\n",
      "Epoch [85/100], Training Loss: 80274.75007404774, Test Loss: 46062512.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [86/100], Training Loss: 79451.75060719151, Test Loss: 45643496.0\n",
      "Epoch [87/100], Training Loss: 78644.66370475682, Test Loss: 45232172.0\n",
      "Epoch [88/100], Training Loss: 77852.66737752502, Test Loss: 44827900.0\n",
      "Epoch [89/100], Training Loss: 77074.97517919555, Test Loss: 44430500.0\n",
      "Epoch [90/100], Training Loss: 76311.05479533203, Test Loss: 44040300.0\n",
      "Epoch [91/100], Training Loss: 75560.88952076298, Test Loss: 43657548.0\n",
      "Epoch [92/100], Training Loss: 74823.84064925063, Test Loss: 43281244.0\n",
      "Epoch [93/100], Training Loss: 74099.52834547717, Test Loss: 42910320.0\n",
      "Epoch [94/100], Training Loss: 73387.58995320182, Test Loss: 42544700.0\n",
      "Epoch [95/100], Training Loss: 72687.04259226349, Test Loss: 42183612.0\n",
      "Epoch [96/100], Training Loss: 71996.69427166638, Test Loss: 41827200.0\n",
      "Epoch [97/100], Training Loss: 71316.35388898762, Test Loss: 41474888.0\n",
      "Epoch [98/100], Training Loss: 70645.06391801433, Test Loss: 41126800.0\n",
      "Epoch [99/100], Training Loss: 69982.14098690836, Test Loss: 40783192.0\n",
      "Epoch [100/100], Training Loss: 69327.46934423316, Test Loss: 40443228.0\n",
      "Epoch [1/100], Training Loss: 7141674.495586755, Test Loss: 148258432.0\n",
      "Epoch [2/100], Training Loss: 4057547.775250281, Test Loss: 117145816.0\n",
      "Epoch [3/100], Training Loss: 3243075.716722943, Test Loss: 95701752.0\n",
      "Epoch [4/100], Training Loss: 2651765.559504769, Test Loss: 80099640.0\n",
      "Epoch [5/100], Training Loss: 2215120.6131153367, Test Loss: 68520768.0\n",
      "Epoch [6/100], Training Loss: 1898279.1826609797, Test Loss: 60107632.0\n",
      "Epoch [7/100], Training Loss: 1672169.3301640898, Test Loss: 54063400.0\n",
      "Epoch [8/100], Training Loss: 1506639.7287186778, Test Loss: 49444164.0\n",
      "Epoch [9/100], Training Loss: 1375528.4802144424, Test Loss: 45627356.0\n",
      "Epoch [10/100], Training Loss: 1264666.0121142112, Test Loss: 42311536.0\n",
      "Epoch [11/100], Training Loss: 1168041.9032640245, Test Loss: 39352272.0\n",
      "Epoch [12/100], Training Loss: 1082868.0059238197, Test Loss: 36690748.0\n",
      "Epoch [13/100], Training Loss: 1007688.1279100764, Test Loss: 34301656.0\n",
      "Epoch [14/100], Training Loss: 941256.718233517, Test Loss: 32181158.0\n",
      "Epoch [15/100], Training Loss: 882681.5120105444, Test Loss: 30304264.0\n",
      "Epoch [16/100], Training Loss: 831087.2553166281, Test Loss: 28649118.0\n",
      "Epoch [17/100], Training Loss: 785698.3904833837, Test Loss: 27186268.0\n",
      "Epoch [18/100], Training Loss: 745716.9924323204, Test Loss: 25887240.0\n",
      "Epoch [19/100], Training Loss: 710413.1928795688, Test Loss: 24738642.0\n",
      "Epoch [20/100], Training Loss: 678835.4130383271, Test Loss: 23717248.0\n",
      "Epoch [21/100], Training Loss: 650322.6964042415, Test Loss: 22799062.0\n",
      "Epoch [22/100], Training Loss: 624413.9306024525, Test Loss: 21983028.0\n",
      "Epoch [23/100], Training Loss: 600726.0670132102, Test Loss: 21255448.0\n",
      "Epoch [24/100], Training Loss: 578980.8726600913, Test Loss: 20605554.0\n",
      "Epoch [25/100], Training Loss: 558980.2884159706, Test Loss: 20026170.0\n",
      "Epoch [26/100], Training Loss: 540596.3215079083, Test Loss: 19511254.0\n",
      "Epoch [27/100], Training Loss: 523647.88267875125, Test Loss: 19041876.0\n",
      "Epoch [28/100], Training Loss: 507945.39917658904, Test Loss: 18632702.0\n",
      "Epoch [29/100], Training Loss: 493356.1068731118, Test Loss: 18248462.0\n",
      "Epoch [30/100], Training Loss: 479794.1039260115, Test Loss: 17904734.0\n",
      "Epoch [31/100], Training Loss: 467156.3032403294, Test Loss: 17582256.0\n",
      "Epoch [32/100], Training Loss: 455378.4941946567, Test Loss: 17301288.0\n",
      "Epoch [33/100], Training Loss: 444432.53164800664, Test Loss: 17034740.0\n",
      "Epoch [34/100], Training Loss: 434200.43477874534, Test Loss: 16803948.0\n",
      "Epoch [35/100], Training Loss: 424618.4098024406, Test Loss: 16582335.0\n",
      "Epoch [36/100], Training Loss: 415608.00365795864, Test Loss: 16403373.0\n",
      "Epoch [37/100], Training Loss: 407147.45122474973, Test Loss: 16213736.0\n",
      "Epoch [38/100], Training Loss: 399196.9990373793, Test Loss: 16077184.0\n",
      "Epoch [39/100], Training Loss: 391771.7811296724, Test Loss: 15928802.0\n",
      "Epoch [40/100], Training Loss: 384739.9116536343, Test Loss: 15824374.0\n",
      "Epoch [41/100], Training Loss: 378090.51882293704, Test Loss: 15696685.0\n",
      "Epoch [42/100], Training Loss: 371854.56297760794, Test Loss: 15624352.0\n",
      "Epoch [43/100], Training Loss: 365944.4284328535, Test Loss: 15518651.0\n",
      "Epoch [44/100], Training Loss: 360434.36469255376, Test Loss: 15475925.0\n",
      "Epoch [45/100], Training Loss: 355172.7816850305, Test Loss: 15385463.0\n",
      "Epoch [46/100], Training Loss: 350210.96287986496, Test Loss: 15351493.0\n",
      "Epoch [47/100], Training Loss: 345464.2071559742, Test Loss: 15266593.0\n",
      "Epoch [48/100], Training Loss: 341033.5321589361, Test Loss: 15232320.0\n",
      "Epoch [49/100], Training Loss: 336743.94849979266, Test Loss: 15125751.0\n",
      "Epoch [50/100], Training Loss: 332775.6351445412, Test Loss: 15140296.0\n",
      "Epoch [51/100], Training Loss: 328987.76371364255, Test Loss: 15024300.0\n",
      "Epoch [52/100], Training Loss: 325401.1847935549, Test Loss: 15041310.0\n",
      "Epoch [53/100], Training Loss: 321991.7208474024, Test Loss: 14938256.0\n",
      "Epoch [54/100], Training Loss: 318799.30545435695, Test Loss: 14947566.0\n",
      "Epoch [55/100], Training Loss: 315684.7084295954, Test Loss: 14858166.0\n",
      "Epoch [56/100], Training Loss: 312756.95267238317, Test Loss: 14866401.0\n",
      "Epoch [57/100], Training Loss: 309949.21685252653, Test Loss: 14788888.0\n",
      "Epoch [58/100], Training Loss: 307324.8442479711, Test Loss: 14794283.0\n",
      "Epoch [59/100], Training Loss: 304742.97500518337, Test Loss: 14728760.0\n",
      "Epoch [60/100], Training Loss: 302370.913419673, Test Loss: 14735041.0\n",
      "Epoch [61/100], Training Loss: 300004.59356673184, Test Loss: 14699826.0\n",
      "Epoch [62/100], Training Loss: 297857.0029841242, Test Loss: 14643000.0\n",
      "Epoch [63/100], Training Loss: 295679.2556276287, Test Loss: 14656402.0\n",
      "Epoch [64/100], Training Loss: 293711.3023036254, Test Loss: 14628195.0\n",
      "Epoch [65/100], Training Loss: 291696.6876740122, Test Loss: 14581538.0\n",
      "Epoch [66/100], Training Loss: 289867.5522073633, Test Loss: 14603846.0\n",
      "Epoch [67/100], Training Loss: 288001.7710517742, Test Loss: 14547963.0\n",
      "Epoch [68/100], Training Loss: 286293.1450817487, Test Loss: 14545101.0\n",
      "Epoch [69/100], Training Loss: 284539.82464383036, Test Loss: 14539937.0\n",
      "Epoch [70/100], Training Loss: 282963.1254183698, Test Loss: 14490210.0\n",
      "Epoch [71/100], Training Loss: 281319.386499615, Test Loss: 14511985.0\n",
      "Epoch [72/100], Training Loss: 279883.20561948343, Test Loss: 14500996.0\n",
      "Epoch [73/100], Training Loss: 278287.77059267816, Test Loss: 14454829.0\n",
      "Epoch [74/100], Training Loss: 276828.122897044, Test Loss: 14509619.0\n",
      "Epoch [75/100], Training Loss: 275327.72730436584, Test Loss: 14445410.0\n",
      "Epoch [76/100], Training Loss: 274010.49647902965, Test Loss: 14473847.0\n",
      "Epoch [77/100], Training Loss: 272605.68552292517, Test Loss: 14440801.0\n",
      "Epoch [78/100], Training Loss: 271330.3972439429, Test Loss: 14419982.0\n",
      "Epoch [79/100], Training Loss: 269998.37915407855, Test Loss: 14454967.0\n",
      "Epoch [80/100], Training Loss: 268874.01753820863, Test Loss: 14432564.0\n",
      "Epoch [81/100], Training Loss: 267641.7597446834, Test Loss: 14402612.0\n",
      "Epoch [82/100], Training Loss: 266559.6731902731, Test Loss: 14448236.0\n",
      "Epoch [83/100], Training Loss: 265382.2405144837, Test Loss: 14371177.0\n",
      "Epoch [84/100], Training Loss: 264349.2510773947, Test Loss: 14429013.0\n",
      "Epoch [85/100], Training Loss: 263174.6882960133, Test Loss: 14386315.0\n",
      "Epoch [86/100], Training Loss: 262189.8498459807, Test Loss: 14364025.0\n",
      "Epoch [87/100], Training Loss: 261060.41717019133, Test Loss: 14413897.0\n",
      "Epoch [88/100], Training Loss: 260149.9918140217, Test Loss: 14374257.0\n",
      "Epoch [89/100], Training Loss: 259021.68447885197, Test Loss: 14363441.0\n",
      "Epoch [90/100], Training Loss: 258118.84279293288, Test Loss: 14370506.0\n",
      "Epoch [91/100], Training Loss: 257073.36820241692, Test Loss: 14303894.0\n",
      "Epoch [92/100], Training Loss: 256185.97708962738, Test Loss: 14368186.0\n",
      "Epoch [93/100], Training Loss: 255200.23887802855, Test Loss: 14312365.0\n",
      "Epoch [94/100], Training Loss: 254354.42555980096, Test Loss: 14323891.0\n",
      "Epoch [95/100], Training Loss: 253370.3928640187, Test Loss: 14335181.0\n",
      "Epoch [96/100], Training Loss: 252570.70098409455, Test Loss: 14272204.0\n",
      "Epoch [97/100], Training Loss: 251627.4881375511, Test Loss: 14307230.0\n",
      "Epoch [98/100], Training Loss: 250889.00036653635, Test Loss: 14299690.0\n",
      "Epoch [99/100], Training Loss: 249988.86297094365, Test Loss: 14250221.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Training Loss: 249261.48573840412, Test Loss: 14310915.0\n",
      "Epoch [1/100], Training Loss: 4401280.839760678, Test Loss: 233558400.0\n",
      "Epoch [2/100], Training Loss: 2705591.403471358, Test Loss: 145760608.0\n",
      "Epoch [3/100], Training Loss: 2089155.5456430307, Test Loss: 126163400.0\n",
      "Epoch [4/100], Training Loss: 1814506.2448907054, Test Loss: 110668512.0\n",
      "Epoch [5/100], Training Loss: 1586605.673360583, Test Loss: 97801856.0\n",
      "Epoch [6/100], Training Loss: 1397582.9710325217, Test Loss: 87288312.0\n",
      "Epoch [7/100], Training Loss: 1241806.3752147385, Test Loss: 78601624.0\n",
      "Epoch [8/100], Training Loss: 1113621.0574018126, Test Loss: 71417792.0\n",
      "Epoch [9/100], Training Loss: 1008890.3903204786, Test Loss: 65492356.0\n",
      "Epoch [10/100], Training Loss: 923761.6646525679, Test Loss: 60656200.0\n",
      "Epoch [11/100], Training Loss: 854689.7511995735, Test Loss: 56702016.0\n",
      "Epoch [12/100], Training Loss: 797778.7630472129, Test Loss: 53382252.0\n",
      "Epoch [13/100], Training Loss: 749270.146318346, Test Loss: 50494316.0\n",
      "Epoch [14/100], Training Loss: 706592.4367632249, Test Loss: 47919420.0\n",
      "Epoch [15/100], Training Loss: 668243.0509448493, Test Loss: 45582080.0\n",
      "Epoch [16/100], Training Loss: 633380.552988567, Test Loss: 43429648.0\n",
      "Epoch [17/100], Training Loss: 601377.4627687933, Test Loss: 41430556.0\n",
      "Epoch [18/100], Training Loss: 571860.8363544814, Test Loss: 39564212.0\n",
      "Epoch [19/100], Training Loss: 544583.593033588, Test Loss: 37818776.0\n",
      "Epoch [20/100], Training Loss: 519390.3527634619, Test Loss: 36192088.0\n",
      "Epoch [21/100], Training Loss: 496149.3397310586, Test Loss: 34678552.0\n",
      "Epoch [22/100], Training Loss: 474694.2362715479, Test Loss: 33275334.0\n",
      "Epoch [23/100], Training Loss: 454911.39372667496, Test Loss: 31976300.0\n",
      "Epoch [24/100], Training Loss: 436647.4640127955, Test Loss: 30773370.0\n",
      "Epoch [25/100], Training Loss: 419788.7117173153, Test Loss: 29663654.0\n",
      "Epoch [26/100], Training Loss: 404240.661364848, Test Loss: 28638966.0\n",
      "Epoch [27/100], Training Loss: 389894.39923582727, Test Loss: 27690370.0\n",
      "Epoch [28/100], Training Loss: 376649.69202061486, Test Loss: 26810890.0\n",
      "Epoch [29/100], Training Loss: 364428.13525561284, Test Loss: 25997060.0\n",
      "Epoch [30/100], Training Loss: 353101.2435578461, Test Loss: 25242902.0\n",
      "Epoch [31/100], Training Loss: 342520.15413778805, Test Loss: 24537660.0\n",
      "Epoch [32/100], Training Loss: 332590.8652331023, Test Loss: 23880986.0\n",
      "Epoch [33/100], Training Loss: 323259.6665926189, Test Loss: 23265554.0\n",
      "Epoch [34/100], Training Loss: 314479.03782358865, Test Loss: 22693518.0\n",
      "Epoch [35/100], Training Loss: 306189.07960132696, Test Loss: 22159612.0\n",
      "Epoch [36/100], Training Loss: 298361.84110834665, Test Loss: 21664830.0\n",
      "Epoch [37/100], Training Loss: 290952.3694834429, Test Loss: 21204396.0\n",
      "Epoch [38/100], Training Loss: 283935.17761092354, Test Loss: 20775688.0\n",
      "Epoch [39/100], Training Loss: 277289.05303299567, Test Loss: 20374946.0\n",
      "Epoch [40/100], Training Loss: 271000.6303684616, Test Loss: 20001092.0\n",
      "Epoch [41/100], Training Loss: 265031.5157721699, Test Loss: 19652986.0\n",
      "Epoch [42/100], Training Loss: 259361.61418162432, Test Loss: 19328724.0\n",
      "Epoch [43/100], Training Loss: 253959.76837865057, Test Loss: 19022644.0\n",
      "Epoch [44/100], Training Loss: 248814.54456193352, Test Loss: 18737682.0\n",
      "Epoch [45/100], Training Loss: 243910.69086547007, Test Loss: 18465784.0\n",
      "Epoch [46/100], Training Loss: 239226.7914223091, Test Loss: 18213778.0\n",
      "Epoch [47/100], Training Loss: 234751.47421657486, Test Loss: 17974730.0\n",
      "Epoch [48/100], Training Loss: 230478.2478674249, Test Loss: 17751872.0\n",
      "Epoch [49/100], Training Loss: 226388.53797168416, Test Loss: 17540552.0\n",
      "Epoch [50/100], Training Loss: 222480.31671701913, Test Loss: 17344344.0\n",
      "Epoch [51/100], Training Loss: 218731.75608672472, Test Loss: 17161482.0\n",
      "Epoch [52/100], Training Loss: 215146.77796042888, Test Loss: 16989078.0\n",
      "Epoch [53/100], Training Loss: 211713.505420295, Test Loss: 16828160.0\n",
      "Epoch [54/100], Training Loss: 208423.04625022213, Test Loss: 16680550.0\n",
      "Epoch [55/100], Training Loss: 205290.2195367573, Test Loss: 16538833.0\n",
      "Epoch [56/100], Training Loss: 202284.00540548546, Test Loss: 16411503.0\n",
      "Epoch [57/100], Training Loss: 199414.34266334932, Test Loss: 16286399.0\n",
      "Epoch [58/100], Training Loss: 196660.38898761923, Test Loss: 16181579.0\n",
      "Epoch [59/100], Training Loss: 194023.7556868669, Test Loss: 16068138.0\n",
      "Epoch [60/100], Training Loss: 191482.95973283573, Test Loss: 15982853.0\n",
      "Epoch [61/100], Training Loss: 189047.15314554825, Test Loss: 15888065.0\n",
      "Epoch [62/100], Training Loss: 186688.9224868195, Test Loss: 15809522.0\n",
      "Epoch [63/100], Training Loss: 184444.33902020022, Test Loss: 15727615.0\n",
      "Epoch [64/100], Training Loss: 182281.81621349446, Test Loss: 15663700.0\n",
      "Epoch [65/100], Training Loss: 180212.87069782597, Test Loss: 15593373.0\n",
      "Epoch [66/100], Training Loss: 178225.46981813875, Test Loss: 15539773.0\n",
      "Epoch [67/100], Training Loss: 176305.10693975474, Test Loss: 15472777.0\n",
      "Epoch [68/100], Training Loss: 174461.72737100883, Test Loss: 15428833.0\n",
      "Epoch [69/100], Training Loss: 172695.77358420708, Test Loss: 15365589.0\n",
      "Epoch [70/100], Training Loss: 170977.3327927848, Test Loss: 15332521.0\n",
      "Epoch [71/100], Training Loss: 169345.95188377466, Test Loss: 15269987.0\n",
      "Epoch [72/100], Training Loss: 167767.19945500858, Test Loss: 15239263.0\n",
      "Epoch [73/100], Training Loss: 166271.57790563355, Test Loss: 15179816.0\n",
      "Epoch [74/100], Training Loss: 164825.60011255258, Test Loss: 15153291.0\n",
      "Epoch [75/100], Training Loss: 163442.16263846928, Test Loss: 15092823.0\n",
      "Epoch [76/100], Training Loss: 162101.10730999347, Test Loss: 15062269.0\n",
      "Epoch [77/100], Training Loss: 160817.11279693147, Test Loss: 15004540.0\n",
      "Epoch [78/100], Training Loss: 159577.45676352113, Test Loss: 14995976.0\n",
      "Epoch [79/100], Training Loss: 158389.42381227415, Test Loss: 14915674.0\n",
      "Epoch [80/100], Training Loss: 157237.14599253598, Test Loss: 14929484.0\n",
      "Epoch [81/100], Training Loss: 156134.6047109176, Test Loss: 14855388.0\n",
      "Epoch [82/100], Training Loss: 155040.71106569516, Test Loss: 14857937.0\n",
      "Epoch [83/100], Training Loss: 154009.12919110243, Test Loss: 14797749.0\n",
      "Epoch [84/100], Training Loss: 152991.56931609503, Test Loss: 14789544.0\n",
      "Epoch [85/100], Training Loss: 152038.78520229843, Test Loss: 14745414.0\n",
      "Epoch [86/100], Training Loss: 151102.41687400037, Test Loss: 14734508.0\n",
      "Epoch [87/100], Training Loss: 150205.89340826965, Test Loss: 14702108.0\n",
      "Epoch [88/100], Training Loss: 149332.90865470056, Test Loss: 14675830.0\n",
      "Epoch [89/100], Training Loss: 148490.72523843375, Test Loss: 14671058.0\n",
      "Epoch [90/100], Training Loss: 147680.12627362122, Test Loss: 14644527.0\n",
      "Epoch [91/100], Training Loss: 146887.5357058231, Test Loss: 14595347.0\n",
      "Epoch [92/100], Training Loss: 146117.37081630234, Test Loss: 14625284.0\n",
      "Epoch [93/100], Training Loss: 145379.1628383982, Test Loss: 14577603.0\n",
      "Epoch [94/100], Training Loss: 144652.28758663585, Test Loss: 14579315.0\n",
      "Epoch [95/100], Training Loss: 143952.0792014691, Test Loss: 14548804.0\n",
      "Epoch [96/100], Training Loss: 143264.50208074166, Test Loss: 14543397.0\n",
      "Epoch [97/100], Training Loss: 142597.52459866123, Test Loss: 14522160.0\n",
      "Epoch [98/100], Training Loss: 141947.52281411053, Test Loss: 14507288.0\n",
      "Epoch [99/100], Training Loss: 141320.74355044132, Test Loss: 14506391.0\n",
      "Epoch [100/100], Training Loss: 140704.32738729933, Test Loss: 14473537.0\n",
      "Epoch [1/100], Training Loss: 2316337.258693205, Test Loss: 291361088.0\n",
      "Epoch [2/100], Training Loss: 2043174.835614004, Test Loss: 223686880.0\n",
      "Epoch [3/100], Training Loss: 1442838.443457141, Test Loss: 159350768.0\n",
      "Epoch [4/100], Training Loss: 1151112.5997275042, Test Loss: 140654912.0\n",
      "Epoch [5/100], Training Loss: 1038397.1783662105, Test Loss: 128831616.0\n",
      "Epoch [6/100], Training Loss: 950809.5634144896, Test Loss: 118641616.0\n",
      "Epoch [7/100], Training Loss: 872873.3748000711, Test Loss: 109468944.0\n",
      "Epoch [8/100], Training Loss: 801386.6647710444, Test Loss: 100999392.0\n",
      "Epoch [9/100], Training Loss: 736564.1220306854, Test Loss: 93653400.0\n",
      "Epoch [10/100], Training Loss: 680749.9456193354, Test Loss: 87353352.0\n",
      "Epoch [11/100], Training Loss: 632469.8125703454, Test Loss: 81872904.0\n",
      "Epoch [12/100], Training Loss: 590527.2250459096, Test Loss: 77083104.0\n",
      "Epoch [13/100], Training Loss: 554155.7265564837, Test Loss: 72897824.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/100], Training Loss: 522599.79254783486, Test Loss: 69222480.0\n",
      "Epoch [15/100], Training Loss: 495154.8293347551, Test Loss: 65990296.0\n",
      "Epoch [16/100], Training Loss: 471187.24080326996, Test Loss: 63147472.0\n",
      "Epoch [17/100], Training Loss: 450168.8125111072, Test Loss: 60634620.0\n",
      "Epoch [18/100], Training Loss: 431573.1564480777, Test Loss: 58388464.0\n",
      "Epoch [19/100], Training Loss: 414952.1657484746, Test Loss: 56371432.0\n",
      "Epoch [20/100], Training Loss: 399909.1822166933, Test Loss: 54526184.0\n",
      "Epoch [21/100], Training Loss: 386066.5535217108, Test Loss: 52817444.0\n",
      "Epoch [22/100], Training Loss: 373216.9463894319, Test Loss: 51230088.0\n",
      "Epoch [23/100], Training Loss: 361198.6182098217, Test Loss: 49736612.0\n",
      "Epoch [24/100], Training Loss: 349900.2728511344, Test Loss: 48332308.0\n",
      "Epoch [25/100], Training Loss: 339257.7661868373, Test Loss: 47011664.0\n",
      "Epoch [26/100], Training Loss: 329197.46756708727, Test Loss: 45765788.0\n",
      "Epoch [27/100], Training Loss: 319681.77015579643, Test Loss: 44584012.0\n",
      "Epoch [28/100], Training Loss: 310669.03027071856, Test Loss: 43456056.0\n",
      "Epoch [29/100], Training Loss: 302125.17647058825, Test Loss: 42378720.0\n",
      "Epoch [30/100], Training Loss: 294037.13198270247, Test Loss: 41355988.0\n",
      "Epoch [31/100], Training Loss: 286387.57455127063, Test Loss: 40388080.0\n",
      "Epoch [32/100], Training Loss: 279166.03015224217, Test Loss: 39468144.0\n",
      "Epoch [33/100], Training Loss: 272322.9245897755, Test Loss: 38588956.0\n",
      "Epoch [34/100], Training Loss: 265825.5698714531, Test Loss: 37751172.0\n",
      "Epoch [35/100], Training Loss: 259643.41739233458, Test Loss: 36951996.0\n",
      "Epoch [36/100], Training Loss: 253715.01350630887, Test Loss: 36186744.0\n",
      "Epoch [37/100], Training Loss: 248020.68745927373, Test Loss: 35452276.0\n",
      "Epoch [38/100], Training Loss: 242561.32723179908, Test Loss: 34747928.0\n",
      "Epoch [39/100], Training Loss: 237341.693856999, Test Loss: 34073132.0\n",
      "Epoch [40/100], Training Loss: 232342.21302055565, Test Loss: 33427254.0\n",
      "Epoch [41/100], Training Loss: 227545.21693027666, Test Loss: 32803562.0\n",
      "Epoch [42/100], Training Loss: 222940.98981103016, Test Loss: 32204170.0\n",
      "Epoch [43/100], Training Loss: 218536.69989929505, Test Loss: 31631940.0\n",
      "Epoch [44/100], Training Loss: 214337.0921450151, Test Loss: 31086302.0\n",
      "Epoch [45/100], Training Loss: 210312.23162134943, Test Loss: 30564516.0\n",
      "Epoch [46/100], Training Loss: 206455.98868550442, Test Loss: 30064770.0\n",
      "Epoch [47/100], Training Loss: 202755.47728215152, Test Loss: 29582060.0\n",
      "Epoch [48/100], Training Loss: 199158.23132515847, Test Loss: 29110544.0\n",
      "Epoch [49/100], Training Loss: 195607.837746579, Test Loss: 28647764.0\n",
      "Epoch [50/100], Training Loss: 192156.04173330963, Test Loss: 28205290.0\n",
      "Epoch [51/100], Training Loss: 188864.24118831824, Test Loss: 27781776.0\n",
      "Epoch [52/100], Training Loss: 185723.354303655, Test Loss: 27376382.0\n",
      "Epoch [53/100], Training Loss: 182697.13144955868, Test Loss: 26984650.0\n",
      "Epoch [54/100], Training Loss: 179798.65396007345, Test Loss: 26614002.0\n",
      "Epoch [55/100], Training Loss: 176993.49688999465, Test Loss: 26250190.0\n",
      "Epoch [56/100], Training Loss: 174206.26269178366, Test Loss: 25889290.0\n",
      "Epoch [57/100], Training Loss: 171405.05754990818, Test Loss: 25527344.0\n",
      "Epoch [58/100], Training Loss: 168631.08921272436, Test Loss: 25172380.0\n",
      "Epoch [59/100], Training Loss: 165960.83694686333, Test Loss: 24838662.0\n",
      "Epoch [60/100], Training Loss: 163441.9887151235, Test Loss: 24524560.0\n",
      "Epoch [61/100], Training Loss: 161056.65073159174, Test Loss: 24226152.0\n",
      "Epoch [62/100], Training Loss: 158776.51353592795, Test Loss: 23941732.0\n",
      "Epoch [63/100], Training Loss: 156576.3268171317, Test Loss: 23666678.0\n",
      "Epoch [64/100], Training Loss: 154436.72445352763, Test Loss: 23400842.0\n",
      "Epoch [65/100], Training Loss: 152326.52502813813, Test Loss: 23129766.0\n",
      "Epoch [66/100], Training Loss: 150201.0721521237, Test Loss: 22864786.0\n",
      "Epoch [67/100], Training Loss: 148094.2106510278, Test Loss: 22604890.0\n",
      "Epoch [68/100], Training Loss: 146079.92864759197, Test Loss: 22368858.0\n",
      "Epoch [69/100], Training Loss: 144165.68742965464, Test Loss: 22137122.0\n",
      "Epoch [70/100], Training Loss: 142321.7583970144, Test Loss: 21919936.0\n",
      "Epoch [71/100], Training Loss: 140517.22925182158, Test Loss: 21702934.0\n",
      "Epoch [72/100], Training Loss: 138739.60023102898, Test Loss: 21498186.0\n",
      "Epoch [73/100], Training Loss: 137008.3785024584, Test Loss: 21295862.0\n",
      "Epoch [74/100], Training Loss: 135307.19915881762, Test Loss: 21099400.0\n",
      "Epoch [75/100], Training Loss: 133601.7046087317, Test Loss: 20901338.0\n",
      "Epoch [76/100], Training Loss: 131892.83516971744, Test Loss: 20709906.0\n",
      "Epoch [77/100], Training Loss: 130192.31813873586, Test Loss: 20518618.0\n",
      "Epoch [78/100], Training Loss: 128545.988863219, Test Loss: 20343378.0\n",
      "Epoch [79/100], Training Loss: 126993.76864522244, Test Loss: 20177266.0\n",
      "Epoch [80/100], Training Loss: 125526.21236893549, Test Loss: 20022514.0\n",
      "Epoch [81/100], Training Loss: 124125.42210177123, Test Loss: 19875008.0\n",
      "Epoch [82/100], Training Loss: 122768.18147621586, Test Loss: 19729596.0\n",
      "Epoch [83/100], Training Loss: 121428.48966293466, Test Loss: 19585108.0\n",
      "Epoch [84/100], Training Loss: 120085.45534920917, Test Loss: 19443280.0\n",
      "Epoch [85/100], Training Loss: 118715.62599964457, Test Loss: 19298276.0\n",
      "Epoch [86/100], Training Loss: 117347.99576446893, Test Loss: 19157784.0\n",
      "Epoch [87/100], Training Loss: 116025.69253894911, Test Loss: 19023952.0\n",
      "Epoch [88/100], Training Loss: 114759.2840767727, Test Loss: 18899456.0\n",
      "Epoch [89/100], Training Loss: 113549.48984064926, Test Loss: 18781522.0\n",
      "Epoch [90/100], Training Loss: 112403.96377584267, Test Loss: 18671660.0\n",
      "Epoch [91/100], Training Loss: 111307.65239026124, Test Loss: 18566544.0\n",
      "Epoch [92/100], Training Loss: 110247.70395711155, Test Loss: 18468416.0\n",
      "Epoch [93/100], Training Loss: 109208.03585391861, Test Loss: 18370262.0\n",
      "Epoch [94/100], Training Loss: 108174.33001599432, Test Loss: 18274726.0\n",
      "Epoch [95/100], Training Loss: 107142.78558734672, Test Loss: 18178520.0\n",
      "Epoch [96/100], Training Loss: 106119.64377110361, Test Loss: 18083308.0\n",
      "Epoch [97/100], Training Loss: 105110.62878383981, Test Loss: 17989944.0\n",
      "Epoch [98/100], Training Loss: 104125.97772643801, Test Loss: 17898720.0\n",
      "Epoch [99/100], Training Loss: 103174.78089271963, Test Loss: 17818162.0\n",
      "Epoch [100/100], Training Loss: 102271.24219536758, Test Loss: 17739722.0\n",
      "Epoch [1/100], Training Loss: 1497804.9613174575, Test Loss: 298762080.0\n",
      "Epoch [2/100], Training Loss: 1468758.1105384752, Test Loss: 285709248.0\n",
      "Epoch [3/100], Training Loss: 1347671.7654167407, Test Loss: 248943264.0\n",
      "Epoch [4/100], Training Loss: 1123733.4797701559, Test Loss: 199617072.0\n",
      "Epoch [5/100], Training Loss: 906391.406670221, Test Loss: 165647584.0\n",
      "Epoch [6/100], Training Loss: 784447.7516734791, Test Loss: 149761536.0\n",
      "Epoch [7/100], Training Loss: 721731.2050233991, Test Loss: 140191520.0\n",
      "Epoch [8/100], Training Loss: 676994.567146496, Test Loss: 132518912.0\n",
      "Epoch [9/100], Training Loss: 638723.9346010308, Test Loss: 125783024.0\n",
      "Epoch [10/100], Training Loss: 604224.7727030389, Test Loss: 119706960.0\n",
      "Epoch [11/100], Training Loss: 572589.7349683075, Test Loss: 114171456.0\n",
      "Epoch [12/100], Training Loss: 543147.7355606896, Test Loss: 108961384.0\n",
      "Epoch [13/100], Training Loss: 514614.2773532374, Test Loss: 103987416.0\n",
      "Epoch [14/100], Training Loss: 487975.4448196197, Test Loss: 99566120.0\n",
      "Epoch [15/100], Training Loss: 464433.4167407144, Test Loss: 95738256.0\n",
      "Epoch [16/100], Training Loss: 443503.51400983357, Test Loss: 92323128.0\n",
      "Epoch [17/100], Training Loss: 424649.6693323855, Test Loss: 89246560.0\n",
      "Epoch [18/100], Training Loss: 407578.94792962505, Test Loss: 86452896.0\n",
      "Epoch [19/100], Training Loss: 392069.9372075114, Test Loss: 83899864.0\n",
      "Epoch [20/100], Training Loss: 377973.4797701558, Test Loss: 81560232.0\n",
      "Epoch [21/100], Training Loss: 365155.5102185889, Test Loss: 79411328.0\n",
      "Epoch [22/100], Training Loss: 353502.974112908, Test Loss: 77430696.0\n",
      "Epoch [23/100], Training Loss: 342889.7950358391, Test Loss: 75601912.0\n",
      "Epoch [24/100], Training Loss: 333226.862745098, Test Loss: 73911048.0\n",
      "Epoch [25/100], Training Loss: 324420.55458799837, Test Loss: 72340416.0\n",
      "Epoch [26/100], Training Loss: 316369.18026183284, Test Loss: 70875536.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/100], Training Loss: 308973.08062318584, Test Loss: 69500472.0\n",
      "Epoch [28/100], Training Loss: 302152.3380131509, Test Loss: 68208008.0\n",
      "Epoch [29/100], Training Loss: 295835.9665007997, Test Loss: 66988196.0\n",
      "Epoch [30/100], Training Loss: 289946.7684082697, Test Loss: 65829368.0\n",
      "Epoch [31/100], Training Loss: 284414.7109620283, Test Loss: 64719900.0\n",
      "Epoch [32/100], Training Loss: 279183.9705289971, Test Loss: 63655468.0\n",
      "Epoch [33/100], Training Loss: 274200.50039060187, Test Loss: 62629648.0\n",
      "Epoch [34/100], Training Loss: 269418.28246866394, Test Loss: 61633100.0\n",
      "Epoch [35/100], Training Loss: 264811.4912332097, Test Loss: 60664864.0\n",
      "Epoch [36/100], Training Loss: 260347.17150846365, Test Loss: 59722860.0\n",
      "Epoch [37/100], Training Loss: 256008.80088561104, Test Loss: 58804668.0\n",
      "Epoch [38/100], Training Loss: 251782.4024013684, Test Loss: 57908680.0\n",
      "Epoch [39/100], Training Loss: 247646.7112878384, Test Loss: 57031728.0\n",
      "Epoch [40/100], Training Loss: 243604.219218352, Test Loss: 56172456.0\n",
      "Epoch [41/100], Training Loss: 239649.51945974765, Test Loss: 55334400.0\n",
      "Epoch [42/100], Training Loss: 235778.7792340501, Test Loss: 54517152.0\n",
      "Epoch [43/100], Training Loss: 231988.5505153723, Test Loss: 53723176.0\n",
      "Epoch [44/100], Training Loss: 228284.44433090457, Test Loss: 52947212.0\n",
      "Epoch [45/100], Training Loss: 224661.80201113678, Test Loss: 52185676.0\n",
      "Epoch [46/100], Training Loss: 221123.55158165985, Test Loss: 51443880.0\n",
      "Epoch [47/100], Training Loss: 217671.89474853384, Test Loss: 50721744.0\n",
      "Epoch [48/100], Training Loss: 214307.9953942302, Test Loss: 50020720.0\n",
      "Epoch [49/100], Training Loss: 211030.14611841715, Test Loss: 49338736.0\n",
      "Epoch [50/100], Training Loss: 207833.16784402583, Test Loss: 48675756.0\n",
      "Epoch [51/100], Training Loss: 204722.45991055033, Test Loss: 48029200.0\n",
      "Epoch [52/100], Training Loss: 201692.7498815236, Test Loss: 47402128.0\n",
      "Epoch [53/100], Training Loss: 198743.0252132575, Test Loss: 46790064.0\n",
      "Epoch [54/100], Training Loss: 195865.54417318286, Test Loss: 46186692.0\n",
      "Epoch [55/100], Training Loss: 193059.85557357385, Test Loss: 45598408.0\n",
      "Epoch [56/100], Training Loss: 190325.1728089272, Test Loss: 45023580.0\n",
      "Epoch [57/100], Training Loss: 187656.54441198686, Test Loss: 44462156.0\n",
      "Epoch [58/100], Training Loss: 185048.7462383745, Test Loss: 43912900.0\n",
      "Epoch [59/100], Training Loss: 182515.44570819265, Test Loss: 43379360.0\n",
      "Epoch [60/100], Training Loss: 180066.22905559506, Test Loss: 42866288.0\n",
      "Epoch [61/100], Training Loss: 177704.1414737723, Test Loss: 42367224.0\n",
      "Epoch [62/100], Training Loss: 175419.5013134219, Test Loss: 41881460.0\n",
      "Epoch [63/100], Training Loss: 173210.11985414446, Test Loss: 41410912.0\n",
      "Epoch [64/100], Training Loss: 171076.37054348268, Test Loss: 40953048.0\n",
      "Epoch [65/100], Training Loss: 169014.22596780403, Test Loss: 40512544.0\n",
      "Epoch [66/100], Training Loss: 167022.9638251307, Test Loss: 40084400.0\n",
      "Epoch [67/100], Training Loss: 165103.9468863617, Test Loss: 39670984.0\n",
      "Epoch [68/100], Training Loss: 163251.30030280323, Test Loss: 39272120.0\n",
      "Epoch [69/100], Training Loss: 161455.7479187377, Test Loss: 38886472.0\n",
      "Epoch [70/100], Training Loss: 159719.03316845218, Test Loss: 38510604.0\n",
      "Epoch [71/100], Training Loss: 158043.97309698595, Test Loss: 38146524.0\n",
      "Epoch [72/100], Training Loss: 156422.73786225836, Test Loss: 37793888.0\n",
      "Epoch [73/100], Training Loss: 154849.40590489702, Test Loss: 37449828.0\n",
      "Epoch [74/100], Training Loss: 153320.5620034173, Test Loss: 37112068.0\n",
      "Epoch [75/100], Training Loss: 151836.8164031261, Test Loss: 36786872.0\n",
      "Epoch [76/100], Training Loss: 150395.63264577687, Test Loss: 36467720.0\n",
      "Epoch [77/100], Training Loss: 148986.86611947697, Test Loss: 36156152.0\n",
      "Epoch [78/100], Training Loss: 147597.79258161914, Test Loss: 35846032.0\n",
      "Epoch [79/100], Training Loss: 146228.41946567147, Test Loss: 35538676.0\n",
      "Epoch [80/100], Training Loss: 144862.57192674087, Test Loss: 35229172.0\n",
      "Epoch [81/100], Training Loss: 143497.4191037631, Test Loss: 34921528.0\n",
      "Epoch [82/100], Training Loss: 142150.11297603443, Test Loss: 34619852.0\n",
      "Epoch [83/100], Training Loss: 140851.8021481251, Test Loss: 34330900.0\n",
      "Epoch [84/100], Training Loss: 139607.10487081445, Test Loss: 34053260.0\n",
      "Epoch [85/100], Training Loss: 138408.24087130133, Test Loss: 33785184.0\n",
      "Epoch [86/100], Training Loss: 137243.4449341623, Test Loss: 33523966.0\n",
      "Epoch [87/100], Training Loss: 136100.85422868165, Test Loss: 33268254.0\n",
      "Epoch [88/100], Training Loss: 134961.35985214516, Test Loss: 33008928.0\n",
      "Epoch [89/100], Training Loss: 133808.28535085672, Test Loss: 32749404.0\n",
      "Epoch [90/100], Training Loss: 132630.8317107621, Test Loss: 32485506.0\n",
      "Epoch [91/100], Training Loss: 131442.31836782108, Test Loss: 32225434.0\n",
      "Epoch [92/100], Training Loss: 130275.98736282655, Test Loss: 31974110.0\n",
      "Epoch [93/100], Training Loss: 129166.70469620061, Test Loss: 31740232.0\n",
      "Epoch [94/100], Training Loss: 128120.28264441162, Test Loss: 31518014.0\n",
      "Epoch [95/100], Training Loss: 127116.10557033426, Test Loss: 31305322.0\n",
      "Epoch [96/100], Training Loss: 126142.72884641016, Test Loss: 31097936.0\n",
      "Epoch [97/100], Training Loss: 125193.11887393742, Test Loss: 30897202.0\n",
      "Epoch [98/100], Training Loss: 124259.10936157884, Test Loss: 30699106.0\n",
      "Epoch [99/100], Training Loss: 123337.49439689962, Test Loss: 30506314.0\n",
      "Epoch [100/100], Training Loss: 122428.9301234561, Test Loss: 30315348.0\n",
      "Epoch [1/100], Training Loss: 1164397.9896925536, Test Loss: 299335616.0\n",
      "Epoch [2/100], Training Loss: 1150016.5686866893, Test Loss: 290706848.0\n",
      "Epoch [3/100], Training Loss: 1083431.7180261833, Test Loss: 263637808.0\n",
      "Epoch [4/100], Training Loss: 941757.9726319531, Test Loss: 219684192.0\n",
      "Epoch [5/100], Training Loss: 768731.0746993661, Test Loss: 178823296.0\n",
      "Epoch [6/100], Training Loss: 643936.3634855755, Test Loss: 156209056.0\n",
      "Epoch [7/100], Training Loss: 580715.766127599, Test Loss: 144863952.0\n",
      "Epoch [8/100], Training Loss: 543600.1381434749, Test Loss: 136908368.0\n",
      "Epoch [9/100], Training Loss: 514290.32948285056, Test Loss: 130114536.0\n",
      "Epoch [10/100], Training Loss: 488246.3951187726, Test Loss: 123938792.0\n",
      "Epoch [11/100], Training Loss: 464170.6815946922, Test Loss: 118188064.0\n",
      "Epoch [12/100], Training Loss: 441540.7155974172, Test Loss: 112765368.0\n",
      "Epoch [13/100], Training Loss: 419907.20573425747, Test Loss: 107540048.0\n",
      "Epoch [14/100], Training Loss: 399106.25721225044, Test Loss: 102617912.0\n",
      "Epoch [15/100], Training Loss: 379674.874474261, Test Loss: 98076384.0\n",
      "Epoch [16/100], Training Loss: 361707.43700017774, Test Loss: 93872208.0\n",
      "Epoch [17/100], Training Loss: 344904.4049523132, Test Loss: 89962944.0\n",
      "Epoch [18/100], Training Loss: 329446.729696108, Test Loss: 86408648.0\n",
      "Epoch [19/100], Training Loss: 315487.52064451156, Test Loss: 83202248.0\n",
      "Epoch [20/100], Training Loss: 302869.76328416565, Test Loss: 80270648.0\n",
      "Epoch [21/100], Training Loss: 291371.00005923817, Test Loss: 77579920.0\n",
      "Epoch [22/100], Training Loss: 280866.35199336533, Test Loss: 75103520.0\n",
      "Epoch [23/100], Training Loss: 271244.4036490729, Test Loss: 72820648.0\n",
      "Epoch [24/100], Training Loss: 262412.05935667315, Test Loss: 70708120.0\n",
      "Epoch [25/100], Training Loss: 254293.32883123038, Test Loss: 68749632.0\n",
      "Epoch [26/100], Training Loss: 246823.1475623482, Test Loss: 66937028.0\n",
      "Epoch [27/100], Training Loss: 239931.77584266334, Test Loss: 65257444.0\n",
      "Epoch [28/100], Training Loss: 233558.22830401044, Test Loss: 63692060.0\n",
      "Epoch [29/100], Training Loss: 227642.22048456845, Test Loss: 62229152.0\n",
      "Epoch [30/100], Training Loss: 222128.40826965228, Test Loss: 60858796.0\n",
      "Epoch [31/100], Training Loss: 216949.74800071085, Test Loss: 59567648.0\n",
      "Epoch [32/100], Training Loss: 212052.70919969195, Test Loss: 58341908.0\n",
      "Epoch [33/100], Training Loss: 207397.68390498194, Test Loss: 57176704.0\n",
      "Epoch [34/100], Training Loss: 202961.0334695812, Test Loss: 56066924.0\n",
      "Epoch [35/100], Training Loss: 198719.74634204136, Test Loss: 55005116.0\n",
      "Epoch [36/100], Training Loss: 194664.79616136485, Test Loss: 53984708.0\n",
      "Epoch [37/100], Training Loss: 190771.64314910254, Test Loss: 53008484.0\n",
      "Epoch [38/100], Training Loss: 187032.32948285053, Test Loss: 52073384.0\n",
      "Epoch [39/100], Training Loss: 183446.0830519519, Test Loss: 51175820.0\n",
      "Epoch [40/100], Training Loss: 180002.84615840294, Test Loss: 50313572.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/100], Training Loss: 176696.7082518808, Test Loss: 49488016.0\n",
      "Epoch [42/100], Training Loss: 173520.60458503643, Test Loss: 48697936.0\n",
      "Epoch [43/100], Training Loss: 170458.32900894497, Test Loss: 47934452.0\n",
      "Epoch [44/100], Training Loss: 167508.1771222084, Test Loss: 47198212.0\n",
      "Epoch [45/100], Training Loss: 164662.84213020556, Test Loss: 46484932.0\n",
      "Epoch [46/100], Training Loss: 161915.10230436586, Test Loss: 45794672.0\n",
      "Epoch [47/100], Training Loss: 159264.18577098512, Test Loss: 45129636.0\n",
      "Epoch [48/100], Training Loss: 156712.80220366092, Test Loss: 44486872.0\n",
      "Epoch [49/100], Training Loss: 154261.24885966472, Test Loss: 43866872.0\n",
      "Epoch [50/100], Training Loss: 151900.97221728571, Test Loss: 43268032.0\n",
      "Epoch [51/100], Training Loss: 149622.19986967597, Test Loss: 42689336.0\n",
      "Epoch [52/100], Training Loss: 147419.46744861087, Test Loss: 42128232.0\n",
      "Epoch [53/100], Training Loss: 145288.08222261714, Test Loss: 41584376.0\n",
      "Epoch [54/100], Training Loss: 143221.60482198923, Test Loss: 41053092.0\n",
      "Epoch [55/100], Training Loss: 141211.61892068005, Test Loss: 40534540.0\n",
      "Epoch [56/100], Training Loss: 139250.8081867188, Test Loss: 40030320.0\n",
      "Epoch [57/100], Training Loss: 137336.33516971744, Test Loss: 39538596.0\n",
      "Epoch [58/100], Training Loss: 135464.75955215923, Test Loss: 39055992.0\n",
      "Epoch [59/100], Training Loss: 133641.60446656003, Test Loss: 38583324.0\n",
      "Epoch [60/100], Training Loss: 131869.08192642615, Test Loss: 38125624.0\n",
      "Epoch [61/100], Training Loss: 130154.66820685979, Test Loss: 37683544.0\n",
      "Epoch [62/100], Training Loss: 128491.53924530538, Test Loss: 37253648.0\n",
      "Epoch [63/100], Training Loss: 126871.66939162371, Test Loss: 36834772.0\n",
      "Epoch [64/100], Training Loss: 125291.3912090516, Test Loss: 36424608.0\n",
      "Epoch [65/100], Training Loss: 123750.044250933, Test Loss: 36026124.0\n",
      "Epoch [66/100], Training Loss: 122252.75439843611, Test Loss: 35639528.0\n",
      "Epoch [67/100], Training Loss: 120806.55239618506, Test Loss: 35263936.0\n",
      "Epoch [68/100], Training Loss: 119409.47923701203, Test Loss: 34899668.0\n",
      "Epoch [69/100], Training Loss: 118057.16521533084, Test Loss: 34547016.0\n",
      "Epoch [70/100], Training Loss: 116744.2765831408, Test Loss: 34204416.0\n",
      "Epoch [71/100], Training Loss: 115467.39683668029, Test Loss: 33872808.0\n",
      "Epoch [72/100], Training Loss: 114222.12475564244, Test Loss: 33547662.0\n",
      "Epoch [73/100], Training Loss: 112999.64699958533, Test Loss: 33229830.0\n",
      "Epoch [74/100], Training Loss: 111802.53811977964, Test Loss: 32919458.0\n",
      "Epoch [75/100], Training Loss: 110631.74586813577, Test Loss: 32615222.0\n",
      "Epoch [76/100], Training Loss: 109481.99342456016, Test Loss: 32315408.0\n",
      "Epoch [77/100], Training Loss: 108354.5756767964, Test Loss: 32023068.0\n",
      "Epoch [78/100], Training Loss: 107255.13684023458, Test Loss: 31737822.0\n",
      "Epoch [79/100], Training Loss: 106183.58616195722, Test Loss: 31460012.0\n",
      "Epoch [80/100], Training Loss: 105141.38818790356, Test Loss: 31190110.0\n",
      "Epoch [81/100], Training Loss: 104123.34245601564, Test Loss: 30925264.0\n",
      "Epoch [82/100], Training Loss: 103123.22196552336, Test Loss: 30664428.0\n",
      "Epoch [83/100], Training Loss: 102141.84278182572, Test Loss: 30409786.0\n",
      "Epoch [84/100], Training Loss: 101181.29080030804, Test Loss: 30161740.0\n",
      "Epoch [85/100], Training Loss: 100246.77483561401, Test Loss: 29920856.0\n",
      "Epoch [86/100], Training Loss: 99340.43391979148, Test Loss: 29686578.0\n",
      "Epoch [87/100], Training Loss: 98459.34026420236, Test Loss: 29459126.0\n",
      "Epoch [88/100], Training Loss: 97599.67288667733, Test Loss: 29237468.0\n",
      "Epoch [89/100], Training Loss: 96757.04650198449, Test Loss: 29022622.0\n",
      "Epoch [90/100], Training Loss: 95928.3541851786, Test Loss: 28812240.0\n",
      "Epoch [91/100], Training Loss: 95110.78105562467, Test Loss: 28609720.0\n",
      "Epoch [92/100], Training Loss: 94303.48160653989, Test Loss: 28406778.0\n",
      "Epoch [93/100], Training Loss: 93505.0607783899, Test Loss: 28209604.0\n",
      "Epoch [94/100], Training Loss: 92713.5166163142, Test Loss: 28013260.0\n",
      "Epoch [95/100], Training Loss: 91917.66927314733, Test Loss: 27815334.0\n",
      "Epoch [96/100], Training Loss: 91111.11711391505, Test Loss: 27613670.0\n",
      "Epoch [97/100], Training Loss: 90297.6236597358, Test Loss: 27411554.0\n",
      "Epoch [98/100], Training Loss: 89498.89206800544, Test Loss: 27217276.0\n",
      "Epoch [99/100], Training Loss: 88729.82062674013, Test Loss: 27030370.0\n",
      "Epoch [100/100], Training Loss: 87987.09946093241, Test Loss: 26850660.0\n",
      "Epoch [1/100], Training Loss: 582423.4687518512, Test Loss: 300033856.0\n",
      "Epoch [2/100], Training Loss: 581934.5346839642, Test Loss: 299457824.0\n",
      "Epoch [3/100], Training Loss: 579568.7942657425, Test Loss: 297346592.0\n",
      "Epoch [4/100], Training Loss: 572959.1810911676, Test Loss: 292363360.0\n",
      "Epoch [5/100], Training Loss: 559564.6637047569, Test Loss: 283315712.0\n",
      "Epoch [6/100], Training Loss: 537518.8484094545, Test Loss: 269563904.0\n",
      "Epoch [7/100], Training Loss: 506401.65203483205, Test Loss: 251392944.0\n",
      "Epoch [8/100], Training Loss: 467842.83300752327, Test Loss: 230223632.0\n",
      "Epoch [9/100], Training Loss: 425604.75611634384, Test Loss: 208443040.0\n",
      "Epoch [10/100], Training Loss: 384750.9313429299, Test Loss: 188716496.0\n",
      "Epoch [11/100], Training Loss: 349902.91902138497, Test Loss: 172936048.0\n",
      "Epoch [12/100], Training Loss: 323301.61530715006, Test Loss: 161448320.0\n",
      "Epoch [13/100], Training Loss: 304245.64895444585, Test Loss: 153317120.0\n",
      "Epoch [14/100], Training Loss: 290453.6271547894, Test Loss: 147289904.0\n",
      "Epoch [15/100], Training Loss: 279764.64522243943, Test Loss: 142441872.0\n",
      "Epoch [16/100], Training Loss: 270793.39375629404, Test Loss: 138249968.0\n",
      "Epoch [17/100], Training Loss: 262806.20437177893, Test Loss: 134451568.0\n",
      "Epoch [18/100], Training Loss: 255439.15786979444, Test Loss: 130915648.0\n",
      "Epoch [19/100], Training Loss: 248504.98998874475, Test Loss: 127571904.0\n",
      "Epoch [20/100], Training Loss: 241898.40317516736, Test Loss: 124379080.0\n",
      "Epoch [21/100], Training Loss: 235555.22208399978, Test Loss: 121311416.0\n",
      "Epoch [22/100], Training Loss: 229434.71358331852, Test Loss: 118352376.0\n",
      "Epoch [23/100], Training Loss: 223510.67922516438, Test Loss: 115491408.0\n",
      "Epoch [24/100], Training Loss: 217766.60553284758, Test Loss: 112721320.0\n",
      "Epoch [25/100], Training Loss: 212187.7879272555, Test Loss: 110033952.0\n",
      "Epoch [26/100], Training Loss: 206757.4226645341, Test Loss: 107416040.0\n",
      "Epoch [27/100], Training Loss: 201432.49238789172, Test Loss: 104835288.0\n",
      "Epoch [28/100], Training Loss: 196151.35146022154, Test Loss: 102289760.0\n",
      "Epoch [29/100], Training Loss: 190939.20052129612, Test Loss: 99800944.0\n",
      "Epoch [30/100], Training Loss: 185875.09460340027, Test Loss: 97408528.0\n",
      "Epoch [31/100], Training Loss: 181018.1695397192, Test Loss: 95121848.0\n",
      "Epoch [32/100], Training Loss: 176403.6696878147, Test Loss: 92962688.0\n",
      "Epoch [33/100], Training Loss: 172048.14169776672, Test Loss: 90938384.0\n",
      "Epoch [34/100], Training Loss: 167936.5350393934, Test Loss: 89018912.0\n",
      "Epoch [35/100], Training Loss: 164046.99934837985, Test Loss: 87200832.0\n",
      "Epoch [36/100], Training Loss: 160357.34968307565, Test Loss: 85472704.0\n",
      "Epoch [37/100], Training Loss: 156852.1919317576, Test Loss: 83828136.0\n",
      "Epoch [38/100], Training Loss: 153518.32971980333, Test Loss: 82258824.0\n",
      "Epoch [39/100], Training Loss: 150339.0998163616, Test Loss: 80754480.0\n",
      "Epoch [40/100], Training Loss: 147284.62152716072, Test Loss: 79308392.0\n",
      "Epoch [41/100], Training Loss: 144357.03927492446, Test Loss: 77920488.0\n",
      "Epoch [42/100], Training Loss: 141551.31876073693, Test Loss: 76582272.0\n",
      "Epoch [43/100], Training Loss: 138850.3228481725, Test Loss: 75290248.0\n",
      "Epoch [44/100], Training Loss: 136259.04744979562, Test Loss: 74044288.0\n",
      "Epoch [45/100], Training Loss: 133793.0260055684, Test Loss: 72860504.0\n",
      "Epoch [46/100], Training Loss: 131452.50139209762, Test Loss: 71729576.0\n",
      "Epoch [47/100], Training Loss: 129220.86866891771, Test Loss: 70645064.0\n",
      "Epoch [48/100], Training Loss: 127080.0002369528, Test Loss: 69600112.0\n",
      "Epoch [49/100], Training Loss: 125027.75949292103, Test Loss: 68595184.0\n",
      "Epoch [50/100], Training Loss: 123061.18737041645, Test Loss: 67628216.0\n",
      "Epoch [51/100], Training Loss: 121178.01954860493, Test Loss: 66698428.0\n",
      "Epoch [52/100], Training Loss: 119368.8745927374, Test Loss: 65801768.0\n",
      "Epoch [53/100], Training Loss: 117626.11906877554, Test Loss: 64936724.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [54/100], Training Loss: 115949.13429299212, Test Loss: 64101876.0\n",
      "Epoch [55/100], Training Loss: 114335.61187133464, Test Loss: 63294944.0\n",
      "Epoch [56/100], Training Loss: 112780.317279782, Test Loss: 62513324.0\n",
      "Epoch [57/100], Training Loss: 111278.19133937563, Test Loss: 61756924.0\n",
      "Epoch [58/100], Training Loss: 109826.12665126474, Test Loss: 61023432.0\n",
      "Epoch [59/100], Training Loss: 108420.09857235946, Test Loss: 60311408.0\n",
      "Epoch [60/100], Training Loss: 107054.4060186008, Test Loss: 59619580.0\n",
      "Epoch [61/100], Training Loss: 105726.77163675138, Test Loss: 58947724.0\n",
      "Epoch [62/100], Training Loss: 104436.67673716012, Test Loss: 58294884.0\n",
      "Epoch [63/100], Training Loss: 103179.86991291985, Test Loss: 57660848.0\n",
      "Epoch [64/100], Training Loss: 101955.76541674072, Test Loss: 57042684.0\n",
      "Epoch [65/100], Training Loss: 100763.47396481251, Test Loss: 56440552.0\n",
      "Epoch [66/100], Training Loss: 99601.45524554233, Test Loss: 55853140.0\n",
      "Epoch [67/100], Training Loss: 98468.41679995261, Test Loss: 55279732.0\n",
      "Epoch [68/100], Training Loss: 97362.98442035425, Test Loss: 54719480.0\n",
      "Epoch [69/100], Training Loss: 96283.83993839228, Test Loss: 54171248.0\n",
      "Epoch [70/100], Training Loss: 95229.6051181802, Test Loss: 53634576.0\n",
      "Epoch [71/100], Training Loss: 94198.22830401042, Test Loss: 53109748.0\n",
      "Epoch [72/100], Training Loss: 93188.19074699366, Test Loss: 52595488.0\n",
      "Epoch [73/100], Training Loss: 92199.01427640543, Test Loss: 52092964.0\n",
      "Epoch [74/100], Training Loss: 91229.6456371068, Test Loss: 51599848.0\n",
      "Epoch [75/100], Training Loss: 90278.96155441029, Test Loss: 51114596.0\n",
      "Epoch [76/100], Training Loss: 89346.71784846869, Test Loss: 50638144.0\n",
      "Epoch [77/100], Training Loss: 88431.61400390972, Test Loss: 50170648.0\n",
      "Epoch [78/100], Training Loss: 87532.42521177656, Test Loss: 49713464.0\n",
      "Epoch [79/100], Training Loss: 86650.19809253006, Test Loss: 49265580.0\n",
      "Epoch [80/100], Training Loss: 85786.01208459215, Test Loss: 48826076.0\n",
      "Epoch [81/100], Training Loss: 84939.02174041822, Test Loss: 48395524.0\n",
      "Epoch [82/100], Training Loss: 84107.92026538712, Test Loss: 47973132.0\n",
      "Epoch [83/100], Training Loss: 83293.5516853267, Test Loss: 47559900.0\n",
      "Epoch [84/100], Training Loss: 82495.50026657188, Test Loss: 47155688.0\n",
      "Epoch [85/100], Training Loss: 81713.2268230555, Test Loss: 46758008.0\n",
      "Epoch [86/100], Training Loss: 80943.60369646348, Test Loss: 46364136.0\n",
      "Epoch [87/100], Training Loss: 80184.9593033588, Test Loss: 45977508.0\n",
      "Epoch [88/100], Training Loss: 79443.08453290683, Test Loss: 45601468.0\n",
      "Epoch [89/100], Training Loss: 78720.41348261359, Test Loss: 45235356.0\n",
      "Epoch [90/100], Training Loss: 78014.28517267935, Test Loss: 44877324.0\n",
      "Epoch [91/100], Training Loss: 77324.44772229133, Test Loss: 44526200.0\n",
      "Epoch [92/100], Training Loss: 76650.02251051478, Test Loss: 44181632.0\n",
      "Epoch [93/100], Training Loss: 75991.01617202772, Test Loss: 43844668.0\n",
      "Epoch [94/100], Training Loss: 75347.06332563236, Test Loss: 43513380.0\n",
      "Epoch [95/100], Training Loss: 74717.89763639595, Test Loss: 43188680.0\n",
      "Epoch [96/100], Training Loss: 74103.27527990048, Test Loss: 42871224.0\n",
      "Epoch [97/100], Training Loss: 73502.65398969255, Test Loss: 42561620.0\n",
      "Epoch [98/100], Training Loss: 72915.41780700196, Test Loss: 42258332.0\n",
      "Epoch [99/100], Training Loss: 72340.94994372371, Test Loss: 41961704.0\n",
      "Epoch [100/100], Training Loss: 71777.93104673894, Test Loss: 41671348.0\n",
      "Epoch [1/100], Training Loss: 7153312.676855637, Test Loss: 148555568.0\n",
      "Epoch [2/100], Training Loss: 4067346.9474557196, Test Loss: 117505568.0\n",
      "Epoch [3/100], Training Loss: 3255647.0677092588, Test Loss: 96108408.0\n",
      "Epoch [4/100], Training Loss: 2665180.3603459513, Test Loss: 80510968.0\n",
      "Epoch [5/100], Training Loss: 2228310.3264617026, Test Loss: 68917000.0\n",
      "Epoch [6/100], Training Loss: 1910514.0386529234, Test Loss: 60454772.0\n",
      "Epoch [7/100], Training Loss: 1682421.4418873289, Test Loss: 54340672.0\n",
      "Epoch [8/100], Training Loss: 1515091.0343877731, Test Loss: 49680504.0\n",
      "Epoch [9/100], Training Loss: 1382928.0370534922, Test Loss: 45842412.0\n",
      "Epoch [10/100], Training Loss: 1271585.0074343937, Test Loss: 42517740.0\n",
      "Epoch [11/100], Training Loss: 1174619.4550085894, Test Loss: 39553992.0\n",
      "Epoch [12/100], Training Loss: 1089055.762751022, Test Loss: 36886568.0\n",
      "Epoch [13/100], Training Loss: 1013478.1263698833, Test Loss: 34488408.0\n",
      "Epoch [14/100], Training Loss: 946793.755124104, Test Loss: 32363676.0\n",
      "Epoch [15/100], Training Loss: 888027.6951454298, Test Loss: 30486132.0\n",
      "Epoch [16/100], Training Loss: 836274.664682187, Test Loss: 28828108.0\n",
      "Epoch [17/100], Training Loss: 790674.450328772, Test Loss: 27361318.0\n",
      "Epoch [18/100], Training Loss: 750471.9554084474, Test Loss: 26058052.0\n",
      "Epoch [19/100], Training Loss: 714960.0558320004, Test Loss: 24902524.0\n",
      "Epoch [20/100], Training Loss: 683220.3870031396, Test Loss: 23872902.0\n",
      "Epoch [21/100], Training Loss: 654528.5619483443, Test Loss: 22946384.0\n",
      "Epoch [22/100], Training Loss: 628425.2623215449, Test Loss: 22118070.0\n",
      "Epoch [23/100], Training Loss: 604583.0294413838, Test Loss: 21379110.0\n",
      "Epoch [24/100], Training Loss: 582742.6675404301, Test Loss: 20721360.0\n",
      "Epoch [25/100], Training Loss: 562631.7712146792, Test Loss: 20136538.0\n",
      "Epoch [26/100], Training Loss: 544101.1780626148, Test Loss: 19613028.0\n",
      "Epoch [27/100], Training Loss: 527018.1773221373, Test Loss: 19145304.0\n",
      "Epoch [28/100], Training Loss: 511162.99794887746, Test Loss: 18717858.0\n",
      "Epoch [29/100], Training Loss: 496462.5450358391, Test Loss: 18335952.0\n",
      "Epoch [30/100], Training Loss: 482785.1486064214, Test Loss: 17978280.0\n",
      "Epoch [31/100], Training Loss: 470033.3644778153, Test Loss: 17661584.0\n",
      "Epoch [32/100], Training Loss: 458131.60309667676, Test Loss: 17362322.0\n",
      "Epoch [33/100], Training Loss: 447000.21561222675, Test Loss: 17094770.0\n",
      "Epoch [34/100], Training Loss: 436614.41289023164, Test Loss: 16852614.0\n",
      "Epoch [35/100], Training Loss: 426854.81510277826, Test Loss: 16635330.0\n",
      "Epoch [36/100], Training Loss: 417729.54565784015, Test Loss: 16438901.0\n",
      "Epoch [37/100], Training Loss: 409185.847535691, Test Loss: 16257421.0\n",
      "Epoch [38/100], Training Loss: 401165.69351637934, Test Loss: 16113865.0\n",
      "Epoch [39/100], Training Loss: 393674.52325099224, Test Loss: 15968985.0\n",
      "Epoch [40/100], Training Loss: 386595.84606954566, Test Loss: 15847113.0\n",
      "Epoch [41/100], Training Loss: 379921.99000355427, Test Loss: 15736964.0\n",
      "Epoch [42/100], Training Loss: 373603.0862211954, Test Loss: 15638630.0\n",
      "Epoch [43/100], Training Loss: 367627.80974912626, Test Loss: 15556410.0\n",
      "Epoch [44/100], Training Loss: 361974.63574432797, Test Loss: 15481785.0\n",
      "Epoch [45/100], Training Loss: 356642.82412179373, Test Loss: 15416543.0\n",
      "Epoch [46/100], Training Loss: 351547.6745231325, Test Loss: 15349897.0\n",
      "Epoch [47/100], Training Loss: 346788.0293229074, Test Loss: 15292809.0\n",
      "Epoch [48/100], Training Loss: 342291.959066406, Test Loss: 15226552.0\n",
      "Epoch [49/100], Training Loss: 338000.4848350216, Test Loss: 15186542.0\n",
      "Epoch [50/100], Training Loss: 333984.32082296663, Test Loss: 15088519.0\n",
      "Epoch [51/100], Training Loss: 330134.0120623778, Test Loss: 15071876.0\n",
      "Epoch [52/100], Training Loss: 326532.54134826135, Test Loss: 14988638.0\n",
      "Epoch [53/100], Training Loss: 323073.1862522955, Test Loss: 14970443.0\n",
      "Epoch [54/100], Training Loss: 319807.8643001007, Test Loss: 14895118.0\n",
      "Epoch [55/100], Training Loss: 316710.1643934009, Test Loss: 14879113.0\n",
      "Epoch [56/100], Training Loss: 313749.3877288075, Test Loss: 14811458.0\n",
      "Epoch [57/100], Training Loss: 310837.18819604884, Test Loss: 14806150.0\n",
      "Epoch [58/100], Training Loss: 308123.32918665954, Test Loss: 14731588.0\n",
      "Epoch [59/100], Training Loss: 305477.85912046087, Test Loss: 14743871.0\n",
      "Epoch [60/100], Training Loss: 303034.13507419586, Test Loss: 14675110.0\n",
      "Epoch [61/100], Training Loss: 300665.84127125173, Test Loss: 14684076.0\n",
      "Epoch [62/100], Training Loss: 298466.3896392394, Test Loss: 14627271.0\n",
      "Epoch [63/100], Training Loss: 296355.3107117469, Test Loss: 14598064.0\n",
      "Epoch [64/100], Training Loss: 294329.60114181624, Test Loss: 14612927.0\n",
      "Epoch [65/100], Training Loss: 292416.0768541556, Test Loss: 14584904.0\n",
      "Epoch [66/100], Training Loss: 290589.546516794, Test Loss: 14553486.0\n",
      "Epoch [67/100], Training Loss: 288818.2807853504, Test Loss: 14549726.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [68/100], Training Loss: 287165.3957814999, Test Loss: 14491280.0\n",
      "Epoch [69/100], Training Loss: 285479.9806143001, Test Loss: 14530246.0\n",
      "Epoch [70/100], Training Loss: 283947.4936207867, Test Loss: 14513359.0\n",
      "Epoch [71/100], Training Loss: 282349.7064451158, Test Loss: 14453246.0\n",
      "Epoch [72/100], Training Loss: 280850.314765861, Test Loss: 14477536.0\n",
      "Epoch [73/100], Training Loss: 279350.28028182575, Test Loss: 14466873.0\n",
      "Epoch [74/100], Training Loss: 277882.85446656006, Test Loss: 14421364.0\n",
      "Epoch [75/100], Training Loss: 276359.01150331734, Test Loss: 14462122.0\n",
      "Epoch [76/100], Training Loss: 274958.6106310349, Test Loss: 14420193.0\n",
      "Epoch [77/100], Training Loss: 273531.3141623719, Test Loss: 14386770.0\n",
      "Epoch [78/100], Training Loss: 272200.15183120075, Test Loss: 14417341.0\n",
      "Epoch [79/100], Training Loss: 270825.42544872937, Test Loss: 14394642.0\n",
      "Epoch [80/100], Training Loss: 269557.1270770393, Test Loss: 14365372.0\n",
      "Epoch [81/100], Training Loss: 268285.5038245661, Test Loss: 14416193.0\n",
      "Epoch [82/100], Training Loss: 267073.64021310944, Test Loss: 14346645.0\n",
      "Epoch [83/100], Training Loss: 265819.25447618624, Test Loss: 14381904.0\n",
      "Epoch [84/100], Training Loss: 264665.0740107221, Test Loss: 14349821.0\n",
      "Epoch [85/100], Training Loss: 263510.42139091284, Test Loss: 14324392.0\n",
      "Epoch [86/100], Training Loss: 262430.4181439192, Test Loss: 14361796.0\n",
      "Epoch [87/100], Training Loss: 261343.37687711036, Test Loss: 14342160.0\n",
      "Epoch [88/100], Training Loss: 260305.7801707541, Test Loss: 14311882.0\n",
      "Epoch [89/100], Training Loss: 259244.98048471654, Test Loss: 14369359.0\n",
      "Epoch [90/100], Training Loss: 258302.0007923109, Test Loss: 14293107.0\n",
      "Epoch [91/100], Training Loss: 257276.28627599077, Test Loss: 14346420.0\n",
      "Epoch [92/100], Training Loss: 256370.27435060128, Test Loss: 14299508.0\n",
      "Epoch [93/100], Training Loss: 255415.76450225106, Test Loss: 14305629.0\n",
      "Epoch [94/100], Training Loss: 254492.55396229489, Test Loss: 14336417.0\n",
      "Epoch [95/100], Training Loss: 253566.35347061785, Test Loss: 14297344.0\n",
      "Epoch [96/100], Training Loss: 252669.88379687222, Test Loss: 14288632.0\n",
      "Epoch [97/100], Training Loss: 251771.11222676383, Test Loss: 14279289.0\n",
      "Epoch [98/100], Training Loss: 250911.23805980096, Test Loss: 14254338.0\n",
      "Epoch [99/100], Training Loss: 250023.88219373854, Test Loss: 14327081.0\n",
      "Epoch [100/100], Training Loss: 249241.94299804515, Test Loss: 14246100.0\n",
      "Epoch [1/100], Training Loss: 4362118.422842248, Test Loss: 224838064.0\n",
      "Epoch [2/100], Training Loss: 2614081.332030093, Test Loss: 143064416.0\n",
      "Epoch [3/100], Training Loss: 2045578.7633434038, Test Loss: 123225608.0\n",
      "Epoch [4/100], Training Loss: 1764131.740181269, Test Loss: 107365376.0\n",
      "Epoch [5/100], Training Loss: 1532398.547242462, Test Loss: 94429864.0\n",
      "Epoch [6/100], Training Loss: 1343125.1891475623, Test Loss: 83970840.0\n",
      "Epoch [7/100], Training Loss: 1188957.5078490612, Test Loss: 75408048.0\n",
      "Epoch [8/100], Training Loss: 1064049.1396244299, Test Loss: 68431792.0\n",
      "Epoch [9/100], Training Loss: 963810.9890409335, Test Loss: 62782668.0\n",
      "Epoch [10/100], Training Loss: 883452.5131212606, Test Loss: 58220264.0\n",
      "Epoch [11/100], Training Loss: 818321.7289852496, Test Loss: 54466408.0\n",
      "Epoch [12/100], Training Loss: 764007.7931402168, Test Loss: 51273816.0\n",
      "Epoch [13/100], Training Loss: 717107.4140157574, Test Loss: 48467388.0\n",
      "Epoch [14/100], Training Loss: 675506.31307387, Test Loss: 45945484.0\n",
      "Epoch [15/100], Training Loss: 637976.8015520407, Test Loss: 43640032.0\n",
      "Epoch [16/100], Training Loss: 603750.3597831882, Test Loss: 41511336.0\n",
      "Epoch [17/100], Training Loss: 572398.8827676085, Test Loss: 39534296.0\n",
      "Epoch [18/100], Training Loss: 543567.621912209, Test Loss: 37693908.0\n",
      "Epoch [19/100], Training Loss: 517054.72966648894, Test Loss: 35982272.0\n",
      "Epoch [20/100], Training Loss: 492707.2049641609, Test Loss: 34399684.0\n",
      "Epoch [21/100], Training Loss: 470363.45287601446, Test Loss: 32941280.0\n",
      "Epoch [22/100], Training Loss: 449854.28161838755, Test Loss: 31597660.0\n",
      "Epoch [23/100], Training Loss: 431026.086102719, Test Loss: 30360652.0\n",
      "Epoch [24/100], Training Loss: 413740.0202002251, Test Loss: 29223872.0\n",
      "Epoch [25/100], Training Loss: 397875.7395592678, Test Loss: 28179890.0\n",
      "Epoch [26/100], Training Loss: 383283.2902523547, Test Loss: 27216794.0\n",
      "Epoch [27/100], Training Loss: 369882.1257330727, Test Loss: 26331270.0\n",
      "Epoch [28/100], Training Loss: 357574.211465553, Test Loss: 25514910.0\n",
      "Epoch [29/100], Training Loss: 346172.4016201647, Test Loss: 24758572.0\n",
      "Epoch [30/100], Training Loss: 335536.9541940643, Test Loss: 24053428.0\n",
      "Epoch [31/100], Training Loss: 325574.7156418459, Test Loss: 23398088.0\n",
      "Epoch [32/100], Training Loss: 316204.62971684145, Test Loss: 22787796.0\n",
      "Epoch [33/100], Training Loss: 307393.87982050824, Test Loss: 22222752.0\n",
      "Epoch [34/100], Training Loss: 299102.1038889876, Test Loss: 21699746.0\n",
      "Epoch [35/100], Training Loss: 291282.73002191813, Test Loss: 21216006.0\n",
      "Epoch [36/100], Training Loss: 283884.7285409632, Test Loss: 20764918.0\n",
      "Epoch [37/100], Training Loss: 276901.327661276, Test Loss: 20344250.0\n",
      "Epoch [38/100], Training Loss: 270299.65425626445, Test Loss: 19953322.0\n",
      "Epoch [39/100], Training Loss: 264050.29856051184, Test Loss: 19590370.0\n",
      "Epoch [40/100], Training Loss: 258120.83358509568, Test Loss: 19252042.0\n",
      "Epoch [41/100], Training Loss: 252511.17975830816, Test Loss: 18934552.0\n",
      "Epoch [42/100], Training Loss: 247184.46453112966, Test Loss: 18637404.0\n",
      "Epoch [43/100], Training Loss: 242117.7105917896, Test Loss: 18363678.0\n",
      "Epoch [44/100], Training Loss: 237293.70152834547, Test Loss: 18099724.0\n",
      "Epoch [45/100], Training Loss: 232684.1922575677, Test Loss: 17856556.0\n",
      "Epoch [46/100], Training Loss: 228285.15815117588, Test Loss: 17625144.0\n",
      "Epoch [47/100], Training Loss: 224089.4262780641, Test Loss: 17412194.0\n",
      "Epoch [48/100], Training Loss: 220085.34056039335, Test Loss: 17211758.0\n",
      "Epoch [49/100], Training Loss: 216252.5056276287, Test Loss: 17025440.0\n",
      "Epoch [50/100], Training Loss: 212593.66721461999, Test Loss: 16851096.0\n",
      "Epoch [51/100], Training Loss: 209094.21845566022, Test Loss: 16691492.0\n",
      "Epoch [52/100], Training Loss: 205769.89864344528, Test Loss: 16536902.0\n",
      "Epoch [53/100], Training Loss: 202584.07665422664, Test Loss: 16401114.0\n",
      "Epoch [54/100], Training Loss: 199544.1634826136, Test Loss: 16266721.0\n",
      "Epoch [55/100], Training Loss: 196628.15742550796, Test Loss: 16157293.0\n",
      "Epoch [56/100], Training Loss: 193852.0987648836, Test Loss: 16036517.0\n",
      "Epoch [57/100], Training Loss: 191185.58528819383, Test Loss: 15949376.0\n",
      "Epoch [58/100], Training Loss: 188652.7382856466, Test Loss: 15848666.0\n",
      "Epoch [59/100], Training Loss: 186218.31406610983, Test Loss: 15769201.0\n",
      "Epoch [60/100], Training Loss: 183897.66690361945, Test Loss: 15686858.0\n",
      "Epoch [61/100], Training Loss: 181666.44908476985, Test Loss: 15622387.0\n",
      "Epoch [62/100], Training Loss: 179533.83586576625, Test Loss: 15552344.0\n",
      "Epoch [63/100], Training Loss: 177474.00142171673, Test Loss: 15500269.0\n",
      "Epoch [64/100], Training Loss: 175507.1235116403, Test Loss: 15438720.0\n",
      "Epoch [65/100], Training Loss: 173604.64454860493, Test Loss: 15394576.0\n",
      "Epoch [66/100], Training Loss: 171800.12752502813, Test Loss: 15332020.0\n",
      "Epoch [67/100], Training Loss: 170048.87669569338, Test Loss: 15295674.0\n",
      "Epoch [68/100], Training Loss: 168397.2433282981, Test Loss: 15232804.0\n",
      "Epoch [69/100], Training Loss: 166792.97553462474, Test Loss: 15196918.0\n",
      "Epoch [70/100], Training Loss: 165268.4196878147, Test Loss: 15140021.0\n",
      "Epoch [71/100], Training Loss: 163792.05461761745, Test Loss: 15105747.0\n",
      "Epoch [72/100], Training Loss: 162373.79042266455, Test Loss: 15044937.0\n",
      "Epoch [73/100], Training Loss: 161004.31230377348, Test Loss: 15034507.0\n",
      "Epoch [74/100], Training Loss: 159700.31405870506, Test Loss: 14944834.0\n",
      "Epoch [75/100], Training Loss: 158437.6647044014, Test Loss: 14962821.0\n",
      "Epoch [76/100], Training Loss: 157232.36226378768, Test Loss: 14882840.0\n",
      "Epoch [77/100], Training Loss: 156050.7830919377, Test Loss: 14883915.0\n",
      "Epoch [78/100], Training Loss: 154942.434526983, Test Loss: 14826262.0\n",
      "Epoch [79/100], Training Loss: 153850.5715079083, Test Loss: 14819464.0\n",
      "Epoch [80/100], Training Loss: 152811.61915022807, Test Loss: 14768245.0\n",
      "Epoch [81/100], Training Loss: 151777.82973461287, Test Loss: 14765244.0\n",
      "Epoch [82/100], Training Loss: 150806.80767578934, Test Loss: 14718923.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [83/100], Training Loss: 149871.3444775191, Test Loss: 14709285.0\n",
      "Epoch [84/100], Training Loss: 148975.0710191932, Test Loss: 14687999.0\n",
      "Epoch [85/100], Training Loss: 148093.88163467805, Test Loss: 14654136.0\n",
      "Epoch [86/100], Training Loss: 147250.08732450684, Test Loss: 14637491.0\n",
      "Epoch [87/100], Training Loss: 146414.67506368106, Test Loss: 14666502.0\n",
      "Epoch [88/100], Training Loss: 145636.5730555062, Test Loss: 14593973.0\n",
      "Epoch [89/100], Training Loss: 144849.69779633908, Test Loss: 14610920.0\n",
      "Epoch [90/100], Training Loss: 144109.50756767963, Test Loss: 14568460.0\n",
      "Epoch [91/100], Training Loss: 143381.31636899474, Test Loss: 14574202.0\n",
      "Epoch [92/100], Training Loss: 142700.70238729933, Test Loss: 14538304.0\n",
      "Epoch [93/100], Training Loss: 142018.5016586695, Test Loss: 14547698.0\n",
      "Epoch [94/100], Training Loss: 141379.38184201173, Test Loss: 14522144.0\n",
      "Epoch [95/100], Training Loss: 140739.91457111546, Test Loss: 14515320.0\n",
      "Epoch [96/100], Training Loss: 140130.04316243113, Test Loss: 14501768.0\n",
      "Epoch [97/100], Training Loss: 139517.79896036963, Test Loss: 14512504.0\n",
      "Epoch [98/100], Training Loss: 138932.98535335585, Test Loss: 14445246.0\n",
      "Epoch [99/100], Training Loss: 138357.513402642, Test Loss: 14484450.0\n",
      "Epoch [100/100], Training Loss: 137812.89148302824, Test Loss: 14455914.0\n",
      "Epoch [1/100], Training Loss: 2317694.6768556363, Test Loss: 292181184.0\n",
      "Epoch [2/100], Training Loss: 2067735.3697055862, Test Loss: 229417760.0\n",
      "Epoch [3/100], Training Loss: 1483934.91688881, Test Loss: 162726032.0\n",
      "Epoch [4/100], Training Loss: 1169003.0374977787, Test Loss: 142469168.0\n",
      "Epoch [5/100], Training Loss: 1052850.5062496297, Test Loss: 130679512.0\n",
      "Epoch [6/100], Training Loss: 966271.5625851549, Test Loss: 120642160.0\n",
      "Epoch [7/100], Training Loss: 889594.2820922931, Test Loss: 111609624.0\n",
      "Epoch [8/100], Training Loss: 820111.6440969137, Test Loss: 103480480.0\n",
      "Epoch [9/100], Training Loss: 757630.6894141342, Test Loss: 96260400.0\n",
      "Epoch [10/100], Training Loss: 702170.1816243114, Test Loss: 89922424.0\n",
      "Epoch [11/100], Training Loss: 653360.1997511996, Test Loss: 84358952.0\n",
      "Epoch [12/100], Training Loss: 610455.4153189977, Test Loss: 79449296.0\n",
      "Epoch [13/100], Training Loss: 572704.3884840945, Test Loss: 75102032.0\n",
      "Epoch [14/100], Training Loss: 539451.4570226882, Test Loss: 71235984.0\n",
      "Epoch [15/100], Training Loss: 510096.16195723, Test Loss: 67789792.0\n",
      "Epoch [16/100], Training Loss: 484117.88033884246, Test Loss: 64717376.0\n",
      "Epoch [17/100], Training Loss: 461089.3945856288, Test Loss: 61981844.0\n",
      "Epoch [18/100], Training Loss: 440615.2168710384, Test Loss: 59536856.0\n",
      "Epoch [19/100], Training Loss: 422293.7174930395, Test Loss: 57337372.0\n",
      "Epoch [20/100], Training Loss: 405756.43700017774, Test Loss: 55330696.0\n",
      "Epoch [21/100], Training Loss: 390608.8989988745, Test Loss: 53485152.0\n",
      "Epoch [22/100], Training Loss: 376609.64492624847, Test Loss: 51770276.0\n",
      "Epoch [23/100], Training Loss: 363541.5275161424, Test Loss: 50159340.0\n",
      "Epoch [24/100], Training Loss: 351252.8700313962, Test Loss: 48638264.0\n",
      "Epoch [25/100], Training Loss: 339653.8801018897, Test Loss: 47197108.0\n",
      "Epoch [26/100], Training Loss: 328655.188140513, Test Loss: 45825668.0\n",
      "Epoch [27/100], Training Loss: 318212.3872400924, Test Loss: 44517328.0\n",
      "Epoch [28/100], Training Loss: 308287.1493987323, Test Loss: 43264780.0\n",
      "Epoch [29/100], Training Loss: 298825.6735382975, Test Loss: 42063812.0\n",
      "Epoch [30/100], Training Loss: 289801.11237485934, Test Loss: 40910036.0\n",
      "Epoch [31/100], Training Loss: 281191.4969492329, Test Loss: 39800924.0\n",
      "Epoch [32/100], Training Loss: 272977.20940702566, Test Loss: 38735960.0\n",
      "Epoch [33/100], Training Loss: 265138.1610094189, Test Loss: 37713148.0\n",
      "Epoch [34/100], Training Loss: 257652.92707777975, Test Loss: 36733572.0\n",
      "Epoch [35/100], Training Loss: 250511.78348439073, Test Loss: 35796256.0\n",
      "Epoch [36/100], Training Loss: 243699.7249570523, Test Loss: 34899184.0\n",
      "Epoch [37/100], Training Loss: 237204.57529174813, Test Loss: 34041992.0\n",
      "Epoch [38/100], Training Loss: 231014.10464427463, Test Loss: 33223096.0\n",
      "Epoch [39/100], Training Loss: 225108.93608198565, Test Loss: 32438988.0\n",
      "Epoch [40/100], Training Loss: 219485.6938866181, Test Loss: 31690184.0\n",
      "Epoch [41/100], Training Loss: 214130.23532373674, Test Loss: 30975106.0\n",
      "Epoch [42/100], Training Loss: 209026.3024406137, Test Loss: 30293942.0\n",
      "Epoch [43/100], Training Loss: 204163.61598838933, Test Loss: 29645094.0\n",
      "Epoch [44/100], Training Loss: 199523.07037497777, Test Loss: 29026390.0\n",
      "Epoch [45/100], Training Loss: 195094.83795391268, Test Loss: 28436216.0\n",
      "Epoch [46/100], Training Loss: 190869.1770629702, Test Loss: 27871452.0\n",
      "Epoch [47/100], Training Loss: 186834.3582726142, Test Loss: 27331952.0\n",
      "Epoch [48/100], Training Loss: 182980.20040281973, Test Loss: 26817612.0\n",
      "Epoch [49/100], Training Loss: 179303.45468277947, Test Loss: 26327390.0\n",
      "Epoch [50/100], Training Loss: 175787.789585925, Test Loss: 25859302.0\n",
      "Epoch [51/100], Training Loss: 172416.27394704105, Test Loss: 25412818.0\n",
      "Epoch [52/100], Training Loss: 169183.20875540547, Test Loss: 24985812.0\n",
      "Epoch [53/100], Training Loss: 166073.91262365974, Test Loss: 24576230.0\n",
      "Epoch [54/100], Training Loss: 163080.66308275575, Test Loss: 24184658.0\n",
      "Epoch [55/100], Training Loss: 160200.6111012381, Test Loss: 23809410.0\n",
      "Epoch [56/100], Training Loss: 157425.96036964635, Test Loss: 23452276.0\n",
      "Epoch [57/100], Training Loss: 154748.3174574966, Test Loss: 23108788.0\n",
      "Epoch [58/100], Training Loss: 152161.87693264618, Test Loss: 22782184.0\n",
      "Epoch [59/100], Training Loss: 149664.43661512944, Test Loss: 22467460.0\n",
      "Epoch [60/100], Training Loss: 147251.10828742373, Test Loss: 22166368.0\n",
      "Epoch [61/100], Training Loss: 144919.19989929505, Test Loss: 21877122.0\n",
      "Epoch [62/100], Training Loss: 142661.67472306144, Test Loss: 21598722.0\n",
      "Epoch [63/100], Training Loss: 140475.74868195012, Test Loss: 21331544.0\n",
      "Epoch [64/100], Training Loss: 138359.38051655708, Test Loss: 21075456.0\n",
      "Epoch [65/100], Training Loss: 136313.86413719566, Test Loss: 20829964.0\n",
      "Epoch [66/100], Training Loss: 134331.8765475979, Test Loss: 20592930.0\n",
      "Epoch [67/100], Training Loss: 132409.65188673657, Test Loss: 20366430.0\n",
      "Epoch [68/100], Training Loss: 130547.23117706297, Test Loss: 20148350.0\n",
      "Epoch [69/100], Training Loss: 128741.26331378473, Test Loss: 19939090.0\n",
      "Epoch [70/100], Training Loss: 126987.38041289023, Test Loss: 19738792.0\n",
      "Epoch [71/100], Training Loss: 125287.62936141224, Test Loss: 19547228.0\n",
      "Epoch [72/100], Training Loss: 123639.46616018009, Test Loss: 19361606.0\n",
      "Epoch [73/100], Training Loss: 122039.47460162312, Test Loss: 19182120.0\n",
      "Epoch [74/100], Training Loss: 120483.59338901725, Test Loss: 19011268.0\n",
      "Epoch [75/100], Training Loss: 118971.38559623245, Test Loss: 18844520.0\n",
      "Epoch [76/100], Training Loss: 117497.70062496298, Test Loss: 18687098.0\n",
      "Epoch [77/100], Training Loss: 116059.79003021149, Test Loss: 18531190.0\n",
      "Epoch [78/100], Training Loss: 114659.52539837688, Test Loss: 18387250.0\n",
      "Epoch [79/100], Training Loss: 113298.32809075291, Test Loss: 18241742.0\n",
      "Epoch [80/100], Training Loss: 111973.3894319057, Test Loss: 18108284.0\n",
      "Epoch [81/100], Training Loss: 110685.23981103016, Test Loss: 17974380.0\n",
      "Epoch [82/100], Training Loss: 109431.65504117055, Test Loss: 17849182.0\n",
      "Epoch [83/100], Training Loss: 108214.23835969433, Test Loss: 17725112.0\n",
      "Epoch [84/100], Training Loss: 107027.13008708015, Test Loss: 17605464.0\n",
      "Epoch [85/100], Training Loss: 105870.13430780167, Test Loss: 17490476.0\n",
      "Epoch [86/100], Training Loss: 104741.19902553166, Test Loss: 17381428.0\n",
      "Epoch [87/100], Training Loss: 103645.83866477104, Test Loss: 17274388.0\n",
      "Epoch [88/100], Training Loss: 102577.01260292636, Test Loss: 17174756.0\n",
      "Epoch [89/100], Training Loss: 101539.94849238789, Test Loss: 17075472.0\n",
      "Epoch [90/100], Training Loss: 100526.64726615722, Test Loss: 16984020.0\n",
      "Epoch [91/100], Training Loss: 99537.68354955275, Test Loss: 16892478.0\n",
      "Epoch [92/100], Training Loss: 98577.4990966175, Test Loss: 16808446.0\n",
      "Epoch [93/100], Training Loss: 97638.37591078728, Test Loss: 16726542.0\n",
      "Epoch [94/100], Training Loss: 96729.63611456667, Test Loss: 16647765.0\n",
      "Epoch [95/100], Training Loss: 95841.15055387715, Test Loss: 16573219.0\n",
      "Epoch [96/100], Training Loss: 94976.5776020378, Test Loss: 16501211.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [97/100], Training Loss: 94134.3062022392, Test Loss: 16432566.0\n",
      "Epoch [98/100], Training Loss: 93313.80907529175, Test Loss: 16369174.0\n",
      "Epoch [99/100], Training Loss: 92513.6622830401, Test Loss: 16301145.0\n",
      "Epoch [100/100], Training Loss: 91740.83928677211, Test Loss: 16249565.0\n",
      "Epoch [1/100], Training Loss: 1497821.1281322197, Test Loss: 298779584.0\n",
      "Epoch [2/100], Training Loss: 1469314.047272081, Test Loss: 285987744.0\n",
      "Epoch [3/100], Training Loss: 1350656.529826432, Test Loss: 249906784.0\n",
      "Epoch [4/100], Training Loss: 1129915.019252414, Test Loss: 200972432.0\n",
      "Epoch [5/100], Training Loss: 912397.2423434631, Test Loss: 166559696.0\n",
      "Epoch [6/100], Training Loss: 788142.9519578224, Test Loss: 150310144.0\n",
      "Epoch [7/100], Training Loss: 724365.1307387003, Test Loss: 140640304.0\n",
      "Epoch [8/100], Training Loss: 679369.4366447486, Test Loss: 132941632.0\n",
      "Epoch [9/100], Training Loss: 641036.752562052, Test Loss: 126196112.0\n",
      "Epoch [10/100], Training Loss: 606524.0791422309, Test Loss: 120114968.0\n",
      "Epoch [11/100], Training Loss: 574901.2108287424, Test Loss: 114584272.0\n",
      "Epoch [12/100], Training Loss: 545781.3629524318, Test Loss: 109547856.0\n",
      "Epoch [13/100], Training Loss: 518923.110479237, Test Loss: 104933112.0\n",
      "Epoch [14/100], Training Loss: 493695.8087791008, Test Loss: 100602760.0\n",
      "Epoch [15/100], Training Loss: 470096.73550145136, Test Loss: 96683056.0\n",
      "Epoch [16/100], Training Loss: 448885.58924234344, Test Loss: 93228240.0\n",
      "Epoch [17/100], Training Loss: 429855.70949588297, Test Loss: 90121688.0\n",
      "Epoch [18/100], Training Loss: 412615.8765475979, Test Loss: 87300560.0\n",
      "Epoch [19/100], Training Loss: 396943.99502399145, Test Loss: 84724280.0\n",
      "Epoch [20/100], Training Loss: 382672.0466796991, Test Loss: 82360992.0\n",
      "Epoch [21/100], Training Loss: 369660.034002725, Test Loss: 80185080.0\n",
      "Epoch [22/100], Training Loss: 357796.08399976307, Test Loss: 78179216.0\n",
      "Epoch [23/100], Training Loss: 346982.065635922, Test Loss: 76325848.0\n",
      "Epoch [24/100], Training Loss: 337114.17025057756, Test Loss: 74609032.0\n",
      "Epoch [25/100], Training Loss: 328101.78520229843, Test Loss: 73014016.0\n",
      "Epoch [26/100], Training Loss: 319853.7770274273, Test Loss: 71526368.0\n",
      "Epoch [27/100], Training Loss: 312274.64332681714, Test Loss: 70131232.0\n",
      "Epoch [28/100], Training Loss: 305273.9888039808, Test Loss: 68816808.0\n",
      "Epoch [29/100], Training Loss: 298784.02642023575, Test Loss: 67574088.0\n",
      "Epoch [30/100], Training Loss: 292733.3825010367, Test Loss: 66393020.0\n",
      "Epoch [31/100], Training Loss: 287045.8160802085, Test Loss: 65264052.0\n",
      "Epoch [32/100], Training Loss: 281663.4302914519, Test Loss: 64176948.0\n",
      "Epoch [33/100], Training Loss: 276533.4243824418, Test Loss: 63125720.0\n",
      "Epoch [34/100], Training Loss: 271605.75094873674, Test Loss: 62107052.0\n",
      "Epoch [35/100], Training Loss: 266852.41152329143, Test Loss: 61116088.0\n",
      "Epoch [36/100], Training Loss: 262244.9120641365, Test Loss: 60147888.0\n",
      "Epoch [37/100], Training Loss: 257758.6489007612, Test Loss: 59202604.0\n",
      "Epoch [38/100], Training Loss: 253376.69386440376, Test Loss: 58278512.0\n",
      "Epoch [39/100], Training Loss: 249083.69293140215, Test Loss: 57373412.0\n",
      "Epoch [40/100], Training Loss: 244874.56924204726, Test Loss: 56486000.0\n",
      "Epoch [41/100], Training Loss: 240737.4937059416, Test Loss: 55615864.0\n",
      "Epoch [42/100], Training Loss: 236680.22066228304, Test Loss: 54762912.0\n",
      "Epoch [43/100], Training Loss: 232703.3953705349, Test Loss: 53927336.0\n",
      "Epoch [44/100], Training Loss: 228807.3504827913, Test Loss: 53109804.0\n",
      "Epoch [45/100], Training Loss: 224989.2778715716, Test Loss: 52312664.0\n",
      "Epoch [46/100], Training Loss: 221247.03530596528, Test Loss: 51534428.0\n",
      "Epoch [47/100], Training Loss: 217584.8609827617, Test Loss: 50772328.0\n",
      "Epoch [48/100], Training Loss: 214010.04733131925, Test Loss: 50028020.0\n",
      "Epoch [49/100], Training Loss: 210515.53345477165, Test Loss: 49298620.0\n",
      "Epoch [50/100], Training Loss: 207096.4120164682, Test Loss: 48581032.0\n",
      "Epoch [51/100], Training Loss: 203743.04323647887, Test Loss: 47881312.0\n",
      "Epoch [52/100], Training Loss: 200455.0955289971, Test Loss: 47196936.0\n",
      "Epoch [53/100], Training Loss: 197237.40062052011, Test Loss: 46529460.0\n",
      "Epoch [54/100], Training Loss: 194091.89060926484, Test Loss: 45876976.0\n",
      "Epoch [55/100], Training Loss: 191030.64626651266, Test Loss: 45240484.0\n",
      "Epoch [56/100], Training Loss: 188051.5101149221, Test Loss: 44617760.0\n",
      "Epoch [57/100], Training Loss: 185146.46648969257, Test Loss: 44010556.0\n",
      "Epoch [58/100], Training Loss: 182316.5445397192, Test Loss: 43417136.0\n",
      "Epoch [59/100], Training Loss: 179556.87922072152, Test Loss: 42835204.0\n",
      "Epoch [60/100], Training Loss: 176862.17543762218, Test Loss: 42265680.0\n",
      "Epoch [61/100], Training Loss: 174233.74747867425, Test Loss: 41706756.0\n",
      "Epoch [62/100], Training Loss: 171669.02033906462, Test Loss: 41159464.0\n",
      "Epoch [63/100], Training Loss: 169163.089548716, Test Loss: 40624268.0\n",
      "Epoch [64/100], Training Loss: 166715.1076515387, Test Loss: 40097956.0\n",
      "Epoch [65/100], Training Loss: 164328.88215162387, Test Loss: 39582888.0\n",
      "Epoch [66/100], Training Loss: 162010.28654117425, Test Loss: 39080868.0\n",
      "Epoch [67/100], Training Loss: 159759.98187079778, Test Loss: 38593168.0\n",
      "Epoch [68/100], Training Loss: 157569.54639889614, Test Loss: 38118236.0\n",
      "Epoch [69/100], Training Loss: 155432.39772531108, Test Loss: 37652876.0\n",
      "Epoch [70/100], Training Loss: 153338.35087512867, Test Loss: 37199396.0\n",
      "Epoch [71/100], Training Loss: 151291.67748712495, Test Loss: 36755672.0\n",
      "Epoch [72/100], Training Loss: 149294.69128649458, Test Loss: 36321896.0\n",
      "Epoch [73/100], Training Loss: 147350.30186229286, Test Loss: 35896492.0\n",
      "Epoch [74/100], Training Loss: 145460.47121869688, Test Loss: 35483484.0\n",
      "Epoch [75/100], Training Loss: 143623.90130259818, Test Loss: 35080792.0\n",
      "Epoch [76/100], Training Loss: 141834.98601434767, Test Loss: 34687700.0\n",
      "Epoch [77/100], Training Loss: 140094.30344199378, Test Loss: 34304620.0\n",
      "Epoch [78/100], Training Loss: 138400.51839172462, Test Loss: 33931348.0\n",
      "Epoch [79/100], Training Loss: 136750.60498512565, Test Loss: 33566628.0\n",
      "Epoch [80/100], Training Loss: 135139.817924923, Test Loss: 33209288.0\n",
      "Epoch [81/100], Training Loss: 133558.11199513322, Test Loss: 32860124.0\n",
      "Epoch [82/100], Training Loss: 132012.88159418319, Test Loss: 32522534.0\n",
      "Epoch [83/100], Training Loss: 130505.43485233028, Test Loss: 32190940.0\n",
      "Epoch [84/100], Training Loss: 129034.89382617662, Test Loss: 31869978.0\n",
      "Epoch [85/100], Training Loss: 127603.13361406686, Test Loss: 31554638.0\n",
      "Epoch [86/100], Training Loss: 126210.9265659247, Test Loss: 31250394.0\n",
      "Epoch [87/100], Training Loss: 124851.42212259716, Test Loss: 30953282.0\n",
      "Epoch [88/100], Training Loss: 123521.25230843849, Test Loss: 30662642.0\n",
      "Epoch [89/100], Training Loss: 122214.26891133153, Test Loss: 30376188.0\n",
      "Epoch [90/100], Training Loss: 120928.09276470217, Test Loss: 30095472.0\n",
      "Epoch [91/100], Training Loss: 119659.86056022673, Test Loss: 29819278.0\n",
      "Epoch [92/100], Training Loss: 118417.68860886499, Test Loss: 29552016.0\n",
      "Epoch [93/100], Training Loss: 117207.06098017002, Test Loss: 29293052.0\n",
      "Epoch [94/100], Training Loss: 116026.79293103193, Test Loss: 29043150.0\n",
      "Epoch [95/100], Training Loss: 114876.90108193013, Test Loss: 28799358.0\n",
      "Epoch [96/100], Training Loss: 113756.89897712295, Test Loss: 28561842.0\n",
      "Epoch [97/100], Training Loss: 112663.48145335363, Test Loss: 28328488.0\n",
      "Epoch [98/100], Training Loss: 111592.72631166327, Test Loss: 28100412.0\n",
      "Epoch [99/100], Training Loss: 110538.86771416459, Test Loss: 27874998.0\n",
      "Epoch [100/100], Training Loss: 109496.90663643667, Test Loss: 27652056.0\n",
      "Epoch [1/100], Training Loss: 1164398.0361353, Test Loss: 299338816.0\n",
      "Epoch [2/100], Training Loss: 1150055.455482495, Test Loss: 290724800.0\n",
      "Epoch [3/100], Training Loss: 1083518.692968426, Test Loss: 263664448.0\n",
      "Epoch [4/100], Training Loss: 941843.1737456312, Test Loss: 219701136.0\n",
      "Epoch [5/100], Training Loss: 768746.3349327646, Test Loss: 178817056.0\n",
      "Epoch [6/100], Training Loss: 643881.5074936319, Test Loss: 156191328.0\n",
      "Epoch [7/100], Training Loss: 580650.8972217286, Test Loss: 144848384.0\n",
      "Epoch [8/100], Training Loss: 543552.7641727385, Test Loss: 136897584.0\n",
      "Epoch [9/100], Training Loss: 514261.347076595, Test Loss: 130108040.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Training Loss: 488232.83312599966, Test Loss: 123935936.0\n",
      "Epoch [11/100], Training Loss: 464171.2374859309, Test Loss: 118189368.0\n",
      "Epoch [12/100], Training Loss: 441579.19673005154, Test Loss: 112792240.0\n",
      "Epoch [13/100], Training Loss: 420272.75327291037, Test Loss: 107721168.0\n",
      "Epoch [14/100], Training Loss: 400217.1916355666, Test Loss: 102975296.0\n",
      "Epoch [15/100], Training Loss: 381424.5236656596, Test Loss: 98554320.0\n",
      "Epoch [16/100], Training Loss: 363910.80338842486, Test Loss: 94462352.0\n",
      "Epoch [17/100], Training Loss: 347691.22255790536, Test Loss: 90693360.0\n",
      "Epoch [18/100], Training Loss: 332742.1458444405, Test Loss: 87228552.0\n",
      "Epoch [19/100], Training Loss: 318997.9844795924, Test Loss: 84041928.0\n",
      "Epoch [20/100], Training Loss: 306367.21651560924, Test Loss: 81104112.0\n",
      "Epoch [21/100], Training Loss: 294747.06924945203, Test Loss: 78388488.0\n",
      "Epoch [22/100], Training Loss: 284036.1315087969, Test Loss: 75869944.0\n",
      "Epoch [23/100], Training Loss: 274141.15052425803, Test Loss: 73526984.0\n",
      "Epoch [24/100], Training Loss: 264977.25383567327, Test Loss: 71340720.0\n",
      "Epoch [25/100], Training Loss: 256470.9816953972, Test Loss: 69296568.0\n",
      "Epoch [26/100], Training Loss: 248557.41401575736, Test Loss: 67383176.0\n",
      "Epoch [27/100], Training Loss: 241183.88886914283, Test Loss: 65591700.0\n",
      "Epoch [28/100], Training Loss: 234307.65262721403, Test Loss: 63916168.0\n",
      "Epoch [29/100], Training Loss: 227886.70185415557, Test Loss: 62345768.0\n",
      "Epoch [30/100], Training Loss: 221869.73662697707, Test Loss: 60868028.0\n",
      "Epoch [31/100], Training Loss: 216216.66216456372, Test Loss: 59475552.0\n",
      "Epoch [32/100], Training Loss: 210880.2195367573, Test Loss: 58155572.0\n",
      "Epoch [33/100], Training Loss: 205820.20626740123, Test Loss: 56901864.0\n",
      "Epoch [34/100], Training Loss: 201006.829927137, Test Loss: 55707008.0\n",
      "Epoch [35/100], Training Loss: 196405.84645459394, Test Loss: 54563536.0\n",
      "Epoch [36/100], Training Loss: 191998.63858776138, Test Loss: 53467216.0\n",
      "Epoch [37/100], Training Loss: 187765.57360345952, Test Loss: 52411780.0\n",
      "Epoch [38/100], Training Loss: 183686.36277471713, Test Loss: 51392328.0\n",
      "Epoch [39/100], Training Loss: 179748.907884604, Test Loss: 50407720.0\n",
      "Epoch [40/100], Training Loss: 175944.2842248682, Test Loss: 49455560.0\n",
      "Epoch [41/100], Training Loss: 172266.7897636396, Test Loss: 48534644.0\n",
      "Epoch [42/100], Training Loss: 168712.28126295836, Test Loss: 47642404.0\n",
      "Epoch [43/100], Training Loss: 165274.29263669215, Test Loss: 46776368.0\n",
      "Epoch [44/100], Training Loss: 161944.71133226703, Test Loss: 45934788.0\n",
      "Epoch [45/100], Training Loss: 158715.26272140277, Test Loss: 45116824.0\n",
      "Epoch [46/100], Training Loss: 155583.91102422844, Test Loss: 44320644.0\n",
      "Epoch [47/100], Training Loss: 152546.45838516674, Test Loss: 43545700.0\n",
      "Epoch [48/100], Training Loss: 149598.3577987086, Test Loss: 42790856.0\n",
      "Epoch [49/100], Training Loss: 146737.96380546177, Test Loss: 42054596.0\n",
      "Epoch [50/100], Training Loss: 143962.8632190036, Test Loss: 41337584.0\n",
      "Epoch [51/100], Training Loss: 141272.5459392216, Test Loss: 40639964.0\n",
      "Epoch [52/100], Training Loss: 138663.29861975, Test Loss: 39960536.0\n",
      "Epoch [53/100], Training Loss: 136129.30152242165, Test Loss: 39299100.0\n",
      "Epoch [54/100], Training Loss: 133668.29565784018, Test Loss: 38654672.0\n",
      "Epoch [55/100], Training Loss: 131276.96374622357, Test Loss: 38026560.0\n",
      "Epoch [56/100], Training Loss: 128953.44517504887, Test Loss: 37415384.0\n",
      "Epoch [57/100], Training Loss: 126696.92399739352, Test Loss: 36821328.0\n",
      "Epoch [58/100], Training Loss: 124507.02967833659, Test Loss: 36243872.0\n",
      "Epoch [59/100], Training Loss: 122381.16924352823, Test Loss: 35683136.0\n",
      "Epoch [60/100], Training Loss: 120319.23476097388, Test Loss: 35138368.0\n",
      "Epoch [61/100], Training Loss: 118317.64030566909, Test Loss: 34609612.0\n",
      "Epoch [62/100], Training Loss: 116373.70884426279, Test Loss: 34095364.0\n",
      "Epoch [63/100], Training Loss: 114484.16924352823, Test Loss: 33594448.0\n",
      "Epoch [64/100], Training Loss: 112646.65239026124, Test Loss: 33107154.0\n",
      "Epoch [65/100], Training Loss: 110860.4944019904, Test Loss: 32632832.0\n",
      "Epoch [66/100], Training Loss: 109124.5810082341, Test Loss: 32171958.0\n",
      "Epoch [67/100], Training Loss: 107440.77282151532, Test Loss: 31724554.0\n",
      "Epoch [68/100], Training Loss: 105808.74705289971, Test Loss: 31290368.0\n",
      "Epoch [69/100], Training Loss: 104226.02671642676, Test Loss: 30868608.0\n",
      "Epoch [70/100], Training Loss: 102692.13447070672, Test Loss: 30459438.0\n",
      "Epoch [71/100], Training Loss: 101203.30110775428, Test Loss: 30062558.0\n",
      "Epoch [72/100], Training Loss: 99757.46638232333, Test Loss: 29677150.0\n",
      "Epoch [73/100], Training Loss: 98353.12291925834, Test Loss: 29302960.0\n",
      "Epoch [74/100], Training Loss: 96988.37041644452, Test Loss: 28939734.0\n",
      "Epoch [75/100], Training Loss: 95661.58746519756, Test Loss: 28587260.0\n",
      "Epoch [76/100], Training Loss: 94371.50536105681, Test Loss: 28244742.0\n",
      "Epoch [77/100], Training Loss: 93115.89982820924, Test Loss: 27911928.0\n",
      "Epoch [78/100], Training Loss: 91895.03477282151, Test Loss: 27589008.0\n",
      "Epoch [79/100], Training Loss: 90707.468692613, Test Loss: 27276292.0\n",
      "Epoch [80/100], Training Loss: 89553.52348794503, Test Loss: 26972552.0\n",
      "Epoch [81/100], Training Loss: 88430.29447307624, Test Loss: 26677824.0\n",
      "Epoch [82/100], Training Loss: 87336.70635625851, Test Loss: 26390948.0\n",
      "Epoch [83/100], Training Loss: 86271.3780581719, Test Loss: 26112902.0\n",
      "Epoch [84/100], Training Loss: 85233.51383211894, Test Loss: 25842934.0\n",
      "Epoch [85/100], Training Loss: 84221.74782299627, Test Loss: 25580054.0\n",
      "Epoch [86/100], Training Loss: 83234.05509152301, Test Loss: 25324040.0\n",
      "Epoch [87/100], Training Loss: 82269.22208399976, Test Loss: 25075386.0\n",
      "Epoch [88/100], Training Loss: 81326.1052070375, Test Loss: 24833572.0\n",
      "Epoch [89/100], Training Loss: 80404.56382915704, Test Loss: 24598734.0\n",
      "Epoch [90/100], Training Loss: 79503.22356495468, Test Loss: 24370170.0\n",
      "Epoch [91/100], Training Loss: 78620.42734435164, Test Loss: 24147186.0\n",
      "Epoch [92/100], Training Loss: 77757.83591019489, Test Loss: 23929958.0\n",
      "Epoch [93/100], Training Loss: 76914.56892364197, Test Loss: 23718726.0\n",
      "Epoch [94/100], Training Loss: 76089.57111545524, Test Loss: 23513104.0\n",
      "Epoch [95/100], Training Loss: 75282.46650079971, Test Loss: 23312204.0\n",
      "Epoch [96/100], Training Loss: 74492.01765298264, Test Loss: 23116810.0\n",
      "Epoch [97/100], Training Loss: 73717.6661335229, Test Loss: 22925620.0\n",
      "Epoch [98/100], Training Loss: 72958.91629642794, Test Loss: 22739952.0\n",
      "Epoch [99/100], Training Loss: 72214.8248326521, Test Loss: 22557002.0\n",
      "Epoch [100/100], Training Loss: 71485.22128428411, Test Loss: 22380210.0\n",
      "Epoch [1/100], Training Loss: 582428.8352585747, Test Loss: 300038176.0\n",
      "Epoch [2/100], Training Loss: 581921.715538179, Test Loss: 299420672.0\n",
      "Epoch [3/100], Training Loss: 579324.3641964339, Test Loss: 297083104.0\n",
      "Epoch [4/100], Training Loss: 571992.0308038624, Test Loss: 291553984.0\n",
      "Epoch [5/100], Training Loss: 557155.443397903, Test Loss: 281553376.0\n",
      "Epoch [6/100], Training Loss: 532883.9827024465, Test Loss: 266479408.0\n",
      "Epoch [7/100], Training Loss: 499004.69213909126, Test Loss: 246843424.0\n",
      "Epoch [8/100], Training Loss: 457770.7661868373, Test Loss: 224471504.0\n",
      "Epoch [9/100], Training Loss: 413798.5038801019, Test Loss: 202186720.0\n",
      "Epoch [10/100], Training Loss: 372819.95663763996, Test Loss: 182852464.0\n",
      "Epoch [11/100], Training Loss: 339408.19098394644, Test Loss: 168095296.0\n",
      "Epoch [12/100], Training Loss: 314937.8401753451, Test Loss: 157706304.0\n",
      "Epoch [13/100], Training Loss: 297730.8131034891, Test Loss: 150368592.0\n",
      "Epoch [14/100], Training Loss: 285112.4829097802, Test Loss: 144788816.0\n",
      "Epoch [15/100], Training Loss: 275027.6912505183, Test Loss: 140153936.0\n",
      "Epoch [16/100], Training Loss: 266319.38629228127, Test Loss: 136049024.0\n",
      "Epoch [17/100], Training Loss: 258420.87933179314, Test Loss: 132275512.0\n",
      "Epoch [18/100], Training Loss: 251056.81132634322, Test Loss: 128733624.0\n",
      "Epoch [19/100], Training Loss: 244081.89941354186, Test Loss: 125368032.0\n",
      "Epoch [20/100], Training Loss: 237411.54907884603, Test Loss: 122145192.0\n",
      "Epoch [21/100], Training Loss: 230992.95871097685, Test Loss: 119043824.0\n",
      "Epoch [22/100], Training Loss: 224791.3649665304, Test Loss: 116048464.0\n",
      "Epoch [23/100], Training Loss: 218778.50696048813, Test Loss: 113143256.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/100], Training Loss: 212908.87246016233, Test Loss: 110295088.0\n",
      "Epoch [25/100], Training Loss: 207130.51004087436, Test Loss: 107505648.0\n",
      "Epoch [26/100], Training Loss: 201479.1699543866, Test Loss: 104802384.0\n",
      "Epoch [27/100], Training Loss: 196018.27830104853, Test Loss: 102203792.0\n",
      "Epoch [28/100], Training Loss: 190756.52816776256, Test Loss: 99701096.0\n",
      "Epoch [29/100], Training Loss: 185674.9611989811, Test Loss: 97291240.0\n",
      "Epoch [30/100], Training Loss: 180778.48942598188, Test Loss: 94981560.0\n",
      "Epoch [31/100], Training Loss: 176079.02659795035, Test Loss: 92778144.0\n",
      "Epoch [32/100], Training Loss: 171600.02819738167, Test Loss: 90677400.0\n",
      "Epoch [33/100], Training Loss: 167345.68497126948, Test Loss: 88687032.0\n",
      "Epoch [34/100], Training Loss: 163322.62069782597, Test Loss: 86808664.0\n",
      "Epoch [35/100], Training Loss: 159520.1478585392, Test Loss: 85034112.0\n",
      "Epoch [36/100], Training Loss: 155927.15550026658, Test Loss: 83350272.0\n",
      "Epoch [37/100], Training Loss: 152522.07309993485, Test Loss: 81750280.0\n",
      "Epoch [38/100], Training Loss: 149291.54907884603, Test Loss: 80227256.0\n",
      "Epoch [39/100], Training Loss: 146223.51424678633, Test Loss: 78775872.0\n",
      "Epoch [40/100], Training Loss: 143306.00793791836, Test Loss: 77390312.0\n",
      "Epoch [41/100], Training Loss: 140527.64149043304, Test Loss: 76065592.0\n",
      "Epoch [42/100], Training Loss: 137879.14294176886, Test Loss: 74797624.0\n",
      "Epoch [43/100], Training Loss: 135352.91297908893, Test Loss: 73583400.0\n",
      "Epoch [44/100], Training Loss: 132940.37012025353, Test Loss: 72420088.0\n",
      "Epoch [45/100], Training Loss: 130634.58989396362, Test Loss: 71303416.0\n",
      "Epoch [46/100], Training Loss: 128427.60310408151, Test Loss: 70229536.0\n",
      "Epoch [47/100], Training Loss: 126313.5515668503, Test Loss: 69195792.0\n",
      "Epoch [48/100], Training Loss: 124285.54659084177, Test Loss: 68200576.0\n",
      "Epoch [49/100], Training Loss: 122342.24986671406, Test Loss: 67243664.0\n",
      "Epoch [50/100], Training Loss: 120477.6290504117, Test Loss: 66321716.0\n",
      "Epoch [51/100], Training Loss: 118686.16574847461, Test Loss: 65432084.0\n",
      "Epoch [52/100], Training Loss: 116962.63515194597, Test Loss: 64573612.0\n",
      "Epoch [53/100], Training Loss: 115302.57994194656, Test Loss: 63743268.0\n",
      "Epoch [54/100], Training Loss: 113701.69705586162, Test Loss: 62939356.0\n",
      "Epoch [55/100], Training Loss: 112155.51424678633, Test Loss: 62159876.0\n",
      "Epoch [56/100], Training Loss: 110660.40566317161, Test Loss: 61403880.0\n",
      "Epoch [57/100], Training Loss: 109212.06314791778, Test Loss: 60670216.0\n",
      "Epoch [58/100], Training Loss: 107806.47900005923, Test Loss: 59957264.0\n",
      "Epoch [59/100], Training Loss: 106436.98750074048, Test Loss: 59263536.0\n",
      "Epoch [60/100], Training Loss: 105104.24417984717, Test Loss: 58588268.0\n",
      "Epoch [61/100], Training Loss: 103806.09880931224, Test Loss: 57931100.0\n",
      "Epoch [62/100], Training Loss: 102539.1790770689, Test Loss: 57290456.0\n",
      "Epoch [63/100], Training Loss: 101301.77939695516, Test Loss: 56664748.0\n",
      "Epoch [64/100], Training Loss: 100090.60944256857, Test Loss: 56052296.0\n",
      "Epoch [65/100], Training Loss: 98907.41366032818, Test Loss: 55453392.0\n",
      "Epoch [66/100], Training Loss: 97752.06267401221, Test Loss: 54868716.0\n",
      "Epoch [67/100], Training Loss: 96623.6919613767, Test Loss: 54297660.0\n",
      "Epoch [68/100], Training Loss: 95523.03880101889, Test Loss: 53739828.0\n",
      "Epoch [69/100], Training Loss: 94448.10414074996, Test Loss: 53194304.0\n",
      "Epoch [70/100], Training Loss: 93395.70108405901, Test Loss: 52659576.0\n",
      "Epoch [71/100], Training Loss: 92363.9592441206, Test Loss: 52136708.0\n",
      "Epoch [72/100], Training Loss: 91353.54197026242, Test Loss: 51625292.0\n",
      "Epoch [73/100], Training Loss: 90364.33516971744, Test Loss: 51123368.0\n",
      "Epoch [74/100], Training Loss: 89396.31609501806, Test Loss: 50630912.0\n",
      "Epoch [75/100], Training Loss: 88449.42479710918, Test Loss: 50147744.0\n",
      "Epoch [76/100], Training Loss: 87522.4151412831, Test Loss: 49676620.0\n",
      "Epoch [77/100], Training Loss: 86615.09436644749, Test Loss: 49215800.0\n",
      "Epoch [78/100], Training Loss: 85727.54019311653, Test Loss: 48764892.0\n",
      "Epoch [79/100], Training Loss: 84859.85711746935, Test Loss: 48323572.0\n",
      "Epoch [80/100], Training Loss: 84010.89627391742, Test Loss: 47891632.0\n",
      "Epoch [81/100], Training Loss: 83179.05669095433, Test Loss: 47468360.0\n",
      "Epoch [82/100], Training Loss: 82363.25525738996, Test Loss: 47053460.0\n",
      "Epoch [83/100], Training Loss: 81564.13731414016, Test Loss: 46647752.0\n",
      "Epoch [84/100], Training Loss: 80782.08956815355, Test Loss: 46250344.0\n",
      "Epoch [85/100], Training Loss: 80016.12522954801, Test Loss: 45860008.0\n",
      "Epoch [86/100], Training Loss: 79265.92725549435, Test Loss: 45476184.0\n",
      "Epoch [87/100], Training Loss: 78530.34085658433, Test Loss: 45099896.0\n",
      "Epoch [88/100], Training Loss: 77808.84580297375, Test Loss: 44731496.0\n",
      "Epoch [89/100], Training Loss: 77099.75013328594, Test Loss: 44369920.0\n",
      "Epoch [90/100], Training Loss: 76403.12244535277, Test Loss: 44013924.0\n",
      "Epoch [91/100], Training Loss: 75719.65689236419, Test Loss: 43663644.0\n",
      "Epoch [92/100], Training Loss: 75049.37420768912, Test Loss: 43318508.0\n",
      "Epoch [93/100], Training Loss: 74391.8856702802, Test Loss: 42979344.0\n",
      "Epoch [94/100], Training Loss: 73746.15532255197, Test Loss: 42645200.0\n",
      "Epoch [95/100], Training Loss: 73111.90024287661, Test Loss: 42317540.0\n",
      "Epoch [96/100], Training Loss: 72490.05876429121, Test Loss: 41996324.0\n",
      "Epoch [97/100], Training Loss: 71881.09815769207, Test Loss: 41681184.0\n",
      "Epoch [98/100], Training Loss: 71284.57117469344, Test Loss: 41371840.0\n",
      "Epoch [99/100], Training Loss: 70699.29814584444, Test Loss: 41068036.0\n",
      "Epoch [100/100], Training Loss: 70125.28250696049, Test Loss: 40769856.0\n",
      "Epoch [1/100], Training Loss: 7095325.946744861, Test Loss: 147455408.0\n",
      "Epoch [2/100], Training Loss: 4032148.103548368, Test Loss: 116248008.0\n",
      "Epoch [3/100], Training Loss: 3213010.7783899056, Test Loss: 94741312.0\n",
      "Epoch [4/100], Training Loss: 2621019.8258989397, Test Loss: 79163288.0\n",
      "Epoch [5/100], Training Loss: 2187103.045494935, Test Loss: 67698816.0\n",
      "Epoch [6/100], Training Loss: 1874632.5350986316, Test Loss: 59402040.0\n",
      "Epoch [7/100], Training Loss: 1652169.7125762692, Test Loss: 53457008.0\n",
      "Epoch [8/100], Training Loss: 1489143.5899532018, Test Loss: 48893068.0\n",
      "Epoch [9/100], Training Loss: 1359129.9594218351, Test Loss: 45094420.0\n",
      "Epoch [10/100], Training Loss: 1248740.792014691, Test Loss: 41779540.0\n",
      "Epoch [11/100], Training Loss: 1152409.9058853148, Test Loss: 38822760.0\n",
      "Epoch [12/100], Training Loss: 1067490.8622119543, Test Loss: 36166900.0\n",
      "Epoch [13/100], Training Loss: 992702.2656240744, Test Loss: 33793164.0\n",
      "Epoch [14/100], Training Loss: 926925.632782418, Test Loss: 31695078.0\n",
      "Epoch [15/100], Training Loss: 869162.0183934601, Test Loss: 29848224.0\n",
      "Epoch [16/100], Training Loss: 818410.7192553759, Test Loss: 28223146.0\n",
      "Epoch [17/100], Training Loss: 773942.4953794207, Test Loss: 26786676.0\n",
      "Epoch [18/100], Training Loss: 734934.0851697174, Test Loss: 25518742.0\n",
      "Epoch [19/100], Training Loss: 700459.1057549908, Test Loss: 24399744.0\n",
      "Epoch [20/100], Training Loss: 669536.0542769978, Test Loss: 23401214.0\n",
      "Epoch [21/100], Training Loss: 641598.3291866595, Test Loss: 22512914.0\n",
      "Epoch [22/100], Training Loss: 616169.785609561, Test Loss: 21720098.0\n",
      "Epoch [23/100], Training Loss: 592927.7351015935, Test Loss: 21015374.0\n",
      "Epoch [24/100], Training Loss: 571610.043791837, Test Loss: 20384906.0\n",
      "Epoch [25/100], Training Loss: 552056.873126592, Test Loss: 19827936.0\n",
      "Epoch [26/100], Training Loss: 534042.8410787275, Test Loss: 19326860.0\n",
      "Epoch [27/100], Training Loss: 517402.70499378, Test Loss: 18881938.0\n",
      "Epoch [28/100], Training Loss: 502016.9965197559, Test Loss: 18477430.0\n",
      "Epoch [29/100], Training Loss: 487734.8259285587, Test Loss: 18108968.0\n",
      "Epoch [30/100], Training Loss: 474454.2009729874, Test Loss: 17773940.0\n",
      "Epoch [31/100], Training Loss: 462144.3521414608, Test Loss: 17468728.0\n",
      "Epoch [32/100], Training Loss: 450622.45025472424, Test Loss: 17185844.0\n",
      "Epoch [33/100], Training Loss: 439838.26727533917, Test Loss: 16939226.0\n",
      "Epoch [34/100], Training Loss: 429711.12629583554, Test Loss: 16706215.0\n",
      "Epoch [35/100], Training Loss: 420265.73013298976, Test Loss: 16509865.0\n",
      "Epoch [36/100], Training Loss: 411377.76238078316, Test Loss: 16319027.0\n",
      "Epoch [37/100], Training Loss: 403126.394941058, Test Loss: 16174509.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/100], Training Loss: 395353.9187325988, Test Loss: 16016500.0\n",
      "Epoch [39/100], Training Loss: 388114.0592233872, Test Loss: 15899395.0\n",
      "Epoch [40/100], Training Loss: 381252.7481339968, Test Loss: 15776032.0\n",
      "Epoch [41/100], Training Loss: 374802.79837539245, Test Loss: 15684984.0\n",
      "Epoch [42/100], Training Loss: 368667.08516971744, Test Loss: 15588166.0\n",
      "Epoch [43/100], Training Loss: 362936.3049212132, Test Loss: 15525229.0\n",
      "Epoch [44/100], Training Loss: 357445.76025561284, Test Loss: 15449609.0\n",
      "Epoch [45/100], Training Loss: 352312.45874800073, Test Loss: 15390323.0\n",
      "Epoch [46/100], Training Loss: 347449.6614833244, Test Loss: 15331119.0\n",
      "Epoch [47/100], Training Loss: 342853.118824418, Test Loss: 15266894.0\n",
      "Epoch [48/100], Training Loss: 338515.12020911084, Test Loss: 15213897.0\n",
      "Epoch [49/100], Training Loss: 334384.7461421124, Test Loss: 15136738.0\n",
      "Epoch [50/100], Training Loss: 330462.239618506, Test Loss: 15119001.0\n",
      "Epoch [51/100], Training Loss: 326724.4450750844, Test Loss: 15052388.0\n",
      "Epoch [52/100], Training Loss: 323128.7958244476, Test Loss: 15019191.0\n",
      "Epoch [53/100], Training Loss: 319789.47257641726, Test Loss: 14957425.0\n",
      "Epoch [54/100], Training Loss: 316580.9045598602, Test Loss: 14940375.0\n",
      "Epoch [55/100], Training Loss: 313632.1756634678, Test Loss: 14883799.0\n",
      "Epoch [56/100], Training Loss: 310790.7496223565, Test Loss: 14867137.0\n",
      "Epoch [57/100], Training Loss: 308123.58171909244, Test Loss: 14804733.0\n",
      "Epoch [58/100], Training Loss: 305512.3873733783, Test Loss: 14799964.0\n",
      "Epoch [59/100], Training Loss: 303073.8319856644, Test Loss: 14760568.0\n",
      "Epoch [60/100], Training Loss: 300745.4089916178, Test Loss: 14738088.0\n",
      "Epoch [61/100], Training Loss: 298491.0898717493, Test Loss: 14698518.0\n",
      "Epoch [62/100], Training Loss: 296328.72570123215, Test Loss: 14684205.0\n",
      "Epoch [63/100], Training Loss: 294291.80165941, Test Loss: 14659721.0\n",
      "Epoch [64/100], Training Loss: 292309.9584407026, Test Loss: 14582300.0\n",
      "Epoch [65/100], Training Loss: 290393.7920961436, Test Loss: 14617159.0\n",
      "Epoch [66/100], Training Loss: 288533.2369083585, Test Loss: 14563284.0\n",
      "Epoch [67/100], Training Loss: 286708.56735753216, Test Loss: 14550395.0\n",
      "Epoch [68/100], Training Loss: 284995.3075425034, Test Loss: 14549958.0\n",
      "Epoch [69/100], Training Loss: 283307.4447566791, Test Loss: 14500243.0\n",
      "Epoch [70/100], Training Loss: 281657.5896347965, Test Loss: 14509364.0\n",
      "Epoch [71/100], Training Loss: 280068.645648214, Test Loss: 14483302.0\n",
      "Epoch [72/100], Training Loss: 278566.8103341034, Test Loss: 14458494.0\n",
      "Epoch [73/100], Training Loss: 277085.0517112434, Test Loss: 14512220.0\n",
      "Epoch [74/100], Training Loss: 275640.7679787927, Test Loss: 14445313.0\n",
      "Epoch [75/100], Training Loss: 274220.67762943543, Test Loss: 14480772.0\n",
      "Epoch [76/100], Training Loss: 272885.09286327823, Test Loss: 14432579.0\n",
      "Epoch [77/100], Training Loss: 271517.72126947454, Test Loss: 14460279.0\n",
      "Epoch [78/100], Training Loss: 270206.58411823947, Test Loss: 14457526.0\n",
      "Epoch [79/100], Training Loss: 268922.6015824003, Test Loss: 14402249.0\n",
      "Epoch [80/100], Training Loss: 267686.31655781646, Test Loss: 14435893.0\n",
      "Epoch [81/100], Training Loss: 266515.6025561282, Test Loss: 14445142.0\n",
      "Epoch [82/100], Training Loss: 265343.117361975, Test Loss: 14412885.0\n",
      "Epoch [83/100], Training Loss: 264185.8371504946, Test Loss: 14449716.0\n",
      "Epoch [84/100], Training Loss: 263122.9593662994, Test Loss: 14464416.0\n",
      "Epoch [85/100], Training Loss: 262007.62216767372, Test Loss: 14414081.0\n",
      "Epoch [86/100], Training Loss: 260944.36341523015, Test Loss: 14427166.0\n",
      "Epoch [87/100], Training Loss: 259934.51071841124, Test Loss: 14459962.0\n",
      "Epoch [88/100], Training Loss: 258916.59221906285, Test Loss: 14400789.0\n",
      "Epoch [89/100], Training Loss: 257919.4154004502, Test Loss: 14441532.0\n",
      "Epoch [90/100], Training Loss: 256937.59047153604, Test Loss: 14465234.0\n",
      "Epoch [91/100], Training Loss: 256022.92466752563, Test Loss: 14417173.0\n",
      "Epoch [92/100], Training Loss: 255094.81644304248, Test Loss: 14425472.0\n",
      "Epoch [93/100], Training Loss: 254157.08142290148, Test Loss: 14446608.0\n",
      "Epoch [94/100], Training Loss: 253337.51057401812, Test Loss: 14399970.0\n",
      "Epoch [95/100], Training Loss: 252449.4283773177, Test Loss: 14439733.0\n",
      "Epoch [96/100], Training Loss: 251666.78276242522, Test Loss: 14464262.0\n",
      "Epoch [97/100], Training Loss: 250853.2601704579, Test Loss: 14415688.0\n",
      "Epoch [98/100], Training Loss: 250079.0128213672, Test Loss: 14411611.0\n",
      "Epoch [99/100], Training Loss: 249287.8151805284, Test Loss: 14433708.0\n",
      "Epoch [100/100], Training Loss: 248544.58623970737, Test Loss: 14393796.0\n",
      "Epoch [1/100], Training Loss: 4427261.192109472, Test Loss: 239193024.0\n",
      "Epoch [2/100], Training Loss: 2770830.320241692, Test Loss: 147613632.0\n",
      "Epoch [3/100], Training Loss: 2117640.681594692, Test Loss: 128053816.0\n",
      "Epoch [4/100], Training Loss: 1846785.3030033766, Test Loss: 112796304.0\n",
      "Epoch [5/100], Training Loss: 1621525.485812452, Test Loss: 99990016.0\n",
      "Epoch [6/100], Training Loss: 1432816.3451217345, Test Loss: 89442168.0\n",
      "Epoch [7/100], Training Loss: 1276216.6460517743, Test Loss: 80692400.0\n",
      "Epoch [8/100], Training Loss: 1146345.8998282093, Test Loss: 73399616.0\n",
      "Epoch [9/100], Training Loss: 1039300.2741543747, Test Loss: 67336784.0\n",
      "Epoch [10/100], Training Loss: 951541.1523014039, Test Loss: 62338516.0\n",
      "Epoch [11/100], Training Loss: 879890.9600142172, Test Loss: 58235412.0\n",
      "Epoch [12/100], Training Loss: 820841.3668621527, Test Loss: 54806456.0\n",
      "Epoch [13/100], Training Loss: 770961.649013684, Test Loss: 51852212.0\n",
      "Epoch [14/100], Training Loss: 727414.4296546413, Test Loss: 49233488.0\n",
      "Epoch [15/100], Training Loss: 688462.5958770216, Test Loss: 46863816.0\n",
      "Epoch [16/100], Training Loss: 653040.0204371779, Test Loss: 44686140.0\n",
      "Epoch [17/100], Training Loss: 620507.6268882175, Test Loss: 42665180.0\n",
      "Epoch [18/100], Training Loss: 590489.141194242, Test Loss: 40778240.0\n",
      "Epoch [19/100], Training Loss: 562700.5092411587, Test Loss: 39011340.0\n",
      "Epoch [20/100], Training Loss: 536949.4370001777, Test Loss: 37357216.0\n",
      "Epoch [21/100], Training Loss: 513102.4815176826, Test Loss: 35808940.0\n",
      "Epoch [22/100], Training Loss: 491033.152923405, Test Loss: 34368228.0\n",
      "Epoch [23/100], Training Loss: 470603.4942242758, Test Loss: 33030300.0\n",
      "Epoch [24/100], Training Loss: 451735.09685445175, Test Loss: 31788884.0\n",
      "Epoch [25/100], Training Loss: 434270.60417036904, Test Loss: 30638110.0\n",
      "Epoch [26/100], Training Loss: 418120.0681831645, Test Loss: 29573048.0\n",
      "Epoch [27/100], Training Loss: 403163.91007641726, Test Loss: 28583222.0\n",
      "Epoch [28/100], Training Loss: 389328.5156240744, Test Loss: 27666556.0\n",
      "Epoch [29/100], Training Loss: 376533.64266038744, Test Loss: 26814296.0\n",
      "Epoch [30/100], Training Loss: 364703.2468307565, Test Loss: 26024212.0\n",
      "Epoch [31/100], Training Loss: 353721.5262129021, Test Loss: 25291384.0\n",
      "Epoch [32/100], Training Loss: 343460.0456134115, Test Loss: 24605856.0\n",
      "Epoch [33/100], Training Loss: 333804.57030093006, Test Loss: 23964352.0\n",
      "Epoch [34/100], Training Loss: 324699.8300159943, Test Loss: 23362700.0\n",
      "Epoch [35/100], Training Loss: 316115.75781944196, Test Loss: 22801712.0\n",
      "Epoch [36/100], Training Loss: 307989.35892423434, Test Loss: 22276044.0\n",
      "Epoch [37/100], Training Loss: 300287.0326698655, Test Loss: 21784004.0\n",
      "Epoch [38/100], Training Loss: 292980.97809667676, Test Loss: 21327302.0\n",
      "Epoch [39/100], Training Loss: 286052.80916414905, Test Loss: 20901730.0\n",
      "Epoch [40/100], Training Loss: 279489.7817812926, Test Loss: 20501976.0\n",
      "Epoch [41/100], Training Loss: 273259.41660742846, Test Loss: 20128918.0\n",
      "Epoch [42/100], Training Loss: 267342.989322315, Test Loss: 19776916.0\n",
      "Epoch [43/100], Training Loss: 261714.34654345122, Test Loss: 19451752.0\n",
      "Epoch [44/100], Training Loss: 256363.72891120194, Test Loss: 19144912.0\n",
      "Epoch [45/100], Training Loss: 251268.6160772466, Test Loss: 18855512.0\n",
      "Epoch [46/100], Training Loss: 246395.40958770216, Test Loss: 18584196.0\n",
      "Epoch [47/100], Training Loss: 241734.09914993186, Test Loss: 18326298.0\n",
      "Epoch [48/100], Training Loss: 237276.56374029975, Test Loss: 18086448.0\n",
      "Epoch [49/100], Training Loss: 233012.94487885787, Test Loss: 17861314.0\n",
      "Epoch [50/100], Training Loss: 228924.9894111723, Test Loss: 17648486.0\n",
      "Epoch [51/100], Training Loss: 225034.66090575204, Test Loss: 17448960.0\n",
      "Epoch [52/100], Training Loss: 221315.71302055565, Test Loss: 17263088.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [53/100], Training Loss: 217755.49198803387, Test Loss: 17085558.0\n",
      "Epoch [54/100], Training Loss: 214333.6580326995, Test Loss: 16921906.0\n",
      "Epoch [55/100], Training Loss: 211063.8597831882, Test Loss: 16767964.0\n",
      "Epoch [56/100], Training Loss: 207914.54324388364, Test Loss: 16622243.0\n",
      "Epoch [57/100], Training Loss: 204899.38980214443, Test Loss: 16486591.0\n",
      "Epoch [58/100], Training Loss: 202014.91792547834, Test Loss: 16360111.0\n",
      "Epoch [59/100], Training Loss: 199245.9144452343, Test Loss: 16237815.0\n",
      "Epoch [60/100], Training Loss: 196582.49563118297, Test Loss: 16133254.0\n",
      "Epoch [61/100], Training Loss: 194018.97214323797, Test Loss: 16024449.0\n",
      "Epoch [62/100], Training Loss: 191550.6440376755, Test Loss: 15932787.0\n",
      "Epoch [63/100], Training Loss: 189194.62142349387, Test Loss: 15844873.0\n",
      "Epoch [64/100], Training Loss: 186913.7583970144, Test Loss: 15764967.0\n",
      "Epoch [65/100], Training Loss: 184732.27222913335, Test Loss: 15691877.0\n",
      "Epoch [66/100], Training Loss: 182630.27505775724, Test Loss: 15627394.0\n",
      "Epoch [67/100], Training Loss: 180621.0994313133, Test Loss: 15565310.0\n",
      "Epoch [68/100], Training Loss: 178677.8938007227, Test Loss: 15511852.0\n",
      "Epoch [69/100], Training Loss: 176807.8126740122, Test Loss: 15454978.0\n",
      "Epoch [70/100], Training Loss: 175006.56680587644, Test Loss: 15405503.0\n",
      "Epoch [71/100], Training Loss: 173273.38140513003, Test Loss: 15358632.0\n",
      "Epoch [72/100], Training Loss: 171596.1528123334, Test Loss: 15310696.0\n",
      "Epoch [73/100], Training Loss: 169993.49425389493, Test Loss: 15271927.0\n",
      "Epoch [74/100], Training Loss: 168445.08326669037, Test Loss: 15223998.0\n",
      "Epoch [75/100], Training Loss: 166951.8944375333, Test Loss: 15185978.0\n",
      "Epoch [76/100], Training Loss: 165514.24457230023, Test Loss: 15143012.0\n",
      "Epoch [77/100], Training Loss: 164141.0089523725, Test Loss: 15102580.0\n",
      "Epoch [78/100], Training Loss: 162805.7066524495, Test Loss: 15064898.0\n",
      "Epoch [79/100], Training Loss: 161529.86591434156, Test Loss: 15019618.0\n",
      "Epoch [80/100], Training Loss: 160287.23407973463, Test Loss: 15000763.0\n",
      "Epoch [81/100], Training Loss: 159106.13563325632, Test Loss: 14947188.0\n",
      "Epoch [82/100], Training Loss: 157964.5146096203, Test Loss: 14926059.0\n",
      "Epoch [83/100], Training Loss: 156843.91004679818, Test Loss: 14884517.0\n",
      "Epoch [84/100], Training Loss: 155778.66725164387, Test Loss: 14861066.0\n",
      "Epoch [85/100], Training Loss: 154749.1742935845, Test Loss: 14832044.0\n",
      "Epoch [86/100], Training Loss: 153755.2721254665, Test Loss: 14802476.0\n",
      "Epoch [87/100], Training Loss: 152787.3401975594, Test Loss: 14779363.0\n",
      "Epoch [88/100], Training Loss: 151852.3484983117, Test Loss: 14748877.0\n",
      "Epoch [89/100], Training Loss: 150946.29856051182, Test Loss: 14732325.0\n",
      "Epoch [90/100], Training Loss: 150068.1383434038, Test Loss: 14699455.0\n",
      "Epoch [91/100], Training Loss: 149222.14322315028, Test Loss: 14699012.0\n",
      "Epoch [92/100], Training Loss: 148397.21713761034, Test Loss: 14648585.0\n",
      "Epoch [93/100], Training Loss: 147583.74683816126, Test Loss: 14653381.0\n",
      "Epoch [94/100], Training Loss: 146805.3461658077, Test Loss: 14652237.0\n",
      "Epoch [95/100], Training Loss: 146040.66426011492, Test Loss: 14602435.0\n",
      "Epoch [96/100], Training Loss: 145288.72667496002, Test Loss: 14590962.0\n",
      "Epoch [97/100], Training Loss: 144556.41552633137, Test Loss: 14579173.0\n",
      "Epoch [98/100], Training Loss: 143850.15766986553, Test Loss: 14553180.0\n",
      "Epoch [99/100], Training Loss: 143162.66104644275, Test Loss: 14542448.0\n",
      "Epoch [100/100], Training Loss: 142477.83473283573, Test Loss: 14514598.0\n",
      "Epoch [1/100], Training Loss: 2317438.239914697, Test Loss: 291987104.0\n",
      "Epoch [2/100], Training Loss: 2059813.7171968485, Test Loss: 227390496.0\n",
      "Epoch [3/100], Training Loss: 1468372.0127954504, Test Loss: 161363184.0\n",
      "Epoch [4/100], Training Loss: 1161733.3200639773, Test Loss: 141728048.0\n",
      "Epoch [5/100], Training Loss: 1046916.7971091759, Test Loss: 129914848.0\n",
      "Epoch [6/100], Training Loss: 959845.1390320478, Test Loss: 119805208.0\n",
      "Epoch [7/100], Training Loss: 882581.6117528583, Test Loss: 110711712.0\n",
      "Epoch [8/100], Training Loss: 812687.9426574255, Test Loss: 102551080.0\n",
      "Epoch [9/100], Training Loss: 750034.9585925004, Test Loss: 95330144.0\n",
      "Epoch [10/100], Training Loss: 694611.7772643801, Test Loss: 89008976.0\n",
      "Epoch [11/100], Training Loss: 645963.9092470825, Test Loss: 83468088.0\n",
      "Epoch [12/100], Training Loss: 603302.3311415201, Test Loss: 78587440.0\n",
      "Epoch [13/100], Training Loss: 565867.8610271903, Test Loss: 74275616.0\n",
      "Epoch [14/100], Training Loss: 532979.6600912268, Test Loss: 70451376.0\n",
      "Epoch [15/100], Training Loss: 504029.6230081156, Test Loss: 67049484.0\n",
      "Epoch [16/100], Training Loss: 478447.6917244239, Test Loss: 64018976.0\n",
      "Epoch [17/100], Training Loss: 455762.23730821634, Test Loss: 61320320.0\n",
      "Epoch [18/100], Training Loss: 435563.0555061904, Test Loss: 58902124.0\n",
      "Epoch [19/100], Training Loss: 417423.23742669274, Test Loss: 56717676.0\n",
      "Epoch [20/100], Training Loss: 400989.3779989337, Test Loss: 54725212.0\n",
      "Epoch [21/100], Training Loss: 385916.3909720988, Test Loss: 52889992.0\n",
      "Epoch [22/100], Training Loss: 371944.80220366095, Test Loss: 51180992.0\n",
      "Epoch [23/100], Training Loss: 358907.9043895504, Test Loss: 49576300.0\n",
      "Epoch [24/100], Training Loss: 346661.6216456371, Test Loss: 48063204.0\n",
      "Epoch [25/100], Training Loss: 335111.0703749778, Test Loss: 46629336.0\n",
      "Epoch [26/100], Training Loss: 324174.9250636811, Test Loss: 45264160.0\n",
      "Epoch [27/100], Training Loss: 313783.24660861323, Test Loss: 43958964.0\n",
      "Epoch [28/100], Training Loss: 303895.91594099876, Test Loss: 42708152.0\n",
      "Epoch [29/100], Training Loss: 294485.2841063918, Test Loss: 41509868.0\n",
      "Epoch [30/100], Training Loss: 285524.66897695634, Test Loss: 40360680.0\n",
      "Epoch [31/100], Training Loss: 276980.5302410995, Test Loss: 39256168.0\n",
      "Epoch [32/100], Training Loss: 268842.94372371305, Test Loss: 38196260.0\n",
      "Epoch [33/100], Training Loss: 261081.56015638885, Test Loss: 37180736.0\n",
      "Epoch [34/100], Training Loss: 253679.93116521533, Test Loss: 36208308.0\n",
      "Epoch [35/100], Training Loss: 246626.40708488834, Test Loss: 35278980.0\n",
      "Epoch [36/100], Training Loss: 239906.54747941473, Test Loss: 34391396.0\n",
      "Epoch [37/100], Training Loss: 233499.60265979503, Test Loss: 33543032.0\n",
      "Epoch [38/100], Training Loss: 227387.44245009182, Test Loss: 32731766.0\n",
      "Epoch [39/100], Training Loss: 221554.8562585155, Test Loss: 31956388.0\n",
      "Epoch [40/100], Training Loss: 215998.8048101416, Test Loss: 31216298.0\n",
      "Epoch [41/100], Training Loss: 210703.18565250875, Test Loss: 30510030.0\n",
      "Epoch [42/100], Training Loss: 205663.0701084059, Test Loss: 29838308.0\n",
      "Epoch [43/100], Training Loss: 200860.28084829097, Test Loss: 29198362.0\n",
      "Epoch [44/100], Training Loss: 196279.8980806824, Test Loss: 28587282.0\n",
      "Epoch [45/100], Training Loss: 191910.2658314081, Test Loss: 28003202.0\n",
      "Epoch [46/100], Training Loss: 187749.68719270185, Test Loss: 27447142.0\n",
      "Epoch [47/100], Training Loss: 183786.36523310232, Test Loss: 26917314.0\n",
      "Epoch [48/100], Training Loss: 180010.20037320064, Test Loss: 26412984.0\n",
      "Epoch [49/100], Training Loss: 176406.49881523606, Test Loss: 25932484.0\n",
      "Epoch [50/100], Training Loss: 172961.1696581956, Test Loss: 25472982.0\n",
      "Epoch [51/100], Training Loss: 169653.82743913276, Test Loss: 25034064.0\n",
      "Epoch [52/100], Training Loss: 166477.4927137018, Test Loss: 24613916.0\n",
      "Epoch [53/100], Training Loss: 163423.30199632724, Test Loss: 24213300.0\n",
      "Epoch [54/100], Training Loss: 160485.41087613293, Test Loss: 23829350.0\n",
      "Epoch [55/100], Training Loss: 157657.74891890291, Test Loss: 23464446.0\n",
      "Epoch [56/100], Training Loss: 154934.25146614536, Test Loss: 23114444.0\n",
      "Epoch [57/100], Training Loss: 152305.20940702566, Test Loss: 22780000.0\n",
      "Epoch [58/100], Training Loss: 149766.6013861738, Test Loss: 22457884.0\n",
      "Epoch [59/100], Training Loss: 147311.813666252, Test Loss: 22149750.0\n",
      "Epoch [60/100], Training Loss: 144938.6365440436, Test Loss: 21852960.0\n",
      "Epoch [61/100], Training Loss: 142642.50082933475, Test Loss: 21569962.0\n",
      "Epoch [62/100], Training Loss: 140422.059297435, Test Loss: 21298150.0\n",
      "Epoch [63/100], Training Loss: 138274.17759611396, Test Loss: 21037712.0\n",
      "Epoch [64/100], Training Loss: 136196.2143534151, Test Loss: 20788196.0\n",
      "Epoch [65/100], Training Loss: 134182.4407321841, Test Loss: 20549060.0\n",
      "Epoch [66/100], Training Loss: 132228.21349446123, Test Loss: 20318030.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [67/100], Training Loss: 130334.14645163201, Test Loss: 20097942.0\n",
      "Epoch [68/100], Training Loss: 128497.21805580238, Test Loss: 19882934.0\n",
      "Epoch [69/100], Training Loss: 126715.5646881109, Test Loss: 19678462.0\n",
      "Epoch [70/100], Training Loss: 124989.48643445293, Test Loss: 19480798.0\n",
      "Epoch [71/100], Training Loss: 123315.34325573129, Test Loss: 19292566.0\n",
      "Epoch [72/100], Training Loss: 121690.81119305728, Test Loss: 19111136.0\n",
      "Epoch [73/100], Training Loss: 120110.0663763995, Test Loss: 18936964.0\n",
      "Epoch [74/100], Training Loss: 118575.55612819146, Test Loss: 18769582.0\n",
      "Epoch [75/100], Training Loss: 117086.70523073278, Test Loss: 18610792.0\n",
      "Epoch [76/100], Training Loss: 115641.42464901369, Test Loss: 18456402.0\n",
      "Epoch [77/100], Training Loss: 114233.29000059239, Test Loss: 18310290.0\n",
      "Epoch [78/100], Training Loss: 112861.91478585392, Test Loss: 18166786.0\n",
      "Epoch [79/100], Training Loss: 111520.82527693857, Test Loss: 18030888.0\n",
      "Epoch [80/100], Training Loss: 110217.39993187608, Test Loss: 17896416.0\n",
      "Epoch [81/100], Training Loss: 108948.95974764528, Test Loss: 17769638.0\n",
      "Epoch [82/100], Training Loss: 107714.29901960785, Test Loss: 17644446.0\n",
      "Epoch [83/100], Training Loss: 106509.98736745454, Test Loss: 17527582.0\n",
      "Epoch [84/100], Training Loss: 105343.74560156389, Test Loss: 17412496.0\n",
      "Epoch [85/100], Training Loss: 104204.94431609502, Test Loss: 17302758.0\n",
      "Epoch [86/100], Training Loss: 103096.60434808364, Test Loss: 17198544.0\n",
      "Epoch [87/100], Training Loss: 102021.59364077958, Test Loss: 17098278.0\n",
      "Epoch [88/100], Training Loss: 100978.59174515727, Test Loss: 17001056.0\n",
      "Epoch [89/100], Training Loss: 99962.60674723062, Test Loss: 16909736.0\n",
      "Epoch [90/100], Training Loss: 98977.97631953083, Test Loss: 16819214.0\n",
      "Epoch [91/100], Training Loss: 98018.9723950003, Test Loss: 16735505.0\n",
      "Epoch [92/100], Training Loss: 97090.50950773059, Test Loss: 16653822.0\n",
      "Epoch [93/100], Training Loss: 96186.68206859783, Test Loss: 16576449.0\n",
      "Epoch [94/100], Training Loss: 95307.53680172976, Test Loss: 16501019.0\n",
      "Epoch [95/100], Training Loss: 94452.88845447544, Test Loss: 16431842.0\n",
      "Epoch [96/100], Training Loss: 93622.76834903145, Test Loss: 16362329.0\n",
      "Epoch [97/100], Training Loss: 92809.78498015521, Test Loss: 16301003.0\n",
      "Epoch [98/100], Training Loss: 92022.95493454179, Test Loss: 16233573.0\n",
      "Epoch [99/100], Training Loss: 91250.60227474675, Test Loss: 16180298.0\n",
      "Epoch [100/100], Training Loss: 90506.64698477578, Test Loss: 16117759.0\n",
      "Epoch [1/100], Training Loss: 1497907.9836502576, Test Loss: 298836160.0\n",
      "Epoch [2/100], Training Loss: 1469470.0484568449, Test Loss: 285869056.0\n",
      "Epoch [3/100], Training Loss: 1347772.6674960013, Test Loss: 248733408.0\n",
      "Epoch [4/100], Training Loss: 1121481.0918784433, Test Loss: 198975088.0\n",
      "Epoch [5/100], Training Loss: 903326.8176055921, Test Loss: 165128720.0\n",
      "Epoch [6/100], Training Loss: 782310.8446182099, Test Loss: 149400848.0\n",
      "Epoch [7/100], Training Loss: 719966.9363189385, Test Loss: 139844304.0\n",
      "Epoch [8/100], Training Loss: 675158.3107635804, Test Loss: 132148040.0\n",
      "Epoch [9/100], Training Loss: 636729.9126828979, Test Loss: 125385872.0\n",
      "Epoch [10/100], Training Loss: 602078.5555358095, Test Loss: 119288824.0\n",
      "Epoch [11/100], Training Loss: 570332.0007108584, Test Loss: 113746600.0\n",
      "Epoch [12/100], Training Loss: 541050.3216634085, Test Loss: 108657352.0\n",
      "Epoch [13/100], Training Loss: 513547.5661394467, Test Loss: 103911312.0\n",
      "Epoch [14/100], Training Loss: 487938.5375273977, Test Loss: 99625792.0\n",
      "Epoch [15/100], Training Loss: 464876.57484746166, Test Loss: 95840912.0\n",
      "Epoch [16/100], Training Loss: 444139.317575973, Test Loss: 92446600.0\n",
      "Epoch [17/100], Training Loss: 425382.68562288966, Test Loss: 89381336.0\n",
      "Epoch [18/100], Training Loss: 408363.6891179432, Test Loss: 86595216.0\n",
      "Epoch [19/100], Training Loss: 392879.71174693445, Test Loss: 84046760.0\n",
      "Epoch [20/100], Training Loss: 378784.52923405013, Test Loss: 81708112.0\n",
      "Epoch [21/100], Training Loss: 365948.5702268823, Test Loss: 79555592.0\n",
      "Epoch [22/100], Training Loss: 354250.816065399, Test Loss: 77569592.0\n",
      "Epoch [23/100], Training Loss: 343590.3929861975, Test Loss: 75733240.0\n",
      "Epoch [24/100], Training Loss: 333867.9690776613, Test Loss: 74033336.0\n",
      "Epoch [25/100], Training Loss: 324984.9450861916, Test Loss: 72450856.0\n",
      "Epoch [26/100], Training Loss: 316839.12825069606, Test Loss: 70971264.0\n",
      "Epoch [27/100], Training Loss: 309335.1861856525, Test Loss: 69578704.0\n",
      "Epoch [28/100], Training Loss: 302384.22741543746, Test Loss: 68263104.0\n",
      "Epoch [29/100], Training Loss: 295912.018660032, Test Loss: 67014296.0\n",
      "Epoch [30/100], Training Loss: 289851.29577631655, Test Loss: 65825748.0\n",
      "Epoch [31/100], Training Loss: 284143.7568123926, Test Loss: 64689348.0\n",
      "Epoch [32/100], Training Loss: 278729.13430780167, Test Loss: 63594788.0\n",
      "Epoch [33/100], Training Loss: 273564.74007760204, Test Loss: 62535828.0\n",
      "Epoch [34/100], Training Loss: 268613.78222604183, Test Loss: 61511716.0\n",
      "Epoch [35/100], Training Loss: 263840.9935941758, Test Loss: 60516364.0\n",
      "Epoch [36/100], Training Loss: 259216.82156436972, Test Loss: 59544488.0\n",
      "Epoch [37/100], Training Loss: 254708.56029707956, Test Loss: 58593364.0\n",
      "Epoch [38/100], Training Loss: 250297.64442642615, Test Loss: 57661024.0\n",
      "Epoch [39/100], Training Loss: 245969.7961169362, Test Loss: 56746256.0\n",
      "Epoch [40/100], Training Loss: 241716.60030507672, Test Loss: 55848976.0\n",
      "Epoch [41/100], Training Loss: 237526.4809993484, Test Loss: 54967228.0\n",
      "Epoch [42/100], Training Loss: 233400.82812037203, Test Loss: 54101648.0\n",
      "Epoch [43/100], Training Loss: 229348.42667792193, Test Loss: 53253428.0\n",
      "Epoch [44/100], Training Loss: 225378.05321071026, Test Loss: 52422584.0\n",
      "Epoch [45/100], Training Loss: 221485.66523014038, Test Loss: 51608708.0\n",
      "Epoch [46/100], Training Loss: 217668.23089568154, Test Loss: 50813844.0\n",
      "Epoch [47/100], Training Loss: 213930.16099460932, Test Loss: 50037240.0\n",
      "Epoch [48/100], Training Loss: 210280.82108583616, Test Loss: 49281284.0\n",
      "Epoch [49/100], Training Loss: 206719.76599431314, Test Loss: 48544056.0\n",
      "Epoch [50/100], Training Loss: 203245.47243942894, Test Loss: 47825776.0\n",
      "Epoch [51/100], Training Loss: 199856.58288164207, Test Loss: 47122620.0\n",
      "Epoch [52/100], Training Loss: 196552.04519874416, Test Loss: 46433116.0\n",
      "Epoch [53/100], Training Loss: 193329.00853770511, Test Loss: 45761120.0\n",
      "Epoch [54/100], Training Loss: 190188.49025531663, Test Loss: 45109432.0\n",
      "Epoch [55/100], Training Loss: 187128.0662875422, Test Loss: 44471656.0\n",
      "Epoch [56/100], Training Loss: 184151.082374415, Test Loss: 43848604.0\n",
      "Epoch [57/100], Training Loss: 181255.5685348913, Test Loss: 43240876.0\n",
      "Epoch [58/100], Training Loss: 178441.23709718027, Test Loss: 42647456.0\n",
      "Epoch [59/100], Training Loss: 175702.58801315087, Test Loss: 42067988.0\n",
      "Epoch [60/100], Training Loss: 173040.52147014395, Test Loss: 41503108.0\n",
      "Epoch [61/100], Training Loss: 170449.92755538772, Test Loss: 40951568.0\n",
      "Epoch [62/100], Training Loss: 167926.52528175167, Test Loss: 40412480.0\n",
      "Epoch [63/100], Training Loss: 165472.26031114862, Test Loss: 39886008.0\n",
      "Epoch [64/100], Training Loss: 163088.0693753332, Test Loss: 39371396.0\n",
      "Epoch [65/100], Training Loss: 160770.7723577913, Test Loss: 38869044.0\n",
      "Epoch [66/100], Training Loss: 158521.30037121061, Test Loss: 38378696.0\n",
      "Epoch [67/100], Training Loss: 156334.49263062948, Test Loss: 37902824.0\n",
      "Epoch [68/100], Training Loss: 154205.86007428842, Test Loss: 37437444.0\n",
      "Epoch [69/100], Training Loss: 152131.38283778037, Test Loss: 36982660.0\n",
      "Epoch [70/100], Training Loss: 150109.5976051108, Test Loss: 36541260.0\n",
      "Epoch [71/100], Training Loss: 148136.55509074204, Test Loss: 36111236.0\n",
      "Epoch [72/100], Training Loss: 146216.78674930427, Test Loss: 35692768.0\n",
      "Epoch [73/100], Training Loss: 144350.95942316574, Test Loss: 35284748.0\n",
      "Epoch [74/100], Training Loss: 142537.6219144362, Test Loss: 34888172.0\n",
      "Epoch [75/100], Training Loss: 140772.91264483277, Test Loss: 34501756.0\n",
      "Epoch [76/100], Training Loss: 139051.90815094448, Test Loss: 34123644.0\n",
      "Epoch [77/100], Training Loss: 137371.41844867196, Test Loss: 33753568.0\n",
      "Epoch [78/100], Training Loss: 135729.91346965524, Test Loss: 33391356.0\n",
      "Epoch [79/100], Training Loss: 134125.89192777753, Test Loss: 33036412.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/100], Training Loss: 132555.84291626865, Test Loss: 32691666.0\n",
      "Epoch [81/100], Training Loss: 131022.54760969248, Test Loss: 32356154.0\n",
      "Epoch [82/100], Training Loss: 129527.9301829257, Test Loss: 32029444.0\n",
      "Epoch [83/100], Training Loss: 128066.02402779937, Test Loss: 31710690.0\n",
      "Epoch [84/100], Training Loss: 126639.93543406788, Test Loss: 31398832.0\n",
      "Epoch [85/100], Training Loss: 125246.45238313415, Test Loss: 31093556.0\n",
      "Epoch [86/100], Training Loss: 123878.77970147651, Test Loss: 30793710.0\n",
      "Epoch [87/100], Training Loss: 122533.19609231532, Test Loss: 30500672.0\n",
      "Epoch [88/100], Training Loss: 121211.8341311978, Test Loss: 30212294.0\n",
      "Epoch [89/100], Training Loss: 119920.86634983488, Test Loss: 29933774.0\n",
      "Epoch [90/100], Training Loss: 118656.6692222395, Test Loss: 29660382.0\n",
      "Epoch [91/100], Training Loss: 117415.21354259226, Test Loss: 29394076.0\n",
      "Epoch [92/100], Training Loss: 116194.3358491055, Test Loss: 29130918.0\n",
      "Epoch [93/100], Training Loss: 114995.12480840145, Test Loss: 28874802.0\n",
      "Epoch [94/100], Training Loss: 113821.36874065147, Test Loss: 28625344.0\n",
      "Epoch [95/100], Training Loss: 112682.7106764817, Test Loss: 28383364.0\n",
      "Epoch [96/100], Training Loss: 111579.1196630087, Test Loss: 28147556.0\n",
      "Epoch [97/100], Training Loss: 110501.89219342382, Test Loss: 27916068.0\n",
      "Epoch [98/100], Training Loss: 109447.65752686081, Test Loss: 27688384.0\n",
      "Epoch [99/100], Training Loss: 108409.64171026228, Test Loss: 27466748.0\n",
      "Epoch [100/100], Training Loss: 107383.31470430884, Test Loss: 27245880.0\n",
      "Epoch [1/100], Training Loss: 1164409.6089094249, Test Loss: 299361984.0\n",
      "Epoch [2/100], Training Loss: 1150838.0318701498, Test Loss: 291270432.0\n",
      "Epoch [3/100], Training Loss: 1088512.7885788756, Test Loss: 265892832.0\n",
      "Epoch [4/100], Training Loss: 954454.657425508, Test Loss: 223808048.0\n",
      "Epoch [5/100], Training Loss: 785126.9915289378, Test Loss: 182684992.0\n",
      "Epoch [6/100], Training Loss: 655810.6287542207, Test Loss: 158467216.0\n",
      "Epoch [7/100], Training Loss: 587909.6612759908, Test Loss: 146402096.0\n",
      "Epoch [8/100], Training Loss: 549404.693560808, Test Loss: 138316896.0\n",
      "Epoch [9/100], Training Loss: 519969.1276583141, Test Loss: 131539632.0\n",
      "Epoch [10/100], Training Loss: 494101.93069130974, Test Loss: 125416920.0\n",
      "Epoch [11/100], Training Loss: 470282.56359220424, Test Loss: 119727320.0\n",
      "Epoch [12/100], Training Loss: 447931.8966885848, Test Loss: 114378784.0\n",
      "Epoch [13/100], Training Loss: 426784.7177299923, Test Loss: 109309032.0\n",
      "Epoch [14/100], Training Loss: 406482.2155085599, Test Loss: 104424536.0\n",
      "Epoch [15/100], Training Loss: 387055.89005390677, Test Loss: 99850216.0\n",
      "Epoch [16/100], Training Loss: 368983.74883004563, Test Loss: 95645392.0\n",
      "Epoch [17/100], Training Loss: 352413.4591552633, Test Loss: 91814280.0\n",
      "Epoch [18/100], Training Loss: 337261.1731532492, Test Loss: 88307696.0\n",
      "Epoch [19/100], Training Loss: 323372.9239973935, Test Loss: 85090808.0\n",
      "Epoch [20/100], Training Loss: 310630.35294117645, Test Loss: 82129528.0\n",
      "Epoch [21/100], Training Loss: 298935.8264320834, Test Loss: 79398256.0\n",
      "Epoch [22/100], Training Loss: 288196.3075647177, Test Loss: 76876960.0\n",
      "Epoch [23/100], Training Loss: 278321.7711036076, Test Loss: 74543224.0\n",
      "Epoch [24/100], Training Loss: 269227.66743676324, Test Loss: 72379552.0\n",
      "Epoch [25/100], Training Loss: 260835.1553817902, Test Loss: 70367384.0\n",
      "Epoch [26/100], Training Loss: 253074.74474261003, Test Loss: 68491608.0\n",
      "Epoch [27/100], Training Loss: 245883.42491558558, Test Loss: 66740576.0\n",
      "Epoch [28/100], Training Loss: 239191.06308867957, Test Loss: 65101748.0\n",
      "Epoch [29/100], Training Loss: 232938.41431194835, Test Loss: 63561664.0\n",
      "Epoch [30/100], Training Loss: 227078.5295894793, Test Loss: 62110412.0\n",
      "Epoch [31/100], Training Loss: 221562.82708370357, Test Loss: 60737748.0\n",
      "Epoch [32/100], Training Loss: 216354.7250755287, Test Loss: 59441632.0\n",
      "Epoch [33/100], Training Loss: 211428.64545939222, Test Loss: 58216460.0\n",
      "Epoch [34/100], Training Loss: 206763.60559208578, Test Loss: 57055580.0\n",
      "Epoch [35/100], Training Loss: 202332.49546827795, Test Loss: 55953088.0\n",
      "Epoch [36/100], Training Loss: 198115.48024406136, Test Loss: 54900752.0\n",
      "Epoch [37/100], Training Loss: 194076.0221550856, Test Loss: 53892208.0\n",
      "Epoch [38/100], Training Loss: 190203.4982524732, Test Loss: 52926520.0\n",
      "Epoch [39/100], Training Loss: 186486.47307623955, Test Loss: 51994112.0\n",
      "Epoch [40/100], Training Loss: 182909.0089449677, Test Loss: 51094848.0\n",
      "Epoch [41/100], Training Loss: 179461.91220899235, Test Loss: 50230160.0\n",
      "Epoch [42/100], Training Loss: 176136.05331437712, Test Loss: 49394932.0\n",
      "Epoch [43/100], Training Loss: 172924.3192938807, Test Loss: 48588144.0\n",
      "Epoch [44/100], Training Loss: 169822.58870919968, Test Loss: 47809064.0\n",
      "Epoch [45/100], Training Loss: 166825.03050767136, Test Loss: 47054320.0\n",
      "Epoch [46/100], Training Loss: 163923.11012380783, Test Loss: 46322472.0\n",
      "Epoch [47/100], Training Loss: 161109.46792251643, Test Loss: 45614112.0\n",
      "Epoch [48/100], Training Loss: 158378.78135181565, Test Loss: 44927188.0\n",
      "Epoch [49/100], Training Loss: 155731.17054676855, Test Loss: 44260236.0\n",
      "Epoch [50/100], Training Loss: 153160.39440791422, Test Loss: 43611980.0\n",
      "Epoch [51/100], Training Loss: 150661.88495942185, Test Loss: 42980736.0\n",
      "Epoch [52/100], Training Loss: 148236.1696581956, Test Loss: 42365852.0\n",
      "Epoch [53/100], Training Loss: 145881.3895503821, Test Loss: 41767372.0\n",
      "Epoch [54/100], Training Loss: 143596.627628695, Test Loss: 41183940.0\n",
      "Epoch [55/100], Training Loss: 141379.27545761506, Test Loss: 40615452.0\n",
      "Epoch [56/100], Training Loss: 139229.71127302884, Test Loss: 40062932.0\n",
      "Epoch [57/100], Training Loss: 137144.3487352645, Test Loss: 39524564.0\n",
      "Epoch [58/100], Training Loss: 135121.07493631894, Test Loss: 39000684.0\n",
      "Epoch [59/100], Training Loss: 133158.56608020852, Test Loss: 38492544.0\n",
      "Epoch [60/100], Training Loss: 131253.32551389135, Test Loss: 37999876.0\n",
      "Epoch [61/100], Training Loss: 129403.44132456608, Test Loss: 37522088.0\n",
      "Epoch [62/100], Training Loss: 127605.30495823707, Test Loss: 37057840.0\n",
      "Epoch [63/100], Training Loss: 125854.3098750074, Test Loss: 36603964.0\n",
      "Epoch [64/100], Training Loss: 124146.16414904331, Test Loss: 36160528.0\n",
      "Epoch [65/100], Training Loss: 122477.00077009657, Test Loss: 35726332.0\n",
      "Epoch [66/100], Training Loss: 120843.35175641254, Test Loss: 35300320.0\n",
      "Epoch [67/100], Training Loss: 119245.79550974468, Test Loss: 34883760.0\n",
      "Epoch [68/100], Training Loss: 117685.65156092649, Test Loss: 34477480.0\n",
      "Epoch [69/100], Training Loss: 116165.04555417332, Test Loss: 34081528.0\n",
      "Epoch [70/100], Training Loss: 114683.98827083704, Test Loss: 33696636.0\n",
      "Epoch [71/100], Training Loss: 113243.58770215034, Test Loss: 33321446.0\n",
      "Epoch [72/100], Training Loss: 111846.37900598306, Test Loss: 32957280.0\n",
      "Epoch [73/100], Training Loss: 110489.69486404833, Test Loss: 32603756.0\n",
      "Epoch [74/100], Training Loss: 109176.07132278894, Test Loss: 32260326.0\n",
      "Epoch [75/100], Training Loss: 107900.7566494876, Test Loss: 31924966.0\n",
      "Epoch [76/100], Training Loss: 106656.19459747645, Test Loss: 31598246.0\n",
      "Epoch [77/100], Training Loss: 105440.50139209762, Test Loss: 31280828.0\n",
      "Epoch [78/100], Training Loss: 104254.15360464428, Test Loss: 30972412.0\n",
      "Epoch [79/100], Training Loss: 103100.89591848824, Test Loss: 30673512.0\n",
      "Epoch [80/100], Training Loss: 101980.18375688644, Test Loss: 30383206.0\n",
      "Epoch [81/100], Training Loss: 100891.70114329719, Test Loss: 30100180.0\n",
      "Epoch [82/100], Training Loss: 99832.59605473609, Test Loss: 29823472.0\n",
      "Epoch [83/100], Training Loss: 98796.93483798353, Test Loss: 29552028.0\n",
      "Epoch [84/100], Training Loss: 97777.06522125467, Test Loss: 29284480.0\n",
      "Epoch [85/100], Training Loss: 96767.6308867958, Test Loss: 29019246.0\n",
      "Epoch [86/100], Training Loss: 95769.3987323026, Test Loss: 28758824.0\n",
      "Epoch [87/100], Training Loss: 94786.40797346129, Test Loss: 28503452.0\n",
      "Epoch [88/100], Training Loss: 93822.25436881701, Test Loss: 28254162.0\n",
      "Epoch [89/100], Training Loss: 92876.57093774066, Test Loss: 28009790.0\n",
      "Epoch [90/100], Training Loss: 91949.25413186423, Test Loss: 27770268.0\n",
      "Epoch [91/100], Training Loss: 91037.6625792311, Test Loss: 27534692.0\n",
      "Epoch [92/100], Training Loss: 90141.89603696464, Test Loss: 27303876.0\n",
      "Epoch [93/100], Training Loss: 89262.50589420059, Test Loss: 27077354.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [94/100], Training Loss: 88400.32616551152, Test Loss: 26856800.0\n",
      "Epoch [95/100], Training Loss: 87558.53794206504, Test Loss: 26641898.0\n",
      "Epoch [96/100], Training Loss: 86734.48729340678, Test Loss: 26433026.0\n",
      "Epoch [97/100], Training Loss: 85929.65250873764, Test Loss: 26229976.0\n",
      "Epoch [98/100], Training Loss: 85142.23760440732, Test Loss: 26030812.0\n",
      "Epoch [99/100], Training Loss: 84372.42900302114, Test Loss: 25836744.0\n",
      "Epoch [100/100], Training Loss: 83616.62591078728, Test Loss: 25644938.0\n",
      "Epoch [1/100], Training Loss: 582425.820271311, Test Loss: 300034400.0\n",
      "Epoch [2/100], Training Loss: 581922.7685563652, Test Loss: 299436480.0\n",
      "Epoch [3/100], Training Loss: 579450.1567442687, Test Loss: 297225088.0\n",
      "Epoch [4/100], Training Loss: 572528.7307623955, Test Loss: 292009600.0\n",
      "Epoch [5/100], Training Loss: 558526.4010425922, Test Loss: 282562528.0\n",
      "Epoch [6/100], Training Loss: 535550.5166755524, Test Loss: 268258896.0\n",
      "Epoch [7/100], Training Loss: 503279.12611812097, Test Loss: 249474032.0\n",
      "Epoch [8/100], Training Loss: 463592.1995142468, Test Loss: 227792368.0\n",
      "Epoch [9/100], Training Loss: 420600.0355429181, Test Loss: 205780848.0\n",
      "Epoch [10/100], Training Loss: 379650.10982761683, Test Loss: 186195744.0\n",
      "Epoch [11/100], Training Loss: 345369.65108702093, Test Loss: 170834480.0\n",
      "Epoch [12/100], Training Loss: 319662.57875718264, Test Loss: 159817632.0\n",
      "Epoch [13/100], Training Loss: 301412.3750962621, Test Loss: 152038112.0\n",
      "Epoch [14/100], Training Loss: 288147.0595343878, Test Loss: 146214288.0\n",
      "Epoch [15/100], Training Loss: 277735.29198507196, Test Loss: 141464864.0\n",
      "Epoch [16/100], Training Loss: 268886.6531603578, Test Loss: 137313696.0\n",
      "Epoch [17/100], Training Loss: 260941.71103607607, Test Loss: 133527248.0\n",
      "Epoch [18/100], Training Loss: 253577.35916118714, Test Loss: 129989120.0\n",
      "Epoch [19/100], Training Loss: 246625.94585628813, Test Loss: 126635928.0\n",
      "Epoch [20/100], Training Loss: 239991.73556068953, Test Loss: 123430016.0\n",
      "Epoch [21/100], Training Loss: 233615.93886618092, Test Loss: 120347776.0\n",
      "Epoch [22/100], Training Loss: 227461.36650672354, Test Loss: 117374296.0\n",
      "Epoch [23/100], Training Loss: 221504.7987678455, Test Loss: 114500280.0\n",
      "Epoch [24/100], Training Loss: 215732.35566613352, Test Loss: 111720416.0\n",
      "Epoch [25/100], Training Loss: 210136.43694093951, Test Loss: 109031984.0\n",
      "Epoch [26/100], Training Loss: 204714.54961198982, Test Loss: 106434336.0\n",
      "Epoch [27/100], Training Loss: 199467.2325099224, Test Loss: 103927704.0\n",
      "Epoch [28/100], Training Loss: 194395.80119661157, Test Loss: 101512448.0\n",
      "Epoch [29/100], Training Loss: 189501.0978022629, Test Loss: 99188224.0\n",
      "Epoch [30/100], Training Loss: 184784.03435815414, Test Loss: 96953952.0\n",
      "Epoch [31/100], Training Loss: 180246.31597654166, Test Loss: 94809816.0\n",
      "Epoch [32/100], Training Loss: 175889.09898702684, Test Loss: 92755432.0\n",
      "Epoch [33/100], Training Loss: 171712.34381849418, Test Loss: 90789808.0\n",
      "Epoch [34/100], Training Loss: 167714.97162490373, Test Loss: 88911472.0\n",
      "Epoch [35/100], Training Loss: 163894.93252769386, Test Loss: 87118248.0\n",
      "Epoch [36/100], Training Loss: 160248.53456548782, Test Loss: 85407120.0\n",
      "Epoch [37/100], Training Loss: 156770.24086250816, Test Loss: 83774376.0\n",
      "Epoch [38/100], Training Loss: 153453.19637462235, Test Loss: 82215896.0\n",
      "Epoch [39/100], Training Loss: 150290.11314495586, Test Loss: 80727032.0\n",
      "Epoch [40/100], Training Loss: 147272.7987678455, Test Loss: 79303416.0\n",
      "Epoch [41/100], Training Loss: 144392.6750784906, Test Loss: 77940080.0\n",
      "Epoch [42/100], Training Loss: 141640.55257389965, Test Loss: 76632776.0\n",
      "Epoch [43/100], Training Loss: 139007.4386588472, Test Loss: 75377080.0\n",
      "Epoch [44/100], Training Loss: 136485.08334814289, Test Loss: 74169272.0\n",
      "Epoch [45/100], Training Loss: 134065.38333037143, Test Loss: 73005616.0\n",
      "Epoch [46/100], Training Loss: 131738.83632486226, Test Loss: 71880800.0\n",
      "Epoch [47/100], Training Loss: 129492.74829690185, Test Loss: 70790944.0\n",
      "Epoch [48/100], Training Loss: 127317.5738404123, Test Loss: 69729960.0\n",
      "Epoch [49/100], Training Loss: 125197.14542977311, Test Loss: 68693656.0\n",
      "Epoch [50/100], Training Loss: 123142.51596469403, Test Loss: 67685536.0\n",
      "Epoch [51/100], Training Loss: 121150.09063444109, Test Loss: 66707528.0\n",
      "Epoch [52/100], Training Loss: 119227.73034772821, Test Loss: 65755136.0\n",
      "Epoch [53/100], Training Loss: 117372.87056454002, Test Loss: 64840456.0\n",
      "Epoch [54/100], Training Loss: 115596.74308394053, Test Loss: 63959140.0\n",
      "Epoch [55/100], Training Loss: 113881.81079319946, Test Loss: 63106148.0\n",
      "Epoch [56/100], Training Loss: 112220.656833126, Test Loss: 62278120.0\n",
      "Epoch [57/100], Training Loss: 110604.51501688288, Test Loss: 61469980.0\n",
      "Epoch [58/100], Training Loss: 109031.7637580712, Test Loss: 60684480.0\n",
      "Epoch [59/100], Training Loss: 107505.83247437948, Test Loss: 59922960.0\n",
      "Epoch [60/100], Training Loss: 106025.17481191873, Test Loss: 59182772.0\n",
      "Epoch [61/100], Training Loss: 104585.00313962443, Test Loss: 58462316.0\n",
      "Epoch [62/100], Training Loss: 103181.25952254013, Test Loss: 57760152.0\n",
      "Epoch [63/100], Training Loss: 101812.09549197322, Test Loss: 57074700.0\n",
      "Epoch [64/100], Training Loss: 100475.6658965701, Test Loss: 56405384.0\n",
      "Epoch [65/100], Training Loss: 99170.85137136426, Test Loss: 55751568.0\n",
      "Epoch [66/100], Training Loss: 97896.24808956815, Test Loss: 55112696.0\n",
      "Epoch [67/100], Training Loss: 96649.23227296962, Test Loss: 54486996.0\n",
      "Epoch [68/100], Training Loss: 95427.31947159528, Test Loss: 53873968.0\n",
      "Epoch [69/100], Training Loss: 94230.24015164978, Test Loss: 53272656.0\n",
      "Epoch [70/100], Training Loss: 93056.0646881109, Test Loss: 52682924.0\n",
      "Epoch [71/100], Training Loss: 91903.41804395475, Test Loss: 52104160.0\n",
      "Epoch [72/100], Training Loss: 90771.56791659261, Test Loss: 51535608.0\n",
      "Epoch [73/100], Training Loss: 89660.0073455364, Test Loss: 50976352.0\n",
      "Epoch [74/100], Training Loss: 88570.11586991292, Test Loss: 50428268.0\n",
      "Epoch [75/100], Training Loss: 87502.38030922339, Test Loss: 49890492.0\n",
      "Epoch [76/100], Training Loss: 86455.64836206386, Test Loss: 49361372.0\n",
      "Epoch [77/100], Training Loss: 85428.70398673064, Test Loss: 48840744.0\n",
      "Epoch [78/100], Training Loss: 84420.90279011907, Test Loss: 48328620.0\n",
      "Epoch [79/100], Training Loss: 83430.68218707423, Test Loss: 47823408.0\n",
      "Epoch [80/100], Training Loss: 82455.58201528345, Test Loss: 47325504.0\n",
      "Epoch [81/100], Training Loss: 81495.63521118417, Test Loss: 46836084.0\n",
      "Epoch [82/100], Training Loss: 80552.77981162253, Test Loss: 46355576.0\n",
      "Epoch [83/100], Training Loss: 79628.91392690006, Test Loss: 45884156.0\n",
      "Epoch [84/100], Training Loss: 78722.63598128073, Test Loss: 45420332.0\n",
      "Epoch [85/100], Training Loss: 77832.2158639891, Test Loss: 44963184.0\n",
      "Epoch [86/100], Training Loss: 76957.4226645341, Test Loss: 44513684.0\n",
      "Epoch [87/100], Training Loss: 76098.33244476038, Test Loss: 44070912.0\n",
      "Epoch [88/100], Training Loss: 75253.94123570879, Test Loss: 43635204.0\n",
      "Epoch [89/100], Training Loss: 74424.35021621942, Test Loss: 43206356.0\n",
      "Epoch [90/100], Training Loss: 73609.55808305195, Test Loss: 42784120.0\n",
      "Epoch [91/100], Training Loss: 72809.33890172384, Test Loss: 42368860.0\n",
      "Epoch [92/100], Training Loss: 72022.98821159884, Test Loss: 41960016.0\n",
      "Epoch [93/100], Training Loss: 71250.16823647889, Test Loss: 41557256.0\n",
      "Epoch [94/100], Training Loss: 70490.79841241632, Test Loss: 41160816.0\n",
      "Epoch [95/100], Training Loss: 69744.74687518513, Test Loss: 40770464.0\n",
      "Epoch [96/100], Training Loss: 69011.85344470115, Test Loss: 40385956.0\n",
      "Epoch [97/100], Training Loss: 68291.55381790179, Test Loss: 40007508.0\n",
      "Epoch [98/100], Training Loss: 67583.60867247201, Test Loss: 39634436.0\n",
      "Epoch [99/100], Training Loss: 66887.26177359161, Test Loss: 39266732.0\n",
      "Epoch [100/100], Training Loss: 66202.09632130798, Test Loss: 38904332.0\n",
      "Epoch [1/100], Training Loss: 7263863.510988685, Test Loss: 150522656.0\n",
      "Epoch [2/100], Training Loss: 4126380.2647947394, Test Loss: 119608512.0\n",
      "Epoch [3/100], Training Loss: 3327712.25164386, Test Loss: 98436560.0\n",
      "Epoch [4/100], Training Loss: 2740985.4676855635, Test Loss: 82850120.0\n",
      "Epoch [5/100], Training Loss: 2300847.822107695, Test Loss: 71074256.0\n",
      "Epoch [6/100], Training Loss: 1974159.2660387417, Test Loss: 62321608.0\n",
      "Epoch [7/100], Training Loss: 1736787.2280078195, Test Loss: 55963212.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100], Training Loss: 1563445.9047153604, Test Loss: 51164432.0\n",
      "Epoch [9/100], Training Loss: 1428181.0828149992, Test Loss: 47256300.0\n",
      "Epoch [10/100], Training Loss: 1315003.774805995, Test Loss: 43894568.0\n",
      "Epoch [11/100], Training Loss: 1216472.1196315384, Test Loss: 40904140.0\n",
      "Epoch [12/100], Training Loss: 1129452.4951424678, Test Loss: 38212036.0\n",
      "Epoch [13/100], Training Loss: 1052240.1591582252, Test Loss: 35777204.0\n",
      "Epoch [14/100], Training Loss: 983745.1784550678, Test Loss: 33592844.0\n",
      "Epoch [15/100], Training Loss: 922993.2040755879, Test Loss: 31647550.0\n",
      "Epoch [16/100], Training Loss: 869196.1903471359, Test Loss: 29919138.0\n",
      "Epoch [17/100], Training Loss: 821471.8236775072, Test Loss: 28385376.0\n",
      "Epoch [18/100], Training Loss: 779210.7746430899, Test Loss: 27019164.0\n",
      "Epoch [19/100], Training Loss: 741823.3149398732, Test Loss: 25802862.0\n",
      "Epoch [20/100], Training Loss: 708590.1781292578, Test Loss: 24722008.0\n",
      "Epoch [21/100], Training Loss: 678703.0534476631, Test Loss: 23751366.0\n",
      "Epoch [22/100], Training Loss: 651568.0074047745, Test Loss: 22880188.0\n",
      "Epoch [23/100], Training Loss: 626778.1154404359, Test Loss: 22094148.0\n",
      "Epoch [24/100], Training Loss: 604016.4021533085, Test Loss: 21390788.0\n",
      "Epoch [25/100], Training Loss: 582993.6242965464, Test Loss: 20755566.0\n",
      "Epoch [26/100], Training Loss: 563612.7850245838, Test Loss: 20191456.0\n",
      "Epoch [27/100], Training Loss: 545707.8428558735, Test Loss: 19675182.0\n",
      "Epoch [28/100], Training Loss: 529099.8435074936, Test Loss: 19218284.0\n",
      "Epoch [29/100], Training Loss: 513676.13831378473, Test Loss: 18802828.0\n",
      "Epoch [30/100], Training Loss: 499319.82154493214, Test Loss: 18421796.0\n",
      "Epoch [31/100], Training Loss: 485933.2333910906, Test Loss: 18074782.0\n",
      "Epoch [32/100], Training Loss: 473432.4144674486, Test Loss: 17751976.0\n",
      "Epoch [33/100], Training Loss: 461732.30583200045, Test Loss: 17456780.0\n",
      "Epoch [34/100], Training Loss: 450815.141327528, Test Loss: 17187526.0\n",
      "Epoch [35/100], Training Loss: 440576.2744653753, Test Loss: 16940230.0\n",
      "Epoch [36/100], Training Loss: 430940.954468041, Test Loss: 16717700.0\n",
      "Epoch [37/100], Training Loss: 421904.4760751733, Test Loss: 16517335.0\n",
      "Epoch [38/100], Training Loss: 413377.0247763758, Test Loss: 16332715.0\n",
      "Epoch [39/100], Training Loss: 405347.9225534625, Test Loss: 16183408.0\n",
      "Epoch [40/100], Training Loss: 397810.6533380724, Test Loss: 16027953.0\n",
      "Epoch [41/100], Training Loss: 390666.9353118891, Test Loss: 15899771.0\n",
      "Epoch [42/100], Training Loss: 383900.83053432853, Test Loss: 15789096.0\n",
      "Epoch [43/100], Training Loss: 377534.1421938866, Test Loss: 15685106.0\n",
      "Epoch [44/100], Training Loss: 371477.79251081095, Test Loss: 15600072.0\n",
      "Epoch [45/100], Training Loss: 365768.53346958116, Test Loss: 15520484.0\n",
      "Epoch [46/100], Training Loss: 360373.6270659321, Test Loss: 15464979.0\n",
      "Epoch [47/100], Training Loss: 355238.3076091464, Test Loss: 15397911.0\n",
      "Epoch [48/100], Training Loss: 350372.50623482023, Test Loss: 15338170.0\n",
      "Epoch [49/100], Training Loss: 345758.0704860494, Test Loss: 15278324.0\n",
      "Epoch [50/100], Training Loss: 341382.8778286239, Test Loss: 15230760.0\n",
      "Epoch [51/100], Training Loss: 337237.9227163675, Test Loss: 15153101.0\n",
      "Epoch [52/100], Training Loss: 333315.4520762988, Test Loss: 15143720.0\n",
      "Epoch [53/100], Training Loss: 329630.5102259937, Test Loss: 15054334.0\n",
      "Epoch [54/100], Training Loss: 326076.7051122564, Test Loss: 15050171.0\n",
      "Epoch [55/100], Training Loss: 322719.66007641726, Test Loss: 14973840.0\n",
      "Epoch [56/100], Training Loss: 319564.60663986136, Test Loss: 14970702.0\n",
      "Epoch [57/100], Training Loss: 316534.62224542385, Test Loss: 14899183.0\n",
      "Epoch [58/100], Training Loss: 313642.3934934246, Test Loss: 14892869.0\n",
      "Epoch [59/100], Training Loss: 310877.80053388426, Test Loss: 14835143.0\n",
      "Epoch [60/100], Training Loss: 308219.90026138857, Test Loss: 14821160.0\n",
      "Epoch [61/100], Training Loss: 305710.37638839526, Test Loss: 14773909.0\n",
      "Epoch [62/100], Training Loss: 303282.01758263726, Test Loss: 14766070.0\n",
      "Epoch [63/100], Training Loss: 300991.11415200523, Test Loss: 14725145.0\n",
      "Epoch [64/100], Training Loss: 298771.97138054617, Test Loss: 14724024.0\n",
      "Epoch [65/100], Training Loss: 296680.6375140691, Test Loss: 14683161.0\n",
      "Epoch [66/100], Training Loss: 294674.49377628695, Test Loss: 14676547.0\n",
      "Epoch [67/100], Training Loss: 292728.4855421776, Test Loss: 14636171.0\n",
      "Epoch [68/100], Training Loss: 290889.5370831112, Test Loss: 14627902.0\n",
      "Epoch [69/100], Training Loss: 289110.17489337124, Test Loss: 14586010.0\n",
      "Epoch [70/100], Training Loss: 287400.5061200462, Test Loss: 14586964.0\n",
      "Epoch [71/100], Training Loss: 285723.56699840055, Test Loss: 14544691.0\n",
      "Epoch [72/100], Training Loss: 284129.04354747944, Test Loss: 14557244.0\n",
      "Epoch [73/100], Training Loss: 282569.0911564777, Test Loss: 14511695.0\n",
      "Epoch [74/100], Training Loss: 281065.8759070849, Test Loss: 14534243.0\n",
      "Epoch [75/100], Training Loss: 279546.7632952728, Test Loss: 14494020.0\n",
      "Epoch [76/100], Training Loss: 278113.9428277353, Test Loss: 14504344.0\n",
      "Epoch [77/100], Training Loss: 276708.47645651916, Test Loss: 14489183.0\n",
      "Epoch [78/100], Training Loss: 275337.4097357976, Test Loss: 14439517.0\n",
      "Epoch [79/100], Training Loss: 273960.67812925775, Test Loss: 14497956.0\n",
      "Epoch [80/100], Training Loss: 272730.8440110183, Test Loss: 14451378.0\n",
      "Epoch [81/100], Training Loss: 271450.87247497187, Test Loss: 14447008.0\n",
      "Epoch [82/100], Training Loss: 270270.9631612464, Test Loss: 14451789.0\n",
      "Epoch [83/100], Training Loss: 269051.67952135537, Test Loss: 14418138.0\n",
      "Epoch [84/100], Training Loss: 267908.38572951837, Test Loss: 14431311.0\n",
      "Epoch [85/100], Training Loss: 266729.7121578994, Test Loss: 14458870.0\n",
      "Epoch [86/100], Training Loss: 265622.87378931936, Test Loss: 14389341.0\n",
      "Epoch [87/100], Training Loss: 264493.5146318346, Test Loss: 14410963.0\n",
      "Epoch [88/100], Training Loss: 263419.6420939222, Test Loss: 14405529.0\n",
      "Epoch [89/100], Training Loss: 262323.13617010246, Test Loss: 14367291.0\n",
      "Epoch [90/100], Training Loss: 261269.9483146733, Test Loss: 14384657.0\n",
      "Epoch [91/100], Training Loss: 260194.61014231975, Test Loss: 14394253.0\n",
      "Epoch [92/100], Training Loss: 259203.73125111073, Test Loss: 14323367.0\n",
      "Epoch [93/100], Training Loss: 258165.5441398614, Test Loss: 14360059.0\n",
      "Epoch [94/100], Training Loss: 257229.60208962738, Test Loss: 14350312.0\n",
      "Epoch [95/100], Training Loss: 256261.3469840353, Test Loss: 14316032.0\n",
      "Epoch [96/100], Training Loss: 255348.8524043303, Test Loss: 14350593.0\n",
      "Epoch [97/100], Training Loss: 254445.87786564778, Test Loss: 14345094.0\n",
      "Epoch [98/100], Training Loss: 253567.2060415556, Test Loss: 14289987.0\n",
      "Epoch [99/100], Training Loss: 252686.11265253834, Test Loss: 14349525.0\n",
      "Epoch [100/100], Training Loss: 251873.47460532552, Test Loss: 14325703.0\n",
      "Epoch [1/100], Training Loss: 4391567.260944257, Test Loss: 231188224.0\n",
      "Epoch [2/100], Training Loss: 2678818.0152834547, Test Loss: 144978912.0\n",
      "Epoch [3/100], Training Loss: 2076562.804928618, Test Loss: 125309872.0\n",
      "Epoch [4/100], Training Loss: 1799758.8574136603, Test Loss: 109696912.0\n",
      "Epoch [5/100], Training Loss: 1569607.9937207513, Test Loss: 96546984.0\n",
      "Epoch [6/100], Training Loss: 1374622.2876606835, Test Loss: 85843456.0\n",
      "Epoch [7/100], Training Loss: 1218353.6243113559, Test Loss: 77218112.0\n",
      "Epoch [8/100], Training Loss: 1092781.5426811208, Test Loss: 70226192.0\n",
      "Epoch [9/100], Training Loss: 992543.595403116, Test Loss: 64570664.0\n",
      "Epoch [10/100], Training Loss: 912384.6937385226, Test Loss: 60017956.0\n",
      "Epoch [11/100], Training Loss: 847826.964457082, Test Loss: 56283724.0\n",
      "Epoch [12/100], Training Loss: 794402.4287068301, Test Loss: 53133920.0\n",
      "Epoch [13/100], Training Loss: 748665.826905989, Test Loss: 50405448.0\n",
      "Epoch [14/100], Training Loss: 708625.7991232747, Test Loss: 47992256.0\n",
      "Epoch [15/100], Training Loss: 672984.9953794207, Test Loss: 45823896.0\n",
      "Epoch [16/100], Training Loss: 640871.956045258, Test Loss: 43856904.0\n",
      "Epoch [17/100], Training Loss: 611728.6805580239, Test Loss: 42054852.0\n",
      "Epoch [18/100], Training Loss: 585133.3021148036, Test Loss: 40389104.0\n",
      "Epoch [19/100], Training Loss: 560838.5288193828, Test Loss: 38852648.0\n",
      "Epoch [20/100], Training Loss: 538584.2068894022, Test Loss: 37431744.0\n",
      "Epoch [21/100], Training Loss: 517991.1638528523, Test Loss: 36110592.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/100], Training Loss: 498761.32856465847, Test Loss: 34879628.0\n",
      "Epoch [23/100], Training Loss: 480723.2306735383, Test Loss: 33726484.0\n",
      "Epoch [24/100], Training Loss: 463782.82758722827, Test Loss: 32641748.0\n",
      "Epoch [25/100], Training Loss: 447875.7452165156, Test Loss: 31613742.0\n",
      "Epoch [26/100], Training Loss: 432890.475919673, Test Loss: 30644348.0\n",
      "Epoch [27/100], Training Loss: 418957.3209525502, Test Loss: 29746684.0\n",
      "Epoch [28/100], Training Loss: 406089.0836147148, Test Loss: 28919298.0\n",
      "Epoch [29/100], Training Loss: 394154.6484509212, Test Loss: 28152064.0\n",
      "Epoch [30/100], Training Loss: 383113.63944671524, Test Loss: 27440878.0\n",
      "Epoch [31/100], Training Loss: 372815.38629228127, Test Loss: 26767432.0\n",
      "Epoch [32/100], Training Loss: 362951.6750488715, Test Loss: 26120148.0\n",
      "Epoch [33/100], Training Loss: 353519.4808364433, Test Loss: 25505862.0\n",
      "Epoch [34/100], Training Loss: 344468.96238374506, Test Loss: 24909334.0\n",
      "Epoch [35/100], Training Loss: 335705.8237959836, Test Loss: 24337002.0\n",
      "Epoch [36/100], Training Loss: 327495.43252769386, Test Loss: 23815226.0\n",
      "Epoch [37/100], Training Loss: 319830.10979799775, Test Loss: 23329020.0\n",
      "Epoch [38/100], Training Loss: 312602.4583703572, Test Loss: 22872824.0\n",
      "Epoch [39/100], Training Loss: 305756.61859487, Test Loss: 22446846.0\n",
      "Epoch [40/100], Training Loss: 299246.8896392394, Test Loss: 22043074.0\n",
      "Epoch [41/100], Training Loss: 292936.6821870742, Test Loss: 21652582.0\n",
      "Epoch [42/100], Training Loss: 286868.98995912564, Test Loss: 21282280.0\n",
      "Epoch [43/100], Training Loss: 281005.4954534684, Test Loss: 20921096.0\n",
      "Epoch [44/100], Training Loss: 275354.0875392453, Test Loss: 20584286.0\n",
      "Epoch [45/100], Training Loss: 270004.54962679936, Test Loss: 20270540.0\n",
      "Epoch [46/100], Training Loss: 264931.0640216812, Test Loss: 19983062.0\n",
      "Epoch [47/100], Training Loss: 260067.11507019727, Test Loss: 19698416.0\n",
      "Epoch [48/100], Training Loss: 255361.6519607843, Test Loss: 19438088.0\n",
      "Epoch [49/100], Training Loss: 250908.82089331202, Test Loss: 19196534.0\n",
      "Epoch [50/100], Training Loss: 246734.1968929566, Test Loss: 18978252.0\n",
      "Epoch [51/100], Training Loss: 242715.49680113737, Test Loss: 18763296.0\n",
      "Epoch [52/100], Training Loss: 238741.8192346425, Test Loss: 18560958.0\n",
      "Epoch [53/100], Training Loss: 234936.10889461526, Test Loss: 18370614.0\n",
      "Epoch [54/100], Training Loss: 231401.960710266, Test Loss: 18207950.0\n",
      "Epoch [55/100], Training Loss: 228074.45377939695, Test Loss: 18043636.0\n",
      "Epoch [56/100], Training Loss: 224871.2048456845, Test Loss: 17896422.0\n",
      "Epoch [57/100], Training Loss: 221742.35301522422, Test Loss: 17741590.0\n",
      "Epoch [58/100], Training Loss: 218688.68514898408, Test Loss: 17604426.0\n",
      "Epoch [59/100], Training Loss: 215688.15122030684, Test Loss: 17457918.0\n",
      "Epoch [60/100], Training Loss: 212772.23502754577, Test Loss: 17331306.0\n",
      "Epoch [61/100], Training Loss: 210004.580223328, Test Loss: 17211150.0\n",
      "Epoch [62/100], Training Loss: 207391.48727859723, Test Loss: 17101848.0\n",
      "Epoch [63/100], Training Loss: 204917.07527693856, Test Loss: 17002602.0\n",
      "Epoch [64/100], Training Loss: 202550.93415674428, Test Loss: 16911774.0\n",
      "Epoch [65/100], Training Loss: 200311.86364848053, Test Loss: 16824566.0\n",
      "Epoch [66/100], Training Loss: 198163.9590071678, Test Loss: 16748495.0\n",
      "Epoch [67/100], Training Loss: 196097.8421746342, Test Loss: 16668973.0\n",
      "Epoch [68/100], Training Loss: 194133.0358242995, Test Loss: 16606265.0\n",
      "Epoch [69/100], Training Loss: 192281.80974172146, Test Loss: 16529993.0\n",
      "Epoch [70/100], Training Loss: 190517.68292755168, Test Loss: 16478345.0\n",
      "Epoch [71/100], Training Loss: 188834.66259404062, Test Loss: 16411336.0\n",
      "Epoch [72/100], Training Loss: 187206.14750311, Test Loss: 16361128.0\n",
      "Epoch [73/100], Training Loss: 185620.15046501983, Test Loss: 16294977.0\n",
      "Epoch [74/100], Training Loss: 184083.25690124993, Test Loss: 16250989.0\n",
      "Epoch [75/100], Training Loss: 182591.11632900895, Test Loss: 16177218.0\n",
      "Epoch [76/100], Training Loss: 181154.4650494639, Test Loss: 16155558.0\n",
      "Epoch [77/100], Training Loss: 179719.74770451986, Test Loss: 16050421.0\n",
      "Epoch [78/100], Training Loss: 178299.25309519577, Test Loss: 16040025.0\n",
      "Epoch [79/100], Training Loss: 176920.20032877199, Test Loss: 15944962.0\n",
      "Epoch [80/100], Training Loss: 175599.01701617203, Test Loss: 15920056.0\n",
      "Epoch [81/100], Training Loss: 174346.79656122267, Test Loss: 15847839.0\n",
      "Epoch [82/100], Training Loss: 173116.1826017416, Test Loss: 15816602.0\n",
      "Epoch [83/100], Training Loss: 171947.97243942894, Test Loss: 15756921.0\n",
      "Epoch [84/100], Training Loss: 170817.635907233, Test Loss: 15721257.0\n",
      "Epoch [85/100], Training Loss: 169683.72714886558, Test Loss: 15672976.0\n",
      "Epoch [86/100], Training Loss: 168559.71610094188, Test Loss: 15627746.0\n",
      "Epoch [87/100], Training Loss: 167465.03015224217, Test Loss: 15565902.0\n",
      "Epoch [88/100], Training Loss: 166423.3183608791, Test Loss: 15583058.0\n",
      "Epoch [89/100], Training Loss: 165425.49246193946, Test Loss: 15493871.0\n",
      "Epoch [90/100], Training Loss: 164460.01828979325, Test Loss: 15496220.0\n",
      "Epoch [91/100], Training Loss: 163523.57421065102, Test Loss: 15424166.0\n",
      "Epoch [92/100], Training Loss: 162623.57489189028, Test Loss: 15427288.0\n",
      "Epoch [93/100], Training Loss: 161741.7091626681, Test Loss: 15357047.0\n",
      "Epoch [94/100], Training Loss: 160877.12970203187, Test Loss: 15357431.0\n",
      "Epoch [95/100], Training Loss: 160016.6956045258, Test Loss: 15299475.0\n",
      "Epoch [96/100], Training Loss: 159162.2148125111, Test Loss: 15284519.0\n",
      "Epoch [97/100], Training Loss: 158307.5771947752, Test Loss: 15247186.0\n",
      "Epoch [98/100], Training Loss: 157463.2275561282, Test Loss: 15212764.0\n",
      "Epoch [99/100], Training Loss: 156621.37087554054, Test Loss: 15180493.0\n",
      "Epoch [100/100], Training Loss: 155807.44062851727, Test Loss: 15188597.0\n",
      "Epoch [1/100], Training Loss: 2318178.3723713052, Test Loss: 292394976.0\n",
      "Epoch [2/100], Training Loss: 2071918.524731947, Test Loss: 230259344.0\n",
      "Epoch [3/100], Training Loss: 1489759.1488655885, Test Loss: 163201680.0\n",
      "Epoch [4/100], Training Loss: 1171377.7769089509, Test Loss: 142694304.0\n",
      "Epoch [5/100], Training Loss: 1054482.3207155974, Test Loss: 130883136.0\n",
      "Epoch [6/100], Training Loss: 967852.1191872519, Test Loss: 120846632.0\n",
      "Epoch [7/100], Training Loss: 891033.6731236301, Test Loss: 111712312.0\n",
      "Epoch [8/100], Training Loss: 819323.0844144304, Test Loss: 103224912.0\n",
      "Epoch [9/100], Training Loss: 754878.3200047391, Test Loss: 95901776.0\n",
      "Epoch [10/100], Training Loss: 698989.9809253006, Test Loss: 89554816.0\n",
      "Epoch [11/100], Training Loss: 650228.4361116048, Test Loss: 84011080.0\n",
      "Epoch [12/100], Training Loss: 607602.1273621231, Test Loss: 79138688.0\n",
      "Epoch [13/100], Training Loss: 570359.9471595285, Test Loss: 74859048.0\n",
      "Epoch [14/100], Training Loss: 537901.8648184349, Test Loss: 71093904.0\n",
      "Epoch [15/100], Training Loss: 509583.32492150937, Test Loss: 67769408.0\n",
      "Epoch [16/100], Training Loss: 484816.95491973223, Test Loss: 64838640.0\n",
      "Epoch [17/100], Training Loss: 463065.71269474557, Test Loss: 62241972.0\n",
      "Epoch [18/100], Training Loss: 443824.30495823704, Test Loss: 59924176.0\n",
      "Epoch [19/100], Training Loss: 426631.516260885, Test Loss: 57840608.0\n",
      "Epoch [20/100], Training Loss: 411145.0597713406, Test Loss: 55955188.0\n",
      "Epoch [21/100], Training Loss: 397060.0777205142, Test Loss: 54228120.0\n",
      "Epoch [22/100], Training Loss: 384116.9368520822, Test Loss: 52630660.0\n",
      "Epoch [23/100], Training Loss: 372120.3317931402, Test Loss: 51140448.0\n",
      "Epoch [24/100], Training Loss: 360903.12120135064, Test Loss: 49742420.0\n",
      "Epoch [25/100], Training Loss: 350334.51395059534, Test Loss: 48422284.0\n",
      "Epoch [26/100], Training Loss: 340315.5890646289, Test Loss: 47172948.0\n",
      "Epoch [27/100], Training Loss: 330751.58207452163, Test Loss: 45979860.0\n",
      "Epoch [28/100], Training Loss: 321609.8489425982, Test Loss: 44837560.0\n",
      "Epoch [29/100], Training Loss: 312910.486464072, Test Loss: 43753964.0\n",
      "Epoch [30/100], Training Loss: 304678.5218885137, Test Loss: 42720880.0\n",
      "Epoch [31/100], Training Loss: 296869.5237248978, Test Loss: 41735204.0\n",
      "Epoch [32/100], Training Loss: 289433.75765653694, Test Loss: 40790800.0\n",
      "Epoch [33/100], Training Loss: 282307.1541970262, Test Loss: 39879508.0\n",
      "Epoch [34/100], Training Loss: 275483.30904567265, Test Loss: 39007668.0\n",
      "Epoch [35/100], Training Loss: 268983.306143001, Test Loss: 38174408.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/100], Training Loss: 262781.79231088207, Test Loss: 37377712.0\n",
      "Epoch [37/100], Training Loss: 256882.7635803566, Test Loss: 36618784.0\n",
      "Epoch [38/100], Training Loss: 251264.11883182277, Test Loss: 35896096.0\n",
      "Epoch [39/100], Training Loss: 245892.8135773947, Test Loss: 35204164.0\n",
      "Epoch [40/100], Training Loss: 240767.98560511819, Test Loss: 34542284.0\n",
      "Epoch [41/100], Training Loss: 235891.45939221611, Test Loss: 33910096.0\n",
      "Epoch [42/100], Training Loss: 231243.9979266631, Test Loss: 33305470.0\n",
      "Epoch [43/100], Training Loss: 226803.13618861442, Test Loss: 32726750.0\n",
      "Epoch [44/100], Training Loss: 222537.973816717, Test Loss: 32171260.0\n",
      "Epoch [45/100], Training Loss: 218430.65274569043, Test Loss: 31632558.0\n",
      "Epoch [46/100], Training Loss: 214431.67395296486, Test Loss: 31107546.0\n",
      "Epoch [47/100], Training Loss: 210527.41209051595, Test Loss: 30600110.0\n",
      "Epoch [48/100], Training Loss: 206768.16539304543, Test Loss: 30114456.0\n",
      "Epoch [49/100], Training Loss: 203144.15082044902, Test Loss: 29641230.0\n",
      "Epoch [50/100], Training Loss: 199589.56388839523, Test Loss: 29178430.0\n",
      "Epoch [51/100], Training Loss: 196108.66204608732, Test Loss: 28723876.0\n",
      "Epoch [52/100], Training Loss: 192716.57541022453, Test Loss: 28286652.0\n",
      "Epoch [53/100], Training Loss: 189427.25904863456, Test Loss: 27864424.0\n",
      "Epoch [54/100], Training Loss: 186276.319382738, Test Loss: 27459986.0\n",
      "Epoch [55/100], Training Loss: 183250.0401634974, Test Loss: 27069278.0\n",
      "Epoch [56/100], Training Loss: 180329.84136010899, Test Loss: 26691660.0\n",
      "Epoch [57/100], Training Loss: 177479.4043303122, Test Loss: 26323766.0\n",
      "Epoch [58/100], Training Loss: 174691.33819086547, Test Loss: 25966146.0\n",
      "Epoch [59/100], Training Loss: 171988.88078312896, Test Loss: 25623090.0\n",
      "Epoch [60/100], Training Loss: 169364.0333511048, Test Loss: 25287876.0\n",
      "Epoch [61/100], Training Loss: 166813.22720810378, Test Loss: 24965574.0\n",
      "Epoch [62/100], Training Loss: 164355.70991055033, Test Loss: 24655580.0\n",
      "Epoch [63/100], Training Loss: 161967.90246430898, Test Loss: 24349722.0\n",
      "Epoch [64/100], Training Loss: 159633.87918369766, Test Loss: 24056444.0\n",
      "Epoch [65/100], Training Loss: 157371.93069130977, Test Loss: 23772000.0\n",
      "Epoch [66/100], Training Loss: 155156.76325454653, Test Loss: 23493178.0\n",
      "Epoch [67/100], Training Loss: 152936.23111782476, Test Loss: 23206002.0\n",
      "Epoch [68/100], Training Loss: 150639.4483442924, Test Loss: 22903178.0\n",
      "Epoch [69/100], Training Loss: 148255.46549375038, Test Loss: 22600134.0\n",
      "Epoch [70/100], Training Loss: 145923.74098098453, Test Loss: 22317984.0\n",
      "Epoch [71/100], Training Loss: 143735.04327350276, Test Loss: 22058656.0\n",
      "Epoch [72/100], Training Loss: 141659.98258397015, Test Loss: 21810668.0\n",
      "Epoch [73/100], Training Loss: 139671.64631834606, Test Loss: 21575604.0\n",
      "Epoch [74/100], Training Loss: 137770.1972928144, Test Loss: 21351660.0\n",
      "Epoch [75/100], Training Loss: 135941.3044843315, Test Loss: 21139722.0\n",
      "Epoch [76/100], Training Loss: 134180.6032225579, Test Loss: 20937408.0\n",
      "Epoch [77/100], Training Loss: 132480.37509626208, Test Loss: 20744496.0\n",
      "Epoch [78/100], Training Loss: 130821.24385403708, Test Loss: 20557918.0\n",
      "Epoch [79/100], Training Loss: 129190.86004976009, Test Loss: 20375684.0\n",
      "Epoch [80/100], Training Loss: 127603.30270718559, Test Loss: 20201510.0\n",
      "Epoch [81/100], Training Loss: 126068.36742491559, Test Loss: 20032218.0\n",
      "Epoch [82/100], Training Loss: 124578.43076535751, Test Loss: 19869958.0\n",
      "Epoch [83/100], Training Loss: 123103.18769622652, Test Loss: 19705384.0\n",
      "Epoch [84/100], Training Loss: 121638.99252117766, Test Loss: 19546248.0\n",
      "Epoch [85/100], Training Loss: 120179.57819441977, Test Loss: 19387664.0\n",
      "Epoch [86/100], Training Loss: 118728.53426929684, Test Loss: 19234514.0\n",
      "Epoch [87/100], Training Loss: 117323.2952431728, Test Loss: 19087938.0\n",
      "Epoch [88/100], Training Loss: 115972.41891771814, Test Loss: 18950960.0\n",
      "Epoch [89/100], Training Loss: 114675.1511758782, Test Loss: 18820972.0\n",
      "Epoch [90/100], Training Loss: 113439.24591256442, Test Loss: 18699632.0\n",
      "Epoch [91/100], Training Loss: 112251.89412653279, Test Loss: 18581712.0\n",
      "Epoch [92/100], Training Loss: 111105.20419406434, Test Loss: 18469504.0\n",
      "Epoch [93/100], Training Loss: 109992.42669273147, Test Loss: 18358308.0\n",
      "Epoch [94/100], Training Loss: 108915.87632545465, Test Loss: 18255462.0\n",
      "Epoch [95/100], Training Loss: 107868.2934364078, Test Loss: 18151988.0\n",
      "Epoch [96/100], Training Loss: 106847.86604762751, Test Loss: 18055572.0\n",
      "Epoch [97/100], Training Loss: 105856.78105562467, Test Loss: 17961728.0\n",
      "Epoch [98/100], Training Loss: 104893.34516616314, Test Loss: 17873196.0\n",
      "Epoch [99/100], Training Loss: 103957.88362656241, Test Loss: 17787240.0\n",
      "Epoch [100/100], Training Loss: 103051.2943397903, Test Loss: 17707012.0\n",
      "Epoch [1/100], Training Loss: 1497960.016586695, Test Loss: 298892128.0\n",
      "Epoch [2/100], Training Loss: 1471011.4329719804, Test Loss: 286631072.0\n",
      "Epoch [3/100], Training Loss: 1355940.999703809, Test Loss: 251378544.0\n",
      "Epoch [4/100], Training Loss: 1138456.997571234, Test Loss: 202701216.0\n",
      "Epoch [5/100], Training Loss: 919774.2413364138, Test Loss: 167616448.0\n",
      "Epoch [6/100], Training Loss: 792377.9977489485, Test Loss: 150902864.0\n",
      "Epoch [7/100], Training Loss: 727290.465967656, Test Loss: 141116352.0\n",
      "Epoch [8/100], Training Loss: 682022.5422664534, Test Loss: 133396024.0\n",
      "Epoch [9/100], Training Loss: 643668.9357265565, Test Loss: 126649664.0\n",
      "Epoch [10/100], Training Loss: 609190.7889343048, Test Loss: 120570760.0\n",
      "Epoch [11/100], Training Loss: 577577.6273917422, Test Loss: 115020640.0\n",
      "Epoch [12/100], Training Loss: 548039.8573544221, Test Loss: 109807056.0\n",
      "Epoch [13/100], Training Loss: 520061.9186067176, Test Loss: 105016064.0\n",
      "Epoch [14/100], Training Loss: 494599.07801670517, Test Loss: 100770544.0\n",
      "Epoch [15/100], Training Loss: 471634.28825306555, Test Loss: 96967120.0\n",
      "Epoch [16/100], Training Loss: 450812.28386943904, Test Loss: 93545768.0\n",
      "Epoch [17/100], Training Loss: 431906.0486937978, Test Loss: 90452320.0\n",
      "Epoch [18/100], Training Loss: 414708.49617913633, Test Loss: 87637016.0\n",
      "Epoch [19/100], Training Loss: 399034.7462828031, Test Loss: 85061936.0\n",
      "Epoch [20/100], Training Loss: 384735.8720454949, Test Loss: 82696304.0\n",
      "Epoch [21/100], Training Loss: 371675.5515668503, Test Loss: 80515224.0\n",
      "Epoch [22/100], Training Loss: 359740.70765949885, Test Loss: 78502768.0\n",
      "Epoch [23/100], Training Loss: 348838.6473550145, Test Loss: 76636752.0\n",
      "Epoch [24/100], Training Loss: 338865.2386706949, Test Loss: 74907616.0\n",
      "Epoch [25/100], Training Loss: 329743.7575972987, Test Loss: 73301416.0\n",
      "Epoch [26/100], Training Loss: 321385.45145429776, Test Loss: 71801040.0\n",
      "Epoch [27/100], Training Loss: 313694.41502280673, Test Loss: 70393096.0\n",
      "Epoch [28/100], Training Loss: 306585.3082163379, Test Loss: 69065280.0\n",
      "Epoch [29/100], Training Loss: 299975.79426574253, Test Loss: 67806432.0\n",
      "Epoch [30/100], Training Loss: 293799.1076358036, Test Loss: 66609036.0\n",
      "Epoch [31/100], Training Loss: 287993.41019489366, Test Loss: 65462544.0\n",
      "Epoch [32/100], Training Loss: 282487.80476571294, Test Loss: 64358984.0\n",
      "Epoch [33/100], Training Loss: 277239.69104318466, Test Loss: 63291560.0\n",
      "Epoch [34/100], Training Loss: 272207.18033402937, Test Loss: 62256560.0\n",
      "Epoch [35/100], Training Loss: 267356.9680743143, Test Loss: 61251368.0\n",
      "Epoch [36/100], Training Loss: 262664.6415193579, Test Loss: 60273520.0\n",
      "Epoch [37/100], Training Loss: 258110.6221769297, Test Loss: 59319548.0\n",
      "Epoch [38/100], Training Loss: 253676.3933379243, Test Loss: 58389340.0\n",
      "Epoch [39/100], Training Loss: 249357.52121467923, Test Loss: 57480724.0\n",
      "Epoch [40/100], Training Loss: 245141.94344233163, Test Loss: 56593224.0\n",
      "Epoch [41/100], Training Loss: 241018.1477622771, Test Loss: 55725584.0\n",
      "Epoch [42/100], Training Loss: 236972.47833362952, Test Loss: 54876760.0\n",
      "Epoch [43/100], Training Loss: 233010.27616847344, Test Loss: 54045116.0\n",
      "Epoch [44/100], Training Loss: 229123.72027723477, Test Loss: 53228876.0\n",
      "Epoch [45/100], Training Loss: 225313.8789615544, Test Loss: 52427984.0\n",
      "Epoch [46/100], Training Loss: 221580.93326817133, Test Loss: 51644488.0\n",
      "Epoch [47/100], Training Loss: 217918.48196196908, Test Loss: 50879900.0\n",
      "Epoch [48/100], Training Loss: 214329.5308779101, Test Loss: 50133444.0\n",
      "Epoch [49/100], Training Loss: 210822.28204786446, Test Loss: 49403876.0\n",
      "Epoch [50/100], Training Loss: 207393.10810970914, Test Loss: 48690660.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/100], Training Loss: 204038.95061015344, Test Loss: 47990048.0\n",
      "Epoch [52/100], Training Loss: 200759.3750592382, Test Loss: 47303680.0\n",
      "Epoch [53/100], Training Loss: 197553.0988537409, Test Loss: 46634856.0\n",
      "Epoch [54/100], Training Loss: 194418.08312599963, Test Loss: 45984192.0\n",
      "Epoch [55/100], Training Loss: 191357.73107339613, Test Loss: 45349008.0\n",
      "Epoch [56/100], Training Loss: 188378.2299478704, Test Loss: 44727780.0\n",
      "Epoch [57/100], Training Loss: 185479.9719877377, Test Loss: 44120552.0\n",
      "Epoch [58/100], Training Loss: 182655.3242735916, Test Loss: 43527360.0\n",
      "Epoch [59/100], Training Loss: 179902.15912860612, Test Loss: 42949608.0\n",
      "Epoch [60/100], Training Loss: 177218.14062962797, Test Loss: 42382916.0\n",
      "Epoch [61/100], Training Loss: 174597.38749185475, Test Loss: 41826176.0\n",
      "Epoch [62/100], Training Loss: 172038.61210643622, Test Loss: 41279016.0\n",
      "Epoch [63/100], Training Loss: 169541.3175981873, Test Loss: 40742532.0\n",
      "Epoch [64/100], Training Loss: 167100.88463083497, Test Loss: 40217056.0\n",
      "Epoch [65/100], Training Loss: 164713.4999162335, Test Loss: 39701852.0\n",
      "Epoch [66/100], Training Loss: 162381.30727222914, Test Loss: 39196808.0\n",
      "Epoch [67/100], Training Loss: 160105.96552383227, Test Loss: 38703280.0\n",
      "Epoch [68/100], Training Loss: 157887.03156470292, Test Loss: 38221996.0\n",
      "Epoch [69/100], Training Loss: 155722.1558169364, Test Loss: 37753580.0\n",
      "Epoch [70/100], Training Loss: 153615.1113422982, Test Loss: 37296156.0\n",
      "Epoch [71/100], Training Loss: 151570.84749395237, Test Loss: 36852008.0\n",
      "Epoch [72/100], Training Loss: 149585.77433626898, Test Loss: 36421044.0\n",
      "Epoch [73/100], Training Loss: 147660.3977575045, Test Loss: 36002920.0\n",
      "Epoch [74/100], Training Loss: 145790.9083960107, Test Loss: 35595772.0\n",
      "Epoch [75/100], Training Loss: 143974.35002855468, Test Loss: 35199008.0\n",
      "Epoch [76/100], Training Loss: 142204.84289046764, Test Loss: 34810940.0\n",
      "Epoch [77/100], Training Loss: 140476.99676075822, Test Loss: 34431108.0\n",
      "Epoch [78/100], Training Loss: 138785.03943644112, Test Loss: 34058132.0\n",
      "Epoch [79/100], Training Loss: 137120.6810599287, Test Loss: 33690496.0\n",
      "Epoch [80/100], Training Loss: 135481.00193380317, Test Loss: 33327804.0\n",
      "Epoch [81/100], Training Loss: 133861.24222660647, Test Loss: 32969296.0\n",
      "Epoch [82/100], Training Loss: 132261.19002410254, Test Loss: 32617626.0\n",
      "Epoch [83/100], Training Loss: 130692.38083681358, Test Loss: 32273642.0\n",
      "Epoch [84/100], Training Loss: 129150.1218805073, Test Loss: 31935308.0\n",
      "Epoch [85/100], Training Loss: 127637.13680089671, Test Loss: 31605098.0\n",
      "Epoch [86/100], Training Loss: 126160.97593401975, Test Loss: 31283282.0\n",
      "Epoch [87/100], Training Loss: 124726.80829131123, Test Loss: 30972590.0\n",
      "Epoch [88/100], Training Loss: 123336.18540907677, Test Loss: 30668834.0\n",
      "Epoch [89/100], Training Loss: 121989.19141388618, Test Loss: 30377560.0\n",
      "Epoch [90/100], Training Loss: 120682.12967611516, Test Loss: 30092802.0\n",
      "Epoch [91/100], Training Loss: 119413.96555113737, Test Loss: 29819178.0\n",
      "Epoch [92/100], Training Loss: 118179.17809177107, Test Loss: 29552548.0\n",
      "Epoch [93/100], Training Loss: 116973.86649515358, Test Loss: 29292896.0\n",
      "Epoch [94/100], Training Loss: 115796.60299439829, Test Loss: 29039400.0\n",
      "Epoch [95/100], Training Loss: 114645.4959190436, Test Loss: 28793004.0\n",
      "Epoch [96/100], Training Loss: 113518.80754851979, Test Loss: 28552386.0\n",
      "Epoch [97/100], Training Loss: 112412.44025179936, Test Loss: 28317358.0\n",
      "Epoch [98/100], Training Loss: 111326.51223824122, Test Loss: 28084540.0\n",
      "Epoch [99/100], Training Loss: 110256.90530913083, Test Loss: 27852446.0\n",
      "Epoch [100/100], Training Loss: 109194.47350803048, Test Loss: 27621656.0\n",
      "Epoch [1/100], Training Loss: 1164356.7267341982, Test Loss: 299299104.0\n",
      "Epoch [2/100], Training Loss: 1149691.5595047686, Test Loss: 290556352.0\n",
      "Epoch [3/100], Training Loss: 1082464.8814643682, Test Loss: 263266640.0\n",
      "Epoch [4/100], Training Loss: 939906.9694923287, Test Loss: 219116176.0\n",
      "Epoch [5/100], Training Loss: 766608.1758189681, Test Loss: 178334624.0\n",
      "Epoch [6/100], Training Loss: 642507.9227533914, Test Loss: 155936560.0\n",
      "Epoch [7/100], Training Loss: 579924.1879035602, Test Loss: 144695760.0\n",
      "Epoch [8/100], Training Loss: 543059.6374622356, Test Loss: 136778416.0\n",
      "Epoch [9/100], Training Loss: 513854.667377525, Test Loss: 130003904.0\n",
      "Epoch [10/100], Training Loss: 487868.91985071974, Test Loss: 123839720.0\n",
      "Epoch [11/100], Training Loss: 463829.58047509036, Test Loss: 118097096.0\n",
      "Epoch [12/100], Training Loss: 441247.674900776, Test Loss: 112701472.0\n",
      "Epoch [13/100], Training Loss: 419944.51063325634, Test Loss: 107630856.0\n",
      "Epoch [14/100], Training Loss: 399889.9209762455, Test Loss: 102885496.0\n",
      "Epoch [15/100], Training Loss: 381094.38421894435, Test Loss: 98462088.0\n",
      "Epoch [16/100], Training Loss: 363538.2029500622, Test Loss: 94342688.0\n",
      "Epoch [17/100], Training Loss: 346976.0497600853, Test Loss: 90443440.0\n",
      "Epoch [18/100], Training Loss: 331416.6866891772, Test Loss: 86844192.0\n",
      "Epoch [19/100], Training Loss: 317258.1422901487, Test Loss: 83599448.0\n",
      "Epoch [20/100], Training Loss: 304482.2264083881, Test Loss: 80643528.0\n",
      "Epoch [21/100], Training Loss: 292855.53580948996, Test Loss: 77928584.0\n",
      "Epoch [22/100], Training Loss: 282229.5642438244, Test Loss: 75429288.0\n",
      "Epoch [23/100], Training Loss: 272487.51566850307, Test Loss: 73119768.0\n",
      "Epoch [24/100], Training Loss: 263525.1403352882, Test Loss: 70979896.0\n",
      "Epoch [25/100], Training Loss: 255259.0478052248, Test Loss: 68990280.0\n",
      "Epoch [26/100], Training Loss: 247619.1199573485, Test Loss: 67138680.0\n",
      "Epoch [27/100], Training Loss: 240538.86997215805, Test Loss: 65413124.0\n",
      "Epoch [28/100], Training Loss: 233954.0092411587, Test Loss: 63799396.0\n",
      "Epoch [29/100], Training Loss: 227810.24050707897, Test Loss: 62285756.0\n",
      "Epoch [30/100], Training Loss: 222058.5186896511, Test Loss: 60864004.0\n",
      "Epoch [31/100], Training Loss: 216660.43397902968, Test Loss: 59523868.0\n",
      "Epoch [32/100], Training Loss: 211555.69847757835, Test Loss: 58250472.0\n",
      "Epoch [33/100], Training Loss: 206703.23191754043, Test Loss: 57039980.0\n",
      "Epoch [34/100], Training Loss: 202080.34287068303, Test Loss: 55888512.0\n",
      "Epoch [35/100], Training Loss: 197670.88987619217, Test Loss: 54789764.0\n",
      "Epoch [36/100], Training Loss: 193442.8042177596, Test Loss: 53738052.0\n",
      "Epoch [37/100], Training Loss: 189387.59729873823, Test Loss: 52729216.0\n",
      "Epoch [38/100], Training Loss: 185504.0515372312, Test Loss: 51760428.0\n",
      "Epoch [39/100], Training Loss: 181780.48966293465, Test Loss: 50825232.0\n",
      "Epoch [40/100], Training Loss: 178198.24643089864, Test Loss: 49924324.0\n",
      "Epoch [41/100], Training Loss: 174746.99164741425, Test Loss: 49058692.0\n",
      "Epoch [42/100], Training Loss: 171420.84260411112, Test Loss: 48224468.0\n",
      "Epoch [43/100], Training Loss: 168205.8779693146, Test Loss: 47420692.0\n",
      "Epoch [44/100], Training Loss: 165095.73117706296, Test Loss: 46640748.0\n",
      "Epoch [45/100], Training Loss: 162087.84846869262, Test Loss: 45883316.0\n",
      "Epoch [46/100], Training Loss: 159171.98483502163, Test Loss: 45146064.0\n",
      "Epoch [47/100], Training Loss: 156344.23482021206, Test Loss: 44428392.0\n",
      "Epoch [48/100], Training Loss: 153597.27196256147, Test Loss: 43730588.0\n",
      "Epoch [49/100], Training Loss: 150922.97316509686, Test Loss: 43052040.0\n",
      "Epoch [50/100], Training Loss: 148320.0975060719, Test Loss: 42390184.0\n",
      "Epoch [51/100], Training Loss: 145785.77607961613, Test Loss: 41742612.0\n",
      "Epoch [52/100], Training Loss: 143315.58787986494, Test Loss: 41111868.0\n",
      "Epoch [53/100], Training Loss: 140911.77009655826, Test Loss: 40496852.0\n",
      "Epoch [54/100], Training Loss: 138572.90344173924, Test Loss: 39896588.0\n",
      "Epoch [55/100], Training Loss: 136296.97375747882, Test Loss: 39310812.0\n",
      "Epoch [56/100], Training Loss: 134083.95515668503, Test Loss: 38739908.0\n",
      "Epoch [57/100], Training Loss: 131933.92571530121, Test Loss: 38184980.0\n",
      "Epoch [58/100], Training Loss: 129845.040341212, Test Loss: 37645680.0\n",
      "Epoch [59/100], Training Loss: 127816.63675137729, Test Loss: 37121428.0\n",
      "Epoch [60/100], Training Loss: 125846.66382323322, Test Loss: 36610372.0\n",
      "Epoch [61/100], Training Loss: 123929.24838575914, Test Loss: 36110816.0\n",
      "Epoch [62/100], Training Loss: 122057.58900539068, Test Loss: 35622120.0\n",
      "Epoch [63/100], Training Loss: 120227.48676026301, Test Loss: 35145920.0\n",
      "Epoch [64/100], Training Loss: 118438.80765357503, Test Loss: 34680476.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [65/100], Training Loss: 116691.56631716131, Test Loss: 34226312.0\n",
      "Epoch [66/100], Training Loss: 114990.5035246727, Test Loss: 33783320.0\n",
      "Epoch [67/100], Training Loss: 113337.07854984894, Test Loss: 33351652.0\n",
      "Epoch [68/100], Training Loss: 111730.21278360287, Test Loss: 32932116.0\n",
      "Epoch [69/100], Training Loss: 110168.6825425034, Test Loss: 32523864.0\n",
      "Epoch [70/100], Training Loss: 108651.69829986375, Test Loss: 32126828.0\n",
      "Epoch [71/100], Training Loss: 107177.16059475149, Test Loss: 31740690.0\n",
      "Epoch [72/100], Training Loss: 105741.45512706593, Test Loss: 31364778.0\n",
      "Epoch [73/100], Training Loss: 104344.26580178899, Test Loss: 30998606.0\n",
      "Epoch [74/100], Training Loss: 102985.27622771163, Test Loss: 30642672.0\n",
      "Epoch [75/100], Training Loss: 101664.63426337302, Test Loss: 30296238.0\n",
      "Epoch [76/100], Training Loss: 100379.18861441857, Test Loss: 29959272.0\n",
      "Epoch [77/100], Training Loss: 99128.45595640069, Test Loss: 29631282.0\n",
      "Epoch [78/100], Training Loss: 97906.76950417629, Test Loss: 29312426.0\n",
      "Epoch [79/100], Training Loss: 96716.56726497246, Test Loss: 29000122.0\n",
      "Epoch [80/100], Training Loss: 95558.37248978141, Test Loss: 28697070.0\n",
      "Epoch [81/100], Training Loss: 94430.0570463835, Test Loss: 28402788.0\n",
      "Epoch [82/100], Training Loss: 93328.04466560038, Test Loss: 28115762.0\n",
      "Epoch [83/100], Training Loss: 92249.11776553522, Test Loss: 27835504.0\n",
      "Epoch [84/100], Training Loss: 91191.23724897814, Test Loss: 27560664.0\n",
      "Epoch [85/100], Training Loss: 90153.52674604586, Test Loss: 27289788.0\n",
      "Epoch [86/100], Training Loss: 89133.07102659794, Test Loss: 27022996.0\n",
      "Epoch [87/100], Training Loss: 88126.42005805344, Test Loss: 26761870.0\n",
      "Epoch [88/100], Training Loss: 87137.00858953853, Test Loss: 26506010.0\n",
      "Epoch [89/100], Training Loss: 86165.71897399443, Test Loss: 26253660.0\n",
      "Epoch [90/100], Training Loss: 85209.56294058409, Test Loss: 26005340.0\n",
      "Epoch [91/100], Training Loss: 84272.38291570405, Test Loss: 25764094.0\n",
      "Epoch [92/100], Training Loss: 83353.76055920857, Test Loss: 25527928.0\n",
      "Epoch [93/100], Training Loss: 82455.00817487115, Test Loss: 25297354.0\n",
      "Epoch [94/100], Training Loss: 81573.87026834903, Test Loss: 25072600.0\n",
      "Epoch [95/100], Training Loss: 80711.57946804099, Test Loss: 24854446.0\n",
      "Epoch [96/100], Training Loss: 79869.17072448315, Test Loss: 24640980.0\n",
      "Epoch [97/100], Training Loss: 79042.767608554, Test Loss: 24429972.0\n",
      "Epoch [98/100], Training Loss: 78228.65150168829, Test Loss: 24221458.0\n",
      "Epoch [99/100], Training Loss: 77426.98844855162, Test Loss: 24019446.0\n",
      "Epoch [100/100], Training Loss: 76637.52704223683, Test Loss: 23819652.0\n",
      "Epoch [1/100], Training Loss: 582418.8434334459, Test Loss: 300027584.0\n",
      "Epoch [2/100], Training Loss: 581885.8683727267, Test Loss: 299397920.0\n",
      "Epoch [3/100], Training Loss: 579320.0194301285, Test Loss: 297125664.0\n",
      "Epoch [4/100], Training Loss: 572279.2166340856, Test Loss: 291854432.0\n",
      "Epoch [5/100], Training Loss: 558200.0829334755, Test Loss: 282389760.0\n",
      "Epoch [6/100], Training Loss: 535243.8315265683, Test Loss: 268126976.0\n",
      "Epoch [7/100], Training Loss: 503109.7589005391, Test Loss: 249440976.0\n",
      "Epoch [8/100], Training Loss: 463652.00450210297, Test Loss: 227889520.0\n",
      "Epoch [9/100], Training Loss: 420907.9973935193, Test Loss: 205993600.0\n",
      "Epoch [10/100], Training Loss: 380133.79444345715, Test Loss: 186466736.0\n",
      "Epoch [11/100], Training Loss: 345901.59682483267, Test Loss: 171096976.0\n",
      "Epoch [12/100], Training Loss: 320136.51323973696, Test Loss: 160033456.0\n",
      "Epoch [13/100], Training Loss: 301790.520466797, Test Loss: 152204608.0\n",
      "Epoch [14/100], Training Loss: 288443.58462176414, Test Loss: 146345248.0\n",
      "Epoch [15/100], Training Loss: 277978.65908417746, Test Loss: 141574336.0\n",
      "Epoch [16/100], Training Loss: 269099.4931579883, Test Loss: 137411008.0\n",
      "Epoch [17/100], Training Loss: 261137.81126710502, Test Loss: 133617808.0\n",
      "Epoch [18/100], Training Loss: 253764.623422783, Test Loss: 130076008.0\n",
      "Epoch [19/100], Training Loss: 246809.1866595581, Test Loss: 126721144.0\n",
      "Epoch [20/100], Training Loss: 240174.21716722942, Test Loss: 123514968.0\n",
      "Epoch [21/100], Training Loss: 233800.0241691843, Test Loss: 120433488.0\n",
      "Epoch [22/100], Training Loss: 227648.8454475446, Test Loss: 117461496.0\n",
      "Epoch [23/100], Training Loss: 221696.97695634144, Test Loss: 114589568.0\n",
      "Epoch [24/100], Training Loss: 215930.1306794621, Test Loss: 111812144.0\n",
      "Epoch [25/100], Training Loss: 210340.4461820982, Test Loss: 109126416.0\n",
      "Epoch [26/100], Training Loss: 204925.1937681417, Test Loss: 106531536.0\n",
      "Epoch [27/100], Training Loss: 199684.5056572478, Test Loss: 104027672.0\n",
      "Epoch [28/100], Training Loss: 194619.33108228186, Test Loss: 101615024.0\n",
      "Epoch [29/100], Training Loss: 189730.1456074877, Test Loss: 99292904.0\n",
      "Epoch [30/100], Training Loss: 185017.46792251643, Test Loss: 97060168.0\n",
      "Epoch [31/100], Training Loss: 180482.0055683905, Test Loss: 94916104.0\n",
      "Epoch [32/100], Training Loss: 176122.86215271606, Test Loss: 92857616.0\n",
      "Epoch [33/100], Training Loss: 171929.6105680943, Test Loss: 90874960.0\n",
      "Epoch [34/100], Training Loss: 167870.32355903086, Test Loss: 88951808.0\n",
      "Epoch [35/100], Training Loss: 163917.79918251288, Test Loss: 87099264.0\n",
      "Epoch [36/100], Training Loss: 160100.6217641135, Test Loss: 85303240.0\n",
      "Epoch [37/100], Training Loss: 156411.4208873882, Test Loss: 83572576.0\n",
      "Epoch [38/100], Training Loss: 152880.81440672945, Test Loss: 81920176.0\n",
      "Epoch [39/100], Training Loss: 149549.41081689473, Test Loss: 80365048.0\n",
      "Epoch [40/100], Training Loss: 146413.7807001955, Test Loss: 78893944.0\n",
      "Epoch [41/100], Training Loss: 143450.83277057047, Test Loss: 77496336.0\n",
      "Epoch [42/100], Training Loss: 140640.97126947457, Test Loss: 76162064.0\n",
      "Epoch [43/100], Training Loss: 137970.43658551032, Test Loss: 74886520.0\n",
      "Epoch [44/100], Training Loss: 135425.4463598128, Test Loss: 73666376.0\n",
      "Epoch [45/100], Training Loss: 132998.49961495172, Test Loss: 72496544.0\n",
      "Epoch [46/100], Training Loss: 130681.36461110123, Test Loss: 71375688.0\n",
      "Epoch [47/100], Training Loss: 128466.20911083466, Test Loss: 70300152.0\n",
      "Epoch [48/100], Training Loss: 126344.85208222261, Test Loss: 69266064.0\n",
      "Epoch [49/100], Training Loss: 124310.82803151473, Test Loss: 68270656.0\n",
      "Epoch [50/100], Training Loss: 122360.53195900717, Test Loss: 67312640.0\n",
      "Epoch [51/100], Training Loss: 120487.17706297021, Test Loss: 66388684.0\n",
      "Epoch [52/100], Training Loss: 118684.38433742076, Test Loss: 65495368.0\n",
      "Epoch [53/100], Training Loss: 116946.7806409573, Test Loss: 64630816.0\n",
      "Epoch [54/100], Training Loss: 115269.38451513536, Test Loss: 63793612.0\n",
      "Epoch [55/100], Training Loss: 113648.03269948463, Test Loss: 62981384.0\n",
      "Epoch [56/100], Training Loss: 112077.10562170488, Test Loss: 62191308.0\n",
      "Epoch [57/100], Training Loss: 110552.2876606836, Test Loss: 61423032.0\n",
      "Epoch [58/100], Training Loss: 109071.88697352052, Test Loss: 60675716.0\n",
      "Epoch [59/100], Training Loss: 107633.01723831527, Test Loss: 59948252.0\n",
      "Epoch [60/100], Training Loss: 106231.64397843729, Test Loss: 59239652.0\n",
      "Epoch [61/100], Training Loss: 104865.34755050056, Test Loss: 58549296.0\n",
      "Epoch [62/100], Training Loss: 103531.31058586577, Test Loss: 57875788.0\n",
      "Epoch [63/100], Training Loss: 102229.54694627096, Test Loss: 57219264.0\n",
      "Epoch [64/100], Training Loss: 100959.30418814051, Test Loss: 56579872.0\n",
      "Epoch [65/100], Training Loss: 99718.8339553344, Test Loss: 55955784.0\n",
      "Epoch [66/100], Training Loss: 98505.93246845566, Test Loss: 55345136.0\n",
      "Epoch [67/100], Training Loss: 97319.40122030686, Test Loss: 54746176.0\n",
      "Epoch [68/100], Training Loss: 96157.38143474913, Test Loss: 54158792.0\n",
      "Epoch [69/100], Training Loss: 95019.58320004739, Test Loss: 53583744.0\n",
      "Epoch [70/100], Training Loss: 93904.45625259167, Test Loss: 53019352.0\n",
      "Epoch [71/100], Training Loss: 92810.22392038387, Test Loss: 52465532.0\n",
      "Epoch [72/100], Training Loss: 91736.95101001125, Test Loss: 51921956.0\n",
      "Epoch [73/100], Training Loss: 90683.78235886499, Test Loss: 51388888.0\n",
      "Epoch [74/100], Training Loss: 89649.91446004383, Test Loss: 50864412.0\n",
      "Epoch [75/100], Training Loss: 88634.14406729459, Test Loss: 50349456.0\n",
      "Epoch [76/100], Training Loss: 87635.49517208696, Test Loss: 49844792.0\n",
      "Epoch [77/100], Training Loss: 86654.62318583022, Test Loss: 49348484.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [78/100], Training Loss: 85690.24761566258, Test Loss: 48860936.0\n",
      "Epoch [79/100], Training Loss: 84743.65108702092, Test Loss: 48380620.0\n",
      "Epoch [80/100], Training Loss: 83813.71826313606, Test Loss: 47909604.0\n",
      "Epoch [81/100], Training Loss: 82901.81458444404, Test Loss: 47446280.0\n",
      "Epoch [82/100], Training Loss: 82005.59907588414, Test Loss: 46989560.0\n",
      "Epoch [83/100], Training Loss: 81122.02322137314, Test Loss: 46539136.0\n",
      "Epoch [84/100], Training Loss: 80249.4872341686, Test Loss: 46098080.0\n",
      "Epoch [85/100], Training Loss: 79392.40435993129, Test Loss: 45664656.0\n",
      "Epoch [86/100], Training Loss: 78547.92749244712, Test Loss: 45236224.0\n",
      "Epoch [87/100], Training Loss: 77714.387417807, Test Loss: 44814480.0\n",
      "Epoch [88/100], Training Loss: 76896.93003968959, Test Loss: 44401416.0\n",
      "Epoch [89/100], Training Loss: 76096.62780640957, Test Loss: 43995908.0\n",
      "Epoch [90/100], Training Loss: 75311.79077068894, Test Loss: 43596148.0\n",
      "Epoch [91/100], Training Loss: 74540.96795213554, Test Loss: 43202328.0\n",
      "Epoch [92/100], Training Loss: 73783.71447189148, Test Loss: 42815256.0\n",
      "Epoch [93/100], Training Loss: 73039.43131331082, Test Loss: 42434388.0\n",
      "Epoch [94/100], Training Loss: 72308.17427877495, Test Loss: 42059572.0\n",
      "Epoch [95/100], Training Loss: 71589.7270303892, Test Loss: 41690632.0\n",
      "Epoch [96/100], Training Loss: 70883.30181861264, Test Loss: 41328596.0\n",
      "Epoch [97/100], Training Loss: 70189.62466678514, Test Loss: 40971848.0\n",
      "Epoch [98/100], Training Loss: 69508.66358628044, Test Loss: 40621400.0\n",
      "Epoch [99/100], Training Loss: 68840.0978615011, Test Loss: 40276624.0\n",
      "Epoch [100/100], Training Loss: 68183.52739766601, Test Loss: 39937132.0\n",
      "Epoch [1/100], Training Loss: 7172629.156921984, Test Loss: 148872640.0\n",
      "Epoch [2/100], Training Loss: 4077054.407914223, Test Loss: 117843344.0\n",
      "Epoch [3/100], Training Loss: 3266897.2520585274, Test Loss: 96468664.0\n",
      "Epoch [4/100], Training Loss: 2676686.904211836, Test Loss: 80867064.0\n",
      "Epoch [5/100], Training Loss: 2239095.077009656, Test Loss: 69241544.0\n",
      "Epoch [6/100], Training Loss: 1920330.646940347, Test Loss: 60757976.0\n",
      "Epoch [7/100], Training Loss: 1691461.1427344352, Test Loss: 54614528.0\n",
      "Epoch [8/100], Training Loss: 1523339.3358805757, Test Loss: 49928976.0\n",
      "Epoch [9/100], Training Loss: 1390587.876843789, Test Loss: 46076372.0\n",
      "Epoch [10/100], Training Loss: 1278770.739351934, Test Loss: 42744160.0\n",
      "Epoch [11/100], Training Loss: 1181511.5700491676, Test Loss: 39778372.0\n",
      "Epoch [12/100], Training Loss: 1095806.061296724, Test Loss: 37108056.0\n",
      "Epoch [13/100], Training Loss: 1020028.1267697412, Test Loss: 34703388.0\n",
      "Epoch [14/100], Training Loss: 952975.724868195, Test Loss: 32562376.0\n",
      "Epoch [15/100], Training Loss: 893740.224409099, Test Loss: 30666472.0\n",
      "Epoch [16/100], Training Loss: 841479.7831437711, Test Loss: 28992080.0\n",
      "Epoch [17/100], Training Loss: 795419.1921242818, Test Loss: 27510254.0\n",
      "Epoch [18/100], Training Loss: 754822.5990018364, Test Loss: 26194878.0\n",
      "Epoch [19/100], Training Loss: 718936.8010781351, Test Loss: 25030310.0\n",
      "Epoch [20/100], Training Loss: 686919.7470528997, Test Loss: 23991192.0\n",
      "Epoch [21/100], Training Loss: 657928.9238789171, Test Loss: 23052312.0\n",
      "Epoch [22/100], Training Loss: 631554.0401634974, Test Loss: 22217204.0\n",
      "Epoch [23/100], Training Loss: 607480.9895148391, Test Loss: 21467368.0\n",
      "Epoch [24/100], Training Loss: 585384.7625807121, Test Loss: 20797042.0\n",
      "Epoch [25/100], Training Loss: 565040.265372312, Test Loss: 20198844.0\n",
      "Epoch [26/100], Training Loss: 546312.7905633552, Test Loss: 19666802.0\n",
      "Epoch [27/100], Training Loss: 529049.7959096025, Test Loss: 19192024.0\n",
      "Epoch [28/100], Training Loss: 513116.3779989337, Test Loss: 18762464.0\n",
      "Epoch [29/100], Training Loss: 498334.72637876903, Test Loss: 18371204.0\n",
      "Epoch [30/100], Training Loss: 484599.9862715479, Test Loss: 18009720.0\n",
      "Epoch [31/100], Training Loss: 471752.8117113915, Test Loss: 17683310.0\n",
      "Epoch [32/100], Training Loss: 459834.5083007523, Test Loss: 17377254.0\n",
      "Epoch [33/100], Training Loss: 448723.940013921, Test Loss: 17104426.0\n",
      "Epoch [34/100], Training Loss: 438345.2591597062, Test Loss: 16852812.0\n",
      "Epoch [35/100], Training Loss: 428580.4005390676, Test Loss: 16631421.0\n",
      "Epoch [36/100], Training Loss: 419436.64999851905, Test Loss: 16425835.0\n",
      "Epoch [37/100], Training Loss: 410855.90423405013, Test Loss: 16256287.0\n",
      "Epoch [38/100], Training Loss: 402836.578416563, Test Loss: 16075015.0\n",
      "Epoch [39/100], Training Loss: 395281.5020140987, Test Loss: 15949445.0\n",
      "Epoch [40/100], Training Loss: 388149.29243676324, Test Loss: 15802939.0\n",
      "Epoch [41/100], Training Loss: 381404.79950832296, Test Loss: 15700844.0\n",
      "Epoch [42/100], Training Loss: 375062.14762899117, Test Loss: 15583174.0\n",
      "Epoch [43/100], Training Loss: 369095.6909098987, Test Loss: 15511371.0\n",
      "Epoch [44/100], Training Loss: 363524.7672309105, Test Loss: 15404132.0\n",
      "Epoch [45/100], Training Loss: 358219.29685741366, Test Loss: 15360759.0\n",
      "Epoch [46/100], Training Loss: 353233.68160209706, Test Loss: 15271970.0\n",
      "Epoch [47/100], Training Loss: 348493.0659469226, Test Loss: 15223474.0\n",
      "Epoch [48/100], Training Loss: 344035.0782388484, Test Loss: 15129993.0\n",
      "Epoch [49/100], Training Loss: 339760.0215775132, Test Loss: 15105236.0\n",
      "Epoch [50/100], Training Loss: 335772.5164385996, Test Loss: 14981579.0\n",
      "Epoch [51/100], Training Loss: 331922.1148554588, Test Loss: 14986077.0\n",
      "Epoch [52/100], Training Loss: 328309.12728807534, Test Loss: 14871795.0\n",
      "Epoch [53/100], Training Loss: 324871.0057757242, Test Loss: 14871286.0\n",
      "Epoch [54/100], Training Loss: 321626.2498593093, Test Loss: 14765326.0\n",
      "Epoch [55/100], Training Loss: 318506.6570848883, Test Loss: 14781677.0\n",
      "Epoch [56/100], Training Loss: 315583.99371334637, Test Loss: 14676908.0\n",
      "Epoch [57/100], Training Loss: 312742.5768763699, Test Loss: 14692472.0\n",
      "Epoch [58/100], Training Loss: 310099.9882560275, Test Loss: 14591410.0\n",
      "Epoch [59/100], Training Loss: 307487.1382582489, Test Loss: 14613981.0\n",
      "Epoch [60/100], Training Loss: 305109.61965005036, Test Loss: 14531593.0\n",
      "Epoch [61/100], Training Loss: 302741.72083999764, Test Loss: 14551257.0\n",
      "Epoch [62/100], Training Loss: 300633.6299834133, Test Loss: 14479233.0\n",
      "Epoch [63/100], Training Loss: 298503.7326839346, Test Loss: 14486473.0\n",
      "Epoch [64/100], Training Loss: 296522.3715715894, Test Loss: 14467229.0\n",
      "Epoch [65/100], Training Loss: 294560.07540652214, Test Loss: 14413644.0\n",
      "Epoch [66/100], Training Loss: 292706.1308016409, Test Loss: 14408683.0\n",
      "Epoch [67/100], Training Loss: 290918.2572085481, Test Loss: 14388168.0\n",
      "Epoch [68/100], Training Loss: 289233.91015046503, Test Loss: 14371988.0\n",
      "Epoch [69/100], Training Loss: 287574.9906477697, Test Loss: 14430199.0\n",
      "Epoch [70/100], Training Loss: 285906.3820826669, Test Loss: 14378884.0\n",
      "Epoch [71/100], Training Loss: 284262.67565236066, Test Loss: 14439893.0\n",
      "Epoch [72/100], Training Loss: 282751.0239951721, Test Loss: 14407868.0\n",
      "Epoch [73/100], Training Loss: 281154.7109657307, Test Loss: 14400736.0\n",
      "Epoch [74/100], Training Loss: 279590.90154241456, Test Loss: 14426603.0\n",
      "Epoch [75/100], Training Loss: 278085.0597861501, Test Loss: 14418076.0\n",
      "Epoch [76/100], Training Loss: 276632.8728341034, Test Loss: 14417253.0\n",
      "Epoch [77/100], Training Loss: 275188.4125162905, Test Loss: 14470682.0\n",
      "Epoch [78/100], Training Loss: 273843.0557209289, Test Loss: 14414636.0\n",
      "Epoch [79/100], Training Loss: 272421.03552810854, Test Loss: 14456829.0\n",
      "Epoch [80/100], Training Loss: 271182.0359464783, Test Loss: 14430411.0\n",
      "Epoch [81/100], Training Loss: 269848.94343122444, Test Loss: 14437692.0\n",
      "Epoch [82/100], Training Loss: 268661.4031307387, Test Loss: 14465683.0\n",
      "Epoch [83/100], Training Loss: 267434.6650561282, Test Loss: 14441698.0\n",
      "Epoch [84/100], Training Loss: 266293.5583829453, Test Loss: 14456776.0\n",
      "Epoch [85/100], Training Loss: 265107.1049848943, Test Loss: 14474690.0\n",
      "Epoch [86/100], Training Loss: 264036.3932897933, Test Loss: 14407791.0\n",
      "Epoch [87/100], Training Loss: 262874.2653612049, Test Loss: 14470286.0\n",
      "Epoch [88/100], Training Loss: 261819.63043510457, Test Loss: 14446459.0\n",
      "Epoch [89/100], Training Loss: 260696.43354955275, Test Loss: 14450312.0\n",
      "Epoch [90/100], Training Loss: 259689.7394481962, Test Loss: 14471730.0\n",
      "Epoch [91/100], Training Loss: 258608.56053773474, Test Loss: 14467452.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [92/100], Training Loss: 257688.46354259228, Test Loss: 14407644.0\n",
      "Epoch [93/100], Training Loss: 256641.41174619395, Test Loss: 14484417.0\n",
      "Epoch [94/100], Training Loss: 255743.374703809, Test Loss: 14463938.0\n",
      "Epoch [95/100], Training Loss: 254783.06354407323, Test Loss: 14484872.0\n",
      "Epoch [96/100], Training Loss: 253936.27981902732, Test Loss: 14482173.0\n",
      "Epoch [97/100], Training Loss: 253037.86250074048, Test Loss: 14486488.0\n",
      "Epoch [98/100], Training Loss: 252173.05397340204, Test Loss: 14449838.0\n",
      "Epoch [99/100], Training Loss: 251275.22903338072, Test Loss: 14544082.0\n",
      "Epoch [100/100], Training Loss: 250478.813140513, Test Loss: 14477803.0\n",
      "Epoch [1/100], Training Loss: 4425462.405307743, Test Loss: 238959760.0\n",
      "Epoch [2/100], Training Loss: 2768602.606243706, Test Loss: 147528416.0\n",
      "Epoch [3/100], Training Loss: 2116286.088857295, Test Loss: 127960648.0\n",
      "Epoch [4/100], Training Loss: 1845322.2684675078, Test Loss: 112699208.0\n",
      "Epoch [5/100], Training Loss: 1620066.708014928, Test Loss: 99897864.0\n",
      "Epoch [6/100], Training Loss: 1431482.7283928678, Test Loss: 89361480.0\n",
      "Epoch [7/100], Training Loss: 1275109.397666015, Test Loss: 80627040.0\n",
      "Epoch [8/100], Training Loss: 1145545.8477578342, Test Loss: 73355064.0\n",
      "Epoch [9/100], Training Loss: 1038869.9360227474, Test Loss: 67311576.0\n",
      "Epoch [10/100], Training Loss: 951260.7215212369, Test Loss: 62311820.0\n",
      "Epoch [11/100], Training Loss: 879541.1597061786, Test Loss: 58211064.0\n",
      "Epoch [12/100], Training Loss: 820548.1725016291, Test Loss: 54786328.0\n",
      "Epoch [13/100], Training Loss: 770699.5321367218, Test Loss: 51833832.0\n",
      "Epoch [14/100], Training Loss: 727132.696285765, Test Loss: 49212356.0\n",
      "Epoch [15/100], Training Loss: 688142.3065576684, Test Loss: 46839956.0\n",
      "Epoch [16/100], Training Loss: 652748.6465256797, Test Loss: 44665256.0\n",
      "Epoch [17/100], Training Loss: 620292.869053966, Test Loss: 42651368.0\n",
      "Epoch [18/100], Training Loss: 590345.3147917778, Test Loss: 40771044.0\n",
      "Epoch [19/100], Training Loss: 562622.4192583378, Test Loss: 39009424.0\n",
      "Epoch [20/100], Training Loss: 536904.394941058, Test Loss: 37359760.0\n",
      "Epoch [21/100], Training Loss: 513084.19059889816, Test Loss: 35813880.0\n",
      "Epoch [22/100], Training Loss: 491062.2449795628, Test Loss: 34375168.0\n",
      "Epoch [23/100], Training Loss: 470670.70979207393, Test Loss: 33039732.0\n",
      "Epoch [24/100], Training Loss: 451814.05230732774, Test Loss: 31799554.0\n",
      "Epoch [25/100], Training Loss: 434368.66225342103, Test Loss: 30649246.0\n",
      "Epoch [26/100], Training Loss: 418241.1981813874, Test Loss: 29584002.0\n",
      "Epoch [27/100], Training Loss: 403330.5880871986, Test Loss: 28598268.0\n",
      "Epoch [28/100], Training Loss: 389522.12752502813, Test Loss: 27681962.0\n",
      "Epoch [29/100], Training Loss: 376726.9181180025, Test Loss: 26830930.0\n",
      "Epoch [30/100], Training Loss: 364859.9942538949, Test Loss: 26040226.0\n",
      "Epoch [31/100], Training Loss: 353830.296353889, Test Loss: 25305594.0\n",
      "Epoch [32/100], Training Loss: 343534.66930276644, Test Loss: 24616848.0\n",
      "Epoch [33/100], Training Loss: 333883.68335702864, Test Loss: 23974554.0\n",
      "Epoch [34/100], Training Loss: 324806.5765209407, Test Loss: 23373556.0\n",
      "Epoch [35/100], Training Loss: 316236.6291688881, Test Loss: 22811608.0\n",
      "Epoch [36/100], Training Loss: 308137.83225223626, Test Loss: 22288280.0\n",
      "Epoch [37/100], Training Loss: 300474.76787512586, Test Loss: 21799410.0\n",
      "Epoch [38/100], Training Loss: 293206.6372104733, Test Loss: 21345346.0\n",
      "Epoch [39/100], Training Loss: 286289.66194242047, Test Loss: 20923988.0\n",
      "Epoch [40/100], Training Loss: 279718.9860642142, Test Loss: 20524424.0\n",
      "Epoch [41/100], Training Loss: 273465.363174575, Test Loss: 20155472.0\n",
      "Epoch [42/100], Training Loss: 267525.7832326284, Test Loss: 19804982.0\n",
      "Epoch [43/100], Training Loss: 261881.01061844677, Test Loss: 19481294.0\n",
      "Epoch [44/100], Training Loss: 256521.64057224098, Test Loss: 19177454.0\n",
      "Epoch [45/100], Training Loss: 251410.21485693977, Test Loss: 18889820.0\n",
      "Epoch [46/100], Training Loss: 246532.37431135596, Test Loss: 18619288.0\n",
      "Epoch [47/100], Training Loss: 241861.0140542622, Test Loss: 18364018.0\n",
      "Epoch [48/100], Training Loss: 237414.11347076594, Test Loss: 18121812.0\n",
      "Epoch [49/100], Training Loss: 233161.29004502104, Test Loss: 17897464.0\n",
      "Epoch [50/100], Training Loss: 229089.47192109472, Test Loss: 17682914.0\n",
      "Epoch [51/100], Training Loss: 225195.62487411883, Test Loss: 17484992.0\n",
      "Epoch [52/100], Training Loss: 221472.6310941295, Test Loss: 17295702.0\n",
      "Epoch [53/100], Training Loss: 217907.4017238315, Test Loss: 17118424.0\n",
      "Epoch [54/100], Training Loss: 214500.60471832237, Test Loss: 16956048.0\n",
      "Epoch [55/100], Training Loss: 211225.1479770156, Test Loss: 16798830.0\n",
      "Epoch [56/100], Training Loss: 208092.40806231857, Test Loss: 16657921.0\n",
      "Epoch [57/100], Training Loss: 205084.8489129791, Test Loss: 16521966.0\n",
      "Epoch [58/100], Training Loss: 202204.41949529055, Test Loss: 16394042.0\n",
      "Epoch [59/100], Training Loss: 199432.6272436467, Test Loss: 16281966.0\n",
      "Epoch [60/100], Training Loss: 196773.50576091465, Test Loss: 16165727.0\n",
      "Epoch [61/100], Training Loss: 194227.9990077602, Test Loss: 16067023.0\n",
      "Epoch [62/100], Training Loss: 191779.1491321604, Test Loss: 15970796.0\n",
      "Epoch [63/100], Training Loss: 189426.82240388604, Test Loss: 15883894.0\n",
      "Epoch [64/100], Training Loss: 187154.8613233813, Test Loss: 15802323.0\n",
      "Epoch [65/100], Training Loss: 184979.74601623128, Test Loss: 15729164.0\n",
      "Epoch [66/100], Training Loss: 182878.2454238493, Test Loss: 15656726.0\n",
      "Epoch [67/100], Training Loss: 180870.76269178366, Test Loss: 15594701.0\n",
      "Epoch [68/100], Training Loss: 178930.90539659973, Test Loss: 15533135.0\n",
      "Epoch [69/100], Training Loss: 177060.25131804988, Test Loss: 15480803.0\n",
      "Epoch [70/100], Training Loss: 175256.9546383508, Test Loss: 15426672.0\n",
      "Epoch [71/100], Training Loss: 173526.47026983, Test Loss: 15379485.0\n",
      "Epoch [72/100], Training Loss: 171857.63002784195, Test Loss: 15334419.0\n",
      "Epoch [73/100], Training Loss: 170258.63162727328, Test Loss: 15283539.0\n",
      "Epoch [74/100], Training Loss: 168707.06510277826, Test Loss: 15241193.0\n",
      "Epoch [75/100], Training Loss: 167233.7170043244, Test Loss: 15194840.0\n",
      "Epoch [76/100], Training Loss: 165807.41893993248, Test Loss: 15152595.0\n",
      "Epoch [77/100], Training Loss: 164444.80298856704, Test Loss: 15106110.0\n",
      "Epoch [78/100], Training Loss: 163132.7579305136, Test Loss: 15068527.0\n",
      "Epoch [79/100], Training Loss: 161866.62997600852, Test Loss: 15019633.0\n",
      "Epoch [80/100], Training Loss: 160644.15775872284, Test Loss: 15007077.0\n",
      "Epoch [81/100], Training Loss: 159479.3133774658, Test Loss: 14941707.0\n",
      "Epoch [82/100], Training Loss: 158331.42073188792, Test Loss: 14937206.0\n",
      "Epoch [83/100], Training Loss: 157247.73698240626, Test Loss: 14883162.0\n",
      "Epoch [84/100], Training Loss: 156179.65311592916, Test Loss: 14868060.0\n",
      "Epoch [85/100], Training Loss: 155173.9818064688, Test Loss: 14823786.0\n",
      "Epoch [86/100], Training Loss: 154181.56079319946, Test Loss: 14803897.0\n",
      "Epoch [87/100], Training Loss: 153232.44669302765, Test Loss: 14767710.0\n",
      "Epoch [88/100], Training Loss: 152302.0334325573, Test Loss: 14745262.0\n",
      "Epoch [89/100], Training Loss: 151401.42888454476, Test Loss: 14713768.0\n",
      "Epoch [90/100], Training Loss: 150521.362811741, Test Loss: 14688887.0\n",
      "Epoch [91/100], Training Loss: 149682.6703912683, Test Loss: 14672197.0\n",
      "Epoch [92/100], Training Loss: 148861.1898288016, Test Loss: 14632238.0\n",
      "Epoch [93/100], Training Loss: 148066.94595255022, Test Loss: 14629372.0\n",
      "Epoch [94/100], Training Loss: 147282.60531810913, Test Loss: 14625018.0\n",
      "Epoch [95/100], Training Loss: 146533.56175582015, Test Loss: 14566072.0\n",
      "Epoch [96/100], Training Loss: 145777.4047671939, Test Loss: 14574947.0\n",
      "Epoch [97/100], Training Loss: 145066.49922990345, Test Loss: 14547975.0\n",
      "Epoch [98/100], Training Loss: 144349.21133967183, Test Loss: 14527730.0\n",
      "Epoch [99/100], Training Loss: 143680.19461969077, Test Loss: 14512337.0\n",
      "Epoch [100/100], Training Loss: 143006.02935993127, Test Loss: 14496976.0\n",
      "Epoch [1/100], Training Loss: 2318201.802262899, Test Loss: 292416896.0\n",
      "Epoch [2/100], Training Loss: 2072763.954268112, Test Loss: 230472464.0\n",
      "Epoch [3/100], Training Loss: 1491445.1167584858, Test Loss: 163363472.0\n",
      "Epoch [4/100], Training Loss: 1172435.2701854156, Test Loss: 142818928.0\n",
      "Epoch [5/100], Training Loss: 1055606.5022214323, Test Loss: 131028960.0\n",
      "Epoch [6/100], Training Loss: 968724.2708370357, Test Loss: 120809640.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100], Training Loss: 889477.7226467626, Test Loss: 111452872.0\n",
      "Epoch [8/100], Training Loss: 818186.0854214798, Test Loss: 103146000.0\n",
      "Epoch [9/100], Training Loss: 753573.4020496416, Test Loss: 95646992.0\n",
      "Epoch [10/100], Training Loss: 696613.6241928795, Test Loss: 89248040.0\n",
      "Epoch [11/100], Training Loss: 647700.667969907, Test Loss: 83706520.0\n",
      "Epoch [12/100], Training Loss: 605196.4536461111, Test Loss: 78853240.0\n",
      "Epoch [13/100], Training Loss: 568223.3814347491, Test Loss: 74605688.0\n",
      "Epoch [14/100], Training Loss: 536117.0166459333, Test Loss: 70883264.0\n",
      "Epoch [15/100], Training Loss: 508246.8006634678, Test Loss: 67614896.0\n",
      "Epoch [16/100], Training Loss: 484001.6064214205, Test Loss: 64745076.0\n",
      "Epoch [17/100], Training Loss: 462786.33659143414, Test Loss: 62210756.0\n",
      "Epoch [18/100], Training Loss: 444103.0959066406, Test Loss: 59961644.0\n",
      "Epoch [19/100], Training Loss: 427492.16681476217, Test Loss: 57946132.0\n",
      "Epoch [20/100], Training Loss: 412558.0313962443, Test Loss: 56106504.0\n",
      "Epoch [21/100], Training Loss: 398940.32770570467, Test Loss: 54427824.0\n",
      "Epoch [22/100], Training Loss: 386421.89325276937, Test Loss: 52875592.0\n",
      "Epoch [23/100], Training Loss: 374793.79823470174, Test Loss: 51430328.0\n",
      "Epoch [24/100], Training Loss: 363968.81796102127, Test Loss: 50085336.0\n",
      "Epoch [25/100], Training Loss: 353855.90687755466, Test Loss: 48825304.0\n",
      "Epoch [26/100], Training Loss: 344357.06545820745, Test Loss: 47641448.0\n",
      "Epoch [27/100], Training Loss: 335388.6805876429, Test Loss: 46522668.0\n",
      "Epoch [28/100], Training Loss: 326930.26639417093, Test Loss: 45462932.0\n",
      "Epoch [29/100], Training Loss: 318991.3931046739, Test Loss: 44463996.0\n",
      "Epoch [30/100], Training Loss: 311523.9228126296, Test Loss: 43521624.0\n",
      "Epoch [31/100], Training Loss: 304492.0018363841, Test Loss: 42633276.0\n",
      "Epoch [32/100], Training Loss: 297822.98068834783, Test Loss: 41785432.0\n",
      "Epoch [33/100], Training Loss: 291445.6381138558, Test Loss: 40970704.0\n",
      "Epoch [34/100], Training Loss: 285307.4045968841, Test Loss: 40179548.0\n",
      "Epoch [35/100], Training Loss: 279414.2139683668, Test Loss: 39423236.0\n",
      "Epoch [36/100], Training Loss: 273811.3057875718, Test Loss: 38708076.0\n",
      "Epoch [37/100], Training Loss: 268459.7226467626, Test Loss: 38017084.0\n",
      "Epoch [38/100], Training Loss: 263211.6067176115, Test Loss: 37339808.0\n",
      "Epoch [39/100], Training Loss: 258109.36674367634, Test Loss: 36690132.0\n",
      "Epoch [40/100], Training Loss: 253309.99745275753, Test Loss: 36089216.0\n",
      "Epoch [41/100], Training Loss: 248856.91985071974, Test Loss: 35528540.0\n",
      "Epoch [42/100], Training Loss: 244641.07754279958, Test Loss: 34986832.0\n",
      "Epoch [43/100], Training Loss: 240547.4705289971, Test Loss: 34452788.0\n",
      "Epoch [44/100], Training Loss: 236541.82015283455, Test Loss: 33934920.0\n",
      "Epoch [45/100], Training Loss: 232720.09175996683, Test Loss: 33447010.0\n",
      "Epoch [46/100], Training Loss: 229116.18215745513, Test Loss: 32988918.0\n",
      "Epoch [47/100], Training Loss: 225712.55837924292, Test Loss: 32552746.0\n",
      "Epoch [48/100], Training Loss: 222472.28854925657, Test Loss: 32136538.0\n",
      "Epoch [49/100], Training Loss: 219360.12457792784, Test Loss: 31730210.0\n",
      "Epoch [50/100], Training Loss: 216347.6588472247, Test Loss: 31338004.0\n",
      "Epoch [51/100], Training Loss: 213386.8692612997, Test Loss: 30943778.0\n",
      "Epoch [52/100], Training Loss: 210345.106865707, Test Loss: 30537530.0\n",
      "Epoch [53/100], Training Loss: 207204.57490669985, Test Loss: 30132542.0\n",
      "Epoch [54/100], Training Loss: 204202.73718973994, Test Loss: 29760914.0\n",
      "Epoch [55/100], Training Loss: 201430.7287482969, Test Loss: 29413514.0\n",
      "Epoch [56/100], Training Loss: 198817.7990640365, Test Loss: 29084200.0\n",
      "Epoch [57/100], Training Loss: 196286.4980155204, Test Loss: 28761486.0\n",
      "Epoch [58/100], Training Loss: 193801.3817901783, Test Loss: 28446598.0\n",
      "Epoch [59/100], Training Loss: 191305.52162194182, Test Loss: 28123736.0\n",
      "Epoch [60/100], Training Loss: 188709.14285291155, Test Loss: 27788716.0\n",
      "Epoch [61/100], Training Loss: 185946.90939517802, Test Loss: 27431304.0\n",
      "Epoch [62/100], Training Loss: 183119.47443871808, Test Loss: 27090228.0\n",
      "Epoch [63/100], Training Loss: 180470.2268230555, Test Loss: 26776324.0\n",
      "Epoch [64/100], Training Loss: 178014.60239322315, Test Loss: 26488692.0\n",
      "Epoch [65/100], Training Loss: 175706.3840412298, Test Loss: 26214334.0\n",
      "Epoch [66/100], Training Loss: 173519.87376340263, Test Loss: 25955678.0\n",
      "Epoch [67/100], Training Loss: 171373.2487115692, Test Loss: 25690112.0\n",
      "Epoch [68/100], Training Loss: 169167.0422960725, Test Loss: 25414852.0\n",
      "Epoch [69/100], Training Loss: 166949.78321781885, Test Loss: 25146586.0\n",
      "Epoch [70/100], Training Loss: 164822.83383685802, Test Loss: 24890922.0\n",
      "Epoch [71/100], Training Loss: 162728.7494224276, Test Loss: 24628536.0\n",
      "Epoch [72/100], Training Loss: 160535.83659143417, Test Loss: 24349242.0\n",
      "Epoch [73/100], Training Loss: 158314.10387417808, Test Loss: 24083680.0\n",
      "Epoch [74/100], Training Loss: 156262.56202239203, Test Loss: 23844814.0\n",
      "Epoch [75/100], Training Loss: 154363.92764054262, Test Loss: 23620562.0\n",
      "Epoch [76/100], Training Loss: 152516.80712043124, Test Loss: 23395366.0\n",
      "Epoch [77/100], Training Loss: 150621.5348616788, Test Loss: 23161950.0\n",
      "Epoch [78/100], Training Loss: 148735.9705586162, Test Loss: 22939164.0\n",
      "Epoch [79/100], Training Loss: 146901.57342574492, Test Loss: 22723372.0\n",
      "Epoch [80/100], Training Loss: 145118.87577750132, Test Loss: 22519670.0\n",
      "Epoch [81/100], Training Loss: 143402.4723950003, Test Loss: 22323216.0\n",
      "Epoch [82/100], Training Loss: 141727.39064628872, Test Loss: 22138572.0\n",
      "Epoch [83/100], Training Loss: 140130.9115277531, Test Loss: 21963218.0\n",
      "Epoch [84/100], Training Loss: 138580.11169362004, Test Loss: 21790600.0\n",
      "Epoch [85/100], Training Loss: 137009.6216752562, Test Loss: 21613608.0\n",
      "Epoch [86/100], Training Loss: 135406.95320182454, Test Loss: 21433106.0\n",
      "Epoch [87/100], Training Loss: 133816.12955393636, Test Loss: 21258346.0\n",
      "Epoch [88/100], Training Loss: 132223.50390972098, Test Loss: 21078192.0\n",
      "Epoch [89/100], Training Loss: 130630.67581896807, Test Loss: 20903996.0\n",
      "Epoch [90/100], Training Loss: 129093.00953734969, Test Loss: 20738882.0\n",
      "Epoch [91/100], Training Loss: 127620.01048516083, Test Loss: 20586528.0\n",
      "Epoch [92/100], Training Loss: 126237.77012617736, Test Loss: 20444316.0\n",
      "Epoch [93/100], Training Loss: 124915.21518274983, Test Loss: 20309280.0\n",
      "Epoch [94/100], Training Loss: 123629.92956578401, Test Loss: 20177274.0\n",
      "Epoch [95/100], Training Loss: 122364.68208340739, Test Loss: 20046644.0\n",
      "Epoch [96/100], Training Loss: 121110.98560511819, Test Loss: 19916734.0\n",
      "Epoch [97/100], Training Loss: 119858.19056927908, Test Loss: 19787192.0\n",
      "Epoch [98/100], Training Loss: 118586.6750784906, Test Loss: 19649224.0\n",
      "Epoch [99/100], Training Loss: 117282.4797405367, Test Loss: 19509054.0\n",
      "Epoch [100/100], Training Loss: 115952.39069071738, Test Loss: 19365984.0\n",
      "Epoch [1/100], Training Loss: 1497783.1436526272, Test Loss: 298725792.0\n",
      "Epoch [2/100], Training Loss: 1467222.3105266276, Test Loss: 284896736.0\n",
      "Epoch [3/100], Training Loss: 1339096.1701321013, Test Loss: 246165312.0\n",
      "Epoch [4/100], Training Loss: 1106517.7963390795, Test Loss: 195902464.0\n",
      "Epoch [5/100], Training Loss: 890594.9784965345, Test Loss: 163271904.0\n",
      "Epoch [6/100], Training Loss: 774928.9861975001, Test Loss: 148266400.0\n",
      "Epoch [7/100], Training Loss: 714498.9747052899, Test Loss: 138863808.0\n",
      "Epoch [8/100], Training Loss: 669997.4096321308, Test Loss: 131188040.0\n",
      "Epoch [9/100], Training Loss: 631537.1729162964, Test Loss: 124421776.0\n",
      "Epoch [10/100], Training Loss: 596796.3632486227, Test Loss: 118320096.0\n",
      "Epoch [11/100], Training Loss: 564983.3455364018, Test Loss: 112783880.0\n",
      "Epoch [12/100], Training Loss: 535766.0821041408, Test Loss: 107762976.0\n",
      "Epoch [13/100], Training Loss: 508966.72448314674, Test Loss: 103217152.0\n",
      "Epoch [14/100], Training Loss: 484369.7138795095, Test Loss: 99066448.0\n",
      "Epoch [15/100], Training Loss: 461366.4112315621, Test Loss: 95192536.0\n",
      "Epoch [16/100], Training Loss: 440016.34855754994, Test Loss: 91699552.0\n",
      "Epoch [17/100], Training Loss: 420969.66459332977, Test Loss: 88619176.0\n",
      "Epoch [18/100], Training Loss: 403917.97642319766, Test Loss: 85831616.0\n",
      "Epoch [19/100], Training Loss: 388486.1052070375, Test Loss: 83289776.0\n",
      "Epoch [20/100], Training Loss: 374484.549967419, Test Loss: 80962504.0\n",
      "Epoch [21/100], Training Loss: 361769.7431431787, Test Loss: 78825608.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/100], Training Loss: 350218.39926544635, Test Loss: 76856480.0\n",
      "Epoch [23/100], Training Loss: 339715.2853503939, Test Loss: 75041016.0\n",
      "Epoch [24/100], Training Loss: 330154.5425034062, Test Loss: 73359888.0\n",
      "Epoch [25/100], Training Loss: 321432.6438007227, Test Loss: 71796208.0\n",
      "Epoch [26/100], Training Loss: 313438.2587524436, Test Loss: 70332280.0\n",
      "Epoch [27/100], Training Loss: 306067.1790178307, Test Loss: 68953512.0\n",
      "Epoch [28/100], Training Loss: 299233.2466678514, Test Loss: 67649720.0\n",
      "Epoch [29/100], Training Loss: 292864.41010603635, Test Loss: 66412172.0\n",
      "Epoch [30/100], Training Loss: 286885.422175819, Test Loss: 65229628.0\n",
      "Epoch [31/100], Training Loss: 281228.26948936674, Test Loss: 64093224.0\n",
      "Epoch [32/100], Training Loss: 275836.25340249395, Test Loss: 62992988.0\n",
      "Epoch [33/100], Training Loss: 270650.3897780789, Test Loss: 61923968.0\n",
      "Epoch [34/100], Training Loss: 265652.51590714074, Test Loss: 60887676.0\n",
      "Epoch [35/100], Training Loss: 260819.18693168354, Test Loss: 59878640.0\n",
      "Epoch [36/100], Training Loss: 256126.94249266927, Test Loss: 58893676.0\n",
      "Epoch [37/100], Training Loss: 251555.14440421184, Test Loss: 57932104.0\n",
      "Epoch [38/100], Training Loss: 247088.12011284876, Test Loss: 56990560.0\n",
      "Epoch [39/100], Training Loss: 242712.28629080032, Test Loss: 56069664.0\n",
      "Epoch [40/100], Training Loss: 238422.96352408032, Test Loss: 55168144.0\n",
      "Epoch [41/100], Training Loss: 234217.2062970203, Test Loss: 54286484.0\n",
      "Epoch [42/100], Training Loss: 230101.09670635627, Test Loss: 53424756.0\n",
      "Epoch [43/100], Training Loss: 226075.35531070436, Test Loss: 52581448.0\n",
      "Epoch [44/100], Training Loss: 222134.25051833421, Test Loss: 51756384.0\n",
      "Epoch [45/100], Training Loss: 218275.76443931047, Test Loss: 50949788.0\n",
      "Epoch [46/100], Training Loss: 214498.6936200462, Test Loss: 50162748.0\n",
      "Epoch [47/100], Training Loss: 210808.82230021918, Test Loss: 49395120.0\n",
      "Epoch [48/100], Training Loss: 207199.5656507316, Test Loss: 48645400.0\n",
      "Epoch [49/100], Training Loss: 203665.13766956935, Test Loss: 47911164.0\n",
      "Epoch [50/100], Training Loss: 200208.92824773415, Test Loss: 47192632.0\n",
      "Epoch [51/100], Training Loss: 196837.4488404123, Test Loss: 46487820.0\n",
      "Epoch [52/100], Training Loss: 193543.46285024585, Test Loss: 45798508.0\n",
      "Epoch [53/100], Training Loss: 190320.74441679995, Test Loss: 45124932.0\n",
      "Epoch [54/100], Training Loss: 187168.2702668681, Test Loss: 44468572.0\n",
      "Epoch [55/100], Training Loss: 184084.72713405604, Test Loss: 43824360.0\n",
      "Epoch [56/100], Training Loss: 181068.7675789349, Test Loss: 43192048.0\n",
      "Epoch [57/100], Training Loss: 178116.05174086252, Test Loss: 42572476.0\n",
      "Epoch [58/100], Training Loss: 175230.7491003199, Test Loss: 41965296.0\n",
      "Epoch [59/100], Training Loss: 172414.58262247496, Test Loss: 41371180.0\n",
      "Epoch [60/100], Training Loss: 169682.45479385107, Test Loss: 40792084.0\n",
      "Epoch [61/100], Training Loss: 167036.6933904982, Test Loss: 40227500.0\n",
      "Epoch [62/100], Training Loss: 164471.92794969195, Test Loss: 39677536.0\n",
      "Epoch [63/100], Training Loss: 161984.56280729815, Test Loss: 39141664.0\n",
      "Epoch [64/100], Training Loss: 159568.57946063622, Test Loss: 38618492.0\n",
      "Epoch [65/100], Training Loss: 157216.54715175345, Test Loss: 38109384.0\n",
      "Epoch [66/100], Training Loss: 154927.75849721025, Test Loss: 37612356.0\n",
      "Epoch [67/100], Training Loss: 152695.90591724793, Test Loss: 37124756.0\n",
      "Epoch [68/100], Training Loss: 150512.5414784234, Test Loss: 36646924.0\n",
      "Epoch [69/100], Training Loss: 148377.001988182, Test Loss: 36177984.0\n",
      "Epoch [70/100], Training Loss: 146285.5035252078, Test Loss: 35717928.0\n",
      "Epoch [71/100], Training Loss: 144241.0446394848, Test Loss: 35268552.0\n",
      "Epoch [72/100], Training Loss: 142234.98102254976, Test Loss: 34828444.0\n",
      "Epoch [73/100], Training Loss: 140265.4979233946, Test Loss: 34397700.0\n",
      "Epoch [74/100], Training Loss: 138335.8381408254, Test Loss: 33975076.0\n",
      "Epoch [75/100], Training Loss: 136445.8215504279, Test Loss: 33561180.0\n",
      "Epoch [76/100], Training Loss: 134589.42727874996, Test Loss: 33154688.0\n",
      "Epoch [77/100], Training Loss: 132771.7169150043, Test Loss: 32756038.0\n",
      "Epoch [78/100], Training Loss: 130991.27685769596, Test Loss: 32366144.0\n",
      "Epoch [79/100], Training Loss: 129254.57538048971, Test Loss: 31984872.0\n",
      "Epoch [80/100], Training Loss: 127551.74630062089, Test Loss: 31611418.0\n",
      "Epoch [81/100], Training Loss: 125881.32977742174, Test Loss: 31246520.0\n",
      "Epoch [82/100], Training Loss: 124251.48541768482, Test Loss: 30890634.0\n",
      "Epoch [83/100], Training Loss: 122662.53300539438, Test Loss: 30543116.0\n",
      "Epoch [84/100], Training Loss: 121116.86997400924, Test Loss: 30207040.0\n",
      "Epoch [85/100], Training Loss: 119607.93512885233, Test Loss: 29877582.0\n",
      "Epoch [86/100], Training Loss: 118135.85112075892, Test Loss: 29558514.0\n",
      "Epoch [87/100], Training Loss: 116694.85137344684, Test Loss: 29239118.0\n",
      "Epoch [88/100], Training Loss: 115285.45726704579, Test Loss: 28931074.0\n",
      "Epoch [89/100], Training Loss: 113903.30557815554, Test Loss: 28626920.0\n",
      "Epoch [90/100], Training Loss: 112550.47257757427, Test Loss: 28331882.0\n",
      "Epoch [91/100], Training Loss: 111226.23553708682, Test Loss: 28043900.0\n",
      "Epoch [92/100], Training Loss: 109930.58026868224, Test Loss: 27762644.0\n",
      "Epoch [93/100], Training Loss: 108661.65729268482, Test Loss: 27486246.0\n",
      "Epoch [94/100], Training Loss: 107413.15964925433, Test Loss: 27214010.0\n",
      "Epoch [95/100], Training Loss: 106184.41617980273, Test Loss: 26947114.0\n",
      "Epoch [96/100], Training Loss: 104976.6541215901, Test Loss: 26687286.0\n",
      "Epoch [97/100], Training Loss: 103795.61967411586, Test Loss: 26432200.0\n",
      "Epoch [98/100], Training Loss: 102640.13403845299, Test Loss: 26184588.0\n",
      "Epoch [99/100], Training Loss: 101513.96304415837, Test Loss: 25944640.0\n",
      "Epoch [100/100], Training Loss: 100416.88384754866, Test Loss: 25711166.0\n",
      "Epoch [1/100], Training Loss: 1164349.6266808838, Test Loss: 299288416.0\n",
      "Epoch [2/100], Training Loss: 1149225.699899295, Test Loss: 290227264.0\n",
      "Epoch [3/100], Training Loss: 1079529.4619986967, Test Loss: 261976592.0\n",
      "Epoch [4/100], Training Loss: 932806.6901249926, Test Loss: 216857872.0\n",
      "Epoch [5/100], Training Loss: 757886.4285291156, Test Loss: 176342768.0\n",
      "Epoch [6/100], Training Loss: 636423.6628161839, Test Loss: 154767552.0\n",
      "Epoch [7/100], Training Loss: 576008.871749304, Test Loss: 143819680.0\n",
      "Epoch [8/100], Training Loss: 539620.101415793, Test Loss: 135928336.0\n",
      "Epoch [9/100], Training Loss: 510354.9981636159, Test Loss: 129122576.0\n",
      "Epoch [10/100], Training Loss: 484197.63307860907, Test Loss: 122915792.0\n",
      "Epoch [11/100], Training Loss: 459971.2829808661, Test Loss: 117132696.0\n",
      "Epoch [12/100], Training Loss: 437224.9577631657, Test Loss: 111705680.0\n",
      "Epoch [13/100], Training Loss: 415801.79065221257, Test Loss: 106616872.0\n",
      "Epoch [14/100], Training Loss: 395681.7406551745, Test Loss: 101866096.0\n",
      "Epoch [15/100], Training Loss: 376876.55067827733, Test Loss: 97453000.0\n",
      "Epoch [16/100], Training Loss: 359403.22563829157, Test Loss: 93380712.0\n",
      "Epoch [17/100], Training Loss: 343269.68331259995, Test Loss: 89638832.0\n",
      "Epoch [18/100], Training Loss: 328436.37912445946, Test Loss: 86204328.0\n",
      "Epoch [19/100], Training Loss: 314822.96925537585, Test Loss: 83048200.0\n",
      "Epoch [20/100], Training Loss: 302329.1389135715, Test Loss: 80140256.0\n",
      "Epoch [21/100], Training Loss: 290845.7740655175, Test Loss: 77452712.0\n",
      "Epoch [22/100], Training Loss: 280268.9577631657, Test Loss: 74961160.0\n",
      "Epoch [23/100], Training Loss: 270502.88596647116, Test Loss: 72643248.0\n",
      "Epoch [24/100], Training Loss: 261463.64646644157, Test Loss: 70481560.0\n",
      "Epoch [25/100], Training Loss: 253077.25786387062, Test Loss: 68462240.0\n",
      "Epoch [26/100], Training Loss: 245283.57360345952, Test Loss: 66575020.0\n",
      "Epoch [27/100], Training Loss: 238025.78401753452, Test Loss: 64809220.0\n",
      "Epoch [28/100], Training Loss: 231253.67264972455, Test Loss: 63154996.0\n",
      "Epoch [29/100], Training Loss: 224923.20419406434, Test Loss: 61604472.0\n",
      "Epoch [30/100], Training Loss: 218983.71838161247, Test Loss: 60144132.0\n",
      "Epoch [31/100], Training Loss: 213394.7121616018, Test Loss: 58767240.0\n",
      "Epoch [32/100], Training Loss: 208116.41597061785, Test Loss: 57461300.0\n",
      "Epoch [33/100], Training Loss: 203106.1566257923, Test Loss: 56219332.0\n",
      "Epoch [34/100], Training Loss: 198332.68538593684, Test Loss: 55034636.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/100], Training Loss: 193762.22356495468, Test Loss: 53899412.0\n",
      "Epoch [36/100], Training Loss: 189376.55020437177, Test Loss: 52808968.0\n",
      "Epoch [37/100], Training Loss: 185166.0021325751, Test Loss: 51759172.0\n",
      "Epoch [38/100], Training Loss: 181112.18506012677, Test Loss: 50746432.0\n",
      "Epoch [39/100], Training Loss: 177202.68526746045, Test Loss: 49768464.0\n",
      "Epoch [40/100], Training Loss: 173428.10236360406, Test Loss: 48822788.0\n",
      "Epoch [41/100], Training Loss: 169781.9094840353, Test Loss: 47907344.0\n",
      "Epoch [42/100], Training Loss: 166258.6602689414, Test Loss: 47020088.0\n",
      "Epoch [43/100], Training Loss: 162850.9589479296, Test Loss: 46159772.0\n",
      "Epoch [44/100], Training Loss: 159551.9208577691, Test Loss: 45324572.0\n",
      "Epoch [45/100], Training Loss: 156355.28333629525, Test Loss: 44512504.0\n",
      "Epoch [46/100], Training Loss: 153255.33333333334, Test Loss: 43721180.0\n",
      "Epoch [47/100], Training Loss: 150245.1440080564, Test Loss: 42950236.0\n",
      "Epoch [48/100], Training Loss: 147323.929861975, Test Loss: 42199856.0\n",
      "Epoch [49/100], Training Loss: 144488.7307031574, Test Loss: 41468520.0\n",
      "Epoch [50/100], Training Loss: 141737.96469403472, Test Loss: 40756056.0\n",
      "Epoch [51/100], Training Loss: 139070.059297435, Test Loss: 40062348.0\n",
      "Epoch [52/100], Training Loss: 136482.65529293288, Test Loss: 39386760.0\n",
      "Epoch [53/100], Training Loss: 133971.5612819146, Test Loss: 38728388.0\n",
      "Epoch [54/100], Training Loss: 131530.97174338013, Test Loss: 38086848.0\n",
      "Epoch [55/100], Training Loss: 129159.86422605296, Test Loss: 37462312.0\n",
      "Epoch [56/100], Training Loss: 126858.63698833008, Test Loss: 36855636.0\n",
      "Epoch [57/100], Training Loss: 124625.71897399443, Test Loss: 36266744.0\n",
      "Epoch [58/100], Training Loss: 122458.74415022807, Test Loss: 35695268.0\n",
      "Epoch [59/100], Training Loss: 120356.86701024821, Test Loss: 35140488.0\n",
      "Epoch [60/100], Training Loss: 118319.44878857887, Test Loss: 34601704.0\n",
      "Epoch [61/100], Training Loss: 116341.77874533499, Test Loss: 34077912.0\n",
      "Epoch [62/100], Training Loss: 114420.4721876666, Test Loss: 33568184.0\n",
      "Epoch [63/100], Training Loss: 112553.85220069901, Test Loss: 33072616.0\n",
      "Epoch [64/100], Training Loss: 110740.97891120195, Test Loss: 32590792.0\n",
      "Epoch [65/100], Training Loss: 108982.38712161602, Test Loss: 32122848.0\n",
      "Epoch [66/100], Training Loss: 107277.51839346011, Test Loss: 31668482.0\n",
      "Epoch [67/100], Training Loss: 105623.78508382205, Test Loss: 31227378.0\n",
      "Epoch [68/100], Training Loss: 104018.91919909958, Test Loss: 30799036.0\n",
      "Epoch [69/100], Training Loss: 102461.9760085303, Test Loss: 30384212.0\n",
      "Epoch [70/100], Training Loss: 100950.82222617144, Test Loss: 29981582.0\n",
      "Epoch [71/100], Training Loss: 99484.2233280019, Test Loss: 29590410.0\n",
      "Epoch [72/100], Training Loss: 98060.51122563829, Test Loss: 29210316.0\n",
      "Epoch [73/100], Training Loss: 96675.785972395, Test Loss: 28841552.0\n",
      "Epoch [74/100], Training Loss: 95329.0664652568, Test Loss: 28483284.0\n",
      "Epoch [75/100], Training Loss: 94020.60043836266, Test Loss: 28135508.0\n",
      "Epoch [76/100], Training Loss: 92749.59883893134, Test Loss: 27798120.0\n",
      "Epoch [77/100], Training Loss: 91514.18375688644, Test Loss: 27470562.0\n",
      "Epoch [78/100], Training Loss: 90313.30010070493, Test Loss: 27152476.0\n",
      "Epoch [79/100], Training Loss: 89146.0886203424, Test Loss: 26844204.0\n",
      "Epoch [80/100], Training Loss: 88012.85267460458, Test Loss: 26545386.0\n",
      "Epoch [81/100], Training Loss: 86910.08086013862, Test Loss: 26255442.0\n",
      "Epoch [82/100], Training Loss: 85836.65795865173, Test Loss: 25974630.0\n",
      "Epoch [83/100], Training Loss: 84791.97820034358, Test Loss: 25702206.0\n",
      "Epoch [84/100], Training Loss: 83773.78857887565, Test Loss: 25438164.0\n",
      "Epoch [85/100], Training Loss: 82781.10923523488, Test Loss: 25181358.0\n",
      "Epoch [86/100], Training Loss: 81812.06504354007, Test Loss: 24932356.0\n",
      "Epoch [87/100], Training Loss: 80866.85865766247, Test Loss: 24689966.0\n",
      "Epoch [88/100], Training Loss: 79943.02102955987, Test Loss: 24454640.0\n",
      "Epoch [89/100], Training Loss: 79039.49007760204, Test Loss: 24225688.0\n",
      "Epoch [90/100], Training Loss: 78155.20342396777, Test Loss: 24002778.0\n",
      "Epoch [91/100], Training Loss: 77289.37302292518, Test Loss: 23786150.0\n",
      "Epoch [92/100], Training Loss: 76443.88389313429, Test Loss: 23575432.0\n",
      "Epoch [93/100], Training Loss: 75617.81245186896, Test Loss: 23370066.0\n",
      "Epoch [94/100], Training Loss: 74808.87411883182, Test Loss: 23170092.0\n",
      "Epoch [95/100], Training Loss: 74016.9724542385, Test Loss: 22974174.0\n",
      "Epoch [96/100], Training Loss: 73242.53429891594, Test Loss: 22783812.0\n",
      "Epoch [97/100], Training Loss: 72483.04845684498, Test Loss: 22596870.0\n",
      "Epoch [98/100], Training Loss: 71738.9632130798, Test Loss: 22415914.0\n",
      "Epoch [99/100], Training Loss: 71008.23405011551, Test Loss: 22237408.0\n",
      "Epoch [100/100], Training Loss: 70291.37405959363, Test Loss: 22064754.0\n",
      "Epoch [1/100], Training Loss: 582420.4935726556, Test Loss: 300027744.0\n",
      "Epoch [2/100], Training Loss: 581865.8506012677, Test Loss: 299362048.0\n",
      "Epoch [3/100], Training Loss: 579093.5968248326, Test Loss: 296877792.0\n",
      "Epoch [4/100], Training Loss: 571329.4473076239, Test Loss: 291038240.0\n",
      "Epoch [5/100], Training Loss: 555704.7094366448, Test Loss: 280532544.0\n",
      "Epoch [6/100], Training Loss: 530286.8777916001, Test Loss: 264795536.0\n",
      "Epoch [7/100], Training Loss: 495063.5663763995, Test Loss: 244470768.0\n",
      "Epoch [8/100], Training Loss: 452629.53711273026, Test Loss: 221595888.0\n",
      "Epoch [9/100], Training Loss: 408021.6456371068, Test Loss: 199193456.0\n",
      "Epoch [10/100], Training Loss: 367233.0752917481, Test Loss: 180168592.0\n",
      "Epoch [11/100], Training Loss: 334684.0329364374, Test Loss: 165951568.0\n",
      "Epoch [12/100], Training Loss: 311243.1988626266, Test Loss: 156053456.0\n",
      "Epoch [13/100], Training Loss: 294807.8028552811, Test Loss: 149024368.0\n",
      "Epoch [14/100], Training Loss: 282614.88679580594, Test Loss: 143594480.0\n",
      "Epoch [15/100], Training Loss: 272712.08956815355, Test Loss: 139016176.0\n",
      "Epoch [16/100], Training Loss: 264056.1961969078, Test Loss: 134922272.0\n",
      "Epoch [17/100], Training Loss: 256149.1712576269, Test Loss: 131138856.0\n",
      "Epoch [18/100], Training Loss: 248748.45708192643, Test Loss: 127577440.0\n",
      "Epoch [19/100], Training Loss: 241723.82536579587, Test Loss: 124188056.0\n",
      "Epoch [20/100], Training Loss: 234998.50956696877, Test Loss: 120940312.0\n",
      "Epoch [21/100], Training Loss: 228525.06557668385, Test Loss: 117815312.0\n",
      "Epoch [22/100], Training Loss: 222274.1477400628, Test Loss: 114801488.0\n",
      "Epoch [23/100], Training Loss: 216228.19311652152, Test Loss: 111892000.0\n",
      "Epoch [24/100], Training Loss: 210377.68402345834, Test Loss: 109083488.0\n",
      "Epoch [25/100], Training Loss: 204719.00811563295, Test Loss: 106374760.0\n",
      "Epoch [26/100], Training Loss: 199252.02345832592, Test Loss: 103765768.0\n",
      "Epoch [27/100], Training Loss: 193977.20537882828, Test Loss: 101255640.0\n",
      "Epoch [28/100], Training Loss: 188888.52698299865, Test Loss: 98834816.0\n",
      "Epoch [29/100], Training Loss: 183956.84236715833, Test Loss: 96487744.0\n",
      "Epoch [30/100], Training Loss: 179164.81440672945, Test Loss: 94215632.0\n",
      "Epoch [31/100], Training Loss: 174530.25199928915, Test Loss: 92031224.0\n",
      "Epoch [32/100], Training Loss: 170082.79130383272, Test Loss: 89945416.0\n",
      "Epoch [33/100], Training Loss: 165858.81499911143, Test Loss: 87969248.0\n",
      "Epoch [34/100], Training Loss: 161860.73028849, Test Loss: 86109160.0\n",
      "Epoch [35/100], Training Loss: 158091.8305787572, Test Loss: 84350472.0\n",
      "Epoch [36/100], Training Loss: 154526.49961495172, Test Loss: 82680768.0\n",
      "Epoch [37/100], Training Loss: 151147.98601978555, Test Loss: 81093560.0\n",
      "Epoch [38/100], Training Loss: 147942.88821752265, Test Loss: 79583480.0\n",
      "Epoch [39/100], Training Loss: 144897.15940998757, Test Loss: 78142888.0\n",
      "Epoch [40/100], Training Loss: 141997.52834547716, Test Loss: 76766056.0\n",
      "Epoch [41/100], Training Loss: 139234.84817250163, Test Loss: 75448976.0\n",
      "Epoch [42/100], Training Loss: 136599.4625910787, Test Loss: 74186792.0\n",
      "Epoch [43/100], Training Loss: 134082.509330016, Test Loss: 72975888.0\n",
      "Epoch [44/100], Training Loss: 131674.944612286, Test Loss: 71813432.0\n",
      "Epoch [45/100], Training Loss: 129369.91742195368, Test Loss: 70696216.0\n",
      "Epoch [46/100], Training Loss: 127160.38267875125, Test Loss: 69621832.0\n",
      "Epoch [47/100], Training Loss: 125039.53367691487, Test Loss: 68585648.0\n",
      "Epoch [48/100], Training Loss: 123000.35187488893, Test Loss: 67584736.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/100], Training Loss: 121038.25484272258, Test Loss: 66618920.0\n",
      "Epoch [50/100], Training Loss: 119150.12286002014, Test Loss: 65686472.0\n",
      "Epoch [51/100], Training Loss: 117331.96161364848, Test Loss: 64785212.0\n",
      "Epoch [52/100], Training Loss: 115577.55630590605, Test Loss: 63911976.0\n",
      "Epoch [53/100], Training Loss: 113882.39796220603, Test Loss: 63066800.0\n",
      "Epoch [54/100], Training Loss: 112243.94858124519, Test Loss: 62247772.0\n",
      "Epoch [55/100], Training Loss: 110658.4594514543, Test Loss: 61452668.0\n",
      "Epoch [56/100], Training Loss: 109121.37385225993, Test Loss: 60680056.0\n",
      "Epoch [57/100], Training Loss: 107629.49315798827, Test Loss: 59928460.0\n",
      "Epoch [58/100], Training Loss: 106179.25810082341, Test Loss: 59197196.0\n",
      "Epoch [59/100], Training Loss: 104768.01445412001, Test Loss: 58485012.0\n",
      "Epoch [60/100], Training Loss: 103392.58361471476, Test Loss: 57790944.0\n",
      "Epoch [61/100], Training Loss: 102049.4298915941, Test Loss: 57114348.0\n",
      "Epoch [62/100], Training Loss: 100737.40323440554, Test Loss: 56453144.0\n",
      "Epoch [63/100], Training Loss: 99456.62437059416, Test Loss: 55807536.0\n",
      "Epoch [64/100], Training Loss: 98205.41922871867, Test Loss: 55176888.0\n",
      "Epoch [65/100], Training Loss: 96982.83904981932, Test Loss: 54559824.0\n",
      "Epoch [66/100], Training Loss: 95785.03880101889, Test Loss: 53955112.0\n",
      "Epoch [67/100], Training Loss: 94610.42817368639, Test Loss: 53361992.0\n",
      "Epoch [68/100], Training Loss: 93456.52437651798, Test Loss: 52778332.0\n",
      "Epoch [69/100], Training Loss: 92322.45530478052, Test Loss: 52205576.0\n",
      "Epoch [70/100], Training Loss: 91210.37557016764, Test Loss: 51644448.0\n",
      "Epoch [71/100], Training Loss: 90119.79574669748, Test Loss: 51093684.0\n",
      "Epoch [72/100], Training Loss: 89051.0810970914, Test Loss: 50553772.0\n",
      "Epoch [73/100], Training Loss: 88003.87394111723, Test Loss: 50024580.0\n",
      "Epoch [74/100], Training Loss: 86976.56181505835, Test Loss: 49504720.0\n",
      "Epoch [75/100], Training Loss: 85968.65588531485, Test Loss: 48993628.0\n",
      "Epoch [76/100], Training Loss: 84979.52633137847, Test Loss: 48491368.0\n",
      "Epoch [77/100], Training Loss: 84008.27877495409, Test Loss: 47996772.0\n",
      "Epoch [78/100], Training Loss: 83055.20573425746, Test Loss: 47510232.0\n",
      "Epoch [79/100], Training Loss: 82119.73852259938, Test Loss: 47032220.0\n",
      "Epoch [80/100], Training Loss: 81201.48308749482, Test Loss: 46563228.0\n",
      "Epoch [81/100], Training Loss: 80300.88466323086, Test Loss: 46103128.0\n",
      "Epoch [82/100], Training Loss: 79417.37716959896, Test Loss: 45650924.0\n",
      "Epoch [83/100], Training Loss: 78550.64699958533, Test Loss: 45206488.0\n",
      "Epoch [84/100], Training Loss: 77700.4060186008, Test Loss: 44770068.0\n",
      "Epoch [85/100], Training Loss: 76865.38475208815, Test Loss: 44342672.0\n",
      "Epoch [86/100], Training Loss: 76044.95515668503, Test Loss: 43923408.0\n",
      "Epoch [87/100], Training Loss: 75238.47911853563, Test Loss: 43510996.0\n",
      "Epoch [88/100], Training Loss: 74444.94058408862, Test Loss: 43104612.0\n",
      "Epoch [89/100], Training Loss: 73664.88679580594, Test Loss: 42703316.0\n",
      "Epoch [90/100], Training Loss: 72898.19714471891, Test Loss: 42307936.0\n",
      "Epoch [91/100], Training Loss: 72144.68384574374, Test Loss: 41918688.0\n",
      "Epoch [92/100], Training Loss: 71404.01694212428, Test Loss: 41535364.0\n",
      "Epoch [93/100], Training Loss: 70675.36259700255, Test Loss: 41157752.0\n",
      "Epoch [94/100], Training Loss: 69958.29500622001, Test Loss: 40785440.0\n",
      "Epoch [95/100], Training Loss: 69252.5018660032, Test Loss: 40419152.0\n",
      "Epoch [96/100], Training Loss: 68558.39950239914, Test Loss: 40058468.0\n",
      "Epoch [97/100], Training Loss: 67876.93193531189, Test Loss: 39703124.0\n",
      "Epoch [98/100], Training Loss: 67208.17214619987, Test Loss: 39354308.0\n",
      "Epoch [99/100], Training Loss: 66551.87512588117, Test Loss: 39011516.0\n",
      "Epoch [100/100], Training Loss: 65907.52242165749, Test Loss: 38674700.0\n",
      "Epoch [1/100], Training Loss: 7230119.601563889, Test Loss: 149939408.0\n",
      "Epoch [2/100], Training Loss: 4109728.308631005, Test Loss: 119034560.0\n",
      "Epoch [3/100], Training Loss: 3307589.2418695576, Test Loss: 97798192.0\n",
      "Epoch [4/100], Training Loss: 2719796.6946863336, Test Loss: 82210368.0\n",
      "Epoch [5/100], Training Loss: 2280595.5945737814, Test Loss: 70486608.0\n",
      "Epoch [6/100], Training Loss: 1956587.825987797, Test Loss: 61828848.0\n",
      "Epoch [7/100], Training Loss: 1722281.44218352, Test Loss: 55550328.0\n",
      "Epoch [8/100], Training Loss: 1550858.357591375, Test Loss: 50793312.0\n",
      "Epoch [9/100], Training Loss: 1416528.5705230732, Test Loss: 46905136.0\n",
      "Epoch [10/100], Training Loss: 1303800.7142349386, Test Loss: 43550436.0\n",
      "Epoch [11/100], Training Loss: 1205642.832592856, Test Loss: 40565884.0\n",
      "Epoch [12/100], Training Loss: 1118902.0509004206, Test Loss: 37875204.0\n",
      "Epoch [13/100], Training Loss: 1042008.3051507613, Test Loss: 35444432.0\n",
      "Epoch [14/100], Training Loss: 973892.5277530951, Test Loss: 33273312.0\n",
      "Epoch [15/100], Training Loss: 913675.903974883, Test Loss: 31345894.0\n",
      "Epoch [16/100], Training Loss: 860370.4913512233, Test Loss: 29637334.0\n",
      "Epoch [17/100], Training Loss: 813193.8301492803, Test Loss: 28122350.0\n",
      "Epoch [18/100], Training Loss: 771467.8228481725, Test Loss: 26771358.0\n",
      "Epoch [19/100], Training Loss: 734543.4473816718, Test Loss: 25570870.0\n",
      "Epoch [20/100], Training Loss: 701681.1410165274, Test Loss: 24505174.0\n",
      "Epoch [21/100], Training Loss: 672095.8546442746, Test Loss: 23547078.0\n",
      "Epoch [22/100], Training Loss: 645179.0119513063, Test Loss: 22685068.0\n",
      "Epoch [23/100], Training Loss: 620563.3978437296, Test Loss: 21910512.0\n",
      "Epoch [24/100], Training Loss: 597975.8241366033, Test Loss: 21215198.0\n",
      "Epoch [25/100], Training Loss: 577185.1043406789, Test Loss: 20592386.0\n",
      "Epoch [26/100], Training Loss: 558015.1088575914, Test Loss: 20035896.0\n",
      "Epoch [27/100], Training Loss: 540308.9659898703, Test Loss: 19537684.0\n",
      "Epoch [28/100], Training Loss: 523875.48033291864, Test Loss: 19086484.0\n",
      "Epoch [29/100], Training Loss: 508615.7380709081, Test Loss: 18678760.0\n",
      "Epoch [30/100], Training Loss: 494434.5137728808, Test Loss: 18307708.0\n",
      "Epoch [31/100], Training Loss: 481232.6258811682, Test Loss: 17971554.0\n",
      "Epoch [32/100], Training Loss: 468942.9134455897, Test Loss: 17657388.0\n",
      "Epoch [33/100], Training Loss: 457445.37402256974, Test Loss: 17373580.0\n",
      "Epoch [34/100], Training Loss: 446677.75776760856, Test Loss: 17106500.0\n",
      "Epoch [35/100], Training Loss: 436557.49161039037, Test Loss: 16873160.0\n",
      "Epoch [36/100], Training Loss: 427109.91348261357, Test Loss: 16657168.0\n",
      "Epoch [37/100], Training Loss: 418220.2957096736, Test Loss: 16472280.0\n",
      "Epoch [38/100], Training Loss: 409871.06436970556, Test Loss: 16292002.0\n",
      "Epoch [39/100], Training Loss: 401999.51846750785, Test Loss: 16144965.0\n",
      "Epoch [40/100], Training Loss: 394575.4753421006, Test Loss: 15999096.0\n",
      "Epoch [41/100], Training Loss: 387512.0853178129, Test Loss: 15876617.0\n",
      "Epoch [42/100], Training Loss: 380895.8767697411, Test Loss: 15761600.0\n",
      "Epoch [43/100], Training Loss: 374627.577661276, Test Loss: 15670974.0\n",
      "Epoch [44/100], Training Loss: 368717.5242728511, Test Loss: 15571093.0\n",
      "Epoch [45/100], Training Loss: 363104.9162668088, Test Loss: 15506124.0\n",
      "Epoch [46/100], Training Loss: 357813.857850542, Test Loss: 15438256.0\n",
      "Epoch [47/100], Training Loss: 352762.946878147, Test Loss: 15386303.0\n",
      "Epoch [48/100], Training Loss: 348022.74975564244, Test Loss: 15310318.0\n",
      "Epoch [49/100], Training Loss: 343488.48266542266, Test Loss: 15263733.0\n",
      "Epoch [50/100], Training Loss: 339236.95919969195, Test Loss: 15185853.0\n",
      "Epoch [51/100], Training Loss: 335245.4044413838, Test Loss: 15146146.0\n",
      "Epoch [52/100], Training Loss: 331426.2020466797, Test Loss: 15058020.0\n",
      "Epoch [53/100], Training Loss: 327796.54630946036, Test Loss: 15066285.0\n",
      "Epoch [54/100], Training Loss: 324438.3721232451, Test Loss: 14979416.0\n",
      "Epoch [55/100], Training Loss: 321134.32236686215, Test Loss: 14976645.0\n",
      "Epoch [56/100], Training Loss: 318096.7702298442, Test Loss: 14885281.0\n",
      "Epoch [57/100], Training Loss: 315134.220795569, Test Loss: 14903230.0\n",
      "Epoch [58/100], Training Loss: 312391.7843618565, Test Loss: 14820754.0\n",
      "Epoch [59/100], Training Loss: 309712.35090486344, Test Loss: 14825847.0\n",
      "Epoch [60/100], Training Loss: 307209.15111664, Test Loss: 14753697.0\n",
      "Epoch [61/100], Training Loss: 304804.87117173156, Test Loss: 14765736.0\n",
      "Epoch [62/100], Training Loss: 302505.86810985726, Test Loss: 14694015.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [63/100], Training Loss: 300315.97046235413, Test Loss: 14702154.0\n",
      "Epoch [64/100], Training Loss: 298234.61376695696, Test Loss: 14639364.0\n",
      "Epoch [65/100], Training Loss: 296223.447700077, Test Loss: 14632236.0\n",
      "Epoch [66/100], Training Loss: 294280.2963872105, Test Loss: 14629105.0\n",
      "Epoch [67/100], Training Loss: 292455.0649139565, Test Loss: 14585766.0\n",
      "Epoch [68/100], Training Loss: 290637.13276760856, Test Loss: 14579924.0\n",
      "Epoch [69/100], Training Loss: 288944.1966523014, Test Loss: 14532682.0\n",
      "Epoch [70/100], Training Loss: 287264.37559608434, Test Loss: 14522993.0\n",
      "Epoch [71/100], Training Loss: 285654.76917096146, Test Loss: 14502935.0\n",
      "Epoch [72/100], Training Loss: 284058.219744091, Test Loss: 14472728.0\n",
      "Epoch [73/100], Training Loss: 282541.14862123097, Test Loss: 14496791.0\n",
      "Epoch [74/100], Training Loss: 281039.3015631479, Test Loss: 14415904.0\n",
      "Epoch [75/100], Training Loss: 279580.88133848703, Test Loss: 14476510.0\n",
      "Epoch [76/100], Training Loss: 278224.33170058054, Test Loss: 14428081.0\n",
      "Epoch [77/100], Training Loss: 276879.88902834547, Test Loss: 14434917.0\n",
      "Epoch [78/100], Training Loss: 275583.85655470646, Test Loss: 14435066.0\n",
      "Epoch [79/100], Training Loss: 274328.5392638173, Test Loss: 14426891.0\n",
      "Epoch [80/100], Training Loss: 273119.3877399147, Test Loss: 14400284.0\n",
      "Epoch [81/100], Training Loss: 271930.923734524, Test Loss: 14451383.0\n",
      "Epoch [82/100], Training Loss: 270753.6982776494, Test Loss: 14364993.0\n",
      "Epoch [83/100], Training Loss: 269606.1549189918, Test Loss: 14420743.0\n",
      "Epoch [84/100], Training Loss: 268519.9001577217, Test Loss: 14368744.0\n",
      "Epoch [85/100], Training Loss: 267424.9559267816, Test Loss: 14393693.0\n",
      "Epoch [86/100], Training Loss: 266362.077727919, Test Loss: 14393205.0\n",
      "Epoch [87/100], Training Loss: 265292.5920635626, Test Loss: 14354391.0\n",
      "Epoch [88/100], Training Loss: 264260.38284906105, Test Loss: 14367947.0\n",
      "Epoch [89/100], Training Loss: 263237.415866951, Test Loss: 14374047.0\n",
      "Epoch [90/100], Training Loss: 262235.54284772824, Test Loss: 14310744.0\n",
      "Epoch [91/100], Training Loss: 261262.08465138322, Test Loss: 14385354.0\n",
      "Epoch [92/100], Training Loss: 260336.95821855933, Test Loss: 14319981.0\n",
      "Epoch [93/100], Training Loss: 259393.3105747586, Test Loss: 14340698.0\n",
      "Epoch [94/100], Training Loss: 258500.1268919199, Test Loss: 14337942.0\n",
      "Epoch [95/100], Training Loss: 257552.29639091287, Test Loss: 14299154.0\n",
      "Epoch [96/100], Training Loss: 256675.49042562646, Test Loss: 14327363.0\n",
      "Epoch [97/100], Training Loss: 255785.4420428292, Test Loss: 14332367.0\n",
      "Epoch [98/100], Training Loss: 254936.31691324565, Test Loss: 14271475.0\n",
      "Epoch [99/100], Training Loss: 254090.85825039985, Test Loss: 14350265.0\n",
      "Epoch [100/100], Training Loss: 253281.60078638708, Test Loss: 14298471.0\n",
      "Epoch [1/100], Training Loss: 4417207.055980096, Test Loss: 237054880.0\n",
      "Epoch [2/100], Training Loss: 2745495.8785616965, Test Loss: 146851008.0\n",
      "Epoch [3/100], Training Loss: 2105551.8520229845, Test Loss: 127235200.0\n",
      "Epoch [4/100], Training Loss: 1832728.8708014928, Test Loss: 111864408.0\n",
      "Epoch [5/100], Training Loss: 1605094.7048160655, Test Loss: 98737872.0\n",
      "Epoch [6/100], Training Loss: 1409820.8974586814, Test Loss: 87977256.0\n",
      "Epoch [7/100], Training Loss: 1252496.4538830637, Test Loss: 79274624.0\n",
      "Epoch [8/100], Training Loss: 1125071.9135714709, Test Loss: 72182408.0\n",
      "Epoch [9/100], Training Loss: 1022806.9905218885, Test Loss: 66417916.0\n",
      "Epoch [10/100], Training Loss: 940771.6030448433, Test Loss: 61727048.0\n",
      "Epoch [11/100], Training Loss: 874012.9651679402, Test Loss: 57861120.0\n",
      "Epoch [12/100], Training Loss: 818745.2444760381, Test Loss: 54628380.0\n",
      "Epoch [13/100], Training Loss: 772321.0100704934, Test Loss: 51879540.0\n",
      "Epoch [14/100], Training Loss: 732228.9911142705, Test Loss: 49458484.0\n",
      "Epoch [15/100], Training Loss: 696457.5666133523, Test Loss: 47274212.0\n",
      "Epoch [16/100], Training Loss: 663958.6360997572, Test Loss: 45282404.0\n",
      "Epoch [17/100], Training Loss: 634194.3852852319, Test Loss: 43446780.0\n",
      "Epoch [18/100], Training Loss: 606673.9796812985, Test Loss: 41730332.0\n",
      "Epoch [19/100], Training Loss: 581249.1120490492, Test Loss: 40135356.0\n",
      "Epoch [20/100], Training Loss: 557924.0143948818, Test Loss: 38666660.0\n",
      "Epoch [21/100], Training Loss: 536649.1355369942, Test Loss: 37318704.0\n",
      "Epoch [22/100], Training Loss: 517114.19116166106, Test Loss: 36077600.0\n",
      "Epoch [23/100], Training Loss: 499048.8437000178, Test Loss: 34933120.0\n",
      "Epoch [24/100], Training Loss: 482422.6169362005, Test Loss: 33877764.0\n",
      "Epoch [25/100], Training Loss: 467115.15745512705, Test Loss: 32900458.0\n",
      "Epoch [26/100], Training Loss: 452945.53382501035, Test Loss: 31985892.0\n",
      "Epoch [27/100], Training Loss: 439656.06039334164, Test Loss: 31125442.0\n",
      "Epoch [28/100], Training Loss: 427017.001806765, Test Loss: 30304838.0\n",
      "Epoch [29/100], Training Loss: 414930.037320064, Test Loss: 29522438.0\n",
      "Epoch [30/100], Training Loss: 403487.43098750076, Test Loss: 28786830.0\n",
      "Epoch [31/100], Training Loss: 392782.22024761565, Test Loss: 28089822.0\n",
      "Epoch [32/100], Training Loss: 382576.5062200107, Test Loss: 27422290.0\n",
      "Epoch [33/100], Training Loss: 372996.76917836623, Test Loss: 26802870.0\n",
      "Epoch [34/100], Training Loss: 363973.9742610035, Test Loss: 26218834.0\n",
      "Epoch [35/100], Training Loss: 355328.0172975535, Test Loss: 25656382.0\n",
      "Epoch [36/100], Training Loss: 347083.5923523488, Test Loss: 25131672.0\n",
      "Epoch [37/100], Training Loss: 339356.41099460935, Test Loss: 24632332.0\n",
      "Epoch [38/100], Training Loss: 331980.33490314556, Test Loss: 24153154.0\n",
      "Epoch [39/100], Training Loss: 324797.0863692909, Test Loss: 23693056.0\n",
      "Epoch [40/100], Training Loss: 317966.2509626207, Test Loss: 23262950.0\n",
      "Epoch [41/100], Training Loss: 311399.41665185714, Test Loss: 22848290.0\n",
      "Epoch [42/100], Training Loss: 305124.5144837391, Test Loss: 22461058.0\n",
      "Epoch [43/100], Training Loss: 299143.5408151176, Test Loss: 22087386.0\n",
      "Epoch [44/100], Training Loss: 293334.15259759495, Test Loss: 21736356.0\n",
      "Epoch [45/100], Training Loss: 287826.9452787157, Test Loss: 21408122.0\n",
      "Epoch [46/100], Training Loss: 282574.92947692674, Test Loss: 21097502.0\n",
      "Epoch [47/100], Training Loss: 277358.6603726083, Test Loss: 20777446.0\n",
      "Epoch [48/100], Training Loss: 272043.8272762277, Test Loss: 20462292.0\n",
      "Epoch [49/100], Training Loss: 267066.81545820745, Test Loss: 20190068.0\n",
      "Epoch [50/100], Training Loss: 262550.31411053846, Test Loss: 19945904.0\n",
      "Epoch [51/100], Training Loss: 258321.03330667614, Test Loss: 19717070.0\n",
      "Epoch [52/100], Training Loss: 254262.89480777204, Test Loss: 19498004.0\n",
      "Epoch [53/100], Training Loss: 250291.6084947574, Test Loss: 19278756.0\n",
      "Epoch [54/100], Training Loss: 246387.57912742137, Test Loss: 19070074.0\n",
      "Epoch [55/100], Training Loss: 242585.24736390024, Test Loss: 18869470.0\n",
      "Epoch [56/100], Training Loss: 238866.5795865174, Test Loss: 18675450.0\n",
      "Epoch [57/100], Training Loss: 235312.189488182, Test Loss: 18499692.0\n",
      "Epoch [58/100], Training Loss: 231966.49486108642, Test Loss: 18333758.0\n",
      "Epoch [59/100], Training Loss: 228705.7657425508, Test Loss: 18172988.0\n",
      "Epoch [60/100], Training Loss: 225515.61807653576, Test Loss: 18020072.0\n",
      "Epoch [61/100], Training Loss: 222468.74292103547, Test Loss: 17882316.0\n",
      "Epoch [62/100], Training Loss: 219636.55898643445, Test Loss: 17764526.0\n",
      "Epoch [63/100], Training Loss: 216954.46961080504, Test Loss: 17637094.0\n",
      "Epoch [64/100], Training Loss: 214310.08742076892, Test Loss: 17529484.0\n",
      "Epoch [65/100], Training Loss: 211685.33759848351, Test Loss: 17406788.0\n",
      "Epoch [66/100], Training Loss: 209088.67165748475, Test Loss: 17300682.0\n",
      "Epoch [67/100], Training Loss: 206575.54405840885, Test Loss: 17188094.0\n",
      "Epoch [68/100], Training Loss: 204123.9482850542, Test Loss: 17089190.0\n",
      "Epoch [69/100], Training Loss: 201773.8226704579, Test Loss: 16988066.0\n",
      "Epoch [70/100], Training Loss: 199540.2184852793, Test Loss: 16902584.0\n",
      "Epoch [71/100], Training Loss: 197456.55978615012, Test Loss: 16822602.0\n",
      "Epoch [72/100], Training Loss: 195448.7049049227, Test Loss: 16740167.0\n",
      "Epoch [73/100], Training Loss: 193521.16320123215, Test Loss: 16663264.0\n",
      "Epoch [74/100], Training Loss: 191653.63140513003, Test Loss: 16589963.0\n",
      "Epoch [75/100], Training Loss: 189843.25860434808, Test Loss: 16512293.0\n",
      "Epoch [76/100], Training Loss: 188060.1330193709, Test Loss: 16443644.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [77/100], Training Loss: 186376.09848350217, Test Loss: 16374807.0\n",
      "Epoch [78/100], Training Loss: 184785.09550678276, Test Loss: 16298406.0\n",
      "Epoch [79/100], Training Loss: 183250.36238966885, Test Loss: 16259495.0\n",
      "Epoch [80/100], Training Loss: 181770.00691605947, Test Loss: 16172155.0\n",
      "Epoch [81/100], Training Loss: 180352.19489366745, Test Loss: 16127562.0\n",
      "Epoch [82/100], Training Loss: 178978.93538593684, Test Loss: 16062757.0\n",
      "Epoch [83/100], Training Loss: 177660.81525087377, Test Loss: 16012771.0\n",
      "Epoch [84/100], Training Loss: 176366.48639002428, Test Loss: 15958216.0\n",
      "Epoch [85/100], Training Loss: 175115.1143593389, Test Loss: 15901374.0\n",
      "Epoch [86/100], Training Loss: 173882.31959007168, Test Loss: 15856616.0\n",
      "Epoch [87/100], Training Loss: 172706.55941591138, Test Loss: 15802963.0\n",
      "Epoch [88/100], Training Loss: 171575.5296487175, Test Loss: 15771717.0\n",
      "Epoch [89/100], Training Loss: 170488.32785380012, Test Loss: 15705789.0\n",
      "Epoch [90/100], Training Loss: 169417.690998756, Test Loss: 15687633.0\n",
      "Epoch [91/100], Training Loss: 168369.8226704579, Test Loss: 15664729.0\n",
      "Epoch [92/100], Training Loss: 167327.65037616255, Test Loss: 15590287.0\n",
      "Epoch [93/100], Training Loss: 166310.57190036136, Test Loss: 15566030.0\n",
      "Epoch [94/100], Training Loss: 165292.28246253185, Test Loss: 15514623.0\n",
      "Epoch [95/100], Training Loss: 164300.08638410046, Test Loss: 15475079.0\n",
      "Epoch [96/100], Training Loss: 163328.3540370831, Test Loss: 15436907.0\n",
      "Epoch [97/100], Training Loss: 162397.36487026836, Test Loss: 15395045.0\n",
      "Epoch [98/100], Training Loss: 161467.73367987678, Test Loss: 15369641.0\n",
      "Epoch [99/100], Training Loss: 160573.41248296903, Test Loss: 15318562.0\n",
      "Epoch [100/100], Training Loss: 159678.01091463777, Test Loss: 15298827.0\n",
      "Epoch [1/100], Training Loss: 2315055.4152005212, Test Loss: 290558720.0\n",
      "Epoch [2/100], Training Loss: 2020688.6975890053, Test Loss: 218714048.0\n",
      "Epoch [3/100], Training Loss: 1410058.9341863634, Test Loss: 156856592.0\n",
      "Epoch [4/100], Training Loss: 1136893.3956519163, Test Loss: 139108304.0\n",
      "Epoch [5/100], Training Loss: 1025728.9795628221, Test Loss: 127196616.0\n",
      "Epoch [6/100], Training Loss: 937030.7673716012, Test Loss: 116863856.0\n",
      "Epoch [7/100], Training Loss: 858055.6194538238, Test Loss: 107614344.0\n",
      "Epoch [8/100], Training Loss: 787201.213435223, Test Loss: 99408472.0\n",
      "Epoch [9/100], Training Loss: 724443.3028849001, Test Loss: 92242456.0\n",
      "Epoch [10/100], Training Loss: 669559.5422072152, Test Loss: 86017392.0\n",
      "Epoch [11/100], Training Loss: 621755.0891534862, Test Loss: 80580312.0\n",
      "Epoch [12/100], Training Loss: 580068.7532729104, Test Loss: 75807840.0\n",
      "Epoch [13/100], Training Loss: 543664.8086013861, Test Loss: 71601232.0\n",
      "Epoch [14/100], Training Loss: 511795.6265624074, Test Loss: 67878768.0\n",
      "Epoch [15/100], Training Loss: 483832.8800426515, Test Loss: 64589744.0\n",
      "Epoch [16/100], Training Loss: 459270.74142527103, Test Loss: 61685388.0\n",
      "Epoch [17/100], Training Loss: 437628.3224927433, Test Loss: 59111512.0\n",
      "Epoch [18/100], Training Loss: 418374.1287838398, Test Loss: 56804036.0\n",
      "Epoch [19/100], Training Loss: 401071.7969906996, Test Loss: 54715288.0\n",
      "Epoch [20/100], Training Loss: 385312.04887151235, Test Loss: 52795416.0\n",
      "Epoch [21/100], Training Loss: 370761.3060245246, Test Loss: 51011832.0\n",
      "Epoch [22/100], Training Loss: 357195.4059593626, Test Loss: 49338724.0\n",
      "Epoch [23/100], Training Loss: 344468.98945560097, Test Loss: 47762196.0\n",
      "Epoch [24/100], Training Loss: 332486.2952431728, Test Loss: 46271304.0\n",
      "Epoch [25/100], Training Loss: 321160.1523014039, Test Loss: 44853488.0\n",
      "Epoch [26/100], Training Loss: 310422.11379657604, Test Loss: 43501040.0\n",
      "Epoch [27/100], Training Loss: 300227.20893312007, Test Loss: 42208608.0\n",
      "Epoch [28/100], Training Loss: 290543.4369409395, Test Loss: 40973576.0\n",
      "Epoch [29/100], Training Loss: 281344.0607783899, Test Loss: 39791908.0\n",
      "Epoch [30/100], Training Loss: 272596.5092707778, Test Loss: 38659648.0\n",
      "Epoch [31/100], Training Loss: 264276.9101356555, Test Loss: 37575468.0\n",
      "Epoch [32/100], Training Loss: 256368.57230021918, Test Loss: 36539748.0\n",
      "Epoch [33/100], Training Loss: 248855.98785616967, Test Loss: 35552696.0\n",
      "Epoch [34/100], Training Loss: 241716.7454534684, Test Loss: 34611412.0\n",
      "Epoch [35/100], Training Loss: 234921.77255494343, Test Loss: 33714188.0\n",
      "Epoch [36/100], Training Loss: 228449.97867424914, Test Loss: 32857174.0\n",
      "Epoch [37/100], Training Loss: 222286.68129850127, Test Loss: 32039862.0\n",
      "Epoch [38/100], Training Loss: 216427.23147325395, Test Loss: 31261890.0\n",
      "Epoch [39/100], Training Loss: 210867.58802796042, Test Loss: 30522472.0\n",
      "Epoch [40/100], Training Loss: 205592.78493572655, Test Loss: 29820334.0\n",
      "Epoch [41/100], Training Loss: 200579.89932468455, Test Loss: 29153210.0\n",
      "Epoch [42/100], Training Loss: 195814.02085184527, Test Loss: 28520480.0\n",
      "Epoch [43/100], Training Loss: 191284.0368757775, Test Loss: 27916988.0\n",
      "Epoch [44/100], Training Loss: 186981.78090752917, Test Loss: 27342648.0\n",
      "Epoch [45/100], Training Loss: 182890.03483205973, Test Loss: 26797704.0\n",
      "Epoch [46/100], Training Loss: 178992.76784550678, Test Loss: 26280468.0\n",
      "Epoch [47/100], Training Loss: 175277.33516971744, Test Loss: 25787624.0\n",
      "Epoch [48/100], Training Loss: 171728.06430306262, Test Loss: 25318110.0\n",
      "Epoch [49/100], Training Loss: 168332.65434512173, Test Loss: 24870434.0\n",
      "Epoch [50/100], Training Loss: 165074.49028493572, Test Loss: 24442882.0\n",
      "Epoch [51/100], Training Loss: 161945.2142941769, Test Loss: 24032706.0\n",
      "Epoch [52/100], Training Loss: 158938.0041170547, Test Loss: 23643586.0\n",
      "Epoch [53/100], Training Loss: 156044.72392038387, Test Loss: 23270032.0\n",
      "Epoch [54/100], Training Loss: 153260.25898939636, Test Loss: 22915316.0\n",
      "Epoch [55/100], Training Loss: 150571.72187666607, Test Loss: 22575238.0\n",
      "Epoch [56/100], Training Loss: 147979.12129020793, Test Loss: 22250972.0\n",
      "Epoch [57/100], Training Loss: 145475.1938866181, Test Loss: 21940090.0\n",
      "Epoch [58/100], Training Loss: 143057.44600438362, Test Loss: 21641990.0\n",
      "Epoch [59/100], Training Loss: 140724.03154433978, Test Loss: 21355878.0\n",
      "Epoch [60/100], Training Loss: 138464.80978615012, Test Loss: 21081602.0\n",
      "Epoch [61/100], Training Loss: 136279.73633078608, Test Loss: 20819020.0\n",
      "Epoch [62/100], Training Loss: 134168.8194419762, Test Loss: 20568630.0\n",
      "Epoch [63/100], Training Loss: 132124.51507612108, Test Loss: 20326276.0\n",
      "Epoch [64/100], Training Loss: 130148.3421450151, Test Loss: 20096326.0\n",
      "Epoch [65/100], Training Loss: 128237.58977548723, Test Loss: 19873582.0\n",
      "Epoch [66/100], Training Loss: 126382.79824951128, Test Loss: 19661620.0\n",
      "Epoch [67/100], Training Loss: 124588.53075943368, Test Loss: 19457224.0\n",
      "Epoch [68/100], Training Loss: 122850.0002369528, Test Loss: 19262650.0\n",
      "Epoch [69/100], Training Loss: 121162.1984035306, Test Loss: 19074908.0\n",
      "Epoch [70/100], Training Loss: 119527.94105799419, Test Loss: 18897602.0\n",
      "Epoch [71/100], Training Loss: 117943.99133641372, Test Loss: 18724450.0\n",
      "Epoch [72/100], Training Loss: 116405.32615070198, Test Loss: 18561532.0\n",
      "Epoch [73/100], Training Loss: 114914.21989218648, Test Loss: 18403810.0\n",
      "Epoch [74/100], Training Loss: 113465.82129316984, Test Loss: 18251440.0\n",
      "Epoch [75/100], Training Loss: 112061.05061903916, Test Loss: 18109584.0\n",
      "Epoch [76/100], Training Loss: 110697.67475268053, Test Loss: 17968546.0\n",
      "Epoch [77/100], Training Loss: 109372.98637521474, Test Loss: 17835832.0\n",
      "Epoch [78/100], Training Loss: 108087.42888454476, Test Loss: 17707844.0\n",
      "Epoch [79/100], Training Loss: 106840.44053965998, Test Loss: 17585776.0\n",
      "Epoch [80/100], Training Loss: 105628.32099697886, Test Loss: 17465096.0\n",
      "Epoch [81/100], Training Loss: 104449.14169776672, Test Loss: 17351202.0\n",
      "Epoch [82/100], Training Loss: 103300.37014987264, Test Loss: 17240580.0\n",
      "Epoch [83/100], Training Loss: 102183.49958533263, Test Loss: 17135176.0\n",
      "Epoch [84/100], Training Loss: 101097.5128546887, Test Loss: 17034766.0\n",
      "Epoch [85/100], Training Loss: 100043.69544162076, Test Loss: 16938134.0\n",
      "Epoch [86/100], Training Loss: 99021.07198921865, Test Loss: 16846362.0\n",
      "Epoch [87/100], Training Loss: 98024.39277886381, Test Loss: 16759615.0\n",
      "Epoch [88/100], Training Loss: 97058.64322315028, Test Loss: 16674460.0\n",
      "Epoch [89/100], Training Loss: 96117.40841774776, Test Loss: 16597048.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/100], Training Loss: 95204.26187725845, Test Loss: 16518515.0\n",
      "Epoch [91/100], Training Loss: 94312.52099994077, Test Loss: 16447752.0\n",
      "Epoch [92/100], Training Loss: 93447.6917836621, Test Loss: 16377568.0\n",
      "Epoch [93/100], Training Loss: 92610.93141697766, Test Loss: 16312238.0\n",
      "Epoch [94/100], Training Loss: 91798.94344233161, Test Loss: 16251915.0\n",
      "Epoch [95/100], Training Loss: 91010.94957348498, Test Loss: 16186773.0\n",
      "Epoch [96/100], Training Loss: 90243.51272140276, Test Loss: 16137239.0\n",
      "Epoch [97/100], Training Loss: 89496.31117824774, Test Loss: 16075580.0\n",
      "Epoch [98/100], Training Loss: 88772.51868965109, Test Loss: 16024428.0\n",
      "Epoch [99/100], Training Loss: 88069.21362774717, Test Loss: 15972197.0\n",
      "Epoch [100/100], Training Loss: 87389.70081748712, Test Loss: 15922951.0\n",
      "Epoch [1/100], Training Loss: 1497736.5407262603, Test Loss: 298717280.0\n",
      "Epoch [2/100], Training Loss: 1467551.9800959658, Test Loss: 285101600.0\n",
      "Epoch [3/100], Training Loss: 1341120.8170132102, Test Loss: 246814448.0\n",
      "Epoch [4/100], Training Loss: 1110283.0922338725, Test Loss: 196703456.0\n",
      "Epoch [5/100], Training Loss: 893754.8690243469, Test Loss: 163741472.0\n",
      "Epoch [6/100], Training Loss: 776571.8163615899, Test Loss: 148522304.0\n",
      "Epoch [7/100], Training Loss: 715503.7147088443, Test Loss: 139054016.0\n",
      "Epoch [8/100], Training Loss: 670798.7529174812, Test Loss: 131352096.0\n",
      "Epoch [9/100], Training Loss: 632250.2624252117, Test Loss: 124571632.0\n",
      "Epoch [10/100], Training Loss: 597464.0343581542, Test Loss: 118461360.0\n",
      "Epoch [11/100], Training Loss: 565626.4654937504, Test Loss: 112918888.0\n",
      "Epoch [12/100], Training Loss: 536378.4730762396, Test Loss: 107880776.0\n",
      "Epoch [13/100], Training Loss: 509220.51015935076, Test Loss: 103187136.0\n",
      "Epoch [14/100], Training Loss: 483606.9372667496, Test Loss: 98884784.0\n",
      "Epoch [15/100], Training Loss: 460474.5695160239, Test Loss: 95103000.0\n",
      "Epoch [16/100], Training Loss: 439792.02559090103, Test Loss: 91725344.0\n",
      "Epoch [17/100], Training Loss: 421137.5432735027, Test Loss: 88677952.0\n",
      "Epoch [18/100], Training Loss: 404239.61589953204, Test Loss: 85910176.0\n",
      "Epoch [19/100], Training Loss: 388894.5496119898, Test Loss: 83378168.0\n",
      "Epoch [20/100], Training Loss: 374939.09803921566, Test Loss: 81055560.0\n",
      "Epoch [21/100], Training Loss: 362240.86641786626, Test Loss: 78917056.0\n",
      "Epoch [22/100], Training Loss: 350679.24992595223, Test Loss: 76943712.0\n",
      "Epoch [23/100], Training Loss: 340154.78798649367, Test Loss: 75121432.0\n",
      "Epoch [24/100], Training Loss: 330563.6140039097, Test Loss: 73434256.0\n",
      "Epoch [25/100], Training Loss: 321812.25425034063, Test Loss: 71865544.0\n",
      "Epoch [26/100], Training Loss: 313795.0996978852, Test Loss: 70399216.0\n",
      "Epoch [27/100], Training Loss: 306417.31283691723, Test Loss: 69021256.0\n",
      "Epoch [28/100], Training Loss: 299591.483739115, Test Loss: 67719552.0\n",
      "Epoch [29/100], Training Loss: 293237.2660683609, Test Loss: 66485544.0\n",
      "Epoch [30/100], Training Loss: 287285.216589657, Test Loss: 65308704.0\n",
      "Epoch [31/100], Training Loss: 281665.61285616964, Test Loss: 64180596.0\n",
      "Epoch [32/100], Training Loss: 276328.4248378354, Test Loss: 63090512.0\n",
      "Epoch [33/100], Training Loss: 271221.05251836387, Test Loss: 62035856.0\n",
      "Epoch [34/100], Training Loss: 266305.04272575193, Test Loss: 61013768.0\n",
      "Epoch [35/100], Training Loss: 261551.71401001865, Test Loss: 60018048.0\n",
      "Epoch [36/100], Training Loss: 256930.97472935548, Test Loss: 59045088.0\n",
      "Epoch [37/100], Training Loss: 252421.17119468632, Test Loss: 58093632.0\n",
      "Epoch [38/100], Training Loss: 248006.7798486464, Test Loss: 57161744.0\n",
      "Epoch [39/100], Training Loss: 243680.09598068835, Test Loss: 56249760.0\n",
      "Epoch [40/100], Training Loss: 239431.51010011256, Test Loss: 55356968.0\n",
      "Epoch [41/100], Training Loss: 235259.53709792075, Test Loss: 54481088.0\n",
      "Epoch [42/100], Training Loss: 231165.93334221907, Test Loss: 53621804.0\n",
      "Epoch [43/100], Training Loss: 227144.56800544992, Test Loss: 52778316.0\n",
      "Epoch [44/100], Training Loss: 223200.3940672946, Test Loss: 51952596.0\n",
      "Epoch [45/100], Training Loss: 219335.40567798115, Test Loss: 51148200.0\n",
      "Epoch [46/100], Training Loss: 215551.32828327705, Test Loss: 50363340.0\n",
      "Epoch [47/100], Training Loss: 211848.43550441324, Test Loss: 49596104.0\n",
      "Epoch [48/100], Training Loss: 208229.100690125, Test Loss: 48845600.0\n",
      "Epoch [49/100], Training Loss: 204693.10187488893, Test Loss: 48113156.0\n",
      "Epoch [50/100], Training Loss: 201244.77129613174, Test Loss: 47395716.0\n",
      "Epoch [51/100], Training Loss: 197879.1742343463, Test Loss: 46696240.0\n",
      "Epoch [52/100], Training Loss: 194599.5036727682, Test Loss: 46017300.0\n",
      "Epoch [53/100], Training Loss: 191401.6839716249, Test Loss: 45355400.0\n",
      "Epoch [54/100], Training Loss: 188293.09233753924, Test Loss: 44708076.0\n",
      "Epoch [55/100], Training Loss: 185274.07678751257, Test Loss: 44076700.0\n",
      "Epoch [56/100], Training Loss: 182343.86248593093, Test Loss: 43462204.0\n",
      "Epoch [57/100], Training Loss: 179495.9463375985, Test Loss: 42863580.0\n",
      "Epoch [58/100], Training Loss: 176724.34875377643, Test Loss: 42279536.0\n",
      "Epoch [59/100], Training Loss: 174025.36183616196, Test Loss: 41707524.0\n",
      "Epoch [60/100], Training Loss: 171396.18431779812, Test Loss: 41147236.0\n",
      "Epoch [61/100], Training Loss: 168838.05041633346, Test Loss: 40598640.0\n",
      "Epoch [62/100], Training Loss: 166346.65224031455, Test Loss: 40061120.0\n",
      "Epoch [63/100], Training Loss: 163915.96389524464, Test Loss: 39534492.0\n",
      "Epoch [64/100], Training Loss: 161542.33687836918, Test Loss: 39018392.0\n",
      "Epoch [65/100], Training Loss: 159221.41705634294, Test Loss: 38512968.0\n",
      "Epoch [66/100], Training Loss: 156943.07429904552, Test Loss: 38016748.0\n",
      "Epoch [67/100], Training Loss: 154700.83927589635, Test Loss: 37526316.0\n",
      "Epoch [68/100], Training Loss: 152487.75962198395, Test Loss: 37042052.0\n",
      "Epoch [69/100], Training Loss: 150305.95850780833, Test Loss: 36565852.0\n",
      "Epoch [70/100], Training Loss: 148161.5264879345, Test Loss: 36098712.0\n",
      "Epoch [71/100], Training Loss: 146062.1132456082, Test Loss: 35642348.0\n",
      "Epoch [72/100], Training Loss: 144022.82426117558, Test Loss: 35198992.0\n",
      "Epoch [73/100], Training Loss: 142045.61854633398, Test Loss: 34768476.0\n",
      "Epoch [74/100], Training Loss: 140130.72191513618, Test Loss: 34350328.0\n",
      "Epoch [75/100], Training Loss: 138272.01615079684, Test Loss: 33943200.0\n",
      "Epoch [76/100], Training Loss: 136465.03882340677, Test Loss: 33546994.0\n",
      "Epoch [77/100], Training Loss: 134705.8581204692, Test Loss: 33161496.0\n",
      "Epoch [78/100], Training Loss: 132994.17990235417, Test Loss: 32786060.0\n",
      "Epoch [79/100], Training Loss: 131322.57380477682, Test Loss: 32417264.0\n",
      "Epoch [80/100], Training Loss: 129685.14551515943, Test Loss: 32058636.0\n",
      "Epoch [81/100], Training Loss: 128094.58225269904, Test Loss: 31710388.0\n",
      "Epoch [82/100], Training Loss: 126549.30919145414, Test Loss: 31373242.0\n",
      "Epoch [83/100], Training Loss: 125048.86633178574, Test Loss: 31043854.0\n",
      "Epoch [84/100], Training Loss: 123587.1705034969, Test Loss: 30725368.0\n",
      "Epoch [85/100], Training Loss: 122162.44361402982, Test Loss: 30413250.0\n",
      "Epoch [86/100], Training Loss: 120771.44667312733, Test Loss: 30111210.0\n",
      "Epoch [87/100], Training Loss: 119410.71474679373, Test Loss: 29812980.0\n",
      "Epoch [88/100], Training Loss: 118077.26877711999, Test Loss: 29522788.0\n",
      "Epoch [89/100], Training Loss: 116768.79339706994, Test Loss: 29237538.0\n",
      "Epoch [90/100], Training Loss: 115489.83927566496, Test Loss: 28960846.0\n",
      "Epoch [91/100], Training Loss: 114237.7414572041, Test Loss: 28690040.0\n",
      "Epoch [92/100], Training Loss: 113011.25426237338, Test Loss: 28423640.0\n",
      "Epoch [93/100], Training Loss: 111805.8838329705, Test Loss: 28163472.0\n",
      "Epoch [94/100], Training Loss: 110624.74272943694, Test Loss: 27909576.0\n",
      "Epoch [95/100], Training Loss: 109469.7058393127, Test Loss: 27661140.0\n",
      "Epoch [96/100], Training Loss: 108342.62200985946, Test Loss: 27419534.0\n",
      "Epoch [97/100], Training Loss: 107242.28212052381, Test Loss: 27182850.0\n",
      "Epoch [98/100], Training Loss: 106166.56590064273, Test Loss: 26952742.0\n",
      "Epoch [99/100], Training Loss: 105109.86496421642, Test Loss: 26726682.0\n",
      "Epoch [100/100], Training Loss: 104065.58213283426, Test Loss: 26502794.0\n",
      "Epoch [1/100], Training Loss: 1164487.4952905634, Test Loss: 299442464.0\n",
      "Epoch [2/100], Training Loss: 1151874.5628813459, Test Loss: 291790240.0\n",
      "Epoch [3/100], Training Loss: 1091818.7529174811, Test Loss: 267092224.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100], Training Loss: 960152.4421539009, Test Loss: 225446176.0\n",
      "Epoch [5/100], Training Loss: 791147.5111664, Test Loss: 184015216.0\n",
      "Epoch [6/100], Training Loss: 659779.4109353712, Test Loss: 159205808.0\n",
      "Epoch [7/100], Training Loss: 590238.1688288609, Test Loss: 146904432.0\n",
      "Epoch [8/100], Training Loss: 551292.553284758, Test Loss: 138783744.0\n",
      "Epoch [9/100], Training Loss: 521837.1492210177, Test Loss: 132016640.0\n",
      "Epoch [10/100], Training Loss: 496039.7687340797, Test Loss: 125913424.0\n",
      "Epoch [11/100], Training Loss: 472306.93655589124, Test Loss: 120243440.0\n",
      "Epoch [12/100], Training Loss: 450025.6411350038, Test Loss: 114901688.0\n",
      "Epoch [13/100], Training Loss: 428738.36597357976, Test Loss: 109703472.0\n",
      "Epoch [14/100], Training Loss: 407433.37527397665, Test Loss: 104480560.0\n",
      "Epoch [15/100], Training Loss: 386611.60571056214, Test Loss: 99603264.0\n",
      "Epoch [16/100], Training Loss: 367580.88999466854, Test Loss: 95255400.0\n",
      "Epoch [17/100], Training Loss: 350588.64498548664, Test Loss: 91368920.0\n",
      "Epoch [18/100], Training Loss: 335285.0923523488, Test Loss: 87842744.0\n",
      "Epoch [19/100], Training Loss: 321356.28564658493, Test Loss: 84619032.0\n",
      "Epoch [20/100], Training Loss: 308625.1194834429, Test Loss: 81657728.0\n",
      "Epoch [21/100], Training Loss: 296966.6015046502, Test Loss: 78929224.0\n",
      "Epoch [22/100], Training Loss: 286276.1108939044, Test Loss: 76411520.0\n",
      "Epoch [23/100], Training Loss: 276452.017771459, Test Loss: 74081592.0\n",
      "Epoch [24/100], Training Loss: 267408.2476156626, Test Loss: 71923280.0\n",
      "Epoch [25/100], Training Loss: 259070.84758011965, Test Loss: 69916568.0\n",
      "Epoch [26/100], Training Loss: 251379.86801729756, Test Loss: 68053456.0\n",
      "Epoch [27/100], Training Loss: 244275.49517208696, Test Loss: 66326328.0\n",
      "Epoch [28/100], Training Loss: 237703.0405781648, Test Loss: 64721048.0\n",
      "Epoch [29/100], Training Loss: 231612.52650909306, Test Loss: 63224768.0\n",
      "Epoch [30/100], Training Loss: 225959.30572833362, Test Loss: 61827428.0\n",
      "Epoch [31/100], Training Loss: 220691.235708785, Test Loss: 60518476.0\n",
      "Epoch [32/100], Training Loss: 215740.59024939282, Test Loss: 59285300.0\n",
      "Epoch [33/100], Training Loss: 211081.44529352526, Test Loss: 58120580.0\n",
      "Epoch [34/100], Training Loss: 206682.04857532136, Test Loss: 57018808.0\n",
      "Epoch [35/100], Training Loss: 202502.3523487945, Test Loss: 55972700.0\n",
      "Epoch [36/100], Training Loss: 198519.96990699603, Test Loss: 54972880.0\n",
      "Epoch [37/100], Training Loss: 194720.00248800428, Test Loss: 54016932.0\n",
      "Epoch [38/100], Training Loss: 191080.93975475387, Test Loss: 53104828.0\n",
      "Epoch [39/100], Training Loss: 187599.45489011315, Test Loss: 52231252.0\n",
      "Epoch [40/100], Training Loss: 184262.28315858066, Test Loss: 51392688.0\n",
      "Epoch [41/100], Training Loss: 181057.87725845625, Test Loss: 50588700.0\n",
      "Epoch [42/100], Training Loss: 177980.61749896334, Test Loss: 49821436.0\n",
      "Epoch [43/100], Training Loss: 175023.46839642202, Test Loss: 49081140.0\n",
      "Epoch [44/100], Training Loss: 172176.94212428175, Test Loss: 48367780.0\n",
      "Epoch [45/100], Training Loss: 169437.202890824, Test Loss: 47679928.0\n",
      "Epoch [46/100], Training Loss: 166798.50044428647, Test Loss: 47014584.0\n",
      "Epoch [47/100], Training Loss: 164261.4078549849, Test Loss: 46373804.0\n",
      "Epoch [48/100], Training Loss: 161818.73325039985, Test Loss: 45756124.0\n",
      "Epoch [49/100], Training Loss: 159466.86227119245, Test Loss: 45162620.0\n",
      "Epoch [50/100], Training Loss: 157200.47283928678, Test Loss: 44588384.0\n",
      "Epoch [51/100], Training Loss: 155019.69634500326, Test Loss: 44034604.0\n",
      "Epoch [52/100], Training Loss: 152918.6036372253, Test Loss: 43501264.0\n",
      "Epoch [53/100], Training Loss: 150892.84769859604, Test Loss: 42986876.0\n",
      "Epoch [54/100], Training Loss: 148940.86606243707, Test Loss: 42490392.0\n",
      "Epoch [55/100], Training Loss: 147059.29399917065, Test Loss: 42010412.0\n",
      "Epoch [56/100], Training Loss: 145246.6737160121, Test Loss: 41546460.0\n",
      "Epoch [57/100], Training Loss: 143502.72768200937, Test Loss: 41101180.0\n",
      "Epoch [58/100], Training Loss: 141821.26520940702, Test Loss: 40672028.0\n",
      "Epoch [59/100], Training Loss: 140198.2620697826, Test Loss: 40255936.0\n",
      "Epoch [60/100], Training Loss: 138632.33836858007, Test Loss: 39852000.0\n",
      "Epoch [61/100], Training Loss: 137120.33712457793, Test Loss: 39462144.0\n",
      "Epoch [62/100], Training Loss: 135662.01054439903, Test Loss: 39085464.0\n",
      "Epoch [63/100], Training Loss: 134252.1679402879, Test Loss: 38722532.0\n",
      "Epoch [64/100], Training Loss: 132887.93981399207, Test Loss: 38371520.0\n",
      "Epoch [65/100], Training Loss: 131565.40193116522, Test Loss: 38031128.0\n",
      "Epoch [66/100], Training Loss: 130272.3875362834, Test Loss: 37697924.0\n",
      "Epoch [67/100], Training Loss: 128998.30845329068, Test Loss: 37368448.0\n",
      "Epoch [68/100], Training Loss: 127750.52473194715, Test Loss: 37047960.0\n",
      "Epoch [69/100], Training Loss: 126549.54131864227, Test Loss: 36740856.0\n",
      "Epoch [70/100], Training Loss: 125391.70955512114, Test Loss: 36443252.0\n",
      "Epoch [71/100], Training Loss: 124264.91771814466, Test Loss: 36150676.0\n",
      "Epoch [72/100], Training Loss: 123159.0444286476, Test Loss: 35862340.0\n",
      "Epoch [73/100], Training Loss: 122067.42094662639, Test Loss: 35576672.0\n",
      "Epoch [74/100], Training Loss: 120999.6180321071, Test Loss: 35299728.0\n",
      "Epoch [75/100], Training Loss: 119964.94188732895, Test Loss: 35028744.0\n",
      "Epoch [76/100], Training Loss: 118945.09341863633, Test Loss: 34760756.0\n",
      "Epoch [77/100], Training Loss: 117938.56323677507, Test Loss: 34495464.0\n",
      "Epoch [78/100], Training Loss: 116960.41656299982, Test Loss: 34239468.0\n",
      "Epoch [79/100], Training Loss: 116022.10058645815, Test Loss: 33994216.0\n",
      "Epoch [80/100], Training Loss: 115117.20253539483, Test Loss: 33757920.0\n",
      "Epoch [81/100], Training Loss: 114241.64611101238, Test Loss: 33527988.0\n",
      "Epoch [82/100], Training Loss: 113391.57330726853, Test Loss: 33306684.0\n",
      "Epoch [83/100], Training Loss: 112561.64356376992, Test Loss: 33087140.0\n",
      "Epoch [84/100], Training Loss: 111742.45121734495, Test Loss: 32872634.0\n",
      "Epoch [85/100], Training Loss: 110922.9632130798, Test Loss: 32653584.0\n",
      "Epoch [86/100], Training Loss: 110082.47023280611, Test Loss: 32431714.0\n",
      "Epoch [87/100], Training Loss: 109222.75214738463, Test Loss: 32205068.0\n",
      "Epoch [88/100], Training Loss: 108365.40086487768, Test Loss: 31985096.0\n",
      "Epoch [89/100], Training Loss: 107529.15105740182, Test Loss: 31773848.0\n",
      "Epoch [90/100], Training Loss: 106734.14021681181, Test Loss: 31576436.0\n",
      "Epoch [91/100], Training Loss: 105977.8613826195, Test Loss: 31387814.0\n",
      "Epoch [92/100], Training Loss: 105248.34541792548, Test Loss: 31205530.0\n",
      "Epoch [93/100], Training Loss: 104538.69048042178, Test Loss: 31026992.0\n",
      "Epoch [94/100], Training Loss: 103842.68870327587, Test Loss: 30853002.0\n",
      "Epoch [95/100], Training Loss: 103156.79906403649, Test Loss: 30680098.0\n",
      "Epoch [96/100], Training Loss: 102478.20123215449, Test Loss: 30511242.0\n",
      "Epoch [97/100], Training Loss: 101805.43237959837, Test Loss: 30340208.0\n",
      "Epoch [98/100], Training Loss: 101129.81120786683, Test Loss: 30169888.0\n",
      "Epoch [99/100], Training Loss: 100438.14282329245, Test Loss: 29992668.0\n",
      "Epoch [100/100], Training Loss: 99737.05675019253, Test Loss: 29818744.0\n",
      "Epoch [1/100], Training Loss: 582414.0712043125, Test Loss: 300017568.0\n",
      "Epoch [2/100], Training Loss: 581798.2138498904, Test Loss: 299282752.0\n",
      "Epoch [3/100], Training Loss: 578755.2756353297, Test Loss: 296562560.0\n",
      "Epoch [4/100], Training Loss: 570263.1711391505, Test Loss: 290179712.0\n",
      "Epoch [5/100], Training Loss: 553210.7169006575, Test Loss: 278734368.0\n",
      "Epoch [6/100], Training Loss: 525619.6509685445, Test Loss: 261721840.0\n",
      "Epoch [7/100], Training Loss: 487795.0917599668, Test Loss: 240060880.0\n",
      "Epoch [8/100], Training Loss: 443054.54700550914, Test Loss: 216239216.0\n",
      "Epoch [9/100], Training Loss: 397320.1018896985, Test Loss: 193689920.0\n",
      "Epoch [10/100], Training Loss: 357080.1928795687, Test Loss: 175357216.0\n",
      "Epoch [11/100], Training Loss: 326327.3194715953, Test Loss: 162214336.0\n",
      "Epoch [12/100], Training Loss: 304860.6883478467, Test Loss: 153227456.0\n",
      "Epoch [13/100], Training Loss: 289830.17593744444, Test Loss: 146751584.0\n",
      "Epoch [14/100], Training Loss: 278402.95053610567, Test Loss: 141595888.0\n",
      "Epoch [15/100], Training Loss: 268851.38889876194, Test Loss: 137137120.0\n",
      "Epoch [16/100], Training Loss: 260334.49061074582, Test Loss: 133088200.0\n",
      "Epoch [17/100], Training Loss: 252465.13216041704, Test Loss: 129313880.0\n",
      "Epoch [18/100], Training Loss: 245051.46661927612, Test Loss: 125743064.0\n",
      "Epoch [19/100], Training Loss: 237986.8894022866, Test Loss: 122334424.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/100], Training Loss: 231207.37870979207, Test Loss: 119062624.0\n",
      "Epoch [21/100], Training Loss: 224673.21272436468, Test Loss: 115911912.0\n",
      "Epoch [22/100], Training Loss: 218359.35880575795, Test Loss: 112871544.0\n",
      "Epoch [23/100], Training Loss: 212248.39760677685, Test Loss: 109931776.0\n",
      "Epoch [24/100], Training Loss: 206309.83306676144, Test Loss: 107069160.0\n",
      "Epoch [25/100], Training Loss: 200495.41875481312, Test Loss: 104269656.0\n",
      "Epoch [26/100], Training Loss: 194792.30495823707, Test Loss: 101537688.0\n",
      "Epoch [27/100], Training Loss: 189232.6087317102, Test Loss: 98892632.0\n",
      "Epoch [28/100], Training Loss: 183847.35927966353, Test Loss: 96350136.0\n",
      "Epoch [29/100], Training Loss: 178704.3490314555, Test Loss: 93941096.0\n",
      "Epoch [30/100], Training Loss: 173852.72412771755, Test Loss: 91683480.0\n",
      "Epoch [31/100], Training Loss: 169288.21467922517, Test Loss: 89556408.0\n",
      "Epoch [32/100], Training Loss: 164989.02932290742, Test Loss: 87554368.0\n",
      "Epoch [33/100], Training Loss: 160935.23772288373, Test Loss: 85661328.0\n",
      "Epoch [34/100], Training Loss: 157105.63189384516, Test Loss: 83870224.0\n",
      "Epoch [35/100], Training Loss: 153485.92239796222, Test Loss: 82171088.0\n",
      "Epoch [36/100], Training Loss: 150059.25359872045, Test Loss: 80559360.0\n",
      "Epoch [37/100], Training Loss: 146811.1618979918, Test Loss: 79025136.0\n",
      "Epoch [38/100], Training Loss: 143729.69326461703, Test Loss: 77565360.0\n",
      "Epoch [39/100], Training Loss: 140803.99478703868, Test Loss: 76173608.0\n",
      "Epoch [40/100], Training Loss: 138023.30217404183, Test Loss: 74844736.0\n",
      "Epoch [41/100], Training Loss: 135377.30205556544, Test Loss: 73574896.0\n",
      "Epoch [42/100], Training Loss: 132856.18861441858, Test Loss: 72360328.0\n",
      "Epoch [43/100], Training Loss: 130450.92921035484, Test Loss: 71195824.0\n",
      "Epoch [44/100], Training Loss: 128152.41063918015, Test Loss: 70077520.0\n",
      "Epoch [45/100], Training Loss: 125953.46578994136, Test Loss: 69002128.0\n",
      "Epoch [46/100], Training Loss: 123847.55144837391, Test Loss: 67968800.0\n",
      "Epoch [47/100], Training Loss: 121832.16634085658, Test Loss: 66975992.0\n",
      "Epoch [48/100], Training Loss: 119900.18979918251, Test Loss: 66020260.0\n",
      "Epoch [49/100], Training Loss: 118045.9669450862, Test Loss: 65099768.0\n",
      "Epoch [50/100], Training Loss: 116264.72554943427, Test Loss: 64212068.0\n",
      "Epoch [51/100], Training Loss: 114550.84817250163, Test Loss: 63354312.0\n",
      "Epoch [52/100], Training Loss: 112898.65908417747, Test Loss: 62522948.0\n",
      "Epoch [53/100], Training Loss: 111302.20958474024, Test Loss: 61717292.0\n",
      "Epoch [54/100], Training Loss: 109757.57218174278, Test Loss: 60935352.0\n",
      "Epoch [55/100], Training Loss: 108260.85930928262, Test Loss: 60176448.0\n",
      "Epoch [56/100], Training Loss: 106805.34375925596, Test Loss: 59438692.0\n",
      "Epoch [57/100], Training Loss: 105388.55518038031, Test Loss: 58721700.0\n",
      "Epoch [58/100], Training Loss: 104010.89769563415, Test Loss: 58025376.0\n",
      "Epoch [59/100], Training Loss: 102669.39055743143, Test Loss: 57349524.0\n",
      "Epoch [60/100], Training Loss: 101362.67211658077, Test Loss: 56690612.0\n",
      "Epoch [61/100], Training Loss: 100088.82495112848, Test Loss: 56048100.0\n",
      "Epoch [62/100], Training Loss: 98848.01860079379, Test Loss: 55420740.0\n",
      "Epoch [63/100], Training Loss: 97637.86090871393, Test Loss: 54807436.0\n",
      "Epoch [64/100], Training Loss: 96456.6144185771, Test Loss: 54208448.0\n",
      "Epoch [65/100], Training Loss: 95302.64605177418, Test Loss: 53623876.0\n",
      "Epoch [66/100], Training Loss: 94176.20271310941, Test Loss: 53052376.0\n",
      "Epoch [67/100], Training Loss: 93074.78549848942, Test Loss: 52494428.0\n",
      "Epoch [68/100], Training Loss: 91997.4520466797, Test Loss: 51949008.0\n",
      "Epoch [69/100], Training Loss: 90942.57058231147, Test Loss: 51416052.0\n",
      "Epoch [70/100], Training Loss: 89909.87133463657, Test Loss: 50893256.0\n",
      "Epoch [71/100], Training Loss: 88901.21758189681, Test Loss: 50382108.0\n",
      "Epoch [72/100], Training Loss: 87915.14353415082, Test Loss: 49880728.0\n",
      "Epoch [73/100], Training Loss: 86949.46744861087, Test Loss: 49391164.0\n",
      "Epoch [74/100], Training Loss: 86004.26195130621, Test Loss: 48912888.0\n",
      "Epoch [75/100], Training Loss: 85080.18103192939, Test Loss: 48444264.0\n",
      "Epoch [76/100], Training Loss: 84177.44375333215, Test Loss: 47984604.0\n",
      "Epoch [77/100], Training Loss: 83294.96901842309, Test Loss: 47535620.0\n",
      "Epoch [78/100], Training Loss: 82431.57976423198, Test Loss: 47097248.0\n",
      "Epoch [79/100], Training Loss: 81587.83046028079, Test Loss: 46668860.0\n",
      "Epoch [80/100], Training Loss: 80763.78520229844, Test Loss: 46250076.0\n",
      "Epoch [81/100], Training Loss: 79958.7712813222, Test Loss: 45840260.0\n",
      "Epoch [82/100], Training Loss: 79171.26781588768, Test Loss: 45438372.0\n",
      "Epoch [83/100], Training Loss: 78401.02896747823, Test Loss: 45044424.0\n",
      "Epoch [84/100], Training Loss: 77647.08322966649, Test Loss: 44659160.0\n",
      "Epoch [85/100], Training Loss: 76908.48362063858, Test Loss: 44281748.0\n",
      "Epoch [86/100], Training Loss: 76185.14791777739, Test Loss: 43911988.0\n",
      "Epoch [87/100], Training Loss: 75478.13352289556, Test Loss: 43548828.0\n",
      "Epoch [88/100], Training Loss: 74787.14448196196, Test Loss: 43192696.0\n",
      "Epoch [89/100], Training Loss: 74112.21112493335, Test Loss: 42843868.0\n",
      "Epoch [90/100], Training Loss: 73452.72780048575, Test Loss: 42501620.0\n",
      "Epoch [91/100], Training Loss: 72807.00136247852, Test Loss: 42166152.0\n",
      "Epoch [92/100], Training Loss: 72174.75137728808, Test Loss: 41837556.0\n",
      "Epoch [93/100], Training Loss: 71556.50755287008, Test Loss: 41515784.0\n",
      "Epoch [94/100], Training Loss: 70951.1588176056, Test Loss: 41200524.0\n",
      "Epoch [95/100], Training Loss: 70357.61969077661, Test Loss: 40891644.0\n",
      "Epoch [96/100], Training Loss: 69775.19495290563, Test Loss: 40588460.0\n",
      "Epoch [97/100], Training Loss: 69202.8411823944, Test Loss: 40291600.0\n",
      "Epoch [98/100], Training Loss: 68639.90119068776, Test Loss: 40000560.0\n",
      "Epoch [99/100], Training Loss: 68086.29583555476, Test Loss: 39714780.0\n",
      "Epoch [100/100], Training Loss: 67542.26692731473, Test Loss: 39433780.0\n",
      "Epoch [1/100], Training Loss: 7112575.890113145, Test Loss: 147755504.0\n",
      "Epoch [2/100], Training Loss: 4042411.213672176, Test Loss: 116613680.0\n",
      "Epoch [3/100], Training Loss: 3222976.94283514, Test Loss: 94760416.0\n",
      "Epoch [4/100], Training Loss: 2616183.411527753, Test Loss: 78984528.0\n",
      "Epoch [5/100], Training Loss: 2182975.1485101595, Test Loss: 67666808.0\n",
      "Epoch [6/100], Training Loss: 1879457.3135477756, Test Loss: 59709520.0\n",
      "Epoch [7/100], Training Loss: 1667957.2938510752, Test Loss: 54000664.0\n",
      "Epoch [8/100], Training Loss: 1512870.4787334874, Test Loss: 49649908.0\n",
      "Epoch [9/100], Training Loss: 1389898.9081511758, Test Loss: 46076452.0\n",
      "Epoch [10/100], Training Loss: 1286669.2617735916, Test Loss: 43026000.0\n",
      "Epoch [11/100], Training Loss: 1197982.8063799539, Test Loss: 40356740.0\n",
      "Epoch [12/100], Training Loss: 1120786.5235471833, Test Loss: 37974800.0\n",
      "Epoch [13/100], Training Loss: 1052052.7089923583, Test Loss: 35819880.0\n",
      "Epoch [14/100], Training Loss: 990084.7572122505, Test Loss: 33888592.0\n",
      "Epoch [15/100], Training Loss: 934663.6007049346, Test Loss: 32160748.0\n",
      "Epoch [16/100], Training Loss: 885593.2509033825, Test Loss: 30625402.0\n",
      "Epoch [17/100], Training Loss: 842255.5791422309, Test Loss: 29253414.0\n",
      "Epoch [18/100], Training Loss: 803317.3444553048, Test Loss: 28022960.0\n",
      "Epoch [19/100], Training Loss: 768530.3840708489, Test Loss: 26918540.0\n",
      "Epoch [20/100], Training Loss: 736556.4716397133, Test Loss: 25893066.0\n",
      "Epoch [21/100], Training Loss: 707355.3841300871, Test Loss: 24974848.0\n",
      "Epoch [22/100], Training Loss: 681289.5192376045, Test Loss: 24157014.0\n",
      "Epoch [23/100], Training Loss: 657602.4454860494, Test Loss: 23410166.0\n",
      "Epoch [24/100], Training Loss: 635586.872800782, Test Loss: 22724584.0\n",
      "Epoch [25/100], Training Loss: 614833.0312037201, Test Loss: 22095860.0\n",
      "Epoch [26/100], Training Loss: 595664.8841597062, Test Loss: 21524810.0\n",
      "Epoch [27/100], Training Loss: 577758.9354747941, Test Loss: 21004190.0\n",
      "Epoch [28/100], Training Loss: 561424.6574551271, Test Loss: 20538528.0\n",
      "Epoch [29/100], Training Loss: 546352.111893549, Test Loss: 20111416.0\n",
      "Epoch [30/100], Training Loss: 532190.3242254605, Test Loss: 19724862.0\n",
      "Epoch [31/100], Training Loss: 518469.3336443339, Test Loss: 19328286.0\n",
      "Epoch [32/100], Training Loss: 504883.06387358566, Test Loss: 18974532.0\n",
      "Epoch [33/100], Training Loss: 492619.88265653694, Test Loss: 18659862.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/100], Training Loss: 481563.0261240448, Test Loss: 18394028.0\n",
      "Epoch [35/100], Training Loss: 471247.5964397844, Test Loss: 18145132.0\n",
      "Epoch [36/100], Training Loss: 461749.194449381, Test Loss: 17941326.0\n",
      "Epoch [37/100], Training Loss: 452754.9702846395, Test Loss: 17721546.0\n",
      "Epoch [38/100], Training Loss: 444176.59027160716, Test Loss: 17546856.0\n",
      "Epoch [39/100], Training Loss: 436191.5401042592, Test Loss: 17366608.0\n",
      "Epoch [40/100], Training Loss: 428949.85856880515, Test Loss: 17243482.0\n",
      "Epoch [41/100], Training Loss: 422194.3224261004, Test Loss: 17101002.0\n",
      "Epoch [42/100], Training Loss: 415816.9980377347, Test Loss: 17014828.0\n",
      "Epoch [43/100], Training Loss: 409647.89754013385, Test Loss: 16882700.0\n",
      "Epoch [44/100], Training Loss: 403761.68025442807, Test Loss: 16806508.0\n",
      "Epoch [45/100], Training Loss: 398194.50417629286, Test Loss: 16698364.0\n",
      "Epoch [46/100], Training Loss: 392729.29129642795, Test Loss: 16616749.0\n",
      "Epoch [47/100], Training Loss: 387449.51035187487, Test Loss: 16510507.0\n",
      "Epoch [48/100], Training Loss: 382549.1432157455, Test Loss: 16438961.0\n",
      "Epoch [49/100], Training Loss: 377976.9396658966, Test Loss: 16364608.0\n",
      "Epoch [50/100], Training Loss: 373543.92448610865, Test Loss: 16250421.0\n",
      "Epoch [51/100], Training Loss: 369259.56178543926, Test Loss: 16195600.0\n",
      "Epoch [52/100], Training Loss: 365262.5014883597, Test Loss: 16114293.0\n",
      "Epoch [53/100], Training Loss: 361509.80740921746, Test Loss: 16042042.0\n",
      "Epoch [54/100], Training Loss: 357823.54759048636, Test Loss: 15973841.0\n",
      "Epoch [55/100], Training Loss: 354328.1812022392, Test Loss: 15897981.0\n",
      "Epoch [56/100], Training Loss: 350990.22840767726, Test Loss: 15883133.0\n",
      "Epoch [57/100], Training Loss: 347903.0599564599, Test Loss: 15759450.0\n",
      "Epoch [58/100], Training Loss: 344894.7566717019, Test Loss: 15779871.0\n",
      "Epoch [59/100], Training Loss: 342083.7883937563, Test Loss: 15666508.0\n",
      "Epoch [60/100], Training Loss: 339282.5202742728, Test Loss: 15676601.0\n",
      "Epoch [61/100], Training Loss: 336673.3060837628, Test Loss: 15581762.0\n",
      "Epoch [62/100], Training Loss: 334027.2097328357, Test Loss: 15573693.0\n",
      "Epoch [63/100], Training Loss: 331562.8172945916, Test Loss: 15533545.0\n",
      "Epoch [64/100], Training Loss: 329081.22642319766, Test Loss: 15475327.0\n",
      "Epoch [65/100], Training Loss: 326708.29094840353, Test Loss: 15464675.0\n",
      "Epoch [66/100], Training Loss: 324348.0327735324, Test Loss: 15452029.0\n",
      "Epoch [67/100], Training Loss: 322014.8007745394, Test Loss: 15414919.0\n",
      "Epoch [68/100], Training Loss: 319614.91848083644, Test Loss: 15402058.0\n",
      "Epoch [69/100], Training Loss: 317335.1602097032, Test Loss: 15338231.0\n",
      "Epoch [70/100], Training Loss: 315010.5157795747, Test Loss: 15376832.0\n",
      "Epoch [71/100], Training Loss: 312919.24544606364, Test Loss: 15291749.0\n",
      "Epoch [72/100], Training Loss: 310707.517149458, Test Loss: 15295149.0\n",
      "Epoch [73/100], Training Loss: 308695.0754102245, Test Loss: 15221516.0\n",
      "Epoch [74/100], Training Loss: 306580.51009270776, Test Loss: 15253938.0\n",
      "Epoch [75/100], Training Loss: 304611.9557120431, Test Loss: 15188010.0\n",
      "Epoch [76/100], Training Loss: 302585.7870757064, Test Loss: 15132928.0\n",
      "Epoch [77/100], Training Loss: 300690.136843937, Test Loss: 15139123.0\n",
      "Epoch [78/100], Training Loss: 298725.6061363367, Test Loss: 15125037.0\n",
      "Epoch [79/100], Training Loss: 296894.4497511996, Test Loss: 15093792.0\n",
      "Epoch [80/100], Training Loss: 295004.9443123926, Test Loss: 15093965.0\n",
      "Epoch [81/100], Training Loss: 293273.8853444701, Test Loss: 15032829.0\n",
      "Epoch [82/100], Training Loss: 291447.3617417511, Test Loss: 15079115.0\n",
      "Epoch [83/100], Training Loss: 289806.80266645935, Test Loss: 15021898.0\n",
      "Epoch [84/100], Training Loss: 288029.8267504887, Test Loss: 14979044.0\n",
      "Epoch [85/100], Training Loss: 286454.0043762218, Test Loss: 15001014.0\n",
      "Epoch [86/100], Training Loss: 284790.16545598605, Test Loss: 14991995.0\n",
      "Epoch [87/100], Training Loss: 283298.72871127306, Test Loss: 14960371.0\n",
      "Epoch [88/100], Training Loss: 281718.4646792252, Test Loss: 14981097.0\n",
      "Epoch [89/100], Training Loss: 280322.4102430247, Test Loss: 14931706.0\n",
      "Epoch [90/100], Training Loss: 278819.9975860435, Test Loss: 14926275.0\n",
      "Epoch [91/100], Training Loss: 277512.86725460575, Test Loss: 14966045.0\n",
      "Epoch [92/100], Training Loss: 276116.6060104555, Test Loss: 14938722.0\n",
      "Epoch [93/100], Training Loss: 274872.64253820863, Test Loss: 14918428.0\n",
      "Epoch [94/100], Training Loss: 273540.2420768912, Test Loss: 14914461.0\n",
      "Epoch [95/100], Training Loss: 272371.56020822225, Test Loss: 14905052.0\n",
      "Epoch [96/100], Training Loss: 271095.19574151415, Test Loss: 14895400.0\n",
      "Epoch [97/100], Training Loss: 269973.4175922635, Test Loss: 14838825.0\n",
      "Epoch [98/100], Training Loss: 268713.8688503347, Test Loss: 14892926.0\n",
      "Epoch [99/100], Training Loss: 267644.39307875716, Test Loss: 14839819.0\n",
      "Epoch [100/100], Training Loss: 266399.65579645755, Test Loss: 14851979.0\n",
      "Epoch [1/100], Training Loss: 4357078.284224868, Test Loss: 224035728.0\n",
      "Epoch [2/100], Training Loss: 2607544.2942953617, Test Loss: 142864848.0\n",
      "Epoch [3/100], Training Loss: 2042035.6996623422, Test Loss: 122980344.0\n",
      "Epoch [4/100], Training Loss: 1759942.6472365381, Test Loss: 107095968.0\n",
      "Epoch [5/100], Training Loss: 1528092.9859605473, Test Loss: 94167472.0\n",
      "Epoch [6/100], Training Loss: 1339023.1167584858, Test Loss: 83724400.0\n",
      "Epoch [7/100], Training Loss: 1185233.1650968546, Test Loss: 75185352.0\n",
      "Epoch [8/100], Training Loss: 1060755.8710976837, Test Loss: 68223800.0\n",
      "Epoch [9/100], Training Loss: 960658.6765594456, Test Loss: 62579948.0\n",
      "Epoch [10/100], Training Loss: 880427.2666311237, Test Loss: 58027248.0\n",
      "Epoch [11/100], Training Loss: 815456.228481725, Test Loss: 54287524.0\n",
      "Epoch [12/100], Training Loss: 761396.5422072152, Test Loss: 51111584.0\n",
      "Epoch [13/100], Training Loss: 714788.5411409277, Test Loss: 48321424.0\n",
      "Epoch [14/100], Training Loss: 673413.5021621942, Test Loss: 45809824.0\n",
      "Epoch [15/100], Training Loss: 636034.9269000652, Test Loss: 43507604.0\n",
      "Epoch [16/100], Training Loss: 601915.6882293703, Test Loss: 41378124.0\n",
      "Epoch [17/100], Training Loss: 570611.3096084355, Test Loss: 39400336.0\n",
      "Epoch [18/100], Training Loss: 541822.5166755524, Test Loss: 37560316.0\n",
      "Epoch [19/100], Training Loss: 515354.1086428529, Test Loss: 35849488.0\n",
      "Epoch [20/100], Training Loss: 491056.1648895208, Test Loss: 34270484.0\n",
      "Epoch [21/100], Training Loss: 468752.57404774596, Test Loss: 32816252.0\n",
      "Epoch [22/100], Training Loss: 448253.84888336, Test Loss: 31475784.0\n",
      "Epoch [23/100], Training Loss: 429435.4297434986, Test Loss: 30242256.0\n",
      "Epoch [24/100], Training Loss: 412165.69613766955, Test Loss: 29107716.0\n",
      "Epoch [25/100], Training Loss: 396342.13411527756, Test Loss: 28065516.0\n",
      "Epoch [26/100], Training Loss: 381822.95290563355, Test Loss: 27104570.0\n",
      "Epoch [27/100], Training Loss: 368505.9893963628, Test Loss: 26221140.0\n",
      "Epoch [28/100], Training Loss: 356255.47282447724, Test Loss: 25407694.0\n",
      "Epoch [29/100], Training Loss: 344898.5326254369, Test Loss: 24651864.0\n",
      "Epoch [30/100], Training Loss: 334314.0301226231, Test Loss: 23950328.0\n",
      "Epoch [31/100], Training Loss: 324398.5952846395, Test Loss: 23297558.0\n",
      "Epoch [32/100], Training Loss: 315093.95853326225, Test Loss: 22693048.0\n",
      "Epoch [33/100], Training Loss: 306342.47036609205, Test Loss: 22131498.0\n",
      "Epoch [34/100], Training Loss: 298097.07773532375, Test Loss: 21611612.0\n",
      "Epoch [35/100], Training Loss: 290315.4907292222, Test Loss: 21130492.0\n",
      "Epoch [36/100], Training Loss: 282965.8228037439, Test Loss: 20684112.0\n",
      "Epoch [37/100], Training Loss: 276031.59664711804, Test Loss: 20267452.0\n",
      "Epoch [38/100], Training Loss: 269471.11121971445, Test Loss: 19880940.0\n",
      "Epoch [39/100], Training Loss: 263247.9107132279, Test Loss: 19522268.0\n",
      "Epoch [40/100], Training Loss: 257365.9133937563, Test Loss: 19185606.0\n",
      "Epoch [41/100], Training Loss: 251788.027397666, Test Loss: 18873570.0\n",
      "Epoch [42/100], Training Loss: 246493.9603548368, Test Loss: 18576958.0\n",
      "Epoch [43/100], Training Loss: 241455.44132456608, Test Loss: 18306476.0\n",
      "Epoch [44/100], Training Loss: 236660.06936792843, Test Loss: 18046870.0\n",
      "Epoch [45/100], Training Loss: 232072.07228540964, Test Loss: 17806898.0\n",
      "Epoch [46/100], Training Loss: 227680.93938451514, Test Loss: 17580052.0\n",
      "Epoch [47/100], Training Loss: 223504.5487382264, Test Loss: 17372674.0\n",
      "Epoch [48/100], Training Loss: 219520.24805994905, Test Loss: 17175646.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/100], Training Loss: 215734.7686007938, Test Loss: 16995556.0\n",
      "Epoch [50/100], Training Loss: 212114.18839227533, Test Loss: 16824484.0\n",
      "Epoch [51/100], Training Loss: 208666.6980184823, Test Loss: 16666992.0\n",
      "Epoch [52/100], Training Loss: 205371.4061963154, Test Loss: 16519431.0\n",
      "Epoch [53/100], Training Loss: 202218.2487115692, Test Loss: 16384167.0\n",
      "Epoch [54/100], Training Loss: 199217.8680321071, Test Loss: 16258152.0\n",
      "Epoch [55/100], Training Loss: 196353.50254724247, Test Loss: 16143973.0\n",
      "Epoch [56/100], Training Loss: 193608.20617854391, Test Loss: 16032833.0\n",
      "Epoch [57/100], Training Loss: 190977.73027368047, Test Loss: 15948396.0\n",
      "Epoch [58/100], Training Loss: 188452.87903560215, Test Loss: 15846144.0\n",
      "Epoch [59/100], Training Loss: 186018.55494342753, Test Loss: 15772987.0\n",
      "Epoch [60/100], Training Loss: 183697.09753569102, Test Loss: 15690314.0\n",
      "Epoch [61/100], Training Loss: 181455.02656833126, Test Loss: 15626053.0\n",
      "Epoch [62/100], Training Loss: 179328.54956756116, Test Loss: 15557553.0\n",
      "Epoch [63/100], Training Loss: 177265.7961465553, Test Loss: 15505867.0\n",
      "Epoch [64/100], Training Loss: 175291.62548131036, Test Loss: 15443971.0\n",
      "Epoch [65/100], Training Loss: 173391.62784343344, Test Loss: 15401883.0\n",
      "Epoch [66/100], Training Loss: 171570.12089775488, Test Loss: 15342261.0\n",
      "Epoch [67/100], Training Loss: 169833.79864196433, Test Loss: 15298509.0\n",
      "Epoch [68/100], Training Loss: 168152.73270985132, Test Loss: 15247932.0\n",
      "Epoch [69/100], Training Loss: 166555.42708518452, Test Loss: 15201834.0\n",
      "Epoch [70/100], Training Loss: 165021.99047005508, Test Loss: 15149530.0\n",
      "Epoch [71/100], Training Loss: 163553.48196937385, Test Loss: 15111634.0\n",
      "Epoch [72/100], Training Loss: 162159.71482732065, Test Loss: 15047146.0\n",
      "Epoch [73/100], Training Loss: 160812.82918665957, Test Loss: 15044833.0\n",
      "Epoch [74/100], Training Loss: 159528.78019667082, Test Loss: 14956915.0\n",
      "Epoch [75/100], Training Loss: 158291.5738700314, Test Loss: 14966533.0\n",
      "Epoch [76/100], Training Loss: 157116.30319590072, Test Loss: 14895922.0\n",
      "Epoch [77/100], Training Loss: 155990.08894615248, Test Loss: 14887966.0\n",
      "Epoch [78/100], Training Loss: 154912.07330726853, Test Loss: 14833441.0\n",
      "Epoch [79/100], Training Loss: 153868.13534447012, Test Loss: 14826722.0\n",
      "Epoch [80/100], Training Loss: 152867.0081896807, Test Loss: 14773858.0\n",
      "Epoch [81/100], Training Loss: 151887.98326520942, Test Loss: 14772524.0\n",
      "Epoch [82/100], Training Loss: 150963.908114152, Test Loss: 14725957.0\n",
      "Epoch [83/100], Training Loss: 150059.81679106687, Test Loss: 14709290.0\n",
      "Epoch [84/100], Training Loss: 149187.46211717316, Test Loss: 14691842.0\n",
      "Epoch [85/100], Training Loss: 148348.32001214384, Test Loss: 14667726.0\n",
      "Epoch [86/100], Training Loss: 147543.71416829573, Test Loss: 14617674.0\n",
      "Epoch [87/100], Training Loss: 146751.12247497187, Test Loss: 14663471.0\n",
      "Epoch [88/100], Training Loss: 145984.2098957408, Test Loss: 14592458.0\n",
      "Epoch [89/100], Training Loss: 145228.72890379716, Test Loss: 14612812.0\n",
      "Epoch [90/100], Training Loss: 144493.10018660032, Test Loss: 14559998.0\n",
      "Epoch [91/100], Training Loss: 143793.69581185948, Test Loss: 14576446.0\n",
      "Epoch [92/100], Training Loss: 143115.4497141757, Test Loss: 14524011.0\n",
      "Epoch [93/100], Training Loss: 142442.41148332445, Test Loss: 14539184.0\n",
      "Epoch [94/100], Training Loss: 141789.76448373913, Test Loss: 14507721.0\n",
      "Epoch [95/100], Training Loss: 141155.47970351283, Test Loss: 14490268.0\n",
      "Epoch [96/100], Training Loss: 140532.24202505776, Test Loss: 14486301.0\n",
      "Epoch [97/100], Training Loss: 139932.28677951545, Test Loss: 14490921.0\n",
      "Epoch [98/100], Training Loss: 139341.3891135004, Test Loss: 14428502.0\n",
      "Epoch [99/100], Training Loss: 138780.06023784136, Test Loss: 14474302.0\n",
      "Epoch [100/100], Training Loss: 138228.4439680706, Test Loss: 14429231.0\n",
      "Epoch [1/100], Training Loss: 2318392.570108406, Test Loss: 292499840.0\n",
      "Epoch [2/100], Training Loss: 2074764.502102956, Test Loss: 230912992.0\n",
      "Epoch [3/100], Training Loss: 1494745.455127066, Test Loss: 163632976.0\n",
      "Epoch [4/100], Training Loss: 1173689.6368698536, Test Loss: 142912816.0\n",
      "Epoch [5/100], Training Loss: 1056269.033114152, Test Loss: 131100664.0\n",
      "Epoch [6/100], Training Loss: 969734.4330312185, Test Loss: 121080424.0\n",
      "Epoch [7/100], Training Loss: 893206.1001125525, Test Loss: 112056720.0\n",
      "Epoch [8/100], Training Loss: 822972.285054203, Test Loss: 103675224.0\n",
      "Epoch [9/100], Training Loss: 758189.9471595285, Test Loss: 96255640.0\n",
      "Epoch [10/100], Training Loss: 701818.159232273, Test Loss: 89872744.0\n",
      "Epoch [11/100], Training Loss: 652832.9908180794, Test Loss: 84303936.0\n",
      "Epoch [12/100], Training Loss: 610023.824773414, Test Loss: 79411776.0\n",
      "Epoch [13/100], Training Loss: 572620.0600675315, Test Loss: 75114832.0\n",
      "Epoch [14/100], Training Loss: 540007.3442331615, Test Loss: 71333968.0\n",
      "Epoch [15/100], Training Loss: 511511.4849831171, Test Loss: 67987488.0\n",
      "Epoch [16/100], Training Loss: 486496.74438718084, Test Loss: 65017364.0\n",
      "Epoch [17/100], Training Loss: 464390.47698596056, Test Loss: 62373732.0\n",
      "Epoch [18/100], Training Loss: 444721.8195604526, Test Loss: 60005884.0\n",
      "Epoch [19/100], Training Loss: 427078.11966115754, Test Loss: 57870468.0\n",
      "Epoch [20/100], Training Loss: 411134.65037616255, Test Loss: 55932520.0\n",
      "Epoch [21/100], Training Loss: 396592.8904685741, Test Loss: 54153184.0\n",
      "Epoch [22/100], Training Loss: 383150.24797109177, Test Loss: 52498224.0\n",
      "Epoch [23/100], Training Loss: 370627.2954208874, Test Loss: 50956024.0\n",
      "Epoch [24/100], Training Loss: 358915.19827024464, Test Loss: 49508464.0\n",
      "Epoch [25/100], Training Loss: 347903.24631242227, Test Loss: 48143336.0\n",
      "Epoch [26/100], Training Loss: 337525.3355251466, Test Loss: 46855048.0\n",
      "Epoch [27/100], Training Loss: 327726.23458325927, Test Loss: 45632424.0\n",
      "Epoch [28/100], Training Loss: 318456.3079201469, Test Loss: 44472804.0\n",
      "Epoch [29/100], Training Loss: 309641.0584681002, Test Loss: 43367032.0\n",
      "Epoch [30/100], Training Loss: 301246.70517149457, Test Loss: 42306804.0\n",
      "Epoch [31/100], Training Loss: 293245.30537290446, Test Loss: 41289724.0\n",
      "Epoch [32/100], Training Loss: 285630.81162253424, Test Loss: 40318708.0\n",
      "Epoch [33/100], Training Loss: 278388.22060304484, Test Loss: 39392256.0\n",
      "Epoch [34/100], Training Loss: 271503.4351637936, Test Loss: 38506812.0\n",
      "Epoch [35/100], Training Loss: 264961.41443042475, Test Loss: 37661148.0\n",
      "Epoch [36/100], Training Loss: 258738.90824003317, Test Loss: 36860512.0\n",
      "Epoch [37/100], Training Loss: 252807.16089094247, Test Loss: 36094852.0\n",
      "Epoch [38/100], Training Loss: 247120.11936496652, Test Loss: 35357704.0\n",
      "Epoch [39/100], Training Loss: 241655.31804987855, Test Loss: 34647924.0\n",
      "Epoch [40/100], Training Loss: 236411.29927137017, Test Loss: 33967208.0\n",
      "Epoch [41/100], Training Loss: 231384.7250755287, Test Loss: 33312364.0\n",
      "Epoch [42/100], Training Loss: 226545.98761921687, Test Loss: 32683286.0\n",
      "Epoch [43/100], Training Loss: 221884.3671583437, Test Loss: 32078720.0\n",
      "Epoch [44/100], Training Loss: 217441.029500622, Test Loss: 31501842.0\n",
      "Epoch [45/100], Training Loss: 213220.74450565726, Test Loss: 30952066.0\n",
      "Epoch [46/100], Training Loss: 209197.44621171732, Test Loss: 30428806.0\n",
      "Epoch [47/100], Training Loss: 205345.08388128664, Test Loss: 29926562.0\n",
      "Epoch [48/100], Training Loss: 201655.7198921865, Test Loss: 29447722.0\n",
      "Epoch [49/100], Training Loss: 198136.64261595876, Test Loss: 28988276.0\n",
      "Epoch [50/100], Training Loss: 194762.41259404062, Test Loss: 28547574.0\n",
      "Epoch [51/100], Training Loss: 191502.16672590486, Test Loss: 28119536.0\n",
      "Epoch [52/100], Training Loss: 188339.44434571412, Test Loss: 27703938.0\n",
      "Epoch [53/100], Training Loss: 185273.78896392393, Test Loss: 27301938.0\n",
      "Epoch [54/100], Training Loss: 182304.81840530774, Test Loss: 26915056.0\n",
      "Epoch [55/100], Training Loss: 179437.8892838102, Test Loss: 26544338.0\n",
      "Epoch [56/100], Training Loss: 176681.17881049702, Test Loss: 26190018.0\n",
      "Epoch [57/100], Training Loss: 174023.92891416384, Test Loss: 25847644.0\n",
      "Epoch [58/100], Training Loss: 171432.0076417274, Test Loss: 25510972.0\n",
      "Epoch [59/100], Training Loss: 168869.49878561698, Test Loss: 25177586.0\n",
      "Epoch [60/100], Training Loss: 166342.49537942064, Test Loss: 24850666.0\n",
      "Epoch [61/100], Training Loss: 163889.9850127362, Test Loss: 24538702.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [62/100], Training Loss: 161543.29237012027, Test Loss: 24242008.0\n",
      "Epoch [63/100], Training Loss: 159286.10366684437, Test Loss: 23957414.0\n",
      "Epoch [64/100], Training Loss: 157107.67042829216, Test Loss: 23684222.0\n",
      "Epoch [65/100], Training Loss: 155001.14069071738, Test Loss: 23421036.0\n",
      "Epoch [66/100], Training Loss: 152961.7512291926, Test Loss: 23165372.0\n",
      "Epoch [67/100], Training Loss: 150979.6272732658, Test Loss: 22914574.0\n",
      "Epoch [68/100], Training Loss: 149041.8559919436, Test Loss: 22673058.0\n",
      "Epoch [69/100], Training Loss: 147128.67647058822, Test Loss: 22435378.0\n",
      "Epoch [70/100], Training Loss: 145207.3526746046, Test Loss: 22196394.0\n",
      "Epoch [71/100], Training Loss: 143260.5186007938, Test Loss: 21951582.0\n",
      "Epoch [72/100], Training Loss: 141328.256767964, Test Loss: 21719054.0\n",
      "Epoch [73/100], Training Loss: 139448.16453409157, Test Loss: 21493416.0\n",
      "Epoch [74/100], Training Loss: 137642.8417451573, Test Loss: 21284292.0\n",
      "Epoch [75/100], Training Loss: 135917.5074343937, Test Loss: 21084348.0\n",
      "Epoch [76/100], Training Loss: 134248.64981339968, Test Loss: 20894142.0\n",
      "Epoch [77/100], Training Loss: 132601.51439488181, Test Loss: 20703236.0\n",
      "Epoch [78/100], Training Loss: 130937.23864107576, Test Loss: 20510042.0\n",
      "Epoch [79/100], Training Loss: 129234.94585628813, Test Loss: 20313188.0\n",
      "Epoch [80/100], Training Loss: 127551.44322018838, Test Loss: 20126526.0\n",
      "Epoch [81/100], Training Loss: 125951.96733013447, Test Loss: 19951746.0\n",
      "Epoch [82/100], Training Loss: 124443.67131686512, Test Loss: 19786232.0\n",
      "Epoch [83/100], Training Loss: 123000.25808601386, Test Loss: 19630996.0\n",
      "Epoch [84/100], Training Loss: 121599.7675196967, Test Loss: 19482030.0\n",
      "Epoch [85/100], Training Loss: 120249.26689769563, Test Loss: 19340272.0\n",
      "Epoch [86/100], Training Loss: 118960.055387714, Test Loss: 19211228.0\n",
      "Epoch [87/100], Training Loss: 117724.71059178958, Test Loss: 19086544.0\n",
      "Epoch [88/100], Training Loss: 116540.02209584741, Test Loss: 18972122.0\n",
      "Epoch [89/100], Training Loss: 115395.21894437533, Test Loss: 18857906.0\n",
      "Epoch [90/100], Training Loss: 114282.97938510752, Test Loss: 18751658.0\n",
      "Epoch [91/100], Training Loss: 113201.88644037675, Test Loss: 18645422.0\n",
      "Epoch [92/100], Training Loss: 112145.52711628457, Test Loss: 18546448.0\n",
      "Epoch [93/100], Training Loss: 111110.39424500919, Test Loss: 18448214.0\n",
      "Epoch [94/100], Training Loss: 110093.62964279368, Test Loss: 18353698.0\n",
      "Epoch [95/100], Training Loss: 109087.89104614656, Test Loss: 18259126.0\n",
      "Epoch [96/100], Training Loss: 108091.85808009004, Test Loss: 18167640.0\n",
      "Epoch [97/100], Training Loss: 107112.0092411587, Test Loss: 18076000.0\n",
      "Epoch [98/100], Training Loss: 106164.67937325987, Test Loss: 17993152.0\n",
      "Epoch [99/100], Training Loss: 105239.14348972218, Test Loss: 17907816.0\n",
      "Epoch [100/100], Training Loss: 104325.50558320005, Test Loss: 17827980.0\n",
      "Epoch [1/100], Training Loss: 1497745.3430483977, Test Loss: 298686560.0\n",
      "Epoch [2/100], Training Loss: 1466465.1222084, Test Loss: 284569888.0\n",
      "Epoch [3/100], Training Loss: 1336192.1161068657, Test Loss: 245305104.0\n",
      "Epoch [4/100], Training Loss: 1101558.3247437947, Test Loss: 194892272.0\n",
      "Epoch [5/100], Training Loss: 886460.2386114567, Test Loss: 162668608.0\n",
      "Epoch [6/100], Training Loss: 772506.6879924175, Test Loss: 147881456.0\n",
      "Epoch [7/100], Training Loss: 712624.5672649725, Test Loss: 138517728.0\n",
      "Epoch [8/100], Training Loss: 668177.956045258, Test Loss: 130841704.0\n",
      "Epoch [9/100], Training Loss: 629676.3867069486, Test Loss: 124069016.0\n",
      "Epoch [10/100], Training Loss: 594883.2976719389, Test Loss: 117962096.0\n",
      "Epoch [11/100], Training Loss: 563029.6645933298, Test Loss: 112423912.0\n",
      "Epoch [12/100], Training Loss: 533784.9537349683, Test Loss: 107400320.0\n",
      "Epoch [13/100], Training Loss: 506812.66133522894, Test Loss: 102774256.0\n",
      "Epoch [14/100], Training Loss: 481473.84752088145, Test Loss: 98487968.0\n",
      "Epoch [15/100], Training Loss: 458155.40287897637, Test Loss: 94666568.0\n",
      "Epoch [16/100], Training Loss: 437373.4041822167, Test Loss: 91294112.0\n",
      "Epoch [17/100], Training Loss: 418744.5551803803, Test Loss: 88256032.0\n",
      "Epoch [18/100], Training Loss: 401890.4095728926, Test Loss: 85496688.0\n",
      "Epoch [19/100], Training Loss: 386612.93679284403, Test Loss: 82977968.0\n",
      "Epoch [20/100], Training Loss: 372755.1553817902, Test Loss: 80670456.0\n",
      "Epoch [21/100], Training Loss: 360172.1157514365, Test Loss: 78550536.0\n",
      "Epoch [22/100], Training Loss: 348741.71601208457, Test Loss: 76597456.0\n",
      "Epoch [23/100], Training Loss: 338354.681831645, Test Loss: 74796728.0\n",
      "Epoch [24/100], Training Loss: 328904.7800485753, Test Loss: 73130272.0\n",
      "Epoch [25/100], Training Loss: 320291.3502754576, Test Loss: 71580856.0\n",
      "Epoch [26/100], Training Loss: 312404.8723416859, Test Loss: 70132136.0\n",
      "Epoch [27/100], Training Loss: 305143.6012676974, Test Loss: 68768920.0\n",
      "Epoch [28/100], Training Loss: 298426.9010722114, Test Loss: 67482384.0\n",
      "Epoch [29/100], Training Loss: 292169.04131864227, Test Loss: 66259172.0\n",
      "Epoch [30/100], Training Loss: 286289.5110775428, Test Loss: 65089704.0\n",
      "Epoch [31/100], Training Loss: 280714.6835051241, Test Loss: 63962108.0\n",
      "Epoch [32/100], Training Loss: 275392.98620120256, Test Loss: 62870196.0\n",
      "Epoch [33/100], Training Loss: 270270.15366897325, Test Loss: 61809368.0\n",
      "Epoch [34/100], Training Loss: 265308.4582274682, Test Loss: 60775520.0\n",
      "Epoch [35/100], Training Loss: 260480.11661039037, Test Loss: 59763996.0\n",
      "Epoch [36/100], Training Loss: 255768.88181609503, Test Loss: 58774084.0\n",
      "Epoch [37/100], Training Loss: 251165.41168325336, Test Loss: 57806164.0\n",
      "Epoch [38/100], Training Loss: 246661.7637358569, Test Loss: 56860596.0\n",
      "Epoch [39/100], Training Loss: 242249.95987352644, Test Loss: 55935000.0\n",
      "Epoch [40/100], Training Loss: 237927.015046502, Test Loss: 55030716.0\n",
      "Epoch [41/100], Training Loss: 233696.41811800248, Test Loss: 54148344.0\n",
      "Epoch [42/100], Training Loss: 229568.32425507967, Test Loss: 53286072.0\n",
      "Epoch [43/100], Training Loss: 225538.0396747823, Test Loss: 52443716.0\n",
      "Epoch [44/100], Training Loss: 221600.3625229548, Test Loss: 51621980.0\n",
      "Epoch [45/100], Training Loss: 217751.1910579942, Test Loss: 50820308.0\n",
      "Epoch [46/100], Training Loss: 213991.24885966472, Test Loss: 50039580.0\n",
      "Epoch [47/100], Training Loss: 210320.12888750667, Test Loss: 49278292.0\n",
      "Epoch [48/100], Training Loss: 206737.36794324982, Test Loss: 48534888.0\n",
      "Epoch [49/100], Training Loss: 203242.07970499378, Test Loss: 47809088.0\n",
      "Epoch [50/100], Training Loss: 199835.53902316213, Test Loss: 47098100.0\n",
      "Epoch [51/100], Training Loss: 196511.7738137551, Test Loss: 46404828.0\n",
      "Epoch [52/100], Training Loss: 193271.55957141164, Test Loss: 45731816.0\n",
      "Epoch [53/100], Training Loss: 190113.0502562052, Test Loss: 45075592.0\n",
      "Epoch [54/100], Training Loss: 187035.02216249038, Test Loss: 44433328.0\n",
      "Epoch [55/100], Training Loss: 184041.61804321426, Test Loss: 43807652.0\n",
      "Epoch [56/100], Training Loss: 181133.89625910786, Test Loss: 43198192.0\n",
      "Epoch [57/100], Training Loss: 178310.14845092114, Test Loss: 42605020.0\n",
      "Epoch [58/100], Training Loss: 175573.3740984687, Test Loss: 42027300.0\n",
      "Epoch [59/100], Training Loss: 172916.09285587346, Test Loss: 41463912.0\n",
      "Epoch [60/100], Training Loss: 170335.8520914786, Test Loss: 40913452.0\n",
      "Epoch [61/100], Training Loss: 167829.12270081748, Test Loss: 40374764.0\n",
      "Epoch [62/100], Training Loss: 165387.6713474098, Test Loss: 39846740.0\n",
      "Epoch [63/100], Training Loss: 163010.1013597943, Test Loss: 39329764.0\n",
      "Epoch [64/100], Training Loss: 160693.45186017195, Test Loss: 38825320.0\n",
      "Epoch [65/100], Training Loss: 158433.15730610598, Test Loss: 38334112.0\n",
      "Epoch [66/100], Training Loss: 156226.2408231703, Test Loss: 37852652.0\n",
      "Epoch [67/100], Training Loss: 154070.42478264673, Test Loss: 37382008.0\n",
      "Epoch [68/100], Training Loss: 151964.96190040995, Test Loss: 36926060.0\n",
      "Epoch [69/100], Training Loss: 149913.066190369, Test Loss: 36481280.0\n",
      "Epoch [70/100], Training Loss: 147917.24860479296, Test Loss: 36046380.0\n",
      "Epoch [71/100], Training Loss: 145981.68357363273, Test Loss: 35623764.0\n",
      "Epoch [72/100], Training Loss: 144102.84873655165, Test Loss: 35213608.0\n",
      "Epoch [73/100], Training Loss: 142281.55018308305, Test Loss: 34814696.0\n",
      "Epoch [74/100], Training Loss: 140510.44551665196, Test Loss: 34425808.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [75/100], Training Loss: 138781.6066604559, Test Loss: 34045052.0\n",
      "Epoch [76/100], Training Loss: 137092.44982177633, Test Loss: 33672252.0\n",
      "Epoch [77/100], Training Loss: 135446.35699575336, Test Loss: 33309254.0\n",
      "Epoch [78/100], Training Loss: 133839.42305259057, Test Loss: 32955144.0\n",
      "Epoch [79/100], Training Loss: 132268.07557058416, Test Loss: 32610472.0\n",
      "Epoch [80/100], Training Loss: 130740.34441272733, Test Loss: 32275616.0\n",
      "Epoch [81/100], Training Loss: 129250.33862520178, Test Loss: 31948734.0\n",
      "Epoch [82/100], Training Loss: 127789.19624966678, Test Loss: 31627762.0\n",
      "Epoch [83/100], Training Loss: 126351.00241210533, Test Loss: 31311932.0\n",
      "Epoch [84/100], Training Loss: 124934.88602339539, Test Loss: 31002308.0\n",
      "Epoch [85/100], Training Loss: 123542.51173240255, Test Loss: 30697110.0\n",
      "Epoch [86/100], Training Loss: 122174.37061035706, Test Loss: 30398822.0\n",
      "Epoch [87/100], Training Loss: 120836.32577259566, Test Loss: 30108256.0\n",
      "Epoch [88/100], Training Loss: 119529.25863674397, Test Loss: 29827166.0\n",
      "Epoch [89/100], Training Loss: 118258.18087457793, Test Loss: 29554056.0\n",
      "Epoch [90/100], Training Loss: 117024.70208740595, Test Loss: 29291600.0\n",
      "Epoch [91/100], Training Loss: 115816.92908910166, Test Loss: 29031524.0\n",
      "Epoch [92/100], Training Loss: 114624.55244663008, Test Loss: 28776756.0\n",
      "Epoch [93/100], Training Loss: 113448.2502013173, Test Loss: 28525096.0\n",
      "Epoch [94/100], Training Loss: 112304.01232894971, Test Loss: 28282836.0\n",
      "Epoch [95/100], Training Loss: 111195.52755224069, Test Loss: 28048406.0\n",
      "Epoch [96/100], Training Loss: 110119.07701289542, Test Loss: 27818468.0\n",
      "Epoch [97/100], Training Loss: 109069.9815510041, Test Loss: 27595984.0\n",
      "Epoch [98/100], Training Loss: 108045.10046705615, Test Loss: 27380342.0\n",
      "Epoch [99/100], Training Loss: 107041.00732933846, Test Loss: 27166934.0\n",
      "Epoch [100/100], Training Loss: 106057.27858983472, Test Loss: 26957928.0\n",
      "Epoch [1/100], Training Loss: 1164524.7433208933, Test Loss: 299459136.0\n",
      "Epoch [2/100], Training Loss: 1151811.9125644215, Test Loss: 291711072.0\n",
      "Epoch [3/100], Training Loss: 1091084.2030685386, Test Loss: 266766800.0\n",
      "Epoch [4/100], Training Loss: 958394.9770748179, Test Loss: 224882720.0\n",
      "Epoch [5/100], Training Loss: 788952.0350690125, Test Loss: 183492528.0\n",
      "Epoch [6/100], Training Loss: 658179.9334162668, Test Loss: 158887968.0\n",
      "Epoch [7/100], Training Loss: 589213.3802499851, Test Loss: 146666096.0\n",
      "Epoch [8/100], Training Loss: 550375.1915170902, Test Loss: 138542320.0\n",
      "Epoch [9/100], Training Loss: 520856.10236360406, Test Loss: 131754544.0\n",
      "Epoch [10/100], Training Loss: 494965.3030033766, Test Loss: 125628768.0\n",
      "Epoch [11/100], Training Loss: 471139.4888928381, Test Loss: 119937680.0\n",
      "Epoch [12/100], Training Loss: 448772.2490373793, Test Loss: 114575392.0\n",
      "Epoch [13/100], Training Loss: 427432.30187785084, Test Loss: 109411976.0\n",
      "Epoch [14/100], Training Loss: 406812.4715360464, Test Loss: 104499360.0\n",
      "Epoch [15/100], Training Loss: 387390.9502991529, Test Loss: 99938680.0\n",
      "Epoch [16/100], Training Loss: 369446.0619631538, Test Loss: 95772360.0\n",
      "Epoch [17/100], Training Loss: 352978.7313547776, Test Loss: 91953232.0\n",
      "Epoch [18/100], Training Loss: 337855.89929506544, Test Loss: 88448384.0\n",
      "Epoch [19/100], Training Loss: 323969.0911675849, Test Loss: 85230496.0\n",
      "Epoch [20/100], Training Loss: 311219.35655470646, Test Loss: 82267232.0\n",
      "Epoch [21/100], Training Loss: 299510.98370949586, Test Loss: 79533384.0\n",
      "Epoch [22/100], Training Loss: 288748.4914400806, Test Loss: 77007624.0\n",
      "Epoch [23/100], Training Loss: 278849.7012025354, Test Loss: 74668056.0\n",
      "Epoch [24/100], Training Loss: 269726.2451276583, Test Loss: 72498680.0\n",
      "Epoch [25/100], Training Loss: 261301.19080623187, Test Loss: 70480440.0\n",
      "Epoch [26/100], Training Loss: 253510.74308394053, Test Loss: 68597792.0\n",
      "Epoch [27/100], Training Loss: 246287.81292577455, Test Loss: 66841812.0\n",
      "Epoch [28/100], Training Loss: 239571.23369468632, Test Loss: 65199680.0\n",
      "Epoch [29/100], Training Loss: 233303.70890350098, Test Loss: 63658380.0\n",
      "Epoch [30/100], Training Loss: 227435.51152182926, Test Loss: 62206844.0\n",
      "Epoch [31/100], Training Loss: 221916.94129494697, Test Loss: 60834972.0\n",
      "Epoch [32/100], Training Loss: 216707.09353711273, Test Loss: 59537284.0\n",
      "Epoch [33/100], Training Loss: 211768.09679521355, Test Loss: 58306248.0\n",
      "Epoch [34/100], Training Loss: 207077.40287897637, Test Loss: 57134808.0\n",
      "Epoch [35/100], Training Loss: 202608.0259463302, Test Loss: 56018744.0\n",
      "Epoch [36/100], Training Loss: 198337.86315976543, Test Loss: 54953480.0\n",
      "Epoch [37/100], Training Loss: 194241.53225519817, Test Loss: 53932244.0\n",
      "Epoch [38/100], Training Loss: 190299.38866180912, Test Loss: 52950580.0\n",
      "Epoch [39/100], Training Loss: 186509.58533262246, Test Loss: 52006596.0\n",
      "Epoch [40/100], Training Loss: 182864.0170606007, Test Loss: 51093228.0\n",
      "Epoch [41/100], Training Loss: 179353.2598779693, Test Loss: 50213020.0\n",
      "Epoch [42/100], Training Loss: 175972.22617143535, Test Loss: 49365820.0\n",
      "Epoch [43/100], Training Loss: 172709.50500562764, Test Loss: 48546560.0\n",
      "Epoch [44/100], Training Loss: 169560.5314851016, Test Loss: 47759004.0\n",
      "Epoch [45/100], Training Loss: 166515.0755287009, Test Loss: 46997432.0\n",
      "Epoch [46/100], Training Loss: 163570.015046502, Test Loss: 46258872.0\n",
      "Epoch [47/100], Training Loss: 160725.16900657545, Test Loss: 45542376.0\n",
      "Epoch [48/100], Training Loss: 157971.8620934779, Test Loss: 44845692.0\n",
      "Epoch [49/100], Training Loss: 155300.31052662758, Test Loss: 44168372.0\n",
      "Epoch [50/100], Training Loss: 152704.32142645578, Test Loss: 43512332.0\n",
      "Epoch [51/100], Training Loss: 150181.3268171317, Test Loss: 42873708.0\n",
      "Epoch [52/100], Training Loss: 147729.8709792074, Test Loss: 42250184.0\n",
      "Epoch [53/100], Training Loss: 145347.11912801376, Test Loss: 41642120.0\n",
      "Epoch [54/100], Training Loss: 143030.36875777502, Test Loss: 41048940.0\n",
      "Epoch [55/100], Training Loss: 140777.40187192702, Test Loss: 40470840.0\n",
      "Epoch [56/100], Training Loss: 138589.38617380487, Test Loss: 39907784.0\n",
      "Epoch [57/100], Training Loss: 136466.15994313132, Test Loss: 39360928.0\n",
      "Epoch [58/100], Training Loss: 134409.64617025058, Test Loss: 38830972.0\n",
      "Epoch [59/100], Training Loss: 132418.50121438302, Test Loss: 38316496.0\n",
      "Epoch [60/100], Training Loss: 130489.37770274273, Test Loss: 37817000.0\n",
      "Epoch [61/100], Training Loss: 128617.80735738404, Test Loss: 37332752.0\n",
      "Epoch [62/100], Training Loss: 126802.04768674841, Test Loss: 36860488.0\n",
      "Epoch [63/100], Training Loss: 125037.40667022095, Test Loss: 36401696.0\n",
      "Epoch [64/100], Training Loss: 123318.50257686156, Test Loss: 35953888.0\n",
      "Epoch [65/100], Training Loss: 121642.68011373734, Test Loss: 35517424.0\n",
      "Epoch [66/100], Training Loss: 120006.2600556839, Test Loss: 35090212.0\n",
      "Epoch [67/100], Training Loss: 118409.19347195071, Test Loss: 34672792.0\n",
      "Epoch [68/100], Training Loss: 116848.23482021208, Test Loss: 34265332.0\n",
      "Epoch [69/100], Training Loss: 115322.12517030981, Test Loss: 33867312.0\n",
      "Epoch [70/100], Training Loss: 113832.15763284166, Test Loss: 33478380.0\n",
      "Epoch [71/100], Training Loss: 112377.17084295955, Test Loss: 33099460.0\n",
      "Epoch [72/100], Training Loss: 110956.97032166341, Test Loss: 32730450.0\n",
      "Epoch [73/100], Training Loss: 109572.08186718797, Test Loss: 32369556.0\n",
      "Epoch [74/100], Training Loss: 108217.42533025295, Test Loss: 32015870.0\n",
      "Epoch [75/100], Training Loss: 106892.38528523191, Test Loss: 31669814.0\n",
      "Epoch [76/100], Training Loss: 105595.24115869912, Test Loss: 31330824.0\n",
      "Epoch [77/100], Training Loss: 104323.96392393815, Test Loss: 30998570.0\n",
      "Epoch [78/100], Training Loss: 103075.47118061726, Test Loss: 30671016.0\n",
      "Epoch [79/100], Training Loss: 101852.02185889462, Test Loss: 30350974.0\n",
      "Epoch [80/100], Training Loss: 100658.5645400154, Test Loss: 30038804.0\n",
      "Epoch [81/100], Training Loss: 99493.3892541911, Test Loss: 29735428.0\n",
      "Epoch [82/100], Training Loss: 98357.75445767431, Test Loss: 29440662.0\n",
      "Epoch [83/100], Training Loss: 97250.08559919435, Test Loss: 29151876.0\n",
      "Epoch [84/100], Training Loss: 96166.25602748652, Test Loss: 28870308.0\n",
      "Epoch [85/100], Training Loss: 95106.58183756887, Test Loss: 28595390.0\n",
      "Epoch [86/100], Training Loss: 94069.59972750429, Test Loss: 28326638.0\n",
      "Epoch [87/100], Training Loss: 93053.70351282507, Test Loss: 28061562.0\n",
      "Epoch [88/100], Training Loss: 92062.21989218648, Test Loss: 27802844.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [89/100], Training Loss: 91094.02825661987, Test Loss: 27550824.0\n",
      "Epoch [90/100], Training Loss: 90144.90148687873, Test Loss: 27304598.0\n",
      "Epoch [91/100], Training Loss: 89215.4465375274, Test Loss: 27064524.0\n",
      "Epoch [92/100], Training Loss: 88306.51744564895, Test Loss: 26829714.0\n",
      "Epoch [93/100], Training Loss: 87415.78822344648, Test Loss: 26598396.0\n",
      "Epoch [94/100], Training Loss: 86539.77738285647, Test Loss: 26371236.0\n",
      "Epoch [95/100], Training Loss: 85679.45725964101, Test Loss: 26149964.0\n",
      "Epoch [96/100], Training Loss: 84835.42301996327, Test Loss: 25933866.0\n",
      "Epoch [97/100], Training Loss: 84007.93856998993, Test Loss: 25722352.0\n",
      "Epoch [98/100], Training Loss: 83194.41662223802, Test Loss: 25514312.0\n",
      "Epoch [99/100], Training Loss: 82393.00426515017, Test Loss: 25310150.0\n",
      "Epoch [100/100], Training Loss: 81605.95320182454, Test Loss: 25110858.0\n",
      "Epoch [1/100], Training Loss: 582416.5004442865, Test Loss: 300021184.0\n",
      "Epoch [2/100], Training Loss: 581801.221254665, Test Loss: 299274688.0\n",
      "Epoch [3/100], Training Loss: 578671.4019311653, Test Loss: 296458304.0\n",
      "Epoch [4/100], Training Loss: 569828.8821752266, Test Loss: 289788608.0\n",
      "Epoch [5/100], Training Loss: 551979.0202002251, Test Loss: 277798592.0\n",
      "Epoch [6/100], Training Loss: 523101.8020259463, Test Loss: 260019904.0\n",
      "Epoch [7/100], Training Loss: 483709.02292518213, Test Loss: 237553520.0\n",
      "Epoch [8/100], Training Loss: 437596.1973816717, Test Loss: 213182688.0\n",
      "Epoch [9/100], Training Loss: 391253.2475564244, Test Loss: 190593008.0\n",
      "Epoch [10/100], Training Loss: 351433.3264617025, Test Loss: 172710976.0\n",
      "Epoch [11/100], Training Loss: 321773.25561281916, Test Loss: 160188640.0\n",
      "Epoch [12/100], Training Loss: 301396.26515016885, Test Loss: 151681984.0\n",
      "Epoch [13/100], Training Loss: 287078.8417747764, Test Loss: 145475808.0\n",
      "Epoch [14/100], Training Loss: 276012.99686037557, Test Loss: 140445424.0\n",
      "Epoch [15/100], Training Loss: 266616.7378709792, Test Loss: 136037984.0\n",
      "Epoch [16/100], Training Loss: 258155.61921687104, Test Loss: 132005992.0\n",
      "Epoch [17/100], Training Loss: 250294.90290859545, Test Loss: 128231784.0\n",
      "Epoch [18/100], Training Loss: 242865.79941946568, Test Loss: 124652264.0\n",
      "Epoch [19/100], Training Loss: 235772.78834192286, Test Loss: 121230328.0\n",
      "Epoch [20/100], Training Loss: 228958.42094662637, Test Loss: 117943408.0\n",
      "Epoch [21/100], Training Loss: 222388.05141875483, Test Loss: 114778024.0\n",
      "Epoch [22/100], Training Loss: 216041.62976127007, Test Loss: 111726624.0\n",
      "Epoch [23/100], Training Loss: 209908.64285291155, Test Loss: 108785576.0\n",
      "Epoch [24/100], Training Loss: 203985.82192998045, Test Loss: 105954184.0\n",
      "Epoch [25/100], Training Loss: 198274.12238611456, Test Loss: 103232736.0\n",
      "Epoch [26/100], Training Loss: 192775.04721284285, Test Loss: 100621408.0\n",
      "Epoch [27/100], Training Loss: 187489.8117410106, Test Loss: 98119592.0\n",
      "Epoch [28/100], Training Loss: 182419.7635211184, Test Loss: 95726216.0\n",
      "Epoch [29/100], Training Loss: 177566.48942598188, Test Loss: 93441264.0\n",
      "Epoch [30/100], Training Loss: 172930.58752443574, Test Loss: 91263560.0\n",
      "Epoch [31/100], Training Loss: 168510.90741069842, Test Loss: 89191152.0\n",
      "Epoch [32/100], Training Loss: 164304.65612226762, Test Loss: 87221288.0\n",
      "Epoch [33/100], Training Loss: 160306.9664119424, Test Loss: 85349920.0\n",
      "Epoch [34/100], Training Loss: 156510.5664356377, Test Loss: 83572152.0\n",
      "Epoch [35/100], Training Loss: 152906.56477696818, Test Loss: 81882344.0\n",
      "Epoch [36/100], Training Loss: 149485.11723239144, Test Loss: 80274640.0\n",
      "Epoch [37/100], Training Loss: 146235.03252177004, Test Loss: 78742768.0\n",
      "Epoch [38/100], Training Loss: 143144.46845566022, Test Loss: 77280936.0\n",
      "Epoch [39/100], Training Loss: 140201.83235590308, Test Loss: 75883816.0\n",
      "Epoch [40/100], Training Loss: 137395.93602274745, Test Loss: 74545928.0\n",
      "Epoch [41/100], Training Loss: 134716.64285291155, Test Loss: 73262320.0\n",
      "Epoch [42/100], Training Loss: 132154.40649250636, Test Loss: 72029072.0\n",
      "Epoch [43/100], Training Loss: 129700.726971151, Test Loss: 70842672.0\n",
      "Epoch [44/100], Training Loss: 127348.00781944198, Test Loss: 69700384.0\n",
      "Epoch [45/100], Training Loss: 125089.29802736804, Test Loss: 68599376.0\n",
      "Epoch [46/100], Training Loss: 122918.11527753095, Test Loss: 67537112.0\n",
      "Epoch [47/100], Training Loss: 120829.11746934423, Test Loss: 66511108.0\n",
      "Epoch [48/100], Training Loss: 118816.39097209882, Test Loss: 65519348.0\n",
      "Epoch [49/100], Training Loss: 116875.15929151117, Test Loss: 64560220.0\n",
      "Epoch [50/100], Training Loss: 115000.94804810142, Test Loss: 63631504.0\n",
      "Epoch [51/100], Training Loss: 113189.49588294532, Test Loss: 62732204.0\n",
      "Epoch [52/100], Training Loss: 111438.14335643622, Test Loss: 61861240.0\n",
      "Epoch [53/100], Training Loss: 109742.55127065931, Test Loss: 61017044.0\n",
      "Epoch [54/100], Training Loss: 108098.10283750962, Test Loss: 60197824.0\n",
      "Epoch [55/100], Training Loss: 106501.68212783603, Test Loss: 59401876.0\n",
      "Epoch [56/100], Training Loss: 104950.07570641549, Test Loss: 58627496.0\n",
      "Epoch [57/100], Training Loss: 103440.54830874948, Test Loss: 57873124.0\n",
      "Epoch [58/100], Training Loss: 101970.48433149695, Test Loss: 57137728.0\n",
      "Epoch [59/100], Training Loss: 100538.37474083289, Test Loss: 56420544.0\n",
      "Epoch [60/100], Training Loss: 99140.74580889758, Test Loss: 55720220.0\n",
      "Epoch [61/100], Training Loss: 97774.72673419821, Test Loss: 55035560.0\n",
      "Epoch [62/100], Training Loss: 96438.45684497364, Test Loss: 54365752.0\n",
      "Epoch [63/100], Training Loss: 95130.68941413423, Test Loss: 53710128.0\n",
      "Epoch [64/100], Training Loss: 93849.76292873645, Test Loss: 53067344.0\n",
      "Epoch [65/100], Training Loss: 92595.05266275695, Test Loss: 52436720.0\n",
      "Epoch [66/100], Training Loss: 91365.81257034536, Test Loss: 51817976.0\n",
      "Epoch [67/100], Training Loss: 90161.02114803625, Test Loss: 51210764.0\n",
      "Epoch [68/100], Training Loss: 88979.48012558497, Test Loss: 50615276.0\n",
      "Epoch [69/100], Training Loss: 87820.0755879391, Test Loss: 50030252.0\n",
      "Epoch [70/100], Training Loss: 86682.65268645223, Test Loss: 49455316.0\n",
      "Epoch [71/100], Training Loss: 85566.60411113086, Test Loss: 48889968.0\n",
      "Epoch [72/100], Training Loss: 84471.46733013447, Test Loss: 48334220.0\n",
      "Epoch [73/100], Training Loss: 83397.18950299153, Test Loss: 47788076.0\n",
      "Epoch [74/100], Training Loss: 82343.19045080268, Test Loss: 47251424.0\n",
      "Epoch [75/100], Training Loss: 81309.0295598602, Test Loss: 46724036.0\n",
      "Epoch [76/100], Training Loss: 80294.27604999703, Test Loss: 46205080.0\n",
      "Epoch [77/100], Training Loss: 79298.25756767964, Test Loss: 45694852.0\n",
      "Epoch [78/100], Training Loss: 78320.8580060423, Test Loss: 45193140.0\n",
      "Epoch [79/100], Training Loss: 77361.46898880399, Test Loss: 44699724.0\n",
      "Epoch [80/100], Training Loss: 76419.74942242759, Test Loss: 44214200.0\n",
      "Epoch [81/100], Training Loss: 75494.9927137018, Test Loss: 43736088.0\n",
      "Epoch [82/100], Training Loss: 74586.71998104377, Test Loss: 43265548.0\n",
      "Epoch [83/100], Training Loss: 73695.00977430247, Test Loss: 42802520.0\n",
      "Epoch [84/100], Training Loss: 72819.35169717434, Test Loss: 42346560.0\n",
      "Epoch [85/100], Training Loss: 71959.53960073456, Test Loss: 41897500.0\n",
      "Epoch [86/100], Training Loss: 71115.23641964339, Test Loss: 41455808.0\n",
      "Epoch [87/100], Training Loss: 70286.12617735915, Test Loss: 41021056.0\n",
      "Epoch [88/100], Training Loss: 69472.09857235946, Test Loss: 40592896.0\n",
      "Epoch [89/100], Training Loss: 68672.86084947574, Test Loss: 40171512.0\n",
      "Epoch [90/100], Training Loss: 67887.66696285765, Test Loss: 39756592.0\n",
      "Epoch [91/100], Training Loss: 67115.87014987264, Test Loss: 39348264.0\n",
      "Epoch [92/100], Training Loss: 66357.24092174634, Test Loss: 38946524.0\n",
      "Epoch [93/100], Training Loss: 65612.17309401101, Test Loss: 38551312.0\n",
      "Epoch [94/100], Training Loss: 64880.337183816126, Test Loss: 38162656.0\n",
      "Epoch [95/100], Training Loss: 64161.16166103904, Test Loss: 37780484.0\n",
      "Epoch [96/100], Training Loss: 63454.12866536342, Test Loss: 37404396.0\n",
      "Epoch [97/100], Training Loss: 62759.12611812096, Test Loss: 37034624.0\n",
      "Epoch [98/100], Training Loss: 62075.674071441266, Test Loss: 36671056.0\n",
      "Epoch [99/100], Training Loss: 61403.98246549375, Test Loss: 36313316.0\n",
      "Epoch [100/100], Training Loss: 60744.18802203661, Test Loss: 35961520.0\n",
      "Epoch [1/100], Training Loss: 7124564.823944079, Test Loss: 148054640.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Training Loss: 4052582.243824418, Test Loss: 116989536.0\n",
      "Epoch [3/100], Training Loss: 3237903.0569279073, Test Loss: 95537248.0\n",
      "Epoch [4/100], Training Loss: 2646429.929625022, Test Loss: 79942352.0\n",
      "Epoch [5/100], Training Loss: 2210835.212724365, Test Loss: 68422000.0\n",
      "Epoch [6/100], Training Loss: 1896762.409395178, Test Loss: 60077392.0\n",
      "Epoch [7/100], Training Loss: 1671679.8207452164, Test Loss: 54013096.0\n",
      "Epoch [8/100], Training Loss: 1505035.4403767549, Test Loss: 49356772.0\n",
      "Epoch [9/100], Training Loss: 1372615.5804454712, Test Loss: 45517692.0\n",
      "Epoch [10/100], Training Loss: 1261139.0391860672, Test Loss: 42191528.0\n",
      "Epoch [11/100], Training Loss: 1164243.7124281737, Test Loss: 39230268.0\n",
      "Epoch [12/100], Training Loss: 1079091.4268112078, Test Loss: 36571572.0\n",
      "Epoch [13/100], Training Loss: 1004150.9873970737, Test Loss: 34190160.0\n",
      "Epoch [14/100], Training Loss: 938030.4952461347, Test Loss: 32080132.0\n",
      "Epoch [15/100], Training Loss: 879775.6042147977, Test Loss: 30216118.0\n",
      "Epoch [16/100], Training Loss: 828350.8675137729, Test Loss: 28568584.0\n",
      "Epoch [17/100], Training Loss: 783163.0816598543, Test Loss: 27111316.0\n",
      "Epoch [18/100], Training Loss: 743408.6699988153, Test Loss: 25821220.0\n",
      "Epoch [19/100], Training Loss: 708268.3143326817, Test Loss: 24679390.0\n",
      "Epoch [20/100], Training Loss: 676864.7612552574, Test Loss: 23664130.0\n",
      "Epoch [21/100], Training Loss: 648433.5826965227, Test Loss: 22752982.0\n",
      "Epoch [22/100], Training Loss: 622568.8503865292, Test Loss: 21937574.0\n",
      "Epoch [23/100], Training Loss: 598943.2898080682, Test Loss: 21210554.0\n",
      "Epoch [24/100], Training Loss: 577245.4984301878, Test Loss: 20559140.0\n",
      "Epoch [25/100], Training Loss: 557281.0552544281, Test Loss: 19979084.0\n",
      "Epoch [26/100], Training Loss: 538923.0441990995, Test Loss: 19461544.0\n",
      "Epoch [27/100], Training Loss: 521946.9707807594, Test Loss: 18993452.0\n",
      "Epoch [28/100], Training Loss: 506251.67898080684, Test Loss: 18577274.0\n",
      "Epoch [29/100], Training Loss: 491680.44890705525, Test Loss: 18192886.0\n",
      "Epoch [30/100], Training Loss: 478147.0963953557, Test Loss: 17844052.0\n",
      "Epoch [31/100], Training Loss: 465564.102430247, Test Loss: 17521272.0\n",
      "Epoch [32/100], Training Loss: 453845.74783040106, Test Loss: 17229518.0\n",
      "Epoch [33/100], Training Loss: 442914.9599697885, Test Loss: 16964116.0\n",
      "Epoch [34/100], Training Loss: 432695.9680335881, Test Loss: 16729954.0\n",
      "Epoch [35/100], Training Loss: 423121.5083673953, Test Loss: 16513516.0\n",
      "Epoch [36/100], Training Loss: 414193.7483931639, Test Loss: 16325891.0\n",
      "Epoch [37/100], Training Loss: 405880.1033114152, Test Loss: 16143991.0\n",
      "Epoch [38/100], Training Loss: 398081.0144615248, Test Loss: 16003236.0\n",
      "Epoch [39/100], Training Loss: 390739.73830045614, Test Loss: 15860080.0\n",
      "Epoch [40/100], Training Loss: 383795.95713375986, Test Loss: 15736411.0\n",
      "Epoch [41/100], Training Loss: 377225.0234435164, Test Loss: 15629186.0\n",
      "Epoch [42/100], Training Loss: 371037.4587554055, Test Loss: 15526489.0\n",
      "Epoch [43/100], Training Loss: 365178.67710739886, Test Loss: 15438892.0\n",
      "Epoch [44/100], Training Loss: 359659.3132886085, Test Loss: 15355639.0\n",
      "Epoch [45/100], Training Loss: 354442.0557431432, Test Loss: 15278249.0\n",
      "Epoch [46/100], Training Loss: 349491.59657307033, Test Loss: 15213551.0\n",
      "Epoch [47/100], Training Loss: 344832.9533943487, Test Loss: 15137137.0\n",
      "Epoch [48/100], Training Loss: 340465.1226971151, Test Loss: 15057484.0\n",
      "Epoch [49/100], Training Loss: 336343.49560896866, Test Loss: 15009483.0\n",
      "Epoch [50/100], Training Loss: 332472.84420354245, Test Loss: 14910252.0\n",
      "Epoch [51/100], Training Loss: 328794.58734672115, Test Loss: 14894040.0\n",
      "Epoch [52/100], Training Loss: 325342.444019904, Test Loss: 14810439.0\n",
      "Epoch [53/100], Training Loss: 322013.7915407855, Test Loss: 14784046.0\n",
      "Epoch [54/100], Training Loss: 318890.3476911913, Test Loss: 14704274.0\n",
      "Epoch [55/100], Training Loss: 315898.388469285, Test Loss: 14686687.0\n",
      "Epoch [56/100], Training Loss: 313100.00374681596, Test Loss: 14614274.0\n",
      "Epoch [57/100], Training Loss: 310403.88472617144, Test Loss: 14608974.0\n",
      "Epoch [58/100], Training Loss: 307848.181553966, Test Loss: 14535444.0\n",
      "Epoch [59/100], Training Loss: 305421.3589168296, Test Loss: 14530530.0\n",
      "Epoch [60/100], Training Loss: 303089.847831882, Test Loss: 14470662.0\n",
      "Epoch [61/100], Training Loss: 300877.2879013388, Test Loss: 14446210.0\n",
      "Epoch [62/100], Training Loss: 298737.8502199218, Test Loss: 14468272.0\n",
      "Epoch [63/100], Training Loss: 296750.1278767549, Test Loss: 14394629.0\n",
      "Epoch [64/100], Training Loss: 294782.53743854037, Test Loss: 14403253.0\n",
      "Epoch [65/100], Training Loss: 292908.50449840055, Test Loss: 14354851.0\n",
      "Epoch [66/100], Training Loss: 291116.3873548664, Test Loss: 14353859.0\n",
      "Epoch [67/100], Training Loss: 289413.38693649665, Test Loss: 14322198.0\n",
      "Epoch [68/100], Training Loss: 287718.7906337006, Test Loss: 14301799.0\n",
      "Epoch [69/100], Training Loss: 286152.56058956817, Test Loss: 14387015.0\n",
      "Epoch [70/100], Training Loss: 284571.13617750723, Test Loss: 14359744.0\n",
      "Epoch [71/100], Training Loss: 282985.4294473076, Test Loss: 14386409.0\n",
      "Epoch [72/100], Training Loss: 281431.97751540196, Test Loss: 14354381.0\n",
      "Epoch [73/100], Training Loss: 279980.5362463717, Test Loss: 14404321.0\n",
      "Epoch [74/100], Training Loss: 278499.01244372374, Test Loss: 14376478.0\n",
      "Epoch [75/100], Training Loss: 277032.10356687993, Test Loss: 14395083.0\n",
      "Epoch [76/100], Training Loss: 275625.5586606244, Test Loss: 14432696.0\n",
      "Epoch [77/100], Training Loss: 274261.302299923, Test Loss: 14378213.0\n",
      "Epoch [78/100], Training Loss: 272887.97843729635, Test Loss: 14424553.0\n",
      "Epoch [79/100], Training Loss: 271642.69127643504, Test Loss: 14420414.0\n",
      "Epoch [80/100], Training Loss: 270376.010562911, Test Loss: 14412179.0\n",
      "Epoch [81/100], Training Loss: 269149.56708725786, Test Loss: 14454618.0\n",
      "Epoch [82/100], Training Loss: 267964.1298834488, Test Loss: 14431752.0\n",
      "Epoch [83/100], Training Loss: 266774.2699188437, Test Loss: 14424676.0\n",
      "Epoch [84/100], Training Loss: 265627.4152634619, Test Loss: 14485849.0\n",
      "Epoch [85/100], Training Loss: 264568.3288904686, Test Loss: 14441700.0\n",
      "Epoch [86/100], Training Loss: 263464.71996623423, Test Loss: 14470653.0\n",
      "Epoch [87/100], Training Loss: 262399.3772991825, Test Loss: 14473150.0\n",
      "Epoch [88/100], Training Loss: 261282.99006649488, Test Loss: 14449130.0\n",
      "Epoch [89/100], Training Loss: 260285.84527723477, Test Loss: 14505253.0\n",
      "Epoch [90/100], Training Loss: 259277.4667340501, Test Loss: 14464632.0\n",
      "Epoch [91/100], Training Loss: 258269.68000266573, Test Loss: 14468557.0\n",
      "Epoch [92/100], Training Loss: 257268.53452476158, Test Loss: 14506913.0\n",
      "Epoch [93/100], Training Loss: 256345.06181876073, Test Loss: 14455689.0\n",
      "Epoch [94/100], Training Loss: 255385.67311252296, Test Loss: 14504916.0\n",
      "Epoch [95/100], Training Loss: 254510.59472928144, Test Loss: 14499590.0\n",
      "Epoch [96/100], Training Loss: 253577.55578386944, Test Loss: 14454889.0\n",
      "Epoch [97/100], Training Loss: 252691.99401323975, Test Loss: 14502374.0\n",
      "Epoch [98/100], Training Loss: 251775.7946248741, Test Loss: 14494429.0\n",
      "Epoch [99/100], Training Loss: 250948.98766364553, Test Loss: 14475941.0\n",
      "Epoch [100/100], Training Loss: 250066.78082237428, Test Loss: 14531800.0\n",
      "Epoch [1/100], Training Loss: 4444764.94449381, Test Loss: 243339888.0\n",
      "Epoch [2/100], Training Loss: 2824573.7844914403, Test Loss: 149121808.0\n",
      "Epoch [3/100], Training Loss: 2139833.816006161, Test Loss: 129500320.0\n",
      "Epoch [4/100], Training Loss: 1871712.3339849536, Test Loss: 114441184.0\n",
      "Epoch [5/100], Training Loss: 1648871.9989337125, Test Loss: 101706728.0\n",
      "Epoch [6/100], Training Loss: 1460708.5864581482, Test Loss: 91140520.0\n",
      "Epoch [7/100], Training Loss: 1303589.2508737633, Test Loss: 82343192.0\n",
      "Epoch [8/100], Training Loss: 1172386.5476571294, Test Loss: 74958376.0\n",
      "Epoch [9/100], Training Loss: 1063358.8236478881, Test Loss: 68785776.0\n",
      "Epoch [10/100], Training Loss: 973523.5009774303, Test Loss: 63669144.0\n",
      "Epoch [11/100], Training Loss: 899944.5408447367, Test Loss: 59459712.0\n",
      "Epoch [12/100], Training Loss: 839411.8533854629, Test Loss: 55950660.0\n",
      "Epoch [13/100], Training Loss: 788521.8637521474, Test Loss: 52944692.0\n",
      "Epoch [14/100], Training Loss: 744297.5095077306, Test Loss: 50290204.0\n",
      "Epoch [15/100], Training Loss: 704882.6654819028, Test Loss: 47899388.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/100], Training Loss: 669130.6056513239, Test Loss: 45711112.0\n",
      "Epoch [17/100], Training Loss: 636347.1434749126, Test Loss: 43687320.0\n",
      "Epoch [18/100], Training Loss: 606080.8263432261, Test Loss: 41796708.0\n",
      "Epoch [19/100], Training Loss: 578000.2948581246, Test Loss: 40023344.0\n",
      "Epoch [20/100], Training Loss: 551900.0953734969, Test Loss: 38358924.0\n",
      "Epoch [21/100], Training Loss: 527670.9262780641, Test Loss: 36797888.0\n",
      "Epoch [22/100], Training Loss: 505214.61581067473, Test Loss: 35336640.0\n",
      "Epoch [23/100], Training Loss: 484364.9265150169, Test Loss: 33974204.0\n",
      "Epoch [24/100], Training Loss: 465028.93821456074, Test Loss: 32707362.0\n",
      "Epoch [25/100], Training Loss: 447109.9384811326, Test Loss: 31525954.0\n",
      "Epoch [26/100], Training Loss: 430482.7292518216, Test Loss: 30427464.0\n",
      "Epoch [27/100], Training Loss: 415051.007523251, Test Loss: 29407370.0\n",
      "Epoch [28/100], Training Loss: 400740.73529411765, Test Loss: 28461088.0\n",
      "Epoch [29/100], Training Loss: 387467.8714679225, Test Loss: 27579822.0\n",
      "Epoch [30/100], Training Loss: 375155.85405189265, Test Loss: 26760930.0\n",
      "Epoch [31/100], Training Loss: 363740.0926337302, Test Loss: 25998882.0\n",
      "Epoch [32/100], Training Loss: 353115.0853030034, Test Loss: 25290344.0\n",
      "Epoch [33/100], Training Loss: 343181.525131805, Test Loss: 24628448.0\n",
      "Epoch [34/100], Training Loss: 333827.0515372312, Test Loss: 24007076.0\n",
      "Epoch [35/100], Training Loss: 324994.68790356023, Test Loss: 23424908.0\n",
      "Epoch [36/100], Training Loss: 316640.19511581067, Test Loss: 22878902.0\n",
      "Epoch [37/100], Training Loss: 308720.01027782715, Test Loss: 22367474.0\n",
      "Epoch [38/100], Training Loss: 301211.99001836387, Test Loss: 21887358.0\n",
      "Epoch [39/100], Training Loss: 294077.62548131036, Test Loss: 21439868.0\n",
      "Epoch [40/100], Training Loss: 287297.74071441265, Test Loss: 21019918.0\n",
      "Epoch [41/100], Training Loss: 280859.2396777442, Test Loss: 20627406.0\n",
      "Epoch [42/100], Training Loss: 274736.6809430721, Test Loss: 20259542.0\n",
      "Epoch [43/100], Training Loss: 268914.67867721105, Test Loss: 19918452.0\n",
      "Epoch [44/100], Training Loss: 263364.0786979444, Test Loss: 19592348.0\n",
      "Epoch [45/100], Training Loss: 258068.13679580594, Test Loss: 19287762.0\n",
      "Epoch [46/100], Training Loss: 253007.23593092826, Test Loss: 19001102.0\n",
      "Epoch [47/100], Training Loss: 248170.57730584682, Test Loss: 18728948.0\n",
      "Epoch [48/100], Training Loss: 243542.94199099578, Test Loss: 18472042.0\n",
      "Epoch [49/100], Training Loss: 239108.92398258398, Test Loss: 18230258.0\n",
      "Epoch [50/100], Training Loss: 234860.36353000414, Test Loss: 18000872.0\n",
      "Epoch [51/100], Training Loss: 230791.4667377525, Test Loss: 17787962.0\n",
      "Epoch [52/100], Training Loss: 226884.99691961377, Test Loss: 17584108.0\n",
      "Epoch [53/100], Training Loss: 223147.82894970677, Test Loss: 17392008.0\n",
      "Epoch [54/100], Training Loss: 219559.90862508144, Test Loss: 17212946.0\n",
      "Epoch [55/100], Training Loss: 216123.2127687933, Test Loss: 17041852.0\n",
      "Epoch [56/100], Training Loss: 212821.68005449913, Test Loss: 16884138.0\n",
      "Epoch [57/100], Training Loss: 209672.4997630472, Test Loss: 16731699.0\n",
      "Epoch [58/100], Training Loss: 206632.75297671938, Test Loss: 16591507.0\n",
      "Epoch [59/100], Training Loss: 203733.97386114567, Test Loss: 16459880.0\n",
      "Epoch [60/100], Training Loss: 200956.2415141283, Test Loss: 16337720.0\n",
      "Epoch [61/100], Training Loss: 198291.0988537409, Test Loss: 16218396.0\n",
      "Epoch [62/100], Training Loss: 195721.27791600022, Test Loss: 16120852.0\n",
      "Epoch [63/100], Training Loss: 193252.19243528228, Test Loss: 16010477.0\n",
      "Epoch [64/100], Training Loss: 190865.23258397015, Test Loss: 15925894.0\n",
      "Epoch [65/100], Training Loss: 188574.0499970381, Test Loss: 15834829.0\n",
      "Epoch [66/100], Training Loss: 186366.03826787512, Test Loss: 15761243.0\n",
      "Epoch [67/100], Training Loss: 184242.0551803803, Test Loss: 15684695.0\n",
      "Epoch [68/100], Training Loss: 182186.1733605829, Test Loss: 15622018.0\n",
      "Epoch [69/100], Training Loss: 180210.22486819502, Test Loss: 15552870.0\n",
      "Epoch [70/100], Training Loss: 178292.74961495172, Test Loss: 15502705.0\n",
      "Epoch [71/100], Training Loss: 176458.96208755404, Test Loss: 15440432.0\n",
      "Epoch [72/100], Training Loss: 174671.06636158994, Test Loss: 15398740.0\n",
      "Epoch [73/100], Training Loss: 172967.8173982584, Test Loss: 15341147.0\n",
      "Epoch [74/100], Training Loss: 171302.8445959955, Test Loss: 15302316.0\n",
      "Epoch [75/100], Training Loss: 169713.82808334814, Test Loss: 15252705.0\n",
      "Epoch [76/100], Training Loss: 168167.44071737456, Test Loss: 15218144.0\n",
      "Epoch [77/100], Training Loss: 166692.63010929446, Test Loss: 15169065.0\n",
      "Epoch [78/100], Training Loss: 165262.74674189917, Test Loss: 15134882.0\n",
      "Epoch [79/100], Training Loss: 163891.65622593448, Test Loss: 15081495.0\n",
      "Epoch [80/100], Training Loss: 162559.05874948166, Test Loss: 15052135.0\n",
      "Epoch [81/100], Training Loss: 161296.98963331556, Test Loss: 15004447.0\n",
      "Epoch [82/100], Training Loss: 160058.76843788876, Test Loss: 14992748.0\n",
      "Epoch [83/100], Training Loss: 158880.98242846987, Test Loss: 14921257.0\n",
      "Epoch [84/100], Training Loss: 157722.88187163082, Test Loss: 14930887.0\n",
      "Epoch [85/100], Training Loss: 156631.4392512292, Test Loss: 14868453.0\n",
      "Epoch [86/100], Training Loss: 155563.8024183994, Test Loss: 14865613.0\n",
      "Epoch [87/100], Training Loss: 154549.89588886916, Test Loss: 14813606.0\n",
      "Epoch [88/100], Training Loss: 153554.0770985131, Test Loss: 14804611.0\n",
      "Epoch [89/100], Training Loss: 152596.13798056988, Test Loss: 14762460.0\n",
      "Epoch [90/100], Training Loss: 151656.2884233754, Test Loss: 14751838.0\n",
      "Epoch [91/100], Training Loss: 150747.57073781174, Test Loss: 14717764.0\n",
      "Epoch [92/100], Training Loss: 149873.05263313785, Test Loss: 14703480.0\n",
      "Epoch [93/100], Training Loss: 149031.3463435223, Test Loss: 14678022.0\n",
      "Epoch [94/100], Training Loss: 148205.22502369527, Test Loss: 14646788.0\n",
      "Epoch [95/100], Training Loss: 147409.08767993603, Test Loss: 14645739.0\n",
      "Epoch [96/100], Training Loss: 146615.91764409692, Test Loss: 14636494.0\n",
      "Epoch [97/100], Training Loss: 145872.67367158344, Test Loss: 14579831.0\n",
      "Epoch [98/100], Training Loss: 145127.0360834666, Test Loss: 14605296.0\n",
      "Epoch [99/100], Training Loss: 144411.70209110834, Test Loss: 14573722.0\n",
      "Epoch [100/100], Training Loss: 143695.99815621114, Test Loss: 14570146.0\n",
      "Epoch [1/100], Training Loss: 2316963.5230140393, Test Loss: 291602944.0\n",
      "Epoch [2/100], Training Loss: 2046896.2563829157, Test Loss: 224331248.0\n",
      "Epoch [3/100], Training Loss: 1446632.6385877614, Test Loss: 159608512.0\n",
      "Epoch [4/100], Training Loss: 1152484.2544872933, Test Loss: 140797200.0\n",
      "Epoch [5/100], Training Loss: 1039561.0961435934, Test Loss: 128982160.0\n",
      "Epoch [6/100], Training Loss: 952069.4757419584, Test Loss: 118803768.0\n",
      "Epoch [7/100], Training Loss: 874221.6124637167, Test Loss: 109651560.0\n",
      "Epoch [8/100], Training Loss: 803901.8484686926, Test Loss: 101442024.0\n",
      "Epoch [9/100], Training Loss: 739856.7653575025, Test Loss: 93946168.0\n",
      "Epoch [10/100], Training Loss: 682612.5657247794, Test Loss: 87534216.0\n",
      "Epoch [11/100], Training Loss: 633764.8824121794, Test Loss: 82013088.0\n",
      "Epoch [12/100], Training Loss: 591536.1970262425, Test Loss: 77194728.0\n",
      "Epoch [13/100], Training Loss: 554947.3176944493, Test Loss: 72985120.0\n",
      "Epoch [14/100], Training Loss: 523206.8450921154, Test Loss: 69291312.0\n",
      "Epoch [15/100], Training Loss: 495594.5555358095, Test Loss: 66039624.0\n",
      "Epoch [16/100], Training Loss: 471463.6926722351, Test Loss: 63177180.0\n",
      "Epoch [17/100], Training Loss: 450239.1858302233, Test Loss: 60632648.0\n",
      "Epoch [18/100], Training Loss: 431361.6843788875, Test Loss: 58353776.0\n",
      "Epoch [19/100], Training Loss: 414399.6379361412, Test Loss: 56295580.0\n",
      "Epoch [20/100], Training Loss: 399011.6552336947, Test Loss: 54417376.0\n",
      "Epoch [21/100], Training Loss: 384869.6753154434, Test Loss: 52677296.0\n",
      "Epoch [22/100], Training Loss: 371720.4487293407, Test Loss: 51057984.0\n",
      "Epoch [23/100], Training Loss: 359448.9914104615, Test Loss: 49540028.0\n",
      "Epoch [24/100], Training Loss: 347918.530715005, Test Loss: 48111024.0\n",
      "Epoch [25/100], Training Loss: 337051.7548723417, Test Loss: 46761628.0\n",
      "Epoch [26/100], Training Loss: 326783.49854866415, Test Loss: 45481256.0\n",
      "Epoch [27/100], Training Loss: 317038.5359872045, Test Loss: 44267420.0\n",
      "Epoch [28/100], Training Loss: 307787.91594099876, Test Loss: 43110660.0\n",
      "Epoch [29/100], Training Loss: 299002.82554351044, Test Loss: 42004168.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/100], Training Loss: 290644.7651205497, Test Loss: 40942968.0\n",
      "Epoch [31/100], Training Loss: 282711.320597121, Test Loss: 39933556.0\n",
      "Epoch [32/100], Training Loss: 275204.9496475327, Test Loss: 38973840.0\n",
      "Epoch [33/100], Training Loss: 268094.7402997453, Test Loss: 38060696.0\n",
      "Epoch [34/100], Training Loss: 261345.91268289794, Test Loss: 37189600.0\n",
      "Epoch [35/100], Training Loss: 254938.2289556306, Test Loss: 36363212.0\n",
      "Epoch [36/100], Training Loss: 248876.81031929387, Test Loss: 35579496.0\n",
      "Epoch [37/100], Training Loss: 243115.51697174337, Test Loss: 34830900.0\n",
      "Epoch [38/100], Training Loss: 237610.12599964457, Test Loss: 34116108.0\n",
      "Epoch [39/100], Training Loss: 232336.60962028315, Test Loss: 33430308.0\n",
      "Epoch [40/100], Training Loss: 227266.16509685444, Test Loss: 32770740.0\n",
      "Epoch [41/100], Training Loss: 222405.0185711747, Test Loss: 32138906.0\n",
      "Epoch [42/100], Training Loss: 217756.0497897044, Test Loss: 31535032.0\n",
      "Epoch [43/100], Training Loss: 213341.44523428706, Test Loss: 30961980.0\n",
      "Epoch [44/100], Training Loss: 209150.54676855638, Test Loss: 30417352.0\n",
      "Epoch [45/100], Training Loss: 205136.6511462591, Test Loss: 29893192.0\n",
      "Epoch [46/100], Training Loss: 201246.8542444168, Test Loss: 29383138.0\n",
      "Epoch [47/100], Training Loss: 197452.0168236479, Test Loss: 28884660.0\n",
      "Epoch [48/100], Training Loss: 193757.2415141283, Test Loss: 28398462.0\n",
      "Epoch [49/100], Training Loss: 190176.22232095254, Test Loss: 27930438.0\n",
      "Epoch [50/100], Training Loss: 186730.70327587228, Test Loss: 27480996.0\n",
      "Epoch [51/100], Training Loss: 183408.57155974171, Test Loss: 27047248.0\n",
      "Epoch [52/100], Training Loss: 180177.90664060187, Test Loss: 26628522.0\n",
      "Epoch [53/100], Training Loss: 177050.83916829573, Test Loss: 26221678.0\n",
      "Epoch [54/100], Training Loss: 174071.95909602512, Test Loss: 25834020.0\n",
      "Epoch [55/100], Training Loss: 171237.7527693857, Test Loss: 25466104.0\n",
      "Epoch [56/100], Training Loss: 168510.40279011906, Test Loss: 25108254.0\n",
      "Epoch [57/100], Training Loss: 165852.99520170607, Test Loss: 24761906.0\n",
      "Epoch [58/100], Training Loss: 163241.19113204195, Test Loss: 24423880.0\n",
      "Epoch [59/100], Training Loss: 160692.8343996209, Test Loss: 24098252.0\n",
      "Epoch [60/100], Training Loss: 158207.7524731947, Test Loss: 23782098.0\n",
      "Epoch [61/100], Training Loss: 155810.94351637937, Test Loss: 23479930.0\n",
      "Epoch [62/100], Training Loss: 153518.94162075708, Test Loss: 23189896.0\n",
      "Epoch [63/100], Training Loss: 151313.62496297612, Test Loss: 22913928.0\n",
      "Epoch [64/100], Training Loss: 149192.80522480895, Test Loss: 22651234.0\n",
      "Epoch [65/100], Training Loss: 147142.05796457556, Test Loss: 22400088.0\n",
      "Epoch [66/100], Training Loss: 145158.9465079083, Test Loss: 22158064.0\n",
      "Epoch [67/100], Training Loss: 143224.4504768675, Test Loss: 21921828.0\n",
      "Epoch [68/100], Training Loss: 141325.4852793081, Test Loss: 21686862.0\n",
      "Epoch [69/100], Training Loss: 139427.30880871986, Test Loss: 21451866.0\n",
      "Epoch [70/100], Training Loss: 137542.75401338784, Test Loss: 21227510.0\n",
      "Epoch [71/100], Training Loss: 135718.80762395592, Test Loss: 21014242.0\n",
      "Epoch [72/100], Training Loss: 133964.55642438243, Test Loss: 20811246.0\n",
      "Epoch [73/100], Training Loss: 132258.38649961495, Test Loss: 20611788.0\n",
      "Epoch [74/100], Training Loss: 130576.3135477756, Test Loss: 20415838.0\n",
      "Epoch [75/100], Training Loss: 128909.19486404833, Test Loss: 20223192.0\n",
      "Epoch [76/100], Training Loss: 127274.84544754458, Test Loss: 20040218.0\n",
      "Epoch [77/100], Training Loss: 125705.92769978082, Test Loss: 19865124.0\n",
      "Epoch [78/100], Training Loss: 124200.18747408329, Test Loss: 19701922.0\n",
      "Epoch [79/100], Training Loss: 122747.71695989574, Test Loss: 19542688.0\n",
      "Epoch [80/100], Training Loss: 121350.82669865529, Test Loss: 19399344.0\n",
      "Epoch [81/100], Training Loss: 120014.5303447663, Test Loss: 19257558.0\n",
      "Epoch [82/100], Training Loss: 118727.65618150584, Test Loss: 19128366.0\n",
      "Epoch [83/100], Training Loss: 117481.65718855518, Test Loss: 18998668.0\n",
      "Epoch [84/100], Training Loss: 116265.47908891653, Test Loss: 18881288.0\n",
      "Epoch [85/100], Training Loss: 115081.87136425567, Test Loss: 18762120.0\n",
      "Epoch [86/100], Training Loss: 113915.66706652449, Test Loss: 18652070.0\n",
      "Epoch [87/100], Training Loss: 112768.409691369, Test Loss: 18538924.0\n",
      "Epoch [88/100], Training Loss: 111645.61746934423, Test Loss: 18434794.0\n",
      "Epoch [89/100], Training Loss: 110562.14596291688, Test Loss: 18330100.0\n",
      "Epoch [90/100], Training Loss: 109508.51537231206, Test Loss: 18237034.0\n",
      "Epoch [91/100], Training Loss: 108488.0497008471, Test Loss: 18141064.0\n",
      "Epoch [92/100], Training Loss: 107505.62970203187, Test Loss: 18057332.0\n",
      "Epoch [93/100], Training Loss: 106555.40723298382, Test Loss: 17971858.0\n",
      "Epoch [94/100], Training Loss: 105634.59954978971, Test Loss: 17898226.0\n",
      "Epoch [95/100], Training Loss: 104742.55384752089, Test Loss: 17821296.0\n",
      "Epoch [96/100], Training Loss: 103876.18834784669, Test Loss: 17755678.0\n",
      "Epoch [97/100], Training Loss: 103033.2599372075, Test Loss: 17681892.0\n",
      "Epoch [98/100], Training Loss: 102208.44917362716, Test Loss: 17625862.0\n",
      "Epoch [99/100], Training Loss: 101405.77652390262, Test Loss: 17553214.0\n",
      "Epoch [100/100], Training Loss: 100621.4344677448, Test Loss: 17503992.0\n",
      "Epoch [1/100], Training Loss: 1497388.161364848, Test Loss: 298389216.0\n",
      "Epoch [2/100], Training Loss: 1459789.8010781351, Test Loss: 281494304.0\n",
      "Epoch [3/100], Training Loss: 1306563.1249333571, Test Loss: 236324256.0\n",
      "Epoch [4/100], Training Loss: 1049559.9526094426, Test Loss: 184648832.0\n",
      "Epoch [5/100], Training Loss: 845248.9715064274, Test Loss: 156869424.0\n",
      "Epoch [6/100], Training Loss: 748251.6218233517, Test Loss: 143993696.0\n",
      "Epoch [7/100], Training Loss: 692847.6770333512, Test Loss: 134911680.0\n",
      "Epoch [8/100], Training Loss: 648632.2980866062, Test Loss: 127208400.0\n",
      "Epoch [9/100], Training Loss: 609618.0555654286, Test Loss: 120368376.0\n",
      "Epoch [10/100], Training Loss: 574278.6536342633, Test Loss: 114217984.0\n",
      "Epoch [11/100], Training Loss: 542056.4272258752, Test Loss: 108681344.0\n",
      "Epoch [12/100], Training Loss: 512699.1701913394, Test Loss: 103710352.0\n",
      "Epoch [13/100], Training Loss: 486027.32776494283, Test Loss: 99253696.0\n",
      "Epoch [14/100], Training Loss: 461861.8543925123, Test Loss: 95264560.0\n",
      "Epoch [15/100], Training Loss: 440030.2595817783, Test Loss: 91691200.0\n",
      "Epoch [16/100], Training Loss: 420332.165866951, Test Loss: 88475664.0\n",
      "Epoch [17/100], Training Loss: 402542.36763224925, Test Loss: 85564400.0\n",
      "Epoch [18/100], Training Loss: 386457.48486464075, Test Loss: 82914480.0\n",
      "Epoch [19/100], Training Loss: 371903.0681831645, Test Loss: 80492480.0\n",
      "Epoch [20/100], Training Loss: 358724.37936141226, Test Loss: 78271360.0\n",
      "Epoch [21/100], Training Loss: 346780.12605888274, Test Loss: 76227416.0\n",
      "Epoch [22/100], Training Loss: 335938.2301996327, Test Loss: 74339488.0\n",
      "Epoch [23/100], Training Loss: 326071.84775783424, Test Loss: 72587776.0\n",
      "Epoch [24/100], Training Loss: 317061.6512054973, Test Loss: 70953648.0\n",
      "Epoch [25/100], Training Loss: 308790.26313607016, Test Loss: 69420232.0\n",
      "Epoch [26/100], Training Loss: 301150.1424086251, Test Loss: 67975064.0\n",
      "Epoch [27/100], Training Loss: 294045.18144659675, Test Loss: 66604324.0\n",
      "Epoch [28/100], Training Loss: 287385.71506427345, Test Loss: 65297728.0\n",
      "Epoch [29/100], Training Loss: 281104.28289941355, Test Loss: 64046900.0\n",
      "Epoch [30/100], Training Loss: 275139.5410705823, Test Loss: 62842504.0\n",
      "Epoch [31/100], Training Loss: 269439.0255902068, Test Loss: 61678352.0\n",
      "Epoch [32/100], Training Loss: 263956.4121978852, Test Loss: 60547580.0\n",
      "Epoch [33/100], Training Loss: 258654.61499429832, Test Loss: 59447696.0\n",
      "Epoch [34/100], Training Loss: 253505.04861975, Test Loss: 58374588.0\n",
      "Epoch [35/100], Training Loss: 248488.8503124815, Test Loss: 57326960.0\n",
      "Epoch [36/100], Training Loss: 243593.67882530656, Test Loss: 56303772.0\n",
      "Epoch [37/100], Training Loss: 238805.43710384457, Test Loss: 55303168.0\n",
      "Epoch [38/100], Training Loss: 234116.46445708192, Test Loss: 54323716.0\n",
      "Epoch [39/100], Training Loss: 229519.71697470528, Test Loss: 53364724.0\n",
      "Epoch [40/100], Training Loss: 225016.77736804693, Test Loss: 52426280.0\n",
      "Epoch [41/100], Training Loss: 220608.7203512825, Test Loss: 51507340.0\n",
      "Epoch [42/100], Training Loss: 216292.02655352172, Test Loss: 50608456.0\n",
      "Epoch [43/100], Training Loss: 212069.82406255553, Test Loss: 49729568.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/100], Training Loss: 207941.60819856645, Test Loss: 48871052.0\n",
      "Epoch [45/100], Training Loss: 203904.8074610509, Test Loss: 48031056.0\n",
      "Epoch [46/100], Training Loss: 199959.79085954622, Test Loss: 47209476.0\n",
      "Epoch [47/100], Training Loss: 196104.90127954504, Test Loss: 46406032.0\n",
      "Epoch [48/100], Training Loss: 192339.23474616432, Test Loss: 45619192.0\n",
      "Epoch [49/100], Training Loss: 188662.3054469522, Test Loss: 44849356.0\n",
      "Epoch [50/100], Training Loss: 185074.55472868905, Test Loss: 44095892.0\n",
      "Epoch [51/100], Training Loss: 181574.26790474498, Test Loss: 43357452.0\n",
      "Epoch [52/100], Training Loss: 178159.35125288786, Test Loss: 42634024.0\n",
      "Epoch [53/100], Training Loss: 174829.14497808187, Test Loss: 41925388.0\n",
      "Epoch [54/100], Training Loss: 171581.55606154847, Test Loss: 41231984.0\n",
      "Epoch [55/100], Training Loss: 168415.25560541436, Test Loss: 40551888.0\n",
      "Epoch [56/100], Training Loss: 165330.58866477106, Test Loss: 39886592.0\n",
      "Epoch [57/100], Training Loss: 162325.669136159, Test Loss: 39235812.0\n",
      "Epoch [58/100], Training Loss: 159396.470499378, Test Loss: 38599132.0\n",
      "Epoch [59/100], Training Loss: 156542.1207755761, Test Loss: 37976568.0\n",
      "Epoch [60/100], Training Loss: 153763.45570648953, Test Loss: 37368920.0\n",
      "Epoch [61/100], Training Loss: 151058.91483398495, Test Loss: 36776192.0\n",
      "Epoch [62/100], Training Loss: 148424.27588246402, Test Loss: 36198000.0\n",
      "Epoch [63/100], Training Loss: 145858.4989263077, Test Loss: 35633704.0\n",
      "Epoch [64/100], Training Loss: 143358.4706169288, Test Loss: 35081328.0\n",
      "Epoch [65/100], Training Loss: 140922.93870744106, Test Loss: 34541876.0\n",
      "Epoch [66/100], Training Loss: 138552.6917889843, Test Loss: 34014992.0\n",
      "Epoch [67/100], Training Loss: 136245.75630169458, Test Loss: 33501740.0\n",
      "Epoch [68/100], Training Loss: 134002.0366414858, Test Loss: 33002150.0\n",
      "Epoch [69/100], Training Loss: 131821.20347713176, Test Loss: 32516274.0\n",
      "Epoch [70/100], Training Loss: 129702.1352144816, Test Loss: 32043104.0\n",
      "Epoch [71/100], Training Loss: 127639.65731121122, Test Loss: 31582494.0\n",
      "Epoch [72/100], Training Loss: 125633.67436900709, Test Loss: 31135290.0\n",
      "Epoch [73/100], Training Loss: 123682.50762049652, Test Loss: 30699788.0\n",
      "Epoch [74/100], Training Loss: 121786.72697689259, Test Loss: 30276406.0\n",
      "Epoch [75/100], Training Loss: 119943.93999587184, Test Loss: 29864508.0\n",
      "Epoch [76/100], Training Loss: 118152.41709079249, Test Loss: 29462944.0\n",
      "Epoch [77/100], Training Loss: 116406.44710341416, Test Loss: 29072254.0\n",
      "Epoch [78/100], Training Loss: 114704.03891909034, Test Loss: 28691920.0\n",
      "Epoch [79/100], Training Loss: 113044.509504491, Test Loss: 28322302.0\n",
      "Epoch [80/100], Training Loss: 111427.97039119888, Test Loss: 27963200.0\n",
      "Epoch [81/100], Training Loss: 109854.17011694453, Test Loss: 27615486.0\n",
      "Epoch [82/100], Training Loss: 108322.60482025372, Test Loss: 27278156.0\n",
      "Epoch [83/100], Training Loss: 106831.62325837387, Test Loss: 26951288.0\n",
      "Epoch [84/100], Training Loss: 105380.82642410013, Test Loss: 26633920.0\n",
      "Epoch [85/100], Training Loss: 103967.1705053481, Test Loss: 26326764.0\n",
      "Epoch [86/100], Training Loss: 102590.39795098317, Test Loss: 26029364.0\n",
      "Epoch [87/100], Training Loss: 101251.97782270008, Test Loss: 25741230.0\n",
      "Epoch [88/100], Training Loss: 99948.20152418029, Test Loss: 25463020.0\n",
      "Epoch [89/100], Training Loss: 98677.35685367425, Test Loss: 25193508.0\n",
      "Epoch [90/100], Training Loss: 97437.53368269985, Test Loss: 24932120.0\n",
      "Epoch [91/100], Training Loss: 96228.2042065599, Test Loss: 24678032.0\n",
      "Epoch [92/100], Training Loss: 95049.57942199254, Test Loss: 24432028.0\n",
      "Epoch [93/100], Training Loss: 93901.06222278374, Test Loss: 24193450.0\n",
      "Epoch [94/100], Training Loss: 92779.41163419673, Test Loss: 23960698.0\n",
      "Epoch [95/100], Training Loss: 91683.08329885485, Test Loss: 23733852.0\n",
      "Epoch [96/100], Training Loss: 90611.30999394659, Test Loss: 23511118.0\n",
      "Epoch [97/100], Training Loss: 89563.78828916386, Test Loss: 23295918.0\n",
      "Epoch [98/100], Training Loss: 88538.74194800181, Test Loss: 23083664.0\n",
      "Epoch [99/100], Training Loss: 87536.4050291378, Test Loss: 22880642.0\n",
      "Epoch [100/100], Training Loss: 86553.35349838575, Test Loss: 22678212.0\n",
      "Epoch [1/100], Training Loss: 1164447.1232746874, Test Loss: 299400800.0\n",
      "Epoch [2/100], Training Loss: 1151524.3845743735, Test Loss: 291682080.0\n",
      "Epoch [3/100], Training Loss: 1091846.8327705704, Test Loss: 267305360.0\n",
      "Epoch [4/100], Training Loss: 962211.2680528405, Test Loss: 226304032.0\n",
      "Epoch [5/100], Training Loss: 795141.2897340205, Test Loss: 185086304.0\n",
      "Epoch [6/100], Training Loss: 663316.8144067294, Test Loss: 159916512.0\n",
      "Epoch [7/100], Training Loss: 592477.760085303, Test Loss: 147369920.0\n",
      "Epoch [8/100], Training Loss: 552997.2911557372, Test Loss: 139185376.0\n",
      "Epoch [9/100], Training Loss: 523433.30845329067, Test Loss: 132409768.0\n",
      "Epoch [10/100], Training Loss: 497645.97500148095, Test Loss: 126313816.0\n",
      "Epoch [11/100], Training Loss: 473962.2422842249, Test Loss: 120657112.0\n",
      "Epoch [12/100], Training Loss: 451755.90687755466, Test Loss: 115340696.0\n",
      "Epoch [13/100], Training Loss: 430771.76517978794, Test Loss: 110326840.0\n",
      "Epoch [14/100], Training Loss: 410931.1642675197, Test Loss: 105609296.0\n",
      "Epoch [15/100], Training Loss: 392237.8233516972, Test Loss: 101188992.0\n",
      "Epoch [16/100], Training Loss: 374698.7839582963, Test Loss: 97064112.0\n",
      "Epoch [17/100], Training Loss: 358313.442805521, Test Loss: 93227192.0\n",
      "Epoch [18/100], Training Loss: 342910.2309104911, Test Loss: 89597536.0\n",
      "Epoch [19/100], Training Loss: 328328.9750607191, Test Loss: 86207696.0\n",
      "Epoch [20/100], Training Loss: 314944.1844677448, Test Loss: 83131544.0\n",
      "Epoch [21/100], Training Loss: 302837.4636573663, Test Loss: 80325904.0\n",
      "Epoch [22/100], Training Loss: 291784.8493572656, Test Loss: 77737752.0\n",
      "Epoch [23/100], Training Loss: 281639.2487411883, Test Loss: 75347584.0\n",
      "Epoch [24/100], Training Loss: 272306.97908891656, Test Loss: 73131104.0\n",
      "Epoch [25/100], Training Loss: 263682.5390675908, Test Loss: 71067784.0\n",
      "Epoch [26/100], Training Loss: 255656.0555654286, Test Loss: 69125824.0\n",
      "Epoch [27/100], Training Loss: 248118.317279782, Test Loss: 67291448.0\n",
      "Epoch [28/100], Training Loss: 241051.7177892305, Test Loss: 65563776.0\n",
      "Epoch [29/100], Training Loss: 234429.31236301168, Test Loss: 63941820.0\n",
      "Epoch [30/100], Training Loss: 228307.10431846455, Test Loss: 62434056.0\n",
      "Epoch [31/100], Training Loss: 222602.243232036, Test Loss: 61024604.0\n",
      "Epoch [32/100], Training Loss: 217249.16758485872, Test Loss: 59696520.0\n",
      "Epoch [33/100], Training Loss: 212201.48841893254, Test Loss: 58441724.0\n",
      "Epoch [34/100], Training Loss: 207432.9205615781, Test Loss: 57257616.0\n",
      "Epoch [35/100], Training Loss: 202909.42633730228, Test Loss: 56133584.0\n",
      "Epoch [36/100], Training Loss: 198596.13174574965, Test Loss: 55060532.0\n",
      "Epoch [37/100], Training Loss: 194467.95106924945, Test Loss: 54033808.0\n",
      "Epoch [38/100], Training Loss: 190508.87601445412, Test Loss: 53048964.0\n",
      "Epoch [39/100], Training Loss: 186710.19441976186, Test Loss: 52098984.0\n",
      "Epoch [40/100], Training Loss: 183054.86760263017, Test Loss: 51182516.0\n",
      "Epoch [41/100], Training Loss: 179521.53450624962, Test Loss: 50298304.0\n",
      "Epoch [42/100], Training Loss: 176103.01877850838, Test Loss: 49442464.0\n",
      "Epoch [43/100], Training Loss: 172782.76997808187, Test Loss: 48609400.0\n",
      "Epoch [44/100], Training Loss: 169555.4302470233, Test Loss: 47803556.0\n",
      "Epoch [45/100], Training Loss: 166429.41448966294, Test Loss: 47022712.0\n",
      "Epoch [46/100], Training Loss: 163395.80498785616, Test Loss: 46264328.0\n",
      "Epoch [47/100], Training Loss: 160446.87482969018, Test Loss: 45520380.0\n",
      "Epoch [48/100], Training Loss: 157567.944908477, Test Loss: 44796588.0\n",
      "Epoch [49/100], Training Loss: 154775.42017652982, Test Loss: 44096316.0\n",
      "Epoch [50/100], Training Loss: 152075.55677981162, Test Loss: 43418212.0\n",
      "Epoch [51/100], Training Loss: 149459.87169006575, Test Loss: 42759484.0\n",
      "Epoch [52/100], Training Loss: 146925.98625673834, Test Loss: 42119408.0\n",
      "Epoch [53/100], Training Loss: 144473.49576446894, Test Loss: 41496692.0\n",
      "Epoch [54/100], Training Loss: 142099.12434097508, Test Loss: 40892368.0\n",
      "Epoch [55/100], Training Loss: 139799.01267697412, Test Loss: 40304240.0\n",
      "Epoch [56/100], Training Loss: 137570.82400331734, Test Loss: 39733168.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [57/100], Training Loss: 135410.63361175286, Test Loss: 39177404.0\n",
      "Epoch [58/100], Training Loss: 133313.19424204726, Test Loss: 38636020.0\n",
      "Epoch [59/100], Training Loss: 131274.73182868314, Test Loss: 38109260.0\n",
      "Epoch [60/100], Training Loss: 129292.99532018245, Test Loss: 37597060.0\n",
      "Epoch [61/100], Training Loss: 127369.13695871098, Test Loss: 37100004.0\n",
      "Epoch [62/100], Training Loss: 125501.97825958178, Test Loss: 36615944.0\n",
      "Epoch [63/100], Training Loss: 123689.04158521415, Test Loss: 36147240.0\n",
      "Epoch [64/100], Training Loss: 121928.68349031455, Test Loss: 35691076.0\n",
      "Epoch [65/100], Training Loss: 120213.88519637463, Test Loss: 35246352.0\n",
      "Epoch [66/100], Training Loss: 118543.48995912564, Test Loss: 34810664.0\n",
      "Epoch [67/100], Training Loss: 116914.27741247557, Test Loss: 34386744.0\n",
      "Epoch [68/100], Training Loss: 115329.33173390201, Test Loss: 33975780.0\n",
      "Epoch [69/100], Training Loss: 113786.56406610983, Test Loss: 33573652.0\n",
      "Epoch [70/100], Training Loss: 112280.76725312481, Test Loss: 33180442.0\n",
      "Epoch [71/100], Training Loss: 110812.57158936081, Test Loss: 32797530.0\n",
      "Epoch [72/100], Training Loss: 109379.00989277886, Test Loss: 32422822.0\n",
      "Epoch [73/100], Training Loss: 107981.11361886145, Test Loss: 32057010.0\n",
      "Epoch [74/100], Training Loss: 106612.75967063563, Test Loss: 31699660.0\n",
      "Epoch [75/100], Training Loss: 105276.93483798353, Test Loss: 31351528.0\n",
      "Epoch [76/100], Training Loss: 103968.74118831822, Test Loss: 31010216.0\n",
      "Epoch [77/100], Training Loss: 102687.1661039038, Test Loss: 30675612.0\n",
      "Epoch [78/100], Training Loss: 101424.93679284402, Test Loss: 30347794.0\n",
      "Epoch [79/100], Training Loss: 100187.14537053493, Test Loss: 30026406.0\n",
      "Epoch [80/100], Training Loss: 98976.24524613471, Test Loss: 29711740.0\n",
      "Epoch [81/100], Training Loss: 97790.14815473017, Test Loss: 29402588.0\n",
      "Epoch [82/100], Training Loss: 96628.65701084059, Test Loss: 29099954.0\n",
      "Epoch [83/100], Training Loss: 95491.15591493394, Test Loss: 28802776.0\n",
      "Epoch [84/100], Training Loss: 94379.22409809845, Test Loss: 28513172.0\n",
      "Epoch [85/100], Training Loss: 93294.3210117884, Test Loss: 28229686.0\n",
      "Epoch [86/100], Training Loss: 92232.32545465315, Test Loss: 27953456.0\n",
      "Epoch [87/100], Training Loss: 91197.62869498253, Test Loss: 27684716.0\n",
      "Epoch [88/100], Training Loss: 90184.5002073337, Test Loss: 27422460.0\n",
      "Epoch [89/100], Training Loss: 89195.78070019548, Test Loss: 27167390.0\n",
      "Epoch [90/100], Training Loss: 88229.81914578521, Test Loss: 26915772.0\n",
      "Epoch [91/100], Training Loss: 87284.20608968663, Test Loss: 26670140.0\n",
      "Epoch [92/100], Training Loss: 86356.43374207689, Test Loss: 26429554.0\n",
      "Epoch [93/100], Training Loss: 85447.41259404064, Test Loss: 26194166.0\n",
      "Epoch [94/100], Training Loss: 84557.17244239086, Test Loss: 25963182.0\n",
      "Epoch [95/100], Training Loss: 83680.84923878917, Test Loss: 25734774.0\n",
      "Epoch [96/100], Training Loss: 82821.40542621883, Test Loss: 25512616.0\n",
      "Epoch [97/100], Training Loss: 81976.48433149695, Test Loss: 25297126.0\n",
      "Epoch [98/100], Training Loss: 81148.91078727563, Test Loss: 25082804.0\n",
      "Epoch [99/100], Training Loss: 80335.7675493158, Test Loss: 24876098.0\n",
      "Epoch [100/100], Training Loss: 79536.54386588473, Test Loss: 24672616.0\n",
      "Epoch [1/100], Training Loss: 582417.6416089095, Test Loss: 300027392.0\n",
      "Epoch [2/100], Training Loss: 581865.7795154315, Test Loss: 299359488.0\n",
      "Epoch [3/100], Training Loss: 579069.6636455186, Test Loss: 296847456.0\n",
      "Epoch [4/100], Training Loss: 571196.4883596944, Test Loss: 290911904.0\n",
      "Epoch [5/100], Training Loss: 555288.8610864285, Test Loss: 280204064.0\n",
      "Epoch [6/100], Training Loss: 529376.8132219656, Test Loss: 264162128.0\n",
      "Epoch [7/100], Training Loss: 493506.5183342219, Test Loss: 243489600.0\n",
      "Epoch [8/100], Training Loss: 450441.9074699366, Test Loss: 220334976.0\n",
      "Epoch [9/100], Training Loss: 405449.8965701084, Test Loss: 197835136.0\n",
      "Epoch [10/100], Training Loss: 364684.473194716, Test Loss: 178930480.0\n",
      "Epoch [11/100], Training Loss: 332509.09685445175, Test Loss: 164958416.0\n",
      "Epoch [12/100], Training Loss: 309544.2270007701, Test Loss: 155290432.0\n",
      "Epoch [13/100], Training Loss: 293475.2249274332, Test Loss: 148410048.0\n",
      "Epoch [14/100], Training Loss: 281492.3893134293, Test Loss: 143056800.0\n",
      "Epoch [15/100], Training Loss: 271689.1368994728, Test Loss: 138512720.0\n",
      "Epoch [16/100], Training Loss: 263073.72217285703, Test Loss: 134432144.0\n",
      "Epoch [17/100], Training Loss: 255178.75149576447, Test Loss: 130652064.0\n",
      "Epoch [18/100], Training Loss: 247776.10236360406, Test Loss: 127088912.0\n",
      "Epoch [19/100], Training Loss: 240742.20721521237, Test Loss: 123695136.0\n",
      "Epoch [20/100], Training Loss: 234003.80498785616, Test Loss: 120441728.0\n",
      "Epoch [21/100], Training Loss: 227515.6324862271, Test Loss: 117310704.0\n",
      "Epoch [22/100], Training Loss: 221249.95675611636, Test Loss: 114291192.0\n",
      "Epoch [23/100], Training Loss: 215190.52331023043, Test Loss: 111376952.0\n",
      "Epoch [24/100], Training Loss: 209328.65422664533, Test Loss: 108564952.0\n",
      "Epoch [25/100], Training Loss: 203661.63758071203, Test Loss: 105854504.0\n",
      "Epoch [26/100], Training Loss: 198189.90273088086, Test Loss: 103245640.0\n",
      "Epoch [27/100], Training Loss: 192914.41123156212, Test Loss: 100738424.0\n",
      "Epoch [28/100], Training Loss: 187835.92180558023, Test Loss: 98331960.0\n",
      "Epoch [29/100], Training Loss: 182955.61116047628, Test Loss: 96025384.0\n",
      "Epoch [30/100], Training Loss: 178274.70173567918, Test Loss: 93818584.0\n",
      "Epoch [31/100], Training Loss: 173793.55749066998, Test Loss: 91710520.0\n",
      "Epoch [32/100], Training Loss: 169510.96404241456, Test Loss: 89699544.0\n",
      "Epoch [33/100], Training Loss: 165424.52721995141, Test Loss: 87783144.0\n",
      "Epoch [34/100], Training Loss: 161530.48658254842, Test Loss: 85958064.0\n",
      "Epoch [35/100], Training Loss: 157822.98963331556, Test Loss: 84220208.0\n",
      "Epoch [36/100], Training Loss: 154294.71168769622, Test Loss: 82564872.0\n",
      "Epoch [37/100], Training Loss: 150937.1620164682, Test Loss: 80986856.0\n",
      "Epoch [38/100], Training Loss: 147740.96818908834, Test Loss: 79481016.0\n",
      "Epoch [39/100], Training Loss: 144696.0310408151, Test Loss: 78041856.0\n",
      "Epoch [40/100], Training Loss: 141791.54694627094, Test Loss: 76664456.0\n",
      "Epoch [41/100], Training Loss: 139017.49327646467, Test Loss: 75343784.0\n",
      "Epoch [42/100], Training Loss: 136364.91084651384, Test Loss: 74075840.0\n",
      "Epoch [43/100], Training Loss: 133825.92145015104, Test Loss: 72857072.0\n",
      "Epoch [44/100], Training Loss: 131392.93833303713, Test Loss: 71684208.0\n",
      "Epoch [45/100], Training Loss: 129058.08779100764, Test Loss: 70553688.0\n",
      "Epoch [46/100], Training Loss: 126814.6292281263, Test Loss: 69463008.0\n",
      "Epoch [47/100], Training Loss: 124656.53148510159, Test Loss: 68409544.0\n",
      "Epoch [48/100], Training Loss: 122578.07546946271, Test Loss: 67391040.0\n",
      "Epoch [49/100], Training Loss: 120573.99135122327, Test Loss: 66405792.0\n",
      "Epoch [50/100], Training Loss: 118639.88815828446, Test Loss: 65451724.0\n",
      "Epoch [51/100], Training Loss: 116771.27847876311, Test Loss: 64527504.0\n",
      "Epoch [52/100], Training Loss: 114964.0443101712, Test Loss: 63631744.0\n",
      "Epoch [53/100], Training Loss: 113215.37610331141, Test Loss: 62763768.0\n",
      "Epoch [54/100], Training Loss: 111521.88045731888, Test Loss: 61922032.0\n",
      "Epoch [55/100], Training Loss: 109879.62348202121, Test Loss: 61104448.0\n",
      "Epoch [56/100], Training Loss: 108285.10645103962, Test Loss: 60309300.0\n",
      "Epoch [57/100], Training Loss: 106734.76974112909, Test Loss: 59535204.0\n",
      "Epoch [58/100], Training Loss: 105226.46810023102, Test Loss: 58781120.0\n",
      "Epoch [59/100], Training Loss: 103757.37859131568, Test Loss: 58046404.0\n",
      "Epoch [60/100], Training Loss: 102324.7195071382, Test Loss: 57330228.0\n",
      "Epoch [61/100], Training Loss: 100927.51365440436, Test Loss: 56631820.0\n",
      "Epoch [62/100], Training Loss: 99563.39411172325, Test Loss: 55949648.0\n",
      "Epoch [63/100], Training Loss: 98229.94431609502, Test Loss: 55282572.0\n",
      "Epoch [64/100], Training Loss: 96925.4392512292, Test Loss: 54630072.0\n",
      "Epoch [65/100], Training Loss: 95649.66032817961, Test Loss: 53991216.0\n",
      "Epoch [66/100], Training Loss: 94400.9998222854, Test Loss: 53364772.0\n",
      "Epoch [67/100], Training Loss: 93177.74089212724, Test Loss: 52749552.0\n",
      "Epoch [68/100], Training Loss: 91978.31123748593, Test Loss: 52145800.0\n",
      "Epoch [69/100], Training Loss: 90802.11124933357, Test Loss: 51553564.0\n",
      "Epoch [70/100], Training Loss: 89647.99763047213, Test Loss: 50972132.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [71/100], Training Loss: 88515.4298915941, Test Loss: 50400908.0\n",
      "Epoch [72/100], Training Loss: 87404.20010662875, Test Loss: 49839800.0\n",
      "Epoch [73/100], Training Loss: 86314.23162134945, Test Loss: 49288432.0\n",
      "Epoch [74/100], Training Loss: 85244.33493276464, Test Loss: 48746248.0\n",
      "Epoch [75/100], Training Loss: 84193.77975238433, Test Loss: 48213088.0\n",
      "Epoch [76/100], Training Loss: 83162.59723950003, Test Loss: 47688416.0\n",
      "Epoch [77/100], Training Loss: 82149.91173508679, Test Loss: 47172372.0\n",
      "Epoch [78/100], Training Loss: 81155.6418458622, Test Loss: 46664792.0\n",
      "Epoch [79/100], Training Loss: 80179.77382856466, Test Loss: 46165508.0\n",
      "Epoch [80/100], Training Loss: 79222.25496119898, Test Loss: 45674212.0\n",
      "Epoch [81/100], Training Loss: 78281.86481843493, Test Loss: 45190884.0\n",
      "Epoch [82/100], Training Loss: 77358.12973165097, Test Loss: 44715064.0\n",
      "Epoch [83/100], Training Loss: 76451.01711983887, Test Loss: 44246840.0\n",
      "Epoch [84/100], Training Loss: 75559.73840412298, Test Loss: 43785564.0\n",
      "Epoch [85/100], Training Loss: 74683.93791836976, Test Loss: 43331552.0\n",
      "Epoch [86/100], Training Loss: 73823.90901012973, Test Loss: 42884708.0\n",
      "Epoch [87/100], Training Loss: 72978.96214679226, Test Loss: 42444776.0\n",
      "Epoch [88/100], Training Loss: 72148.47023280611, Test Loss: 42011828.0\n",
      "Epoch [89/100], Training Loss: 71332.10994609325, Test Loss: 41585548.0\n",
      "Epoch [90/100], Training Loss: 70529.82607665422, Test Loss: 41165792.0\n",
      "Epoch [91/100], Training Loss: 69741.46934423316, Test Loss: 40752476.0\n",
      "Epoch [92/100], Training Loss: 68967.11533676914, Test Loss: 40345248.0\n",
      "Epoch [93/100], Training Loss: 68206.07440317517, Test Loss: 39944088.0\n",
      "Epoch [94/100], Training Loss: 67457.93507493632, Test Loss: 39548988.0\n",
      "Epoch [95/100], Training Loss: 66722.5535217108, Test Loss: 39159708.0\n",
      "Epoch [96/100], Training Loss: 65999.66885847994, Test Loss: 38776360.0\n",
      "Epoch [97/100], Training Loss: 65288.86487767312, Test Loss: 38398772.0\n",
      "Epoch [98/100], Training Loss: 64589.766127599076, Test Loss: 38026620.0\n",
      "Epoch [99/100], Training Loss: 63902.036490729224, Test Loss: 37660088.0\n",
      "Epoch [100/100], Training Loss: 63225.77797523843, Test Loss: 37299120.0\n",
      "Epoch [1/100], Training Loss: 7058454.069604881, Test Loss: 146873152.0\n",
      "Epoch [2/100], Training Loss: 4012980.9736390025, Test Loss: 115545728.0\n",
      "Epoch [3/100], Training Loss: 3188920.538001303, Test Loss: 93967024.0\n",
      "Epoch [4/100], Training Loss: 2595740.291451928, Test Loss: 78391616.0\n",
      "Epoch [5/100], Training Loss: 2163321.445767431, Test Loss: 66998448.0\n",
      "Epoch [6/100], Training Loss: 1854083.4779633908, Test Loss: 58807808.0\n",
      "Epoch [7/100], Training Loss: 1634617.1759374444, Test Loss: 52925276.0\n",
      "Epoch [8/100], Training Loss: 1472666.246519756, Test Loss: 48374168.0\n",
      "Epoch [9/100], Training Loss: 1342887.1518867365, Test Loss: 44583028.0\n",
      "Epoch [10/100], Training Loss: 1232805.4049523133, Test Loss: 41278960.0\n",
      "Epoch [11/100], Training Loss: 1137072.6909543274, Test Loss: 38337072.0\n",
      "Epoch [12/100], Training Loss: 1053097.0928114448, Test Loss: 35701276.0\n",
      "Epoch [13/100], Training Loss: 979426.077039275, Test Loss: 33360300.0\n",
      "Epoch [14/100], Training Loss: 914631.2649428351, Test Loss: 31292908.0\n",
      "Epoch [15/100], Training Loss: 857712.2547242462, Test Loss: 29472978.0\n",
      "Epoch [16/100], Training Loss: 807783.7809519578, Test Loss: 27874838.0\n",
      "Epoch [17/100], Training Loss: 764025.9097061786, Test Loss: 26462350.0\n",
      "Epoch [18/100], Training Loss: 725640.1921983295, Test Loss: 25217892.0\n",
      "Epoch [19/100], Training Loss: 691705.8631301463, Test Loss: 24117108.0\n",
      "Epoch [20/100], Training Loss: 661273.1114418578, Test Loss: 23136188.0\n",
      "Epoch [21/100], Training Loss: 633708.0728926011, Test Loss: 22263174.0\n",
      "Epoch [22/100], Training Loss: 608564.7256531011, Test Loss: 21483326.0\n",
      "Epoch [23/100], Training Loss: 585565.247371305, Test Loss: 20789176.0\n",
      "Epoch [24/100], Training Loss: 564494.2766127599, Test Loss: 20174428.0\n",
      "Epoch [25/100], Training Loss: 545182.4919732243, Test Loss: 19626798.0\n",
      "Epoch [26/100], Training Loss: 527422.3508604348, Test Loss: 19134986.0\n",
      "Epoch [27/100], Training Loss: 511030.57158195606, Test Loss: 18694634.0\n",
      "Epoch [28/100], Training Loss: 495834.92378265504, Test Loss: 18299978.0\n",
      "Epoch [29/100], Training Loss: 481761.53173686395, Test Loss: 17935212.0\n",
      "Epoch [30/100], Training Loss: 468705.8833377762, Test Loss: 17601878.0\n",
      "Epoch [31/100], Training Loss: 456553.95456430304, Test Loss: 17293542.0\n",
      "Epoch [32/100], Training Loss: 445185.42556720576, Test Loss: 17026396.0\n",
      "Epoch [33/100], Training Loss: 434626.8975623482, Test Loss: 16779804.0\n",
      "Epoch [34/100], Training Loss: 424814.1859783188, Test Loss: 16563289.0\n",
      "Epoch [35/100], Training Loss: 415665.619579705, Test Loss: 16361219.0\n",
      "Epoch [36/100], Training Loss: 407113.4058038623, Test Loss: 16183338.0\n",
      "Epoch [37/100], Training Loss: 399056.29479888635, Test Loss: 16028749.0\n",
      "Epoch [38/100], Training Loss: 391460.3081570997, Test Loss: 15876950.0\n",
      "Epoch [39/100], Training Loss: 384313.49870416446, Test Loss: 15755164.0\n",
      "Epoch [40/100], Training Loss: 377598.08330371423, Test Loss: 15643095.0\n",
      "Epoch [41/100], Training Loss: 371271.6503835673, Test Loss: 15540531.0\n",
      "Epoch [42/100], Training Loss: 365297.7884752088, Test Loss: 15458967.0\n",
      "Epoch [43/100], Training Loss: 359706.29688703275, Test Loss: 15372268.0\n",
      "Epoch [44/100], Training Loss: 354413.0193042474, Test Loss: 15300247.0\n",
      "Epoch [45/100], Training Loss: 349424.47225430957, Test Loss: 15232516.0\n",
      "Epoch [46/100], Training Loss: 344755.0451913394, Test Loss: 15165923.0\n",
      "Epoch [47/100], Training Loss: 340311.2231206682, Test Loss: 15068217.0\n",
      "Epoch [48/100], Training Loss: 336118.114092767, Test Loss: 15045564.0\n",
      "Epoch [49/100], Training Loss: 332162.7001214383, Test Loss: 14923668.0\n",
      "Epoch [50/100], Training Loss: 328391.38278612046, Test Loss: 14906440.0\n",
      "Epoch [51/100], Training Loss: 324881.502265861, Test Loss: 14808561.0\n",
      "Epoch [52/100], Training Loss: 321481.46133226703, Test Loss: 14787590.0\n",
      "Epoch [53/100], Training Loss: 318307.62991306797, Test Loss: 14697983.0\n",
      "Epoch [54/100], Training Loss: 315287.58825010364, Test Loss: 14701129.0\n",
      "Epoch [55/100], Training Loss: 312422.23415378237, Test Loss: 14607242.0\n",
      "Epoch [56/100], Training Loss: 309669.7766831053, Test Loss: 14615107.0\n",
      "Epoch [57/100], Training Loss: 307050.47117691487, Test Loss: 14532358.0\n",
      "Epoch [58/100], Training Loss: 304586.00786387065, Test Loss: 14539643.0\n",
      "Epoch [59/100], Training Loss: 302224.3588835081, Test Loss: 14493793.0\n",
      "Epoch [60/100], Training Loss: 300026.53269207984, Test Loss: 14446531.0\n",
      "Epoch [61/100], Training Loss: 297880.8944671524, Test Loss: 14440029.0\n",
      "Epoch [62/100], Training Loss: 295828.92918443814, Test Loss: 14427965.0\n",
      "Epoch [63/100], Training Loss: 293824.48479429533, Test Loss: 14371825.0\n",
      "Epoch [64/100], Training Loss: 291931.950328772, Test Loss: 14402162.0\n",
      "Epoch [65/100], Training Loss: 290081.85779130383, Test Loss: 14344039.0\n",
      "Epoch [66/100], Training Loss: 288299.640801789, Test Loss: 14404615.0\n",
      "Epoch [67/100], Training Loss: 286529.1760633256, Test Loss: 14382735.0\n",
      "Epoch [68/100], Training Loss: 284860.4848350216, Test Loss: 14351685.0\n",
      "Epoch [69/100], Training Loss: 283138.78508752445, Test Loss: 14379806.0\n",
      "Epoch [70/100], Training Loss: 281530.2626288431, Test Loss: 14389863.0\n",
      "Epoch [71/100], Training Loss: 279955.7869794443, Test Loss: 14362517.0\n",
      "Epoch [72/100], Training Loss: 278475.43010263017, Test Loss: 14421573.0\n",
      "Epoch [73/100], Training Loss: 276978.9128087791, Test Loss: 14363449.0\n",
      "Epoch [74/100], Training Loss: 275575.4462524436, Test Loss: 14426030.0\n",
      "Epoch [75/100], Training Loss: 274128.2704779042, Test Loss: 14386954.0\n",
      "Epoch [76/100], Training Loss: 272812.81409202656, Test Loss: 14407892.0\n",
      "Epoch [77/100], Training Loss: 271425.49457230023, Test Loss: 14439528.0\n",
      "Epoch [78/100], Training Loss: 270215.5753472839, Test Loss: 14408368.0\n",
      "Epoch [79/100], Training Loss: 268816.22440909897, Test Loss: 14429417.0\n",
      "Epoch [80/100], Training Loss: 267583.8767697411, Test Loss: 14415100.0\n",
      "Epoch [81/100], Training Loss: 266310.7154456193, Test Loss: 14415469.0\n",
      "Epoch [82/100], Training Loss: 265127.12819516024, Test Loss: 14447156.0\n",
      "Epoch [83/100], Training Loss: 263877.5104666489, Test Loss: 14367010.0\n",
      "Epoch [84/100], Training Loss: 262749.0332178188, Test Loss: 14454362.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [85/100], Training Loss: 261587.7625288786, Test Loss: 14428151.0\n",
      "Epoch [86/100], Training Loss: 260513.3622304662, Test Loss: 14445827.0\n",
      "Epoch [87/100], Training Loss: 259375.9897666015, Test Loss: 14453626.0\n",
      "Epoch [88/100], Training Loss: 258364.8235738404, Test Loss: 14493204.0\n",
      "Epoch [89/100], Training Loss: 257351.58966071322, Test Loss: 14411289.0\n",
      "Epoch [90/100], Training Loss: 256377.75759359635, Test Loss: 14493161.0\n",
      "Epoch [91/100], Training Loss: 255326.55268867366, Test Loss: 14473841.0\n",
      "Epoch [92/100], Training Loss: 254411.51234005686, Test Loss: 14486557.0\n",
      "Epoch [93/100], Training Loss: 253422.6415015402, Test Loss: 14490094.0\n",
      "Epoch [94/100], Training Loss: 252584.30557653576, Test Loss: 14543236.0\n",
      "Epoch [95/100], Training Loss: 251694.08571396838, Test Loss: 14466046.0\n",
      "Epoch [96/100], Training Loss: 250901.84400731593, Test Loss: 14531166.0\n",
      "Epoch [97/100], Training Loss: 250049.4308505124, Test Loss: 14527173.0\n",
      "Epoch [98/100], Training Loss: 249244.2395296487, Test Loss: 14510332.0\n",
      "Epoch [99/100], Training Loss: 248441.8342182039, Test Loss: 14532017.0\n",
      "Epoch [100/100], Training Loss: 247729.96006605058, Test Loss: 14577935.0\n",
      "Epoch [1/100], Training Loss: 4413503.770392749, Test Loss: 236000928.0\n",
      "Epoch [2/100], Training Loss: 2731716.5000888575, Test Loss: 146526208.0\n",
      "Epoch [3/100], Training Loss: 2101576.494875896, Test Loss: 127011144.0\n",
      "Epoch [4/100], Training Loss: 1829013.6413719566, Test Loss: 111623152.0\n",
      "Epoch [5/100], Training Loss: 1602146.6654819027, Test Loss: 98765168.0\n",
      "Epoch [6/100], Training Loss: 1409561.0303891948, Test Loss: 87812600.0\n",
      "Epoch [7/100], Training Loss: 1249141.118891061, Test Loss: 79038648.0\n",
      "Epoch [8/100], Training Loss: 1121024.7288667732, Test Loss: 71918248.0\n",
      "Epoch [9/100], Training Loss: 1018431.7912445945, Test Loss: 66140076.0\n",
      "Epoch [10/100], Training Loss: 936118.9764824358, Test Loss: 61439772.0\n",
      "Epoch [11/100], Training Loss: 869313.7929625022, Test Loss: 57585368.0\n",
      "Epoch [12/100], Training Loss: 814325.8550441324, Test Loss: 54367760.0\n",
      "Epoch [13/100], Training Loss: 768002.4974823766, Test Loss: 51612356.0\n",
      "Epoch [14/100], Training Loss: 727716.8485279308, Test Loss: 49176432.0\n",
      "Epoch [15/100], Training Loss: 691793.2575676796, Test Loss: 46987592.0\n",
      "Epoch [16/100], Training Loss: 659188.6725312482, Test Loss: 44989800.0\n",
      "Epoch [17/100], Training Loss: 629323.8603755701, Test Loss: 43150280.0\n",
      "Epoch [18/100], Training Loss: 601897.5031396244, Test Loss: 41442128.0\n",
      "Epoch [19/100], Training Loss: 576486.0775724187, Test Loss: 39841984.0\n",
      "Epoch [20/100], Training Loss: 552768.2013506308, Test Loss: 38339976.0\n",
      "Epoch [21/100], Training Loss: 530771.8035069013, Test Loss: 36941796.0\n",
      "Epoch [22/100], Training Loss: 510482.24791185354, Test Loss: 35643240.0\n",
      "Epoch [23/100], Training Loss: 491617.45971802616, Test Loss: 34435280.0\n",
      "Epoch [24/100], Training Loss: 473979.8246549375, Test Loss: 33301568.0\n",
      "Epoch [25/100], Training Loss: 457382.952727919, Test Loss: 32230242.0\n",
      "Epoch [26/100], Training Loss: 441685.98065872875, Test Loss: 31214514.0\n",
      "Epoch [27/100], Training Loss: 426911.98068834783, Test Loss: 30263728.0\n",
      "Epoch [28/100], Training Loss: 413264.4436644749, Test Loss: 29386576.0\n",
      "Epoch [29/100], Training Loss: 400683.612878384, Test Loss: 28571676.0\n",
      "Epoch [30/100], Training Loss: 389035.8896688585, Test Loss: 27817776.0\n",
      "Epoch [31/100], Training Loss: 378202.4271962561, Test Loss: 27110114.0\n",
      "Epoch [32/100], Training Loss: 367995.2764054262, Test Loss: 26446408.0\n",
      "Epoch [33/100], Training Loss: 358346.7764942835, Test Loss: 25820750.0\n",
      "Epoch [34/100], Training Loss: 349232.1329008945, Test Loss: 25216312.0\n",
      "Epoch [35/100], Training Loss: 340513.18660031987, Test Loss: 24641272.0\n",
      "Epoch [36/100], Training Loss: 332143.9177773829, Test Loss: 24095618.0\n",
      "Epoch [37/100], Training Loss: 324197.63559623243, Test Loss: 23584692.0\n",
      "Epoch [38/100], Training Loss: 316733.5429625022, Test Loss: 23102776.0\n",
      "Epoch [39/100], Training Loss: 309646.22729696106, Test Loss: 22653380.0\n",
      "Epoch [40/100], Training Loss: 302844.7375007405, Test Loss: 22226320.0\n",
      "Epoch [41/100], Training Loss: 296329.86050885613, Test Loss: 21821120.0\n",
      "Epoch [42/100], Training Loss: 290114.0020437178, Test Loss: 21439802.0\n",
      "Epoch [43/100], Training Loss: 284175.85742846987, Test Loss: 21069238.0\n",
      "Epoch [44/100], Training Loss: 278504.8326669036, Test Loss: 20729478.0\n",
      "Epoch [45/100], Training Loss: 273158.4551863041, Test Loss: 20411240.0\n",
      "Epoch [46/100], Training Loss: 268118.362582193, Test Loss: 20120076.0\n",
      "Epoch [47/100], Training Loss: 263354.8717937326, Test Loss: 19840420.0\n",
      "Epoch [48/100], Training Loss: 258819.68655589124, Test Loss: 19581772.0\n",
      "Epoch [49/100], Training Loss: 254494.10276346188, Test Loss: 19335080.0\n",
      "Epoch [50/100], Training Loss: 250354.8541851786, Test Loss: 19110468.0\n",
      "Epoch [51/100], Training Loss: 246385.79213316747, Test Loss: 18893878.0\n",
      "Epoch [52/100], Training Loss: 242586.0581570997, Test Loss: 18697048.0\n",
      "Epoch [53/100], Training Loss: 238928.65717374563, Test Loss: 18505398.0\n",
      "Epoch [54/100], Training Loss: 235378.89229014868, Test Loss: 18325660.0\n",
      "Epoch [55/100], Training Loss: 231936.23830045614, Test Loss: 18151362.0\n",
      "Epoch [56/100], Training Loss: 228588.697174338, Test Loss: 17989816.0\n",
      "Epoch [57/100], Training Loss: 225356.934526983, Test Loss: 17832778.0\n",
      "Epoch [58/100], Training Loss: 222275.09096025117, Test Loss: 17692336.0\n",
      "Epoch [59/100], Training Loss: 219372.06452520585, Test Loss: 17555648.0\n",
      "Epoch [60/100], Training Loss: 216622.9841389728, Test Loss: 17438232.0\n",
      "Epoch [61/100], Training Loss: 214000.32027131095, Test Loss: 17321124.0\n",
      "Epoch [62/100], Training Loss: 211471.2722439429, Test Loss: 17227316.0\n",
      "Epoch [63/100], Training Loss: 209010.12558497718, Test Loss: 17115896.0\n",
      "Epoch [64/100], Training Loss: 206595.40049463895, Test Loss: 17028484.0\n",
      "Epoch [65/100], Training Loss: 204303.14027605, Test Loss: 16933758.0\n",
      "Epoch [66/100], Training Loss: 202088.5083970144, Test Loss: 16857276.0\n",
      "Epoch [67/100], Training Loss: 199934.43725194005, Test Loss: 16770245.0\n",
      "Epoch [68/100], Training Loss: 197858.9792962502, Test Loss: 16703218.0\n",
      "Epoch [69/100], Training Loss: 195930.73322078076, Test Loss: 16633902.0\n",
      "Epoch [70/100], Training Loss: 194091.74660861323, Test Loss: 16570962.0\n",
      "Epoch [71/100], Training Loss: 192344.7697707482, Test Loss: 16513717.0\n",
      "Epoch [72/100], Training Loss: 190636.78117410105, Test Loss: 16460932.0\n",
      "Epoch [73/100], Training Loss: 189014.66302351755, Test Loss: 16402740.0\n",
      "Epoch [74/100], Training Loss: 187419.1088798057, Test Loss: 16351533.0\n",
      "Epoch [75/100], Training Loss: 185884.12568864404, Test Loss: 16291545.0\n",
      "Epoch [76/100], Training Loss: 184354.1810911676, Test Loss: 16233285.0\n",
      "Epoch [77/100], Training Loss: 182861.7307031574, Test Loss: 16179421.0\n",
      "Epoch [78/100], Training Loss: 181384.32957170784, Test Loss: 16104541.0\n",
      "Epoch [79/100], Training Loss: 179972.32481784254, Test Loss: 16078685.0\n",
      "Epoch [80/100], Training Loss: 178617.49383922754, Test Loss: 15997308.0\n",
      "Epoch [81/100], Training Loss: 177307.8872697115, Test Loss: 15970468.0\n",
      "Epoch [82/100], Training Loss: 176028.57394407914, Test Loss: 15903880.0\n",
      "Epoch [83/100], Training Loss: 174804.39928025592, Test Loss: 15873011.0\n",
      "Epoch [84/100], Training Loss: 173618.3892541911, Test Loss: 15818671.0\n",
      "Epoch [85/100], Training Loss: 172499.44011018306, Test Loss: 15785645.0\n",
      "Epoch [86/100], Training Loss: 171392.4299804514, Test Loss: 15739495.0\n",
      "Epoch [87/100], Training Loss: 170328.4291363071, Test Loss: 15692821.0\n",
      "Epoch [88/100], Training Loss: 169267.1250222143, Test Loss: 15671309.0\n",
      "Epoch [89/100], Training Loss: 168233.49284698773, Test Loss: 15619673.0\n",
      "Epoch [90/100], Training Loss: 167160.38550737515, Test Loss: 15556606.0\n",
      "Epoch [91/100], Training Loss: 166124.34784669155, Test Loss: 15553974.0\n",
      "Epoch [92/100], Training Loss: 165109.2852689414, Test Loss: 15494128.0\n",
      "Epoch [93/100], Training Loss: 164125.20316480065, Test Loss: 15465523.0\n",
      "Epoch [94/100], Training Loss: 163180.69862567383, Test Loss: 15428836.0\n",
      "Epoch [95/100], Training Loss: 162269.54111130856, Test Loss: 15393678.0\n",
      "Epoch [96/100], Training Loss: 161381.04214797702, Test Loss: 15377836.0\n",
      "Epoch [97/100], Training Loss: 160529.96737456313, Test Loss: 15320042.0\n",
      "Epoch [98/100], Training Loss: 159670.9235975357, Test Loss: 15322974.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [99/100], Training Loss: 158849.55170013625, Test Loss: 15281472.0\n",
      "Epoch [100/100], Training Loss: 158032.4511729163, Test Loss: 15241399.0\n",
      "Epoch [1/100], Training Loss: 2319312.6388247143, Test Loss: 293079648.0\n",
      "Epoch [2/100], Training Loss: 2091335.1915170902, Test Loss: 234787440.0\n",
      "Epoch [3/100], Training Loss: 1524379.5964694035, Test Loss: 166281568.0\n",
      "Epoch [4/100], Training Loss: 1187301.0406966412, Test Loss: 144227360.0\n",
      "Epoch [5/100], Training Loss: 1066565.387121616, Test Loss: 132411088.0\n",
      "Epoch [6/100], Training Loss: 980656.8873881879, Test Loss: 122495312.0\n",
      "Epoch [7/100], Training Loss: 905019.1988626266, Test Loss: 113571152.0\n",
      "Epoch [8/100], Training Loss: 836262.8543332741, Test Loss: 105492272.0\n",
      "Epoch [9/100], Training Loss: 774020.7428469878, Test Loss: 98259528.0\n",
      "Epoch [10/100], Training Loss: 718359.8876843789, Test Loss: 91869600.0\n",
      "Epoch [11/100], Training Loss: 669090.7954505065, Test Loss: 86243264.0\n",
      "Epoch [12/100], Training Loss: 625608.5661986849, Test Loss: 81267240.0\n",
      "Epoch [13/100], Training Loss: 587217.243528227, Test Loss: 76849312.0\n",
      "Epoch [14/100], Training Loss: 553288.8251880812, Test Loss: 72915456.0\n",
      "Epoch [15/100], Training Loss: 523257.5015698122, Test Loss: 69397760.0\n",
      "Epoch [16/100], Training Loss: 496603.2714886559, Test Loss: 66251392.0\n",
      "Epoch [17/100], Training Loss: 472939.29269593034, Test Loss: 63443496.0\n",
      "Epoch [18/100], Training Loss: 451900.7179669451, Test Loss: 60934548.0\n",
      "Epoch [19/100], Training Loss: 433090.4814880635, Test Loss: 58678568.0\n",
      "Epoch [20/100], Training Loss: 416141.01172916294, Test Loss: 56628652.0\n",
      "Epoch [21/100], Training Loss: 400695.90983946447, Test Loss: 54744520.0\n",
      "Epoch [22/100], Training Loss: 386435.94064332685, Test Loss: 52996492.0\n",
      "Epoch [23/100], Training Loss: 373115.0126769741, Test Loss: 51358172.0\n",
      "Epoch [24/100], Training Loss: 360608.5609264854, Test Loss: 49815256.0\n",
      "Epoch [25/100], Training Loss: 348832.27960428887, Test Loss: 48356192.0\n",
      "Epoch [26/100], Training Loss: 337695.6738344885, Test Loss: 46971552.0\n",
      "Epoch [27/100], Training Loss: 327119.01415792905, Test Loss: 45650912.0\n",
      "Epoch [28/100], Training Loss: 317052.63758071203, Test Loss: 44388136.0\n",
      "Epoch [29/100], Training Loss: 307449.9058705053, Test Loss: 43177592.0\n",
      "Epoch [30/100], Training Loss: 298285.49594218354, Test Loss: 42015188.0\n",
      "Epoch [31/100], Training Loss: 289536.75339138677, Test Loss: 40898224.0\n",
      "Epoch [32/100], Training Loss: 281184.85279308097, Test Loss: 39823888.0\n",
      "Epoch [33/100], Training Loss: 273205.5879983413, Test Loss: 38791516.0\n",
      "Epoch [34/100], Training Loss: 265585.4912031278, Test Loss: 37799440.0\n",
      "Epoch [35/100], Training Loss: 258308.35353355843, Test Loss: 36847420.0\n",
      "Epoch [36/100], Training Loss: 251351.95794088027, Test Loss: 35935304.0\n",
      "Epoch [37/100], Training Loss: 244714.4461820982, Test Loss: 35062252.0\n",
      "Epoch [38/100], Training Loss: 238380.11178247735, Test Loss: 34227524.0\n",
      "Epoch [39/100], Training Loss: 232325.80715005036, Test Loss: 33428934.0\n",
      "Epoch [40/100], Training Loss: 226543.20828149992, Test Loss: 32661864.0\n",
      "Epoch [41/100], Training Loss: 221028.64611101238, Test Loss: 31927738.0\n",
      "Epoch [42/100], Training Loss: 215771.5461761744, Test Loss: 31225552.0\n",
      "Epoch [43/100], Training Loss: 210761.27237722883, Test Loss: 30556102.0\n",
      "Epoch [44/100], Training Loss: 205979.84746164328, Test Loss: 29916434.0\n",
      "Epoch [45/100], Training Loss: 201403.36028671288, Test Loss: 29305954.0\n",
      "Epoch [46/100], Training Loss: 197025.32163378946, Test Loss: 28722356.0\n",
      "Epoch [47/100], Training Loss: 192839.7472306143, Test Loss: 28164238.0\n",
      "Epoch [48/100], Training Loss: 188839.40314554825, Test Loss: 27630222.0\n",
      "Epoch [49/100], Training Loss: 185018.4851608317, Test Loss: 27119386.0\n",
      "Epoch [50/100], Training Loss: 181366.35480717968, Test Loss: 26630880.0\n",
      "Epoch [51/100], Training Loss: 177871.29272554943, Test Loss: 26164540.0\n",
      "Epoch [52/100], Training Loss: 174523.23461287838, Test Loss: 25718644.0\n",
      "Epoch [53/100], Training Loss: 171309.3358213376, Test Loss: 25292030.0\n",
      "Epoch [54/100], Training Loss: 168219.70404596883, Test Loss: 24883650.0\n",
      "Epoch [55/100], Training Loss: 165238.8561696582, Test Loss: 24490714.0\n",
      "Epoch [56/100], Training Loss: 162359.39014276405, Test Loss: 24113574.0\n",
      "Epoch [57/100], Training Loss: 159578.93504531722, Test Loss: 23750500.0\n",
      "Epoch [58/100], Training Loss: 156896.5388010189, Test Loss: 23405734.0\n",
      "Epoch [59/100], Training Loss: 154304.538534447, Test Loss: 23074022.0\n",
      "Epoch [60/100], Training Loss: 151796.93285350394, Test Loss: 22756098.0\n",
      "Epoch [61/100], Training Loss: 149372.1807061193, Test Loss: 22449094.0\n",
      "Epoch [62/100], Training Loss: 147028.03065576684, Test Loss: 22155668.0\n",
      "Epoch [63/100], Training Loss: 144760.30940110184, Test Loss: 21872622.0\n",
      "Epoch [64/100], Training Loss: 142568.38107931995, Test Loss: 21601378.0\n",
      "Epoch [65/100], Training Loss: 140444.8079793851, Test Loss: 21340686.0\n",
      "Epoch [66/100], Training Loss: 138386.24971861858, Test Loss: 21092574.0\n",
      "Epoch [67/100], Training Loss: 136391.21725608673, Test Loss: 20852132.0\n",
      "Epoch [68/100], Training Loss: 134457.43316450447, Test Loss: 20622852.0\n",
      "Epoch [69/100], Training Loss: 132584.83093418635, Test Loss: 20400946.0\n",
      "Epoch [70/100], Training Loss: 130766.90973579764, Test Loss: 20188010.0\n",
      "Epoch [71/100], Training Loss: 129003.34011610686, Test Loss: 19980774.0\n",
      "Epoch [72/100], Training Loss: 127285.48197677862, Test Loss: 19782590.0\n",
      "Epoch [73/100], Training Loss: 125617.26138854334, Test Loss: 19590484.0\n",
      "Epoch [74/100], Training Loss: 123995.4964160891, Test Loss: 19407272.0\n",
      "Epoch [75/100], Training Loss: 122417.63658847225, Test Loss: 19228462.0\n",
      "Epoch [76/100], Training Loss: 120881.39769563415, Test Loss: 19058456.0\n",
      "Epoch [77/100], Training Loss: 119384.84038267875, Test Loss: 18891246.0\n",
      "Epoch [78/100], Training Loss: 117926.48373911498, Test Loss: 18733092.0\n",
      "Epoch [79/100], Training Loss: 116503.61483324447, Test Loss: 18577376.0\n",
      "Epoch [80/100], Training Loss: 115117.16742195368, Test Loss: 18431768.0\n",
      "Epoch [81/100], Training Loss: 113767.37751021859, Test Loss: 18289598.0\n",
      "Epoch [82/100], Training Loss: 112454.91751081098, Test Loss: 18153328.0\n",
      "Epoch [83/100], Training Loss: 111177.68520822226, Test Loss: 18021712.0\n",
      "Epoch [84/100], Training Loss: 109934.44955867543, Test Loss: 17895038.0\n",
      "Epoch [85/100], Training Loss: 108721.36834310764, Test Loss: 17770962.0\n",
      "Epoch [86/100], Training Loss: 107545.27412475564, Test Loss: 17654362.0\n",
      "Epoch [87/100], Training Loss: 106394.01415792904, Test Loss: 17538622.0\n",
      "Epoch [88/100], Training Loss: 105276.75359872046, Test Loss: 17431338.0\n",
      "Epoch [89/100], Training Loss: 104186.66429713879, Test Loss: 17325160.0\n",
      "Epoch [90/100], Training Loss: 103123.21713761034, Test Loss: 17226564.0\n",
      "Epoch [91/100], Training Loss: 102086.19627095551, Test Loss: 17127040.0\n",
      "Epoch [92/100], Training Loss: 101080.80953438778, Test Loss: 17035642.0\n",
      "Epoch [93/100], Training Loss: 100100.60719151709, Test Loss: 16944196.0\n",
      "Epoch [94/100], Training Loss: 99149.89925063681, Test Loss: 16859580.0\n",
      "Epoch [95/100], Training Loss: 98224.40270126177, Test Loss: 16775533.0\n",
      "Epoch [96/100], Training Loss: 97322.67026538712, Test Loss: 16699577.0\n",
      "Epoch [97/100], Training Loss: 96443.75451691251, Test Loss: 16621541.0\n",
      "Epoch [98/100], Training Loss: 95590.2674308394, Test Loss: 16553264.0\n",
      "Epoch [99/100], Training Loss: 94755.5645103963, Test Loss: 16482828.0\n",
      "Epoch [100/100], Training Loss: 93941.66724423908, Test Loss: 16417447.0\n",
      "Epoch [1/100], Training Loss: 1497716.3333925714, Test Loss: 298634784.0\n",
      "Epoch [2/100], Training Loss: 1463913.8705053017, Test Loss: 283193728.0\n",
      "Epoch [3/100], Training Loss: 1321249.99277294, Test Loss: 240505264.0\n",
      "Epoch [4/100], Training Loss: 1072410.8898761922, Test Loss: 188913168.0\n",
      "Epoch [5/100], Training Loss: 861876.8205675019, Test Loss: 159170848.0\n",
      "Epoch [6/100], Training Loss: 758072.4025827853, Test Loss: 145613840.0\n",
      "Epoch [7/100], Training Loss: 701184.7464012796, Test Loss: 136451504.0\n",
      "Epoch [8/100], Training Loss: 656977.5868728156, Test Loss: 128768768.0\n",
      "Epoch [9/100], Training Loss: 618203.059297435, Test Loss: 121958200.0\n",
      "Epoch [10/100], Training Loss: 583087.339849535, Test Loss: 115823120.0\n",
      "Epoch [11/100], Training Loss: 550997.3466026894, Test Loss: 110281320.0\n",
      "Epoch [12/100], Training Loss: 521660.75374681596, Test Loss: 105285152.0\n",
      "Epoch [13/100], Training Loss: 494892.2746282803, Test Loss: 100778344.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/100], Training Loss: 470273.03453586873, Test Loss: 96628208.0\n",
      "Epoch [15/100], Training Loss: 447288.49688999465, Test Loss: 92831808.0\n",
      "Epoch [16/100], Training Loss: 426630.9920028434, Test Loss: 89498976.0\n",
      "Epoch [17/100], Training Loss: 408268.73763402645, Test Loss: 86507656.0\n",
      "Epoch [18/100], Training Loss: 391738.051774184, Test Loss: 83796696.0\n",
      "Epoch [19/100], Training Loss: 376817.2666311237, Test Loss: 81325272.0\n",
      "Epoch [20/100], Training Loss: 363322.7251940051, Test Loss: 79062456.0\n",
      "Epoch [21/100], Training Loss: 351106.7155974172, Test Loss: 76985256.0\n",
      "Epoch [22/100], Training Loss: 340035.94905515073, Test Loss: 75072232.0\n",
      "Epoch [23/100], Training Loss: 329984.6972335762, Test Loss: 73304592.0\n",
      "Epoch [24/100], Training Loss: 320834.86783958296, Test Loss: 71661944.0\n",
      "Epoch [25/100], Training Loss: 312466.51323973696, Test Loss: 70127056.0\n",
      "Epoch [26/100], Training Loss: 304767.6885255613, Test Loss: 68684520.0\n",
      "Epoch [27/100], Training Loss: 297643.1802025946, Test Loss: 67323104.0\n",
      "Epoch [28/100], Training Loss: 291007.2954208874, Test Loss: 66031664.0\n",
      "Epoch [29/100], Training Loss: 284787.05192227947, Test Loss: 64800308.0\n",
      "Epoch [30/100], Training Loss: 278909.6826905989, Test Loss: 63621000.0\n",
      "Epoch [31/100], Training Loss: 273326.81418273505, Test Loss: 62485164.0\n",
      "Epoch [32/100], Training Loss: 267994.25629347994, Test Loss: 61386612.0\n",
      "Epoch [33/100], Training Loss: 262874.3372657314, Test Loss: 60322712.0\n",
      "Epoch [34/100], Training Loss: 257933.64684223387, Test Loss: 59288048.0\n",
      "Epoch [35/100], Training Loss: 253147.48219892188, Test Loss: 58282516.0\n",
      "Epoch [36/100], Training Loss: 248489.56610242283, Test Loss: 57304136.0\n",
      "Epoch [37/100], Training Loss: 243943.81428825305, Test Loss: 56347028.0\n",
      "Epoch [38/100], Training Loss: 239495.79414726616, Test Loss: 55410524.0\n",
      "Epoch [39/100], Training Loss: 235122.26516497837, Test Loss: 54488944.0\n",
      "Epoch [40/100], Training Loss: 230816.45059534386, Test Loss: 53588084.0\n",
      "Epoch [41/100], Training Loss: 226613.71359812806, Test Loss: 52710284.0\n",
      "Epoch [42/100], Training Loss: 222513.47389076476, Test Loss: 51853708.0\n",
      "Epoch [43/100], Training Loss: 218508.34420354245, Test Loss: 51015252.0\n",
      "Epoch [44/100], Training Loss: 214592.09193768143, Test Loss: 50196964.0\n",
      "Epoch [45/100], Training Loss: 210767.67678158876, Test Loss: 49399968.0\n",
      "Epoch [46/100], Training Loss: 207038.65832889048, Test Loss: 48623588.0\n",
      "Epoch [47/100], Training Loss: 203397.50188081275, Test Loss: 47867060.0\n",
      "Epoch [48/100], Training Loss: 199841.33611752858, Test Loss: 47128296.0\n",
      "Epoch [49/100], Training Loss: 196373.4616358628, Test Loss: 46404552.0\n",
      "Epoch [50/100], Training Loss: 192987.97562348202, Test Loss: 45696440.0\n",
      "Epoch [51/100], Training Loss: 189678.7364196434, Test Loss: 45005076.0\n",
      "Epoch [52/100], Training Loss: 186446.02810852436, Test Loss: 44327512.0\n",
      "Epoch [53/100], Training Loss: 183288.05023399089, Test Loss: 43668416.0\n",
      "Epoch [54/100], Training Loss: 180208.23992580417, Test Loss: 43023060.0\n",
      "Epoch [55/100], Training Loss: 177203.74049226943, Test Loss: 42391988.0\n",
      "Epoch [56/100], Training Loss: 174273.25737885787, Test Loss: 41775112.0\n",
      "Epoch [57/100], Training Loss: 171417.90917488598, Test Loss: 41171708.0\n",
      "Epoch [58/100], Training Loss: 168638.8271040667, Test Loss: 40581136.0\n",
      "Epoch [59/100], Training Loss: 165931.5520074344, Test Loss: 40004352.0\n",
      "Epoch [60/100], Training Loss: 163301.0229085214, Test Loss: 39441828.0\n",
      "Epoch [61/100], Training Loss: 160748.0914211984, Test Loss: 38893800.0\n",
      "Epoch [62/100], Training Loss: 158269.31239910994, Test Loss: 38360092.0\n",
      "Epoch [63/100], Training Loss: 155861.95463233444, Test Loss: 37839268.0\n",
      "Epoch [64/100], Training Loss: 153521.15102546872, Test Loss: 37331932.0\n",
      "Epoch [65/100], Training Loss: 151245.1780291776, Test Loss: 36837224.0\n",
      "Epoch [66/100], Training Loss: 149032.33994811104, Test Loss: 36353452.0\n",
      "Epoch [67/100], Training Loss: 146876.86625409345, Test Loss: 35881680.0\n",
      "Epoch [68/100], Training Loss: 144776.07087613063, Test Loss: 35422300.0\n",
      "Epoch [69/100], Training Loss: 142725.60951316793, Test Loss: 34974364.0\n",
      "Epoch [70/100], Training Loss: 140727.38624638107, Test Loss: 34538244.0\n",
      "Epoch [71/100], Training Loss: 138778.42329973937, Test Loss: 34113020.0\n",
      "Epoch [72/100], Training Loss: 136882.6413794771, Test Loss: 33697632.0\n",
      "Epoch [73/100], Training Loss: 135038.98739435468, Test Loss: 33293282.0\n",
      "Epoch [74/100], Training Loss: 133240.5360064107, Test Loss: 32899390.0\n",
      "Epoch [75/100], Training Loss: 131480.26538122093, Test Loss: 32513464.0\n",
      "Epoch [76/100], Training Loss: 129754.14300227974, Test Loss: 32135188.0\n",
      "Epoch [77/100], Training Loss: 128053.15756712428, Test Loss: 31764742.0\n",
      "Epoch [78/100], Training Loss: 126384.72859163964, Test Loss: 31401222.0\n",
      "Epoch [79/100], Training Loss: 124752.53947531618, Test Loss: 31044482.0\n",
      "Epoch [80/100], Training Loss: 123159.60254918621, Test Loss: 30697306.0\n",
      "Epoch [81/100], Training Loss: 121610.35480162609, Test Loss: 30360502.0\n",
      "Epoch [82/100], Training Loss: 120100.63698809867, Test Loss: 30033910.0\n",
      "Epoch [83/100], Training Loss: 118633.70997187111, Test Loss: 29715852.0\n",
      "Epoch [84/100], Training Loss: 117209.15623750445, Test Loss: 29407312.0\n",
      "Epoch [85/100], Training Loss: 115821.06243983621, Test Loss: 29102482.0\n",
      "Epoch [86/100], Training Loss: 114466.96713807313, Test Loss: 28805970.0\n",
      "Epoch [87/100], Training Loss: 113140.40773141771, Test Loss: 28514070.0\n",
      "Epoch [88/100], Training Loss: 111846.73885720262, Test Loss: 28232156.0\n",
      "Epoch [89/100], Training Loss: 110583.13877102955, Test Loss: 27955854.0\n",
      "Epoch [90/100], Training Loss: 109345.89886234894, Test Loss: 27688174.0\n",
      "Epoch [91/100], Training Loss: 108131.20337074596, Test Loss: 27424786.0\n",
      "Epoch [92/100], Training Loss: 106927.51727996713, Test Loss: 27165728.0\n",
      "Epoch [93/100], Training Loss: 105738.99138130517, Test Loss: 26912504.0\n",
      "Epoch [94/100], Training Loss: 104573.57799356525, Test Loss: 26665876.0\n",
      "Epoch [95/100], Training Loss: 103434.12724688629, Test Loss: 26425636.0\n",
      "Epoch [96/100], Training Loss: 102325.65676509464, Test Loss: 26191014.0\n",
      "Epoch [97/100], Training Loss: 101244.83499801923, Test Loss: 25962670.0\n",
      "Epoch [98/100], Training Loss: 100195.49427240685, Test Loss: 25741260.0\n",
      "Epoch [99/100], Training Loss: 99177.07832076373, Test Loss: 25527436.0\n",
      "Epoch [100/100], Training Loss: 98181.31403787913, Test Loss: 25320070.0\n",
      "Epoch [1/100], Training Loss: 1164606.2361234524, Test Loss: 299580032.0\n",
      "Epoch [2/100], Training Loss: 1154504.0507078965, Test Loss: 293435552.0\n",
      "Epoch [3/100], Training Loss: 1105937.5070197263, Test Loss: 273283968.0\n",
      "Epoch [4/100], Training Loss: 995662.7934364078, Test Loss: 237321360.0\n",
      "Epoch [5/100], Training Loss: 841314.9249452046, Test Loss: 196755440.0\n",
      "Epoch [6/100], Training Loss: 701499.3609383331, Test Loss: 167565568.0\n",
      "Epoch [7/100], Training Loss: 616121.4231384397, Test Loss: 152192432.0\n",
      "Epoch [8/100], Training Loss: 570199.5384159706, Test Loss: 143216736.0\n",
      "Epoch [9/100], Training Loss: 539270.6275694568, Test Loss: 136339616.0\n",
      "Epoch [10/100], Training Loss: 513572.8937859132, Test Loss: 130320936.0\n",
      "Epoch [11/100], Training Loss: 490381.53474320244, Test Loss: 124793664.0\n",
      "Epoch [12/100], Training Loss: 468772.80326994846, Test Loss: 119611664.0\n",
      "Epoch [13/100], Training Loss: 448352.54285883537, Test Loss: 114710568.0\n",
      "Epoch [14/100], Training Loss: 428954.6024524614, Test Loss: 110065744.0\n",
      "Epoch [15/100], Training Loss: 410525.2030093004, Test Loss: 105668096.0\n",
      "Epoch [16/100], Training Loss: 392946.612878384, Test Loss: 101448736.0\n",
      "Epoch [17/100], Training Loss: 375965.7342574492, Test Loss: 97412336.0\n",
      "Epoch [18/100], Training Loss: 359885.0568094307, Test Loss: 93661920.0\n",
      "Epoch [19/100], Training Loss: 345059.13062022394, Test Loss: 90233128.0\n",
      "Epoch [20/100], Training Loss: 331426.43611160474, Test Loss: 87057256.0\n",
      "Epoch [21/100], Training Loss: 318672.46466441563, Test Loss: 84077616.0\n",
      "Epoch [22/100], Training Loss: 306748.5370534921, Test Loss: 81297568.0\n",
      "Epoch [23/100], Training Loss: 295794.8174871157, Test Loss: 78751480.0\n",
      "Epoch [24/100], Training Loss: 285813.59931283694, Test Loss: 76410816.0\n",
      "Epoch [25/100], Training Loss: 276645.06581363664, Test Loss: 74236776.0\n",
      "Epoch [26/100], Training Loss: 268162.91250518337, Test Loss: 72208728.0\n",
      "Epoch [27/100], Training Loss: 260296.16491913988, Test Loss: 70318408.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/100], Training Loss: 253007.7987086073, Test Loss: 68554232.0\n",
      "Epoch [29/100], Training Loss: 246242.88869142823, Test Loss: 66905900.0\n",
      "Epoch [30/100], Training Loss: 239938.34073810795, Test Loss: 65364884.0\n",
      "Epoch [31/100], Training Loss: 234051.94775191043, Test Loss: 63918744.0\n",
      "Epoch [32/100], Training Loss: 228551.39363781767, Test Loss: 62560312.0\n",
      "Epoch [33/100], Training Loss: 223399.34589183106, Test Loss: 61281780.0\n",
      "Epoch [34/100], Training Loss: 218553.23914460043, Test Loss: 60075424.0\n",
      "Epoch [35/100], Training Loss: 213980.6605058942, Test Loss: 58938072.0\n",
      "Epoch [36/100], Training Loss: 209651.37065339732, Test Loss: 57856544.0\n",
      "Epoch [37/100], Training Loss: 205535.17504887152, Test Loss: 56825152.0\n",
      "Epoch [38/100], Training Loss: 201606.593922161, Test Loss: 55839596.0\n",
      "Epoch [39/100], Training Loss: 197842.92518215746, Test Loss: 54896440.0\n",
      "Epoch [40/100], Training Loss: 194232.3670398673, Test Loss: 53986484.0\n",
      "Epoch [41/100], Training Loss: 190747.1883182276, Test Loss: 53108148.0\n",
      "Epoch [42/100], Training Loss: 187376.15567798115, Test Loss: 52261072.0\n",
      "Epoch [43/100], Training Loss: 184122.8785024584, Test Loss: 51443168.0\n",
      "Epoch [44/100], Training Loss: 180983.15147206918, Test Loss: 50652548.0\n",
      "Epoch [45/100], Training Loss: 177947.80084118238, Test Loss: 49888640.0\n",
      "Epoch [46/100], Training Loss: 175009.37349683075, Test Loss: 49152136.0\n",
      "Epoch [47/100], Training Loss: 172164.72602333984, Test Loss: 48441980.0\n",
      "Epoch [48/100], Training Loss: 169408.40886203424, Test Loss: 47752656.0\n",
      "Epoch [49/100], Training Loss: 166741.20217996565, Test Loss: 47085772.0\n",
      "Epoch [50/100], Training Loss: 164164.68692612997, Test Loss: 46438180.0\n",
      "Epoch [51/100], Training Loss: 161669.3655589124, Test Loss: 45810124.0\n",
      "Epoch [52/100], Training Loss: 159256.67105029323, Test Loss: 45203116.0\n",
      "Epoch [53/100], Training Loss: 156921.01249925952, Test Loss: 44613828.0\n",
      "Epoch [54/100], Training Loss: 154652.5128843078, Test Loss: 44040036.0\n",
      "Epoch [55/100], Training Loss: 152443.77702742728, Test Loss: 43479488.0\n",
      "Epoch [56/100], Training Loss: 150285.4651975594, Test Loss: 42930996.0\n",
      "Epoch [57/100], Training Loss: 148176.36940939518, Test Loss: 42396024.0\n",
      "Epoch [58/100], Training Loss: 146116.91831052664, Test Loss: 41872256.0\n",
      "Epoch [59/100], Training Loss: 144112.55873467211, Test Loss: 41363472.0\n",
      "Epoch [60/100], Training Loss: 142167.4039452639, Test Loss: 40866612.0\n",
      "Epoch [61/100], Training Loss: 140281.14874711214, Test Loss: 40384616.0\n",
      "Epoch [62/100], Training Loss: 138450.62851726793, Test Loss: 39918664.0\n",
      "Epoch [63/100], Training Loss: 136680.8632190036, Test Loss: 39466876.0\n",
      "Epoch [64/100], Training Loss: 134967.50642734434, Test Loss: 39028196.0\n",
      "Epoch [65/100], Training Loss: 133302.61708429595, Test Loss: 38599780.0\n",
      "Epoch [66/100], Training Loss: 131680.6326047035, Test Loss: 38181344.0\n",
      "Epoch [67/100], Training Loss: 130100.9796812985, Test Loss: 37773308.0\n",
      "Epoch [68/100], Training Loss: 128558.39346010308, Test Loss: 37376120.0\n",
      "Epoch [69/100], Training Loss: 127054.15905455837, Test Loss: 36988888.0\n",
      "Epoch [70/100], Training Loss: 125586.58971624904, Test Loss: 36609024.0\n",
      "Epoch [71/100], Training Loss: 124151.5606895326, Test Loss: 36236212.0\n",
      "Epoch [72/100], Training Loss: 122746.19513062022, Test Loss: 35870564.0\n",
      "Epoch [73/100], Training Loss: 121370.67075410225, Test Loss: 35513372.0\n",
      "Epoch [74/100], Training Loss: 120026.29897517919, Test Loss: 35164708.0\n",
      "Epoch [75/100], Training Loss: 118712.87708074166, Test Loss: 34823200.0\n",
      "Epoch [76/100], Training Loss: 117429.01634974232, Test Loss: 34488992.0\n",
      "Epoch [77/100], Training Loss: 116175.59090101297, Test Loss: 34161256.0\n",
      "Epoch [78/100], Training Loss: 114954.85516260884, Test Loss: 33842948.0\n",
      "Epoch [79/100], Training Loss: 113768.24589775487, Test Loss: 33534244.0\n",
      "Epoch [80/100], Training Loss: 112616.69646347965, Test Loss: 33235808.0\n",
      "Epoch [81/100], Training Loss: 111499.25786387062, Test Loss: 32944122.0\n",
      "Epoch [82/100], Training Loss: 110410.04656122268, Test Loss: 32661552.0\n",
      "Epoch [83/100], Training Loss: 109353.37622178781, Test Loss: 32387666.0\n",
      "Epoch [84/100], Training Loss: 108324.38955038208, Test Loss: 32122064.0\n",
      "Epoch [85/100], Training Loss: 107320.13932823885, Test Loss: 31863404.0\n",
      "Epoch [86/100], Training Loss: 106338.89597772644, Test Loss: 31610710.0\n",
      "Epoch [87/100], Training Loss: 105378.95432735028, Test Loss: 31362154.0\n",
      "Epoch [88/100], Training Loss: 104435.3082163379, Test Loss: 31116546.0\n",
      "Epoch [89/100], Training Loss: 103509.34719507139, Test Loss: 30876744.0\n",
      "Epoch [90/100], Training Loss: 102598.06735382976, Test Loss: 30640930.0\n",
      "Epoch [91/100], Training Loss: 101701.63029441384, Test Loss: 30408696.0\n",
      "Epoch [92/100], Training Loss: 100817.70872578639, Test Loss: 30178624.0\n",
      "Epoch [93/100], Training Loss: 99944.80475090338, Test Loss: 29952532.0\n",
      "Epoch [94/100], Training Loss: 99076.96824832652, Test Loss: 29726756.0\n",
      "Epoch [95/100], Training Loss: 98211.72531248149, Test Loss: 29503614.0\n",
      "Epoch [96/100], Training Loss: 97352.29358450329, Test Loss: 29279812.0\n",
      "Epoch [97/100], Training Loss: 96503.58770215034, Test Loss: 29062774.0\n",
      "Epoch [98/100], Training Loss: 95665.78822344648, Test Loss: 28846762.0\n",
      "Epoch [99/100], Training Loss: 94842.70404596884, Test Loss: 28637174.0\n",
      "Epoch [100/100], Training Loss: 94033.95711154552, Test Loss: 28431526.0\n",
      "Epoch [1/100], Training Loss: 582432.2833955334, Test Loss: 300047552.0\n",
      "Epoch [2/100], Training Loss: 582015.5905455837, Test Loss: 299544928.0\n",
      "Epoch [3/100], Training Loss: 579914.9130975653, Test Loss: 297658816.0\n",
      "Epoch [4/100], Training Loss: 574001.4814288253, Test Loss: 293196576.0\n",
      "Epoch [5/100], Training Loss: 561990.3816124637, Test Loss: 285069216.0\n",
      "Epoch [6/100], Training Loss: 542121.0279011907, Test Loss: 272627744.0\n",
      "Epoch [7/100], Training Loss: 513797.6072507553, Test Loss: 255974960.0\n",
      "Epoch [8/100], Training Loss: 478116.02630175935, Test Loss: 236169024.0\n",
      "Epoch [9/100], Training Loss: 438032.7791007642, Test Loss: 215159696.0\n",
      "Epoch [10/100], Training Loss: 397861.34138972807, Test Loss: 195323456.0\n",
      "Epoch [11/100], Training Loss: 362011.1988626266, Test Loss: 178658560.0\n",
      "Epoch [12/100], Training Loss: 333334.89959125646, Test Loss: 165994144.0\n",
      "Epoch [13/100], Training Loss: 312140.2566198685, Test Loss: 156873056.0\n",
      "Epoch [14/100], Training Loss: 296789.9721580475, Test Loss: 150211936.0\n",
      "Epoch [15/100], Training Loss: 285192.8146436823, Test Loss: 145023776.0\n",
      "Epoch [16/100], Training Loss: 275769.3828564659, Test Loss: 140671360.0\n",
      "Epoch [17/100], Training Loss: 267587.9329423612, Test Loss: 136807632.0\n",
      "Epoch [18/100], Training Loss: 260159.84266334932, Test Loss: 133255008.0\n",
      "Epoch [19/100], Training Loss: 253233.67952135537, Test Loss: 129920240.0\n",
      "Epoch [20/100], Training Loss: 246672.58811681773, Test Loss: 126750416.0\n",
      "Epoch [21/100], Training Loss: 240395.6675552396, Test Loss: 123713248.0\n",
      "Epoch [22/100], Training Loss: 234351.9597180262, Test Loss: 120787960.0\n",
      "Epoch [23/100], Training Loss: 228508.4447603815, Test Loss: 117961080.0\n",
      "Epoch [24/100], Training Loss: 222843.65807712814, Test Loss: 115223848.0\n",
      "Epoch [25/100], Training Loss: 217343.93625970025, Test Loss: 112570184.0\n",
      "Epoch [26/100], Training Loss: 211998.21645637107, Test Loss: 109993936.0\n",
      "Epoch [27/100], Training Loss: 206787.93412712516, Test Loss: 107478184.0\n",
      "Epoch [28/100], Training Loss: 201668.9132160417, Test Loss: 105002912.0\n",
      "Epoch [29/100], Training Loss: 196622.39393400864, Test Loss: 102576368.0\n",
      "Epoch [30/100], Training Loss: 191664.87340797347, Test Loss: 100202152.0\n",
      "Epoch [31/100], Training Loss: 186808.95065458206, Test Loss: 97888816.0\n",
      "Epoch [32/100], Training Loss: 182099.236301167, Test Loss: 95668896.0\n",
      "Epoch [33/100], Training Loss: 177589.7878087791, Test Loss: 93549152.0\n",
      "Epoch [34/100], Training Loss: 173295.56163734375, Test Loss: 91541144.0\n",
      "Epoch [35/100], Training Loss: 169227.70523073277, Test Loss: 89639192.0\n",
      "Epoch [36/100], Training Loss: 165377.15846217642, Test Loss: 87841576.0\n",
      "Epoch [37/100], Training Loss: 161735.37776198093, Test Loss: 86136832.0\n",
      "Epoch [38/100], Training Loss: 158277.86481843493, Test Loss: 84515680.0\n",
      "Epoch [39/100], Training Loss: 154991.67229429536, Test Loss: 82969552.0\n",
      "Epoch [40/100], Training Loss: 151862.55482495113, Test Loss: 81494784.0\n",
      "Epoch [41/100], Training Loss: 148882.23185830223, Test Loss: 80085720.0\n",
      "Epoch [42/100], Training Loss: 146040.2751021859, Test Loss: 78737928.0\n",
      "Epoch [43/100], Training Loss: 143328.3502162194, Test Loss: 77447792.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/100], Training Loss: 140736.68052840472, Test Loss: 76210560.0\n",
      "Epoch [45/100], Training Loss: 138257.46768556366, Test Loss: 75021640.0\n",
      "Epoch [46/100], Training Loss: 135884.4871749304, Test Loss: 73880040.0\n",
      "Epoch [47/100], Training Loss: 133611.09886855044, Test Loss: 72782464.0\n",
      "Epoch [48/100], Training Loss: 131430.08708014927, Test Loss: 71725992.0\n",
      "Epoch [49/100], Training Loss: 129337.10088264913, Test Loss: 70707944.0\n",
      "Epoch [50/100], Training Loss: 127327.85640661098, Test Loss: 69725632.0\n",
      "Epoch [51/100], Training Loss: 125395.7102067413, Test Loss: 68777640.0\n",
      "Epoch [52/100], Training Loss: 123537.77098513121, Test Loss: 67863704.0\n",
      "Epoch [53/100], Training Loss: 121751.78129257745, Test Loss: 66982192.0\n",
      "Epoch [54/100], Training Loss: 120032.69048042178, Test Loss: 66130132.0\n",
      "Epoch [55/100], Training Loss: 118376.44357561755, Test Loss: 65306548.0\n",
      "Epoch [56/100], Training Loss: 116779.70096558261, Test Loss: 64508780.0\n",
      "Epoch [57/100], Training Loss: 115238.8173686393, Test Loss: 63736624.0\n",
      "Epoch [58/100], Training Loss: 113750.36052366566, Test Loss: 62987352.0\n",
      "Epoch [59/100], Training Loss: 112310.10911675848, Test Loss: 62259532.0\n",
      "Epoch [60/100], Training Loss: 110915.2085776909, Test Loss: 61552656.0\n",
      "Epoch [61/100], Training Loss: 109562.41383804278, Test Loss: 60865588.0\n",
      "Epoch [62/100], Training Loss: 108248.13897280967, Test Loss: 60197480.0\n",
      "Epoch [63/100], Training Loss: 106966.38090160536, Test Loss: 59547044.0\n",
      "Epoch [64/100], Training Loss: 105717.52597594929, Test Loss: 58913716.0\n",
      "Epoch [65/100], Training Loss: 104499.4354599846, Test Loss: 58297396.0\n",
      "Epoch [66/100], Training Loss: 103311.1512351164, Test Loss: 57695428.0\n",
      "Epoch [67/100], Training Loss: 102151.80356613945, Test Loss: 57108156.0\n",
      "Epoch [68/100], Training Loss: 101019.79882708371, Test Loss: 56534532.0\n",
      "Epoch [69/100], Training Loss: 99913.88993543037, Test Loss: 55973400.0\n",
      "Epoch [70/100], Training Loss: 98832.26147740062, Test Loss: 55423592.0\n",
      "Epoch [71/100], Training Loss: 97774.7711628458, Test Loss: 54886120.0\n",
      "Epoch [72/100], Training Loss: 96741.32871275398, Test Loss: 54360344.0\n",
      "Epoch [73/100], Training Loss: 95730.49463894319, Test Loss: 53845744.0\n",
      "Epoch [74/100], Training Loss: 94739.98021444227, Test Loss: 53341976.0\n",
      "Epoch [75/100], Training Loss: 93767.78520229844, Test Loss: 52847324.0\n",
      "Epoch [76/100], Training Loss: 92814.31277767905, Test Loss: 52362200.0\n",
      "Epoch [77/100], Training Loss: 91879.84669154671, Test Loss: 51887064.0\n",
      "Epoch [78/100], Training Loss: 90962.52331023043, Test Loss: 51420028.0\n",
      "Epoch [79/100], Training Loss: 90062.60257093774, Test Loss: 50960520.0\n",
      "Epoch [80/100], Training Loss: 89180.27995971803, Test Loss: 50511236.0\n",
      "Epoch [81/100], Training Loss: 88315.52988567029, Test Loss: 50071860.0\n",
      "Epoch [82/100], Training Loss: 87468.94567857354, Test Loss: 49641792.0\n",
      "Epoch [83/100], Training Loss: 86640.64699958533, Test Loss: 49220852.0\n",
      "Epoch [84/100], Training Loss: 85829.6162549612, Test Loss: 48808300.0\n",
      "Epoch [85/100], Training Loss: 85034.21384989041, Test Loss: 48403136.0\n",
      "Epoch [86/100], Training Loss: 84253.3378354363, Test Loss: 48005160.0\n",
      "Epoch [87/100], Training Loss: 83485.4558379243, Test Loss: 47614216.0\n",
      "Epoch [88/100], Training Loss: 82728.50174752681, Test Loss: 47229596.0\n",
      "Epoch [89/100], Training Loss: 81982.35175641254, Test Loss: 46849956.0\n",
      "Epoch [90/100], Training Loss: 81247.21343522304, Test Loss: 46475456.0\n",
      "Epoch [91/100], Training Loss: 80522.91345299449, Test Loss: 46106096.0\n",
      "Epoch [92/100], Training Loss: 79809.2473194716, Test Loss: 45742012.0\n",
      "Epoch [93/100], Training Loss: 79105.04401398021, Test Loss: 45382808.0\n",
      "Epoch [94/100], Training Loss: 78409.77335465909, Test Loss: 45028304.0\n",
      "Epoch [95/100], Training Loss: 77724.65209407026, Test Loss: 44679936.0\n",
      "Epoch [96/100], Training Loss: 77052.3208340738, Test Loss: 44338116.0\n",
      "Epoch [97/100], Training Loss: 76393.32041940643, Test Loss: 44002204.0\n",
      "Epoch [98/100], Training Loss: 75747.72631953083, Test Loss: 43672284.0\n",
      "Epoch [99/100], Training Loss: 75116.56240744032, Test Loss: 43348792.0\n",
      "Epoch [100/100], Training Loss: 74499.79586517386, Test Loss: 43031920.0\n",
      "Epoch [1/100], Training Loss: 7155335.306913097, Test Loss: 148569344.0\n",
      "Epoch [2/100], Training Loss: 4067944.8558734674, Test Loss: 117529032.0\n",
      "Epoch [3/100], Training Loss: 3246905.67312363, Test Loss: 95363664.0\n",
      "Epoch [4/100], Training Loss: 2631838.6839642203, Test Loss: 79464040.0\n",
      "Epoch [5/100], Training Loss: 2199912.221847047, Test Loss: 68261568.0\n",
      "Epoch [6/100], Training Loss: 1902194.4669747052, Test Loss: 60480260.0\n",
      "Epoch [7/100], Training Loss: 1698693.1930572833, Test Loss: 55029220.0\n",
      "Epoch [8/100], Training Loss: 1552829.2649724542, Test Loss: 50950220.0\n",
      "Epoch [9/100], Training Loss: 1438733.3390202003, Test Loss: 47628948.0\n",
      "Epoch [10/100], Training Loss: 1343726.38075351, Test Loss: 44853944.0\n",
      "Epoch [11/100], Training Loss: 1263805.6783365915, Test Loss: 42482060.0\n",
      "Epoch [12/100], Training Loss: 1195425.3409454415, Test Loss: 40409900.0\n",
      "Epoch [13/100], Training Loss: 1135448.7286890587, Test Loss: 38594428.0\n",
      "Epoch [14/100], Training Loss: 1083077.71977371, Test Loss: 37017084.0\n",
      "Epoch [15/100], Training Loss: 1037153.145696345, Test Loss: 35623772.0\n",
      "Epoch [16/100], Training Loss: 996981.5121734494, Test Loss: 34381360.0\n",
      "Epoch [17/100], Training Loss: 959906.9080030804, Test Loss: 33194302.0\n",
      "Epoch [18/100], Training Loss: 926214.3129553937, Test Loss: 32167060.0\n",
      "Epoch [19/100], Training Loss: 895186.5927966352, Test Loss: 31143686.0\n",
      "Epoch [20/100], Training Loss: 864769.2182038978, Test Loss: 30227700.0\n",
      "Epoch [21/100], Training Loss: 837799.1161068657, Test Loss: 29381200.0\n",
      "Epoch [22/100], Training Loss: 811835.2989751792, Test Loss: 28607612.0\n",
      "Epoch [23/100], Training Loss: 788395.2156862745, Test Loss: 27891246.0\n",
      "Epoch [24/100], Training Loss: 765737.790474498, Test Loss: 27209112.0\n",
      "Epoch [25/100], Training Loss: 744097.4149931876, Test Loss: 26578352.0\n",
      "Epoch [26/100], Training Loss: 723904.773028849, Test Loss: 25972600.0\n",
      "Epoch [27/100], Training Loss: 703985.3775842664, Test Loss: 25367914.0\n",
      "Epoch [28/100], Training Loss: 683640.4787186778, Test Loss: 24754016.0\n",
      "Epoch [29/100], Training Loss: 664746.4739203838, Test Loss: 24205824.0\n",
      "Epoch [30/100], Training Loss: 647237.8308897577, Test Loss: 23686604.0\n",
      "Epoch [31/100], Training Loss: 629111.0411113086, Test Loss: 23123692.0\n",
      "Epoch [32/100], Training Loss: 610847.366358628, Test Loss: 22595024.0\n",
      "Epoch [33/100], Training Loss: 594044.2076743084, Test Loss: 22132152.0\n",
      "Epoch [34/100], Training Loss: 579052.7176855636, Test Loss: 21723762.0\n",
      "Epoch [35/100], Training Loss: 565520.1317901784, Test Loss: 21357406.0\n",
      "Epoch [36/100], Training Loss: 552502.2377969314, Test Loss: 21006496.0\n",
      "Epoch [37/100], Training Loss: 540015.3431816836, Test Loss: 20660738.0\n",
      "Epoch [38/100], Training Loss: 527574.4061666963, Test Loss: 20327806.0\n",
      "Epoch [39/100], Training Loss: 515447.4791333452, Test Loss: 20008554.0\n",
      "Epoch [40/100], Training Loss: 503973.8754665008, Test Loss: 19721292.0\n",
      "Epoch [41/100], Training Loss: 492927.36193057284, Test Loss: 19422526.0\n",
      "Epoch [42/100], Training Loss: 481673.6781440673, Test Loss: 19141754.0\n",
      "Epoch [43/100], Training Loss: 470438.8831008234, Test Loss: 18833998.0\n",
      "Epoch [44/100], Training Loss: 460464.9519207985, Test Loss: 18625944.0\n",
      "Epoch [45/100], Training Loss: 451143.46300574613, Test Loss: 18383410.0\n",
      "Epoch [46/100], Training Loss: 441949.499311356, Test Loss: 18183352.0\n",
      "Epoch [47/100], Training Loss: 433274.649539423, Test Loss: 17951524.0\n",
      "Epoch [48/100], Training Loss: 425055.2758352586, Test Loss: 17799208.0\n",
      "Epoch [49/100], Training Loss: 417345.74951128487, Test Loss: 17567464.0\n",
      "Epoch [50/100], Training Loss: 410146.99258041586, Test Loss: 17455786.0\n",
      "Epoch [51/100], Training Loss: 403468.90122771164, Test Loss: 17263918.0\n",
      "Epoch [52/100], Training Loss: 397214.1640824003, Test Loss: 17150890.0\n",
      "Epoch [53/100], Training Loss: 391337.25450950774, Test Loss: 16976356.0\n",
      "Epoch [54/100], Training Loss: 385766.76549078844, Test Loss: 16878178.0\n",
      "Epoch [55/100], Training Loss: 380489.259722469, Test Loss: 16740446.0\n",
      "Epoch [56/100], Training Loss: 375368.152234761, Test Loss: 16604311.0\n",
      "Epoch [57/100], Training Loss: 370439.1063769919, Test Loss: 16511091.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [58/100], Training Loss: 365711.7144644867, Test Loss: 16409527.0\n",
      "Epoch [59/100], Training Loss: 361245.73425004445, Test Loss: 16290323.0\n",
      "Epoch [60/100], Training Loss: 356984.8397014395, Test Loss: 16212044.0\n",
      "Epoch [61/100], Training Loss: 352948.857161898, Test Loss: 16080922.0\n",
      "Epoch [62/100], Training Loss: 349118.6795139506, Test Loss: 16058326.0\n",
      "Epoch [63/100], Training Loss: 345543.44601919316, Test Loss: 15936050.0\n",
      "Epoch [64/100], Training Loss: 342036.971780404, Test Loss: 15905934.0\n",
      "Epoch [65/100], Training Loss: 338760.12595521595, Test Loss: 15790039.0\n",
      "Epoch [66/100], Training Loss: 335564.67236093833, Test Loss: 15802430.0\n",
      "Epoch [67/100], Training Loss: 332559.3665437474, Test Loss: 15689736.0\n",
      "Epoch [68/100], Training Loss: 329657.4062777679, Test Loss: 15695280.0\n",
      "Epoch [69/100], Training Loss: 326924.7581600616, Test Loss: 15580932.0\n",
      "Epoch [70/100], Training Loss: 324279.83344440494, Test Loss: 15612272.0\n",
      "Epoch [71/100], Training Loss: 321784.52811592916, Test Loss: 15511106.0\n",
      "Epoch [72/100], Training Loss: 319318.9359190806, Test Loss: 15531046.0\n",
      "Epoch [73/100], Training Loss: 317012.78385462944, Test Loss: 15442851.0\n",
      "Epoch [74/100], Training Loss: 314749.5680424738, Test Loss: 15466503.0\n",
      "Epoch [75/100], Training Loss: 312584.4968307565, Test Loss: 15430716.0\n",
      "Epoch [76/100], Training Loss: 310466.32856465847, Test Loss: 15391893.0\n",
      "Epoch [77/100], Training Loss: 308417.76312126056, Test Loss: 15379080.0\n",
      "Epoch [78/100], Training Loss: 306448.0912638469, Test Loss: 15376609.0\n",
      "Epoch [79/100], Training Loss: 304533.67582637287, Test Loss: 15332670.0\n",
      "Epoch [80/100], Training Loss: 302662.28777175525, Test Loss: 15355796.0\n",
      "Epoch [81/100], Training Loss: 300846.6757004917, Test Loss: 15284120.0\n",
      "Epoch [82/100], Training Loss: 299007.46972928144, Test Loss: 15289733.0\n",
      "Epoch [83/100], Training Loss: 297285.60158610274, Test Loss: 15312142.0\n",
      "Epoch [84/100], Training Loss: 295578.87297849654, Test Loss: 15281118.0\n",
      "Epoch [85/100], Training Loss: 293893.1409795036, Test Loss: 15260315.0\n",
      "Epoch [86/100], Training Loss: 292244.6549671228, Test Loss: 15255609.0\n",
      "Epoch [87/100], Training Loss: 290659.88196789287, Test Loss: 15235603.0\n",
      "Epoch [88/100], Training Loss: 289114.0940258278, Test Loss: 15274570.0\n",
      "Epoch [89/100], Training Loss: 287668.32689117943, Test Loss: 15198006.0\n",
      "Epoch [90/100], Training Loss: 286208.28173686395, Test Loss: 15263119.0\n",
      "Epoch [91/100], Training Loss: 284842.0845625259, Test Loss: 15214592.0\n",
      "Epoch [92/100], Training Loss: 283439.9589701439, Test Loss: 15234021.0\n",
      "Epoch [93/100], Training Loss: 282113.3323188792, Test Loss: 15240181.0\n",
      "Epoch [94/100], Training Loss: 280777.7223579764, Test Loss: 15206296.0\n",
      "Epoch [95/100], Training Loss: 279416.20480125584, Test Loss: 15232761.0\n",
      "Epoch [96/100], Training Loss: 278071.13822862983, Test Loss: 15226945.0\n",
      "Epoch [97/100], Training Loss: 276780.79729429533, Test Loss: 15214554.0\n",
      "Epoch [98/100], Training Loss: 275509.51974853384, Test Loss: 15262596.0\n",
      "Epoch [99/100], Training Loss: 274313.44780744624, Test Loss: 15192766.0\n",
      "Epoch [100/100], Training Loss: 273099.0964694035, Test Loss: 15274144.0\n",
      "Epoch [1/100], Training Loss: 4371245.175522777, Test Loss: 226612432.0\n",
      "Epoch [2/100], Training Loss: 2631501.9582963097, Test Loss: 143619824.0\n",
      "Epoch [3/100], Training Loss: 2054474.0459688406, Test Loss: 123811568.0\n",
      "Epoch [4/100], Training Loss: 1773909.8574728984, Test Loss: 107986160.0\n",
      "Epoch [5/100], Training Loss: 1538140.305787572, Test Loss: 94555848.0\n",
      "Epoch [6/100], Training Loss: 1344283.9274924472, Test Loss: 84026960.0\n",
      "Epoch [7/100], Training Loss: 1190283.4846869262, Test Loss: 75527192.0\n",
      "Epoch [8/100], Training Loss: 1067588.7155974172, Test Loss: 68707928.0\n",
      "Epoch [9/100], Training Loss: 970587.2755760915, Test Loss: 63234400.0\n",
      "Epoch [10/100], Training Loss: 893222.9472187667, Test Loss: 58817784.0\n",
      "Epoch [11/100], Training Loss: 830731.0598898169, Test Loss: 55199608.0\n",
      "Epoch [12/100], Training Loss: 779142.4484923879, Test Loss: 52160524.0\n",
      "Epoch [13/100], Training Loss: 734950.2505775724, Test Loss: 49503268.0\n",
      "Epoch [14/100], Training Loss: 695871.9703809016, Test Loss: 47130068.0\n",
      "Epoch [15/100], Training Loss: 660709.9094247972, Test Loss: 44981180.0\n",
      "Epoch [16/100], Training Loss: 628592.3329779041, Test Loss: 43009828.0\n",
      "Epoch [17/100], Training Loss: 599236.865736627, Test Loss: 41194676.0\n",
      "Epoch [18/100], Training Loss: 572502.7919554529, Test Loss: 39525120.0\n",
      "Epoch [19/100], Training Loss: 548120.397932587, Test Loss: 37984644.0\n",
      "Epoch [20/100], Training Loss: 525663.9508322966, Test Loss: 36551064.0\n",
      "Epoch [21/100], Training Loss: 504803.73014039453, Test Loss: 35223684.0\n",
      "Epoch [22/100], Training Loss: 485677.96596765594, Test Loss: 34011996.0\n",
      "Epoch [23/100], Training Loss: 468150.17282743915, Test Loss: 32894596.0\n",
      "Epoch [24/100], Training Loss: 451988.6263254547, Test Loss: 31860028.0\n",
      "Epoch [25/100], Training Loss: 437096.4157929033, Test Loss: 30904456.0\n",
      "Epoch [26/100], Training Loss: 423439.77208103787, Test Loss: 30024056.0\n",
      "Epoch [27/100], Training Loss: 410693.92784787633, Test Loss: 29197128.0\n",
      "Epoch [28/100], Training Loss: 398591.57863870624, Test Loss: 28400368.0\n",
      "Epoch [29/100], Training Loss: 386923.2933771696, Test Loss: 27638542.0\n",
      "Epoch [30/100], Training Loss: 375978.73493868846, Test Loss: 26934694.0\n",
      "Epoch [31/100], Training Loss: 365858.35516260884, Test Loss: 26282792.0\n",
      "Epoch [32/100], Training Loss: 356456.9481369587, Test Loss: 25678212.0\n",
      "Epoch [33/100], Training Loss: 347710.5332326284, Test Loss: 25115366.0\n",
      "Epoch [34/100], Training Loss: 339462.15236064215, Test Loss: 24581982.0\n",
      "Epoch [35/100], Training Loss: 331580.919317576, Test Loss: 24073710.0\n",
      "Epoch [36/100], Training Loss: 323937.7910076417, Test Loss: 23577388.0\n",
      "Epoch [37/100], Training Loss: 316617.760085303, Test Loss: 23123484.0\n",
      "Epoch [38/100], Training Loss: 309680.9815176826, Test Loss: 22680860.0\n",
      "Epoch [39/100], Training Loss: 302884.27930809784, Test Loss: 22256866.0\n",
      "Epoch [40/100], Training Loss: 296307.8999170665, Test Loss: 21856950.0\n",
      "Epoch [41/100], Training Loss: 290202.7706741307, Test Loss: 21494410.0\n",
      "Epoch [42/100], Training Loss: 284511.29747941473, Test Loss: 21155038.0\n",
      "Epoch [43/100], Training Loss: 279117.4582222617, Test Loss: 20839536.0\n",
      "Epoch [44/100], Training Loss: 273942.66577809374, Test Loss: 20535258.0\n",
      "Epoch [45/100], Training Loss: 268851.15068716306, Test Loss: 20244910.0\n",
      "Epoch [46/100], Training Loss: 263850.35116403055, Test Loss: 19960366.0\n",
      "Epoch [47/100], Training Loss: 259004.86181209644, Test Loss: 19694592.0\n",
      "Epoch [48/100], Training Loss: 254330.37299330608, Test Loss: 19435958.0\n",
      "Epoch [49/100], Training Loss: 249882.82828327705, Test Loss: 19200818.0\n",
      "Epoch [50/100], Training Loss: 245699.50088857295, Test Loss: 18980482.0\n",
      "Epoch [51/100], Training Loss: 241584.0714560749, Test Loss: 18758010.0\n",
      "Epoch [52/100], Training Loss: 237446.31578401753, Test Loss: 18549138.0\n",
      "Epoch [53/100], Training Loss: 233622.28829749423, Test Loss: 18364476.0\n",
      "Epoch [54/100], Training Loss: 230074.87999822287, Test Loss: 18194150.0\n",
      "Epoch [55/100], Training Loss: 226724.02849357267, Test Loss: 18033134.0\n",
      "Epoch [56/100], Training Loss: 223504.32418103193, Test Loss: 17885128.0\n",
      "Epoch [57/100], Training Loss: 220420.67487115692, Test Loss: 17743258.0\n",
      "Epoch [58/100], Training Loss: 217464.07598779694, Test Loss: 17607990.0\n",
      "Epoch [59/100], Training Loss: 214681.82240388604, Test Loss: 17496434.0\n",
      "Epoch [60/100], Training Loss: 212062.41524494995, Test Loss: 17377978.0\n",
      "Epoch [61/100], Training Loss: 209521.6418458622, Test Loss: 17278134.0\n",
      "Epoch [62/100], Training Loss: 207050.7906966412, Test Loss: 17171820.0\n",
      "Epoch [63/100], Training Loss: 204654.92321248743, Test Loss: 17074898.0\n",
      "Epoch [64/100], Training Loss: 202376.54257745395, Test Loss: 16987158.0\n",
      "Epoch [65/100], Training Loss: 200240.19390142764, Test Loss: 16901858.0\n",
      "Epoch [66/100], Training Loss: 198167.04472483858, Test Loss: 16829368.0\n",
      "Epoch [67/100], Training Loss: 196126.9228422487, Test Loss: 16741172.0\n",
      "Epoch [68/100], Training Loss: 194138.82650613115, Test Loss: 16667106.0\n",
      "Epoch [69/100], Training Loss: 192261.25007404774, Test Loss: 16591686.0\n",
      "Epoch [70/100], Training Loss: 190479.20083229666, Test Loss: 16522800.0\n",
      "Epoch [71/100], Training Loss: 188732.0923227297, Test Loss: 16447252.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [72/100], Training Loss: 187024.3828416563, Test Loss: 16385500.0\n",
      "Epoch [73/100], Training Loss: 185354.67421953677, Test Loss: 16306629.0\n",
      "Epoch [74/100], Training Loss: 183690.65487826552, Test Loss: 16250065.0\n",
      "Epoch [75/100], Training Loss: 182067.6758930158, Test Loss: 16143329.0\n",
      "Epoch [76/100], Training Loss: 180504.63159765417, Test Loss: 16134357.0\n",
      "Epoch [77/100], Training Loss: 179036.7989455601, Test Loss: 16021395.0\n",
      "Epoch [78/100], Training Loss: 177626.2747467567, Test Loss: 16002658.0\n",
      "Epoch [79/100], Training Loss: 176285.6299834133, Test Loss: 15914385.0\n",
      "Epoch [80/100], Training Loss: 174978.33559919437, Test Loss: 15881280.0\n",
      "Epoch [81/100], Training Loss: 173681.2106510278, Test Loss: 15804883.0\n",
      "Epoch [82/100], Training Loss: 172415.58282980867, Test Loss: 15771285.0\n",
      "Epoch [83/100], Training Loss: 171184.70262425212, Test Loss: 15710837.0\n",
      "Epoch [84/100], Training Loss: 169999.8036401872, Test Loss: 15662811.0\n",
      "Epoch [85/100], Training Loss: 168842.38267875125, Test Loss: 15616214.0\n",
      "Epoch [86/100], Training Loss: 167745.10307446241, Test Loss: 15588615.0\n",
      "Epoch [87/100], Training Loss: 166701.80572833362, Test Loss: 15493651.0\n",
      "Epoch [88/100], Training Loss: 165674.2445204668, Test Loss: 15506153.0\n",
      "Epoch [89/100], Training Loss: 164683.73068094306, Test Loss: 15438858.0\n",
      "Epoch [90/100], Training Loss: 163715.4821322789, Test Loss: 15421827.0\n",
      "Epoch [91/100], Training Loss: 162755.91232746874, Test Loss: 15364956.0\n",
      "Epoch [92/100], Training Loss: 161808.30982317397, Test Loss: 15336295.0\n",
      "Epoch [93/100], Training Loss: 160850.19967715183, Test Loss: 15298665.0\n",
      "Epoch [94/100], Training Loss: 159899.31501392098, Test Loss: 15263175.0\n",
      "Epoch [95/100], Training Loss: 158961.44621171732, Test Loss: 15213334.0\n",
      "Epoch [96/100], Training Loss: 158049.4328016705, Test Loss: 15229920.0\n",
      "Epoch [97/100], Training Loss: 157161.58913867662, Test Loss: 15153117.0\n",
      "Epoch [98/100], Training Loss: 156285.6671257627, Test Loss: 15154215.0\n",
      "Epoch [99/100], Training Loss: 155449.40598898169, Test Loss: 15091126.0\n",
      "Epoch [100/100], Training Loss: 154623.41300130324, Test Loss: 15088402.0\n",
      "Epoch [1/100], Training Loss: 2317426.905041171, Test Loss: 291966816.0\n",
      "Epoch [2/100], Training Loss: 2060566.3906166696, Test Loss: 227702528.0\n",
      "Epoch [3/100], Training Loss: 1471217.5020437178, Test Loss: 161638784.0\n",
      "Epoch [4/100], Training Loss: 1163136.4513950595, Test Loss: 141861488.0\n",
      "Epoch [5/100], Training Loss: 1047920.1094721877, Test Loss: 130038504.0\n",
      "Epoch [6/100], Training Loss: 960870.0986908359, Test Loss: 119937120.0\n",
      "Epoch [7/100], Training Loss: 883708.073455364, Test Loss: 110857280.0\n",
      "Epoch [8/100], Training Loss: 813928.0490492269, Test Loss: 102708712.0\n",
      "Epoch [9/100], Training Loss: 751358.6148924826, Test Loss: 95495032.0\n",
      "Epoch [10/100], Training Loss: 695982.7305254427, Test Loss: 89177136.0\n",
      "Epoch [11/100], Training Loss: 647347.8761921687, Test Loss: 83636808.0\n",
      "Epoch [12/100], Training Loss: 604675.5582015284, Test Loss: 78754544.0\n",
      "Epoch [13/100], Training Loss: 567201.035246727, Test Loss: 74437616.0\n",
      "Epoch [14/100], Training Loss: 534247.8070019549, Test Loss: 70604232.0\n",
      "Epoch [15/100], Training Loss: 505201.6395948107, Test Loss: 67191184.0\n",
      "Epoch [16/100], Training Loss: 479507.6680291452, Test Loss: 64148452.0\n",
      "Epoch [17/100], Training Loss: 456723.66494875896, Test Loss: 61440676.0\n",
      "Epoch [18/100], Training Loss: 436478.46608613234, Test Loss: 59022008.0\n",
      "Epoch [19/100], Training Loss: 418338.80208518455, Test Loss: 56838188.0\n",
      "Epoch [20/100], Training Loss: 401912.3923938155, Test Loss: 54844052.0\n",
      "Epoch [21/100], Training Loss: 386835.26449854864, Test Loss: 53003324.0\n",
      "Epoch [22/100], Training Loss: 372838.6091463776, Test Loss: 51287552.0\n",
      "Epoch [23/100], Training Loss: 359754.2145607488, Test Loss: 49675628.0\n",
      "Epoch [24/100], Training Loss: 347454.4576150702, Test Loss: 48154016.0\n",
      "Epoch [25/100], Training Loss: 335866.19246490137, Test Loss: 46715292.0\n",
      "Epoch [26/100], Training Loss: 324916.4994964753, Test Loss: 45348628.0\n",
      "Epoch [27/100], Training Loss: 314524.1422309105, Test Loss: 44042956.0\n",
      "Epoch [28/100], Training Loss: 304636.03169243527, Test Loss: 42793292.0\n",
      "Epoch [29/100], Training Loss: 295217.9811622534, Test Loss: 41594660.0\n",
      "Epoch [30/100], Training Loss: 286242.89781411056, Test Loss: 40444512.0\n",
      "Epoch [31/100], Training Loss: 277693.77175522776, Test Loss: 39340912.0\n",
      "Epoch [32/100], Training Loss: 269547.17202772346, Test Loss: 38280452.0\n",
      "Epoch [33/100], Training Loss: 261779.05106332563, Test Loss: 37263732.0\n",
      "Epoch [34/100], Training Loss: 254371.03376577218, Test Loss: 36291424.0\n",
      "Epoch [35/100], Training Loss: 247305.29399917065, Test Loss: 35360920.0\n",
      "Epoch [36/100], Training Loss: 240572.2690006516, Test Loss: 34473372.0\n",
      "Epoch [37/100], Training Loss: 234154.91759966826, Test Loss: 33624720.0\n",
      "Epoch [38/100], Training Loss: 228033.05657247794, Test Loss: 32812358.0\n",
      "Epoch [39/100], Training Loss: 222194.6439784373, Test Loss: 32035806.0\n",
      "Epoch [40/100], Training Loss: 216629.46217641135, Test Loss: 31293930.0\n",
      "Epoch [41/100], Training Loss: 211332.07807594337, Test Loss: 30588486.0\n",
      "Epoch [42/100], Training Loss: 206285.84423316154, Test Loss: 29916530.0\n",
      "Epoch [43/100], Training Loss: 201477.1552040756, Test Loss: 29276358.0\n",
      "Epoch [44/100], Training Loss: 196893.5571648599, Test Loss: 28665618.0\n",
      "Epoch [45/100], Training Loss: 192518.51575736035, Test Loss: 28081970.0\n",
      "Epoch [46/100], Training Loss: 188351.03456548782, Test Loss: 27525270.0\n",
      "Epoch [47/100], Training Loss: 184380.19136899474, Test Loss: 26994358.0\n",
      "Epoch [48/100], Training Loss: 180595.07401812688, Test Loss: 26489420.0\n",
      "Epoch [49/100], Training Loss: 176983.4443753332, Test Loss: 26007196.0\n",
      "Epoch [50/100], Training Loss: 173529.59507730586, Test Loss: 25547304.0\n",
      "Epoch [51/100], Training Loss: 170214.5311000533, Test Loss: 25108358.0\n",
      "Epoch [52/100], Training Loss: 167028.08589538533, Test Loss: 24688298.0\n",
      "Epoch [53/100], Training Loss: 163966.3171020674, Test Loss: 24287162.0\n",
      "Epoch [54/100], Training Loss: 161020.95995497896, Test Loss: 23904020.0\n",
      "Epoch [55/100], Training Loss: 158181.90521888513, Test Loss: 23536312.0\n",
      "Epoch [56/100], Training Loss: 155445.34287068303, Test Loss: 23184612.0\n",
      "Epoch [57/100], Training Loss: 152803.5867247201, Test Loss: 22848388.0\n",
      "Epoch [58/100], Training Loss: 150251.89748830046, Test Loss: 22525974.0\n",
      "Epoch [59/100], Training Loss: 147791.75771577514, Test Loss: 22218648.0\n",
      "Epoch [60/100], Training Loss: 145413.987352645, Test Loss: 21922902.0\n",
      "Epoch [61/100], Training Loss: 143114.15176826017, Test Loss: 21639954.0\n",
      "Epoch [62/100], Training Loss: 140884.15253835672, Test Loss: 21367520.0\n",
      "Epoch [63/100], Training Loss: 138722.35303003376, Test Loss: 21106464.0\n",
      "Epoch [64/100], Training Loss: 136630.5499970381, Test Loss: 20857348.0\n",
      "Epoch [65/100], Training Loss: 134608.86446300574, Test Loss: 20619398.0\n",
      "Epoch [66/100], Training Loss: 132651.3228777916, Test Loss: 20387844.0\n",
      "Epoch [67/100], Training Loss: 130757.34405544696, Test Loss: 20169284.0\n",
      "Epoch [68/100], Training Loss: 128921.36076061845, Test Loss: 19955362.0\n",
      "Epoch [69/100], Training Loss: 127141.85341508205, Test Loss: 19753936.0\n",
      "Epoch [70/100], Training Loss: 125416.99663823233, Test Loss: 19556246.0\n",
      "Epoch [71/100], Training Loss: 123745.33416266809, Test Loss: 19371602.0\n",
      "Epoch [72/100], Training Loss: 122125.40781055625, Test Loss: 19191424.0\n",
      "Epoch [73/100], Training Loss: 120553.58839819916, Test Loss: 19021760.0\n",
      "Epoch [74/100], Training Loss: 119026.64365262722, Test Loss: 18854502.0\n",
      "Epoch [75/100], Training Loss: 117539.64996149518, Test Loss: 18698466.0\n",
      "Epoch [76/100], Training Loss: 116093.95472720811, Test Loss: 18543300.0\n",
      "Epoch [77/100], Training Loss: 114689.75497600852, Test Loss: 18401522.0\n",
      "Epoch [78/100], Training Loss: 113324.76138854334, Test Loss: 18256488.0\n",
      "Epoch [79/100], Training Loss: 111997.65752917482, Test Loss: 18124574.0\n",
      "Epoch [80/100], Training Loss: 110705.63974290622, Test Loss: 17988528.0\n",
      "Epoch [81/100], Training Loss: 109448.25339138677, Test Loss: 17863686.0\n",
      "Epoch [82/100], Training Loss: 108224.68132812036, Test Loss: 17739504.0\n",
      "Epoch [83/100], Training Loss: 107033.55994905515, Test Loss: 17622958.0\n",
      "Epoch [84/100], Training Loss: 105877.37656240744, Test Loss: 17507606.0\n",
      "Epoch [85/100], Training Loss: 104749.35033469582, Test Loss: 17400616.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [86/100], Training Loss: 103650.42094662639, Test Loss: 17293800.0\n",
      "Epoch [87/100], Training Loss: 102578.25436881701, Test Loss: 17195134.0\n",
      "Epoch [88/100], Training Loss: 101535.95095077305, Test Loss: 17095366.0\n",
      "Epoch [89/100], Training Loss: 100525.71768556365, Test Loss: 17004662.0\n",
      "Epoch [90/100], Training Loss: 99541.01165511522, Test Loss: 16911322.0\n",
      "Epoch [91/100], Training Loss: 98584.14329719804, Test Loss: 16828014.0\n",
      "Epoch [92/100], Training Loss: 97647.8072833363, Test Loss: 16741797.0\n",
      "Epoch [93/100], Training Loss: 96736.99592737397, Test Loss: 16666041.0\n",
      "Epoch [94/100], Training Loss: 95850.84601030745, Test Loss: 16589472.0\n",
      "Epoch [95/100], Training Loss: 94989.29342159825, Test Loss: 16518736.0\n",
      "Epoch [96/100], Training Loss: 94147.34076772703, Test Loss: 16448546.0\n",
      "Epoch [97/100], Training Loss: 93335.08046028079, Test Loss: 16382246.0\n",
      "Epoch [98/100], Training Loss: 92539.29019311653, Test Loss: 16321862.0\n",
      "Epoch [99/100], Training Loss: 91769.89399324685, Test Loss: 16257634.0\n",
      "Epoch [100/100], Training Loss: 91015.61735086783, Test Loss: 16201991.0\n",
      "Epoch [1/100], Training Loss: 1497621.5532255198, Test Loss: 298579968.0\n",
      "Epoch [2/100], Training Loss: 1463801.7292814406, Test Loss: 283305312.0\n",
      "Epoch [3/100], Training Loss: 1323668.654700551, Test Loss: 241434272.0\n",
      "Epoch [4/100], Training Loss: 1078543.196256146, Test Loss: 190209840.0\n",
      "Epoch [5/100], Training Loss: 867181.1691250518, Test Loss: 159897776.0\n",
      "Epoch [6/100], Training Loss: 761014.6301759374, Test Loss: 146053552.0\n",
      "Epoch [7/100], Training Loss: 703398.0650435401, Test Loss: 136835312.0\n",
      "Epoch [8/100], Training Loss: 659081.369350157, Test Loss: 129144288.0\n",
      "Epoch [9/100], Training Loss: 620322.049641609, Test Loss: 122335016.0\n",
      "Epoch [10/100], Training Loss: 585246.217641135, Test Loss: 116201832.0\n",
      "Epoch [11/100], Training Loss: 553190.0212072744, Test Loss: 110658736.0\n",
      "Epoch [12/100], Training Loss: 523866.8927196256, Test Loss: 105657688.0\n",
      "Epoch [13/100], Training Loss: 497103.55571352405, Test Loss: 101154408.0\n",
      "Epoch [14/100], Training Loss: 472739.4152005213, Test Loss: 97105064.0\n",
      "Epoch [15/100], Training Loss: 450618.30223328003, Test Loss: 93465776.0\n",
      "Epoch [16/100], Training Loss: 430572.18529707956, Test Loss: 90186024.0\n",
      "Epoch [17/100], Training Loss: 412405.70226882293, Test Loss: 87214640.0\n",
      "Epoch [18/100], Training Loss: 395923.13038327114, Test Loss: 84507720.0\n",
      "Epoch [19/100], Training Loss: 380954.57449203247, Test Loss: 82030432.0\n",
      "Epoch [20/100], Training Loss: 367349.81114862865, Test Loss: 79754984.0\n",
      "Epoch [21/100], Training Loss: 354976.33647295774, Test Loss: 77659064.0\n",
      "Epoch [22/100], Training Loss: 343712.03601682367, Test Loss: 75721208.0\n",
      "Epoch [23/100], Training Loss: 333439.6777442095, Test Loss: 73923104.0\n",
      "Epoch [24/100], Training Loss: 324049.81600616075, Test Loss: 72247280.0\n",
      "Epoch [25/100], Training Loss: 315433.59113796573, Test Loss: 70677712.0\n",
      "Epoch [26/100], Training Loss: 307488.16953971924, Test Loss: 69200120.0\n",
      "Epoch [27/100], Training Loss: 300117.2439428944, Test Loss: 67802592.0\n",
      "Epoch [28/100], Training Loss: 293236.4931579883, Test Loss: 66474324.0\n",
      "Epoch [29/100], Training Loss: 286777.49127717555, Test Loss: 65208288.0\n",
      "Epoch [30/100], Training Loss: 280675.97317990643, Test Loss: 63994128.0\n",
      "Epoch [31/100], Training Loss: 274869.43236478884, Test Loss: 62824624.0\n",
      "Epoch [32/100], Training Loss: 269314.36720554915, Test Loss: 61693480.0\n",
      "Epoch [33/100], Training Loss: 263973.4569183272, Test Loss: 60597240.0\n",
      "Epoch [34/100], Training Loss: 258821.3751406907, Test Loss: 59530732.0\n",
      "Epoch [35/100], Training Loss: 253822.78820123215, Test Loss: 58490272.0\n",
      "Epoch [36/100], Training Loss: 248955.80700935965, Test Loss: 57474404.0\n",
      "Epoch [37/100], Training Loss: 244206.66497097327, Test Loss: 56480932.0\n",
      "Epoch [38/100], Training Loss: 239562.7404034121, Test Loss: 55508560.0\n",
      "Epoch [39/100], Training Loss: 235015.68985842072, Test Loss: 54556916.0\n",
      "Epoch [40/100], Training Loss: 230560.43232036018, Test Loss: 53624968.0\n",
      "Epoch [41/100], Training Loss: 226193.87743617085, Test Loss: 52711988.0\n",
      "Epoch [42/100], Training Loss: 221914.3996653042, Test Loss: 51818252.0\n",
      "Epoch [43/100], Training Loss: 217719.93517860316, Test Loss: 50942556.0\n",
      "Epoch [44/100], Training Loss: 213610.52615366387, Test Loss: 50084884.0\n",
      "Epoch [45/100], Training Loss: 209588.38285646585, Test Loss: 49246040.0\n",
      "Epoch [46/100], Training Loss: 205654.06036372253, Test Loss: 48425824.0\n",
      "Epoch [47/100], Training Loss: 201805.93757775013, Test Loss: 47622988.0\n",
      "Epoch [48/100], Training Loss: 198041.6423197678, Test Loss: 46836304.0\n",
      "Epoch [49/100], Training Loss: 194360.50884130088, Test Loss: 46065652.0\n",
      "Epoch [50/100], Training Loss: 190761.16388247142, Test Loss: 45311312.0\n",
      "Epoch [51/100], Training Loss: 187242.50805639476, Test Loss: 44572008.0\n",
      "Epoch [52/100], Training Loss: 183802.95068420118, Test Loss: 43846704.0\n",
      "Epoch [53/100], Training Loss: 180443.8977400628, Test Loss: 43135380.0\n",
      "Epoch [54/100], Training Loss: 177166.04003761624, Test Loss: 42438144.0\n",
      "Epoch [55/100], Training Loss: 173967.89174960015, Test Loss: 41754836.0\n",
      "Epoch [56/100], Training Loss: 170846.72400183638, Test Loss: 41085512.0\n",
      "Epoch [57/100], Training Loss: 167800.63305269237, Test Loss: 40428296.0\n",
      "Epoch [58/100], Training Loss: 164826.9776746046, Test Loss: 39784524.0\n",
      "Epoch [59/100], Training Loss: 161926.83617676678, Test Loss: 39154288.0\n",
      "Epoch [60/100], Training Loss: 159097.4445271311, Test Loss: 38537380.0\n",
      "Epoch [61/100], Training Loss: 156340.18920680054, Test Loss: 37934268.0\n",
      "Epoch [62/100], Training Loss: 153652.16607613588, Test Loss: 37345188.0\n",
      "Epoch [63/100], Training Loss: 151032.2518419377, Test Loss: 36770104.0\n",
      "Epoch [64/100], Training Loss: 148478.7967815147, Test Loss: 36208600.0\n",
      "Epoch [65/100], Training Loss: 145990.04469614508, Test Loss: 35659872.0\n",
      "Epoch [66/100], Training Loss: 143563.30463288978, Test Loss: 35123836.0\n",
      "Epoch [67/100], Training Loss: 141195.78434519577, Test Loss: 34600036.0\n",
      "Epoch [68/100], Training Loss: 138887.39299961866, Test Loss: 34087952.0\n",
      "Epoch [69/100], Training Loss: 136639.6689900304, Test Loss: 33587852.0\n",
      "Epoch [70/100], Training Loss: 134452.0326735679, Test Loss: 33100478.0\n",
      "Epoch [71/100], Training Loss: 132324.13978999597, Test Loss: 32625104.0\n",
      "Epoch [72/100], Training Loss: 130253.64895479292, Test Loss: 32162476.0\n",
      "Epoch [73/100], Training Loss: 128238.33817242585, Test Loss: 31712480.0\n",
      "Epoch [74/100], Training Loss: 126276.76033372541, Test Loss: 31274408.0\n",
      "Epoch [75/100], Training Loss: 124367.25803687407, Test Loss: 30848210.0\n",
      "Epoch [76/100], Training Loss: 122509.85953582244, Test Loss: 30434100.0\n",
      "Epoch [77/100], Training Loss: 120703.22029840779, Test Loss: 30030982.0\n",
      "Epoch [78/100], Training Loss: 118942.85573925567, Test Loss: 29637334.0\n",
      "Epoch [79/100], Training Loss: 117225.60583760033, Test Loss: 29253376.0\n",
      "Epoch [80/100], Training Loss: 115550.75193472876, Test Loss: 28879208.0\n",
      "Epoch [81/100], Training Loss: 113917.8121800906, Test Loss: 28515456.0\n",
      "Epoch [82/100], Training Loss: 112327.0038503671, Test Loss: 28161756.0\n",
      "Epoch [83/100], Training Loss: 110776.71182225486, Test Loss: 27817752.0\n",
      "Epoch [84/100], Training Loss: 109266.07825863305, Test Loss: 27483692.0\n",
      "Epoch [85/100], Training Loss: 107794.02525421517, Test Loss: 27159156.0\n",
      "Epoch [86/100], Training Loss: 106359.32213557865, Test Loss: 26844534.0\n",
      "Epoch [87/100], Training Loss: 104962.61152562422, Test Loss: 26539980.0\n",
      "Epoch [88/100], Training Loss: 103602.22231354777, Test Loss: 26244458.0\n",
      "Epoch [89/100], Training Loss: 102277.31013162913, Test Loss: 25958144.0\n",
      "Epoch [90/100], Training Loss: 100986.07888630338, Test Loss: 25680700.0\n",
      "Epoch [91/100], Training Loss: 99727.2075009904, Test Loss: 25411938.0\n",
      "Epoch [92/100], Training Loss: 98498.521398873, Test Loss: 25150434.0\n",
      "Epoch [93/100], Training Loss: 97298.10035482755, Test Loss: 24897408.0\n",
      "Epoch [94/100], Training Loss: 96125.25152399518, Test Loss: 24651520.0\n",
      "Epoch [95/100], Training Loss: 94979.68336258219, Test Loss: 24413308.0\n",
      "Epoch [96/100], Training Loss: 93862.4673213413, Test Loss: 24181184.0\n",
      "Epoch [97/100], Training Loss: 92772.67924182513, Test Loss: 23955350.0\n",
      "Epoch [98/100], Training Loss: 91706.58087217137, Test Loss: 23734144.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [99/100], Training Loss: 90662.70752945248, Test Loss: 23519440.0\n",
      "Epoch [100/100], Training Loss: 89640.57431570627, Test Loss: 23308588.0\n",
      "Epoch [1/100], Training Loss: 1164475.3680469166, Test Loss: 299433056.0\n",
      "Epoch [2/100], Training Loss: 1152195.7827142943, Test Loss: 292111776.0\n",
      "Epoch [3/100], Training Loss: 1095605.3238552217, Test Loss: 268962720.0\n",
      "Epoch [4/100], Training Loss: 971697.9614951721, Test Loss: 229453312.0\n",
      "Epoch [5/100], Training Loss: 808214.3773473136, Test Loss: 188326656.0\n",
      "Epoch [6/100], Training Loss: 673663.6187429655, Test Loss: 161933280.0\n",
      "Epoch [7/100], Training Loss: 598705.7989455601, Test Loss: 148644480.0\n",
      "Epoch [8/100], Training Loss: 557607.3858183757, Test Loss: 140271440.0\n",
      "Epoch [9/100], Training Loss: 527724.1661039038, Test Loss: 133472832.0\n",
      "Epoch [10/100], Training Loss: 501966.4699958533, Test Loss: 127397800.0\n",
      "Epoch [11/100], Training Loss: 478415.9561637344, Test Loss: 121776184.0\n",
      "Epoch [12/100], Training Loss: 456371.4130679462, Test Loss: 116495816.0\n",
      "Epoch [13/100], Training Loss: 435526.59131568036, Test Loss: 111501736.0\n",
      "Epoch [14/100], Training Loss: 415625.804632427, Test Loss: 106700448.0\n",
      "Epoch [15/100], Training Loss: 396372.7954505065, Test Loss: 102113328.0\n",
      "Epoch [16/100], Training Loss: 378209.86079023755, Test Loss: 97860168.0\n",
      "Epoch [17/100], Training Loss: 361428.4874118832, Test Loss: 93972808.0\n",
      "Epoch [18/100], Training Loss: 346048.07321841124, Test Loss: 90410056.0\n",
      "Epoch [19/100], Training Loss: 331919.4407914223, Test Loss: 87135480.0\n",
      "Epoch [20/100], Training Loss: 318921.1534861679, Test Loss: 84116456.0\n",
      "Epoch [21/100], Training Loss: 306950.8077720514, Test Loss: 81326496.0\n",
      "Epoch [22/100], Training Loss: 295927.5577276228, Test Loss: 78744880.0\n",
      "Epoch [23/100], Training Loss: 285774.9773117706, Test Loss: 76354176.0\n",
      "Epoch [24/100], Training Loss: 276414.9147562348, Test Loss: 74135464.0\n",
      "Epoch [25/100], Training Loss: 267768.9763639595, Test Loss: 72071824.0\n",
      "Epoch [26/100], Training Loss: 259765.6256145963, Test Loss: 70148168.0\n",
      "Epoch [27/100], Training Loss: 252350.5864581482, Test Loss: 68351232.0\n",
      "Epoch [28/100], Training Loss: 245469.62644393105, Test Loss: 66673216.0\n",
      "Epoch [29/100], Training Loss: 239059.0551507612, Test Loss: 65101624.0\n",
      "Epoch [30/100], Training Loss: 233071.46211717316, Test Loss: 63625148.0\n",
      "Epoch [31/100], Training Loss: 227458.29761270067, Test Loss: 62232096.0\n",
      "Epoch [32/100], Training Loss: 222170.0883833896, Test Loss: 60912708.0\n",
      "Epoch [33/100], Training Loss: 217164.70256501393, Test Loss: 59661524.0\n",
      "Epoch [34/100], Training Loss: 212409.6586695101, Test Loss: 58471632.0\n",
      "Epoch [35/100], Training Loss: 207873.1031337006, Test Loss: 57336268.0\n",
      "Epoch [36/100], Training Loss: 203538.16326047035, Test Loss: 56252152.0\n",
      "Epoch [37/100], Training Loss: 199382.4224868195, Test Loss: 55213812.0\n",
      "Epoch [38/100], Training Loss: 195384.95326106274, Test Loss: 54217384.0\n",
      "Epoch [39/100], Training Loss: 191544.325099224, Test Loss: 53261048.0\n",
      "Epoch [40/100], Training Loss: 187854.40341212013, Test Loss: 52337332.0\n",
      "Epoch [41/100], Training Loss: 184304.1347076595, Test Loss: 51447672.0\n",
      "Epoch [42/100], Training Loss: 180891.1105977134, Test Loss: 50593604.0\n",
      "Epoch [43/100], Training Loss: 177602.4337420769, Test Loss: 49770772.0\n",
      "Epoch [44/100], Training Loss: 174425.15940998757, Test Loss: 48978788.0\n",
      "Epoch [45/100], Training Loss: 171360.27557609146, Test Loss: 48212636.0\n",
      "Epoch [46/100], Training Loss: 168402.43078016705, Test Loss: 47471980.0\n",
      "Epoch [47/100], Training Loss: 165544.4800663468, Test Loss: 46752740.0\n",
      "Epoch [48/100], Training Loss: 162776.3653811978, Test Loss: 46053800.0\n",
      "Epoch [49/100], Training Loss: 160091.4705289971, Test Loss: 45377348.0\n",
      "Epoch [50/100], Training Loss: 157484.42367158344, Test Loss: 44718088.0\n",
      "Epoch [51/100], Training Loss: 154951.97867424914, Test Loss: 44077876.0\n",
      "Epoch [52/100], Training Loss: 152492.8157099698, Test Loss: 43457348.0\n",
      "Epoch [53/100], Training Loss: 150107.69136899474, Test Loss: 42853748.0\n",
      "Epoch [54/100], Training Loss: 147794.01220306853, Test Loss: 42265020.0\n",
      "Epoch [55/100], Training Loss: 145547.4648421302, Test Loss: 41690428.0\n",
      "Epoch [56/100], Training Loss: 143361.1972039571, Test Loss: 41128544.0\n",
      "Epoch [57/100], Training Loss: 141231.34375925598, Test Loss: 40579776.0\n",
      "Epoch [58/100], Training Loss: 139158.42379005984, Test Loss: 40044840.0\n",
      "Epoch [59/100], Training Loss: 137140.19832948284, Test Loss: 39522432.0\n",
      "Epoch [60/100], Training Loss: 135175.0904567265, Test Loss: 39013008.0\n",
      "Epoch [61/100], Training Loss: 133259.47621586398, Test Loss: 38515432.0\n",
      "Epoch [62/100], Training Loss: 131392.4149635685, Test Loss: 38030428.0\n",
      "Epoch [63/100], Training Loss: 129572.42710739885, Test Loss: 37558144.0\n",
      "Epoch [64/100], Training Loss: 127797.77104436941, Test Loss: 37099600.0\n",
      "Epoch [65/100], Training Loss: 126072.5814229015, Test Loss: 36654172.0\n",
      "Epoch [66/100], Training Loss: 124397.26100349505, Test Loss: 36220880.0\n",
      "Epoch [67/100], Training Loss: 122768.6623422783, Test Loss: 35798000.0\n",
      "Epoch [68/100], Training Loss: 121180.86339671821, Test Loss: 35384232.0\n",
      "Epoch [69/100], Training Loss: 119632.48747112138, Test Loss: 34979936.0\n",
      "Epoch [70/100], Training Loss: 118124.41454890113, Test Loss: 34585748.0\n",
      "Epoch [71/100], Training Loss: 116653.1827498371, Test Loss: 34201368.0\n",
      "Epoch [72/100], Training Loss: 115216.9389846573, Test Loss: 33826816.0\n",
      "Epoch [73/100], Training Loss: 113816.8100231029, Test Loss: 33462036.0\n",
      "Epoch [74/100], Training Loss: 112449.52153308454, Test Loss: 33105562.0\n",
      "Epoch [75/100], Training Loss: 111113.82003435816, Test Loss: 32756456.0\n",
      "Epoch [76/100], Training Loss: 109806.92500444286, Test Loss: 32414008.0\n",
      "Epoch [77/100], Training Loss: 108530.38256027487, Test Loss: 32079372.0\n",
      "Epoch [78/100], Training Loss: 107282.07215212368, Test Loss: 31751792.0\n",
      "Epoch [79/100], Training Loss: 106062.76583140809, Test Loss: 31432606.0\n",
      "Epoch [80/100], Training Loss: 104872.73224335052, Test Loss: 31121204.0\n",
      "Epoch [81/100], Training Loss: 103711.88946152478, Test Loss: 30818952.0\n",
      "Epoch [82/100], Training Loss: 102581.5275161424, Test Loss: 30524842.0\n",
      "Epoch [83/100], Training Loss: 101479.62377821219, Test Loss: 30237994.0\n",
      "Epoch [84/100], Training Loss: 100404.55926781589, Test Loss: 29957598.0\n",
      "Epoch [85/100], Training Loss: 99354.13571470884, Test Loss: 29682920.0\n",
      "Epoch [86/100], Training Loss: 98324.96131745749, Test Loss: 29413888.0\n",
      "Epoch [87/100], Training Loss: 97316.83318523785, Test Loss: 29149834.0\n",
      "Epoch [88/100], Training Loss: 96329.51774183994, Test Loss: 28891008.0\n",
      "Epoch [89/100], Training Loss: 95362.04028197382, Test Loss: 28639516.0\n",
      "Epoch [90/100], Training Loss: 94419.21337598484, Test Loss: 28395834.0\n",
      "Epoch [91/100], Training Loss: 93501.92091700729, Test Loss: 28159498.0\n",
      "Epoch [92/100], Training Loss: 92606.87257863871, Test Loss: 27929904.0\n",
      "Epoch [93/100], Training Loss: 91732.09857235946, Test Loss: 27705704.0\n",
      "Epoch [94/100], Training Loss: 90872.95817783306, Test Loss: 27482768.0\n",
      "Epoch [95/100], Training Loss: 90023.19205023399, Test Loss: 27262192.0\n",
      "Epoch [96/100], Training Loss: 89179.18351993365, Test Loss: 27041756.0\n",
      "Epoch [97/100], Training Loss: 88334.5479533203, Test Loss: 26821686.0\n",
      "Epoch [98/100], Training Loss: 87488.0093003969, Test Loss: 26601574.0\n",
      "Epoch [99/100], Training Loss: 86641.8855518038, Test Loss: 26385394.0\n",
      "Epoch [100/100], Training Loss: 85802.04875303595, Test Loss: 26169290.0\n",
      "Epoch [1/100], Training Loss: 582423.6649487589, Test Loss: 300030528.0\n",
      "Epoch [2/100], Training Loss: 581854.6285172679, Test Loss: 299334816.0\n",
      "Epoch [3/100], Training Loss: 578931.7230021919, Test Loss: 296705376.0\n",
      "Epoch [4/100], Training Loss: 570697.5378235887, Test Loss: 290504192.0\n",
      "Epoch [5/100], Training Loss: 554106.9130975653, Test Loss: 279353344.0\n",
      "Epoch [6/100], Training Loss: 527183.4104614656, Test Loss: 262723088.0\n",
      "Epoch [7/100], Training Loss: 490122.8638113856, Test Loss: 241442992.0\n",
      "Epoch [8/100], Training Loss: 446016.62555535807, Test Loss: 217864240.0\n",
      "Epoch [9/100], Training Loss: 400527.9834133049, Test Loss: 195306608.0\n",
      "Epoch [10/100], Training Loss: 360029.17978792725, Test Loss: 176725488.0\n",
      "Epoch [11/100], Training Loss: 328686.05840886204, Test Loss: 163249920.0\n",
      "Epoch [12/100], Training Loss: 306632.79000059236, Test Loss: 154003728.0\n",
      "Epoch [13/100], Training Loss: 291215.7071263551, Test Loss: 147382080.0\n",
      "Epoch [14/100], Training Loss: 279594.27190332324, Test Loss: 142160592.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/100], Training Loss: 269963.8888691428, Test Loss: 137677600.0\n",
      "Epoch [16/100], Training Loss: 261424.12842841065, Test Loss: 133623616.0\n",
      "Epoch [17/100], Training Loss: 253557.57170783723, Test Loss: 129853064.0\n",
      "Epoch [18/100], Training Loss: 246158.99342456015, Test Loss: 126290312.0\n",
      "Epoch [19/100], Training Loss: 239115.51780107812, Test Loss: 122891720.0\n",
      "Epoch [20/100], Training Loss: 232359.6843788875, Test Loss: 119630640.0\n",
      "Epoch [21/100], Training Loss: 225850.01552040756, Test Loss: 116490704.0\n",
      "Epoch [22/100], Training Loss: 219561.5110479237, Test Loss: 113462200.0\n",
      "Epoch [23/100], Training Loss: 213479.88294532313, Test Loss: 110539712.0\n",
      "Epoch [24/100], Training Loss: 207598.24749718618, Test Loss: 107721192.0\n",
      "Epoch [25/100], Training Loss: 201915.5538179018, Test Loss: 105006400.0\n",
      "Epoch [26/100], Training Loss: 196432.88786209348, Test Loss: 102395680.0\n",
      "Epoch [27/100], Training Loss: 191151.50050352467, Test Loss: 99888984.0\n",
      "Epoch [28/100], Training Loss: 186072.38315265684, Test Loss: 97485176.0\n",
      "Epoch [29/100], Training Loss: 181196.9705586162, Test Loss: 95183832.0\n",
      "Epoch [30/100], Training Loss: 176526.5446359813, Test Loss: 92984584.0\n",
      "Epoch [31/100], Training Loss: 172061.16912505182, Test Loss: 90886240.0\n",
      "Epoch [32/100], Training Loss: 167799.84763935785, Test Loss: 88887368.0\n",
      "Epoch [33/100], Training Loss: 163739.89218648185, Test Loss: 86985032.0\n",
      "Epoch [34/100], Training Loss: 159876.50020733368, Test Loss: 85175368.0\n",
      "Epoch [35/100], Training Loss: 156202.73656773887, Test Loss: 83453848.0\n",
      "Epoch [36/100], Training Loss: 152710.660742847, Test Loss: 81815448.0\n",
      "Epoch [37/100], Training Loss: 149391.13441146852, Test Loss: 80254680.0\n",
      "Epoch [38/100], Training Loss: 146233.85297079556, Test Loss: 78765888.0\n",
      "Epoch [39/100], Training Loss: 143227.96422012913, Test Loss: 77343416.0\n",
      "Epoch [40/100], Training Loss: 140362.5704638351, Test Loss: 75982368.0\n",
      "Epoch [41/100], Training Loss: 137627.67395296486, Test Loss: 74677944.0\n",
      "Epoch [42/100], Training Loss: 135014.05082637284, Test Loss: 73425824.0\n",
      "Epoch [43/100], Training Loss: 132512.61465552988, Test Loss: 72221928.0\n",
      "Epoch [44/100], Training Loss: 130114.78893430484, Test Loss: 71062584.0\n",
      "Epoch [45/100], Training Loss: 127813.04401398021, Test Loss: 69944704.0\n",
      "Epoch [46/100], Training Loss: 125600.58953853445, Test Loss: 68865672.0\n",
      "Epoch [47/100], Training Loss: 123471.64172738582, Test Loss: 67823640.0\n",
      "Epoch [48/100], Training Loss: 121420.88644037675, Test Loss: 66816388.0\n",
      "Epoch [49/100], Training Loss: 119443.02067413068, Test Loss: 65841656.0\n",
      "Epoch [50/100], Training Loss: 117533.1549078846, Test Loss: 64897352.0\n",
      "Epoch [51/100], Training Loss: 115687.0903382501, Test Loss: 63982668.0\n",
      "Epoch [52/100], Training Loss: 113901.512943546, Test Loss: 63096492.0\n",
      "Epoch [53/100], Training Loss: 112172.74687518513, Test Loss: 62237208.0\n",
      "Epoch [54/100], Training Loss: 110498.04762751021, Test Loss: 61403752.0\n",
      "Epoch [55/100], Training Loss: 108873.22433505124, Test Loss: 60593336.0\n",
      "Epoch [56/100], Training Loss: 107294.72294295362, Test Loss: 59805456.0\n",
      "Epoch [57/100], Training Loss: 105760.29180735738, Test Loss: 59039376.0\n",
      "Epoch [58/100], Training Loss: 104267.42444167999, Test Loss: 58293140.0\n",
      "Epoch [59/100], Training Loss: 102812.46513832119, Test Loss: 57564944.0\n",
      "Epoch [60/100], Training Loss: 101392.78158876844, Test Loss: 56854000.0\n",
      "Epoch [61/100], Training Loss: 100007.08263728452, Test Loss: 56159056.0\n",
      "Epoch [62/100], Training Loss: 98652.75967063563, Test Loss: 55479764.0\n",
      "Epoch [63/100], Training Loss: 97327.66506723536, Test Loss: 54815460.0\n",
      "Epoch [64/100], Training Loss: 96031.08832415141, Test Loss: 54165748.0\n",
      "Epoch [65/100], Training Loss: 94761.47100290268, Test Loss: 53529228.0\n",
      "Epoch [66/100], Training Loss: 93517.1993365322, Test Loss: 52905032.0\n",
      "Epoch [67/100], Training Loss: 92297.41591137966, Test Loss: 52292100.0\n",
      "Epoch [68/100], Training Loss: 91101.24222498666, Test Loss: 51689980.0\n",
      "Epoch [69/100], Training Loss: 89927.70155796458, Test Loss: 51098896.0\n",
      "Epoch [70/100], Training Loss: 88776.3414489663, Test Loss: 50518320.0\n",
      "Epoch [71/100], Training Loss: 87646.62117173153, Test Loss: 49947840.0\n",
      "Epoch [72/100], Training Loss: 86537.80569871452, Test Loss: 49387312.0\n",
      "Epoch [73/100], Training Loss: 85449.20620816303, Test Loss: 48836600.0\n",
      "Epoch [74/100], Training Loss: 84381.07232983828, Test Loss: 48295508.0\n",
      "Epoch [75/100], Training Loss: 83333.47384633611, Test Loss: 47763432.0\n",
      "Epoch [76/100], Training Loss: 82305.73022925181, Test Loss: 47240380.0\n",
      "Epoch [77/100], Training Loss: 81296.83857591375, Test Loss: 46725748.0\n",
      "Epoch [78/100], Training Loss: 80306.25946330193, Test Loss: 46219732.0\n",
      "Epoch [79/100], Training Loss: 79334.01066287543, Test Loss: 45722156.0\n",
      "Epoch [80/100], Training Loss: 78379.56175582015, Test Loss: 45232288.0\n",
      "Epoch [81/100], Training Loss: 77442.18778508382, Test Loss: 44749996.0\n",
      "Epoch [82/100], Training Loss: 76521.90806231859, Test Loss: 44275620.0\n",
      "Epoch [83/100], Training Loss: 75618.13980214442, Test Loss: 43809112.0\n",
      "Epoch [84/100], Training Loss: 74730.59546235413, Test Loss: 43350092.0\n",
      "Epoch [85/100], Training Loss: 73858.83277057046, Test Loss: 42898204.0\n",
      "Epoch [86/100], Training Loss: 73001.8764291215, Test Loss: 42453072.0\n",
      "Epoch [87/100], Training Loss: 72159.5350986316, Test Loss: 42014176.0\n",
      "Epoch [88/100], Training Loss: 71331.94147266157, Test Loss: 41581972.0\n",
      "Epoch [89/100], Training Loss: 70519.13073870032, Test Loss: 41156452.0\n",
      "Epoch [90/100], Training Loss: 69720.79829393994, Test Loss: 40737476.0\n",
      "Epoch [91/100], Training Loss: 68936.48219892186, Test Loss: 40324812.0\n",
      "Epoch [92/100], Training Loss: 68165.75925596825, Test Loss: 39918340.0\n",
      "Epoch [93/100], Training Loss: 67407.81434749126, Test Loss: 39518124.0\n",
      "Epoch [94/100], Training Loss: 66662.70469758901, Test Loss: 39124344.0\n",
      "Epoch [95/100], Training Loss: 65930.36087909483, Test Loss: 38736568.0\n",
      "Epoch [96/100], Training Loss: 65210.2601741603, Test Loss: 38354616.0\n",
      "Epoch [97/100], Training Loss: 64502.17759611397, Test Loss: 37978396.0\n",
      "Epoch [98/100], Training Loss: 63805.586043480835, Test Loss: 37607884.0\n",
      "Epoch [99/100], Training Loss: 63120.444404952315, Test Loss: 37243080.0\n",
      "Epoch [100/100], Training Loss: 62447.08003080386, Test Loss: 36884424.0\n",
      "Epoch [1/100], Training Loss: 7174425.125525739, Test Loss: 148916176.0\n",
      "Epoch [2/100], Training Loss: 4078168.9324092176, Test Loss: 117876992.0\n",
      "Epoch [3/100], Training Loss: 3260179.537942065, Test Loss: 95945120.0\n",
      "Epoch [4/100], Training Loss: 2658827.1448966293, Test Loss: 80349248.0\n",
      "Epoch [5/100], Training Loss: 2226476.3418043954, Test Loss: 68989336.0\n",
      "Epoch [6/100], Training Loss: 1919867.4796516795, Test Loss: 60887152.0\n",
      "Epoch [7/100], Training Loss: 1704065.731769445, Test Loss: 55091588.0\n",
      "Epoch [8/100], Training Loss: 1547521.3884248564, Test Loss: 50727300.0\n",
      "Epoch [9/100], Training Loss: 1424748.813103489, Test Loss: 47171776.0\n",
      "Epoch [10/100], Training Loss: 1322079.377525028, Test Loss: 44140004.0\n",
      "Epoch [11/100], Training Loss: 1233577.1615129435, Test Loss: 41489352.0\n",
      "Epoch [12/100], Training Loss: 1156780.159202654, Test Loss: 39153848.0\n",
      "Epoch [13/100], Training Loss: 1089274.2714590367, Test Loss: 37052360.0\n",
      "Epoch [14/100], Training Loss: 1028626.0107221137, Test Loss: 35168060.0\n",
      "Epoch [15/100], Training Loss: 974406.5196374622, Test Loss: 33486564.0\n",
      "Epoch [16/100], Training Loss: 925819.2252828624, Test Loss: 31964870.0\n",
      "Epoch [17/100], Training Loss: 882268.3414933949, Test Loss: 30611076.0\n",
      "Epoch [18/100], Training Loss: 843830.5323884841, Test Loss: 29412814.0\n",
      "Epoch [19/100], Training Loss: 809039.0361797287, Test Loss: 28307966.0\n",
      "Epoch [20/100], Training Loss: 777418.0789497067, Test Loss: 27308300.0\n",
      "Epoch [21/100], Training Loss: 748657.8177536876, Test Loss: 26399178.0\n",
      "Epoch [22/100], Training Loss: 722670.7098661216, Test Loss: 25585788.0\n",
      "Epoch [23/100], Training Loss: 698813.1195574907, Test Loss: 24844408.0\n",
      "Epoch [24/100], Training Loss: 676596.7661720277, Test Loss: 24152624.0\n",
      "Epoch [25/100], Training Loss: 655379.6289023162, Test Loss: 23462342.0\n",
      "Epoch [26/100], Training Loss: 634432.7640986908, Test Loss: 22812980.0\n",
      "Epoch [27/100], Training Loss: 614498.5190006517, Test Loss: 22208130.0\n",
      "Epoch [28/100], Training Loss: 596093.2425656063, Test Loss: 21676900.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/100], Training Loss: 579217.2006693917, Test Loss: 21187442.0\n",
      "Epoch [30/100], Training Loss: 563266.1169213909, Test Loss: 20731738.0\n",
      "Epoch [31/100], Training Loss: 548063.538238256, Test Loss: 20291012.0\n",
      "Epoch [32/100], Training Loss: 533250.1245112849, Test Loss: 19867202.0\n",
      "Epoch [33/100], Training Loss: 518743.1724572004, Test Loss: 19483028.0\n",
      "Epoch [34/100], Training Loss: 505981.32790563355, Test Loss: 19171648.0\n",
      "Epoch [35/100], Training Loss: 494695.6590175345, Test Loss: 18885520.0\n",
      "Epoch [36/100], Training Loss: 484310.75344322016, Test Loss: 18637408.0\n",
      "Epoch [37/100], Training Loss: 474585.33939784375, Test Loss: 18394174.0\n",
      "Epoch [38/100], Training Loss: 465214.2381967893, Test Loss: 18173678.0\n",
      "Epoch [39/100], Training Loss: 456277.4238122742, Test Loss: 17953810.0\n",
      "Epoch [40/100], Training Loss: 447795.6394763343, Test Loss: 17764418.0\n",
      "Epoch [41/100], Training Loss: 439789.5354244417, Test Loss: 17588830.0\n",
      "Epoch [42/100], Training Loss: 432381.3674989633, Test Loss: 17453194.0\n",
      "Epoch [43/100], Training Loss: 425469.9669673005, Test Loss: 17308628.0\n",
      "Epoch [44/100], Training Loss: 419049.2031277768, Test Loss: 17197888.0\n",
      "Epoch [45/100], Training Loss: 412949.72663793614, Test Loss: 17080608.0\n",
      "Epoch [46/100], Training Loss: 407214.6718500089, Test Loss: 16992176.0\n",
      "Epoch [47/100], Training Loss: 401852.6786179729, Test Loss: 16896760.0\n",
      "Epoch [48/100], Training Loss: 396712.69883300754, Test Loss: 16810164.0\n",
      "Epoch [49/100], Training Loss: 391874.9790148688, Test Loss: 16738335.0\n",
      "Epoch [50/100], Training Loss: 387291.37623659737, Test Loss: 16614181.0\n",
      "Epoch [51/100], Training Loss: 382968.18247586046, Test Loss: 16580887.0\n",
      "Epoch [52/100], Training Loss: 378745.8555254428, Test Loss: 16476746.0\n",
      "Epoch [53/100], Training Loss: 374662.5043762218, Test Loss: 16419002.0\n",
      "Epoch [54/100], Training Loss: 370621.4210873171, Test Loss: 16323097.0\n",
      "Epoch [55/100], Training Loss: 366788.84572152124, Test Loss: 16251373.0\n",
      "Epoch [56/100], Training Loss: 363132.84361116047, Test Loss: 16212924.0\n",
      "Epoch [57/100], Training Loss: 359718.4687222321, Test Loss: 16113096.0\n",
      "Epoch [58/100], Training Loss: 356234.09330756473, Test Loss: 16071629.0\n",
      "Epoch [59/100], Training Loss: 352721.8615603341, Test Loss: 15958385.0\n",
      "Epoch [60/100], Training Loss: 349154.26234375924, Test Loss: 15914040.0\n",
      "Epoch [61/100], Training Loss: 345778.6884663231, Test Loss: 15828266.0\n",
      "Epoch [62/100], Training Loss: 342529.5563503347, Test Loss: 15778720.0\n",
      "Epoch [63/100], Training Loss: 339511.85286712873, Test Loss: 15714633.0\n",
      "Epoch [64/100], Training Loss: 336404.5863618861, Test Loss: 15651276.0\n",
      "Epoch [65/100], Training Loss: 333359.3482095255, Test Loss: 15658844.0\n",
      "Epoch [66/100], Training Loss: 330433.69634500326, Test Loss: 15557896.0\n",
      "Epoch [67/100], Training Loss: 327621.5829038564, Test Loss: 15565319.0\n",
      "Epoch [68/100], Training Loss: 324802.8026405426, Test Loss: 15464707.0\n",
      "Epoch [69/100], Training Loss: 322046.94746312423, Test Loss: 15461193.0\n",
      "Epoch [70/100], Training Loss: 319331.0109960903, Test Loss: 15380698.0\n",
      "Epoch [71/100], Training Loss: 316750.5862286002, Test Loss: 15372398.0\n",
      "Epoch [72/100], Training Loss: 314226.21360553283, Test Loss: 15306694.0\n",
      "Epoch [73/100], Training Loss: 311845.9452935253, Test Loss: 15259528.0\n",
      "Epoch [74/100], Training Loss: 309484.4721358332, Test Loss: 15254343.0\n",
      "Epoch [75/100], Training Loss: 307281.62959836505, Test Loss: 15216225.0\n",
      "Epoch [76/100], Training Loss: 305109.2620623778, Test Loss: 15171159.0\n",
      "Epoch [77/100], Training Loss: 303081.34893519344, Test Loss: 15151720.0\n",
      "Epoch [78/100], Training Loss: 301087.54189621465, Test Loss: 15088607.0\n",
      "Epoch [79/100], Training Loss: 299154.18865144247, Test Loss: 15130549.0\n",
      "Epoch [80/100], Training Loss: 297258.8297346129, Test Loss: 15048747.0\n",
      "Epoch [81/100], Training Loss: 295430.1723757479, Test Loss: 15063664.0\n",
      "Epoch [82/100], Training Loss: 293633.6920576388, Test Loss: 14994684.0\n",
      "Epoch [83/100], Training Loss: 291932.3744964753, Test Loss: 15026582.0\n",
      "Epoch [84/100], Training Loss: 290237.82729103725, Test Loss: 14969022.0\n",
      "Epoch [85/100], Training Loss: 288641.76915615186, Test Loss: 14934494.0\n",
      "Epoch [86/100], Training Loss: 287021.9867972869, Test Loss: 14969119.0\n",
      "Epoch [87/100], Training Loss: 285551.1532936437, Test Loss: 14926102.0\n",
      "Epoch [88/100], Training Loss: 283998.52648687875, Test Loss: 14917996.0\n",
      "Epoch [89/100], Training Loss: 282585.3938599609, Test Loss: 14895055.0\n",
      "Epoch [90/100], Training Loss: 281136.44791481545, Test Loss: 14879077.0\n",
      "Epoch [91/100], Training Loss: 279789.21347965166, Test Loss: 14911321.0\n",
      "Epoch [92/100], Training Loss: 278394.2941990996, Test Loss: 14826947.0\n",
      "Epoch [93/100], Training Loss: 277064.9412060897, Test Loss: 14869742.0\n",
      "Epoch [94/100], Training Loss: 275695.0422960725, Test Loss: 14818613.0\n",
      "Epoch [95/100], Training Loss: 274367.5022732658, Test Loss: 14807286.0\n",
      "Epoch [96/100], Training Loss: 272999.52876014455, Test Loss: 14815235.0\n",
      "Epoch [97/100], Training Loss: 271749.2149087732, Test Loss: 14766492.0\n",
      "Epoch [98/100], Training Loss: 270442.32977163675, Test Loss: 14780598.0\n",
      "Epoch [99/100], Training Loss: 269226.00559060485, Test Loss: 14765605.0\n",
      "Epoch [100/100], Training Loss: 267987.139520763, Test Loss: 14703035.0\n",
      "Epoch [1/100], Training Loss: 4371666.052958948, Test Loss: 226589296.0\n",
      "Epoch [2/100], Training Loss: 2630252.4412060897, Test Loss: 143581776.0\n",
      "Epoch [3/100], Training Loss: 2054368.3970143949, Test Loss: 123823680.0\n",
      "Epoch [4/100], Training Loss: 1774288.3442923997, Test Loss: 108017776.0\n",
      "Epoch [5/100], Training Loss: 1542974.6428529115, Test Loss: 95074448.0\n",
      "Epoch [6/100], Training Loss: 1353522.3881286653, Test Loss: 84595728.0\n",
      "Epoch [7/100], Training Loss: 1198962.9964457082, Test Loss: 76005576.0\n",
      "Epoch [8/100], Training Loss: 1073317.1942420474, Test Loss: 68974288.0\n",
      "Epoch [9/100], Training Loss: 971919.8644630058, Test Loss: 63249416.0\n",
      "Epoch [10/100], Training Loss: 890433.5220069902, Test Loss: 58639588.0\n",
      "Epoch [11/100], Training Loss: 824687.9002428766, Test Loss: 54855012.0\n",
      "Epoch [12/100], Training Loss: 770025.5536994254, Test Loss: 51643304.0\n",
      "Epoch [13/100], Training Loss: 722863.6400094781, Test Loss: 48821072.0\n",
      "Epoch [14/100], Training Loss: 681095.3193531189, Test Loss: 46291116.0\n",
      "Epoch [15/100], Training Loss: 643487.084888336, Test Loss: 43983372.0\n",
      "Epoch [16/100], Training Loss: 609160.679728689, Test Loss: 41848904.0\n",
      "Epoch [17/100], Training Loss: 577692.7118654108, Test Loss: 39868524.0\n",
      "Epoch [18/100], Training Loss: 548738.2980866062, Test Loss: 38024956.0\n",
      "Epoch [19/100], Training Loss: 522107.0568094307, Test Loss: 36311916.0\n",
      "Epoch [20/100], Training Loss: 497645.4340382679, Test Loss: 34721776.0\n",
      "Epoch [21/100], Training Loss: 475168.708311119, Test Loss: 33255712.0\n",
      "Epoch [22/100], Training Loss: 454535.37302292517, Test Loss: 31902850.0\n",
      "Epoch [23/100], Training Loss: 435543.9328238848, Test Loss: 30655978.0\n",
      "Epoch [24/100], Training Loss: 418074.7560867247, Test Loss: 29506016.0\n",
      "Epoch [25/100], Training Loss: 401979.5902493928, Test Loss: 28444662.0\n",
      "Epoch [26/100], Training Loss: 387150.9658936082, Test Loss: 27464114.0\n",
      "Epoch [27/100], Training Loss: 373527.90267164266, Test Loss: 26560036.0\n",
      "Epoch [28/100], Training Loss: 361013.5502043718, Test Loss: 25727316.0\n",
      "Epoch [29/100], Training Loss: 349441.8315117588, Test Loss: 24956466.0\n",
      "Epoch [30/100], Training Loss: 338673.91518571175, Test Loss: 24241470.0\n",
      "Epoch [31/100], Training Loss: 328612.22707481787, Test Loss: 23574868.0\n",
      "Epoch [32/100], Training Loss: 319164.9607250755, Test Loss: 22956466.0\n",
      "Epoch [33/100], Training Loss: 310280.75543510454, Test Loss: 22379458.0\n",
      "Epoch [34/100], Training Loss: 301909.91644452343, Test Loss: 21845668.0\n",
      "Epoch [35/100], Training Loss: 294006.90769207984, Test Loss: 21352960.0\n",
      "Epoch [36/100], Training Loss: 286518.57400331734, Test Loss: 20892090.0\n",
      "Epoch [37/100], Training Loss: 279443.6766038742, Test Loss: 20466280.0\n",
      "Epoch [38/100], Training Loss: 272758.7822403886, Test Loss: 20068160.0\n",
      "Epoch [39/100], Training Loss: 266439.9135714709, Test Loss: 19698696.0\n",
      "Epoch [40/100], Training Loss: 260471.67845506783, Test Loss: 19354848.0\n",
      "Epoch [41/100], Training Loss: 254816.5758248919, Test Loss: 19034778.0\n",
      "Epoch [42/100], Training Loss: 249444.26704579112, Test Loss: 18736054.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/100], Training Loss: 244338.3403678692, Test Loss: 18454412.0\n",
      "Epoch [44/100], Training Loss: 239463.54208873882, Test Loss: 18194732.0\n",
      "Epoch [45/100], Training Loss: 234824.5205260352, Test Loss: 17945584.0\n",
      "Epoch [46/100], Training Loss: 230388.34642497482, Test Loss: 17715240.0\n",
      "Epoch [47/100], Training Loss: 226152.78598720455, Test Loss: 17497944.0\n",
      "Epoch [48/100], Training Loss: 222110.34595106926, Test Loss: 17293238.0\n",
      "Epoch [49/100], Training Loss: 218243.71279841242, Test Loss: 17104738.0\n",
      "Epoch [50/100], Training Loss: 214531.22086961672, Test Loss: 16925978.0\n",
      "Epoch [51/100], Training Loss: 211000.52843433447, Test Loss: 16765179.0\n",
      "Epoch [52/100], Training Loss: 207616.30837924292, Test Loss: 16608476.0\n",
      "Epoch [53/100], Training Loss: 204383.4294769267, Test Loss: 16465301.0\n",
      "Epoch [54/100], Training Loss: 201281.66163141993, Test Loss: 16331485.0\n",
      "Epoch [55/100], Training Loss: 198330.25798234702, Test Loss: 16206893.0\n",
      "Epoch [56/100], Training Loss: 195508.76491321603, Test Loss: 16095297.0\n",
      "Epoch [57/100], Training Loss: 192818.5423256916, Test Loss: 15989098.0\n",
      "Epoch [58/100], Training Loss: 190235.16253480245, Test Loss: 15889480.0\n",
      "Epoch [59/100], Training Loss: 187758.86388543333, Test Loss: 15802779.0\n",
      "Epoch [60/100], Training Loss: 185393.32424027013, Test Loss: 15717574.0\n",
      "Epoch [61/100], Training Loss: 183125.4403027072, Test Loss: 15641451.0\n",
      "Epoch [62/100], Training Loss: 180963.30589123868, Test Loss: 15571785.0\n",
      "Epoch [63/100], Training Loss: 178869.34195249097, Test Loss: 15505270.0\n",
      "Epoch [64/100], Training Loss: 176873.93319412356, Test Loss: 15445695.0\n",
      "Epoch [65/100], Training Loss: 174957.12232687636, Test Loss: 15391758.0\n",
      "Epoch [66/100], Training Loss: 173115.97812629584, Test Loss: 15334349.0\n",
      "Epoch [67/100], Training Loss: 171353.0753583911, Test Loss: 15288620.0\n",
      "Epoch [68/100], Training Loss: 169650.70314258634, Test Loss: 15231018.0\n",
      "Epoch [69/100], Training Loss: 168041.4664489663, Test Loss: 15194571.0\n",
      "Epoch [70/100], Training Loss: 166480.5413112375, Test Loss: 15139257.0\n",
      "Epoch [71/100], Training Loss: 164974.95800011847, Test Loss: 15098846.0\n",
      "Epoch [72/100], Training Loss: 163538.3073055506, Test Loss: 15048745.0\n",
      "Epoch [73/100], Training Loss: 162171.9634722469, Test Loss: 15019689.0\n",
      "Epoch [74/100], Training Loss: 160860.4850497601, Test Loss: 14973898.0\n",
      "Epoch [75/100], Training Loss: 159608.0151279545, Test Loss: 14933034.0\n",
      "Epoch [76/100], Training Loss: 158394.51674960015, Test Loss: 14917234.0\n",
      "Epoch [77/100], Training Loss: 157240.43734820213, Test Loss: 14871880.0\n",
      "Epoch [78/100], Training Loss: 156102.73066613352, Test Loss: 14852009.0\n",
      "Epoch [79/100], Training Loss: 155029.87625881168, Test Loss: 14818988.0\n",
      "Epoch [80/100], Training Loss: 153985.49589775488, Test Loss: 14785881.0\n",
      "Epoch [81/100], Training Loss: 152986.30809045673, Test Loss: 14766391.0\n",
      "Epoch [82/100], Training Loss: 152016.13592204254, Test Loss: 14732393.0\n",
      "Epoch [83/100], Training Loss: 151093.0515076121, Test Loss: 14718155.0\n",
      "Epoch [84/100], Training Loss: 150195.99465375274, Test Loss: 14678150.0\n",
      "Epoch [85/100], Training Loss: 149327.32195959956, Test Loss: 14686259.0\n",
      "Epoch [86/100], Training Loss: 148497.3532225579, Test Loss: 14620157.0\n",
      "Epoch [87/100], Training Loss: 147671.38502606482, Test Loss: 14640890.0\n",
      "Epoch [88/100], Training Loss: 146869.8838338961, Test Loss: 14608388.0\n",
      "Epoch [89/100], Training Loss: 146095.31901249927, Test Loss: 14572898.0\n",
      "Epoch [90/100], Training Loss: 145325.10775427995, Test Loss: 14571544.0\n",
      "Epoch [91/100], Training Loss: 144592.58895355725, Test Loss: 14560023.0\n",
      "Epoch [92/100], Training Loss: 143856.45211332268, Test Loss: 14522790.0\n",
      "Epoch [93/100], Training Loss: 143148.87813962443, Test Loss: 14530013.0\n",
      "Epoch [94/100], Training Loss: 142463.9283439962, Test Loss: 14495073.0\n",
      "Epoch [95/100], Training Loss: 141785.26115899533, Test Loss: 14492232.0\n",
      "Epoch [96/100], Training Loss: 141131.5146318346, Test Loss: 14480213.0\n",
      "Epoch [97/100], Training Loss: 140499.54118535633, Test Loss: 14462173.0\n",
      "Epoch [98/100], Training Loss: 139867.36789882116, Test Loss: 14432670.0\n",
      "Epoch [99/100], Training Loss: 139272.278449144, Test Loss: 14474565.0\n",
      "Epoch [100/100], Training Loss: 138681.60009033824, Test Loss: 14394884.0\n",
      "Epoch [1/100], Training Loss: 2316521.715064273, Test Loss: 291464128.0\n",
      "Epoch [2/100], Training Loss: 2046986.9448492387, Test Loss: 224637280.0\n",
      "Epoch [3/100], Training Loss: 1449579.9391031337, Test Loss: 159866688.0\n",
      "Epoch [4/100], Training Loss: 1153514.56833126, Test Loss: 140869952.0\n",
      "Epoch [5/100], Training Loss: 1039959.671583437, Test Loss: 129025368.0\n",
      "Epoch [6/100], Training Loss: 952363.6400687163, Test Loss: 118844608.0\n",
      "Epoch [7/100], Training Loss: 874560.2097032167, Test Loss: 109702912.0\n",
      "Epoch [8/100], Training Loss: 804380.9790889166, Test Loss: 101530632.0\n",
      "Epoch [9/100], Training Loss: 741586.8042177596, Test Loss: 94259488.0\n",
      "Epoch [10/100], Training Loss: 684939.7430247023, Test Loss: 87751392.0\n",
      "Epoch [11/100], Training Loss: 635226.1826905989, Test Loss: 82166472.0\n",
      "Epoch [12/100], Training Loss: 592653.5797642319, Test Loss: 77319992.0\n",
      "Epoch [13/100], Training Loss: 555822.5353948226, Test Loss: 73083432.0\n",
      "Epoch [14/100], Training Loss: 523824.95314258634, Test Loss: 69360032.0\n",
      "Epoch [15/100], Training Loss: 495923.3531188911, Test Loss: 66077896.0\n",
      "Epoch [16/100], Training Loss: 471508.7947396481, Test Loss: 63186144.0\n",
      "Epoch [17/100], Training Loss: 450018.49416503764, Test Loss: 60620104.0\n",
      "Epoch [18/100], Training Loss: 430926.5590901013, Test Loss: 58313976.0\n",
      "Epoch [19/100], Training Loss: 413726.63965404895, Test Loss: 56227124.0\n",
      "Epoch [20/100], Training Loss: 398102.0588235294, Test Loss: 54320332.0\n",
      "Epoch [21/100], Training Loss: 383720.4855162609, Test Loss: 52555972.0\n",
      "Epoch [22/100], Training Loss: 370345.752858243, Test Loss: 50911668.0\n",
      "Epoch [23/100], Training Loss: 357825.25525739, Test Loss: 49368320.0\n",
      "Epoch [24/100], Training Loss: 346084.28096676734, Test Loss: 47912948.0\n",
      "Epoch [25/100], Training Loss: 335036.3809608435, Test Loss: 46539272.0\n",
      "Epoch [26/100], Training Loss: 324605.4853385463, Test Loss: 45241020.0\n",
      "Epoch [27/100], Training Loss: 314761.2384337421, Test Loss: 44012712.0\n",
      "Epoch [28/100], Training Loss: 305467.5197559386, Test Loss: 42845440.0\n",
      "Epoch [29/100], Training Loss: 296666.8372134352, Test Loss: 41731604.0\n",
      "Epoch [30/100], Training Loss: 288341.3018186126, Test Loss: 40671068.0\n",
      "Epoch [31/100], Training Loss: 280439.2419880339, Test Loss: 39660296.0\n",
      "Epoch [32/100], Training Loss: 272929.0754102245, Test Loss: 38694312.0\n",
      "Epoch [33/100], Training Loss: 265781.83442923997, Test Loss: 37768872.0\n",
      "Epoch [34/100], Training Loss: 258948.6810023103, Test Loss: 36880104.0\n",
      "Epoch [35/100], Training Loss: 252396.01155144838, Test Loss: 36027168.0\n",
      "Epoch [36/100], Training Loss: 246094.63817309402, Test Loss: 35207496.0\n",
      "Epoch [37/100], Training Loss: 240038.02896747823, Test Loss: 34417460.0\n",
      "Epoch [38/100], Training Loss: 234225.34956459925, Test Loss: 33656696.0\n",
      "Epoch [39/100], Training Loss: 228666.99250636811, Test Loss: 32928308.0\n",
      "Epoch [40/100], Training Loss: 223374.21438303418, Test Loss: 32235212.0\n",
      "Epoch [41/100], Training Loss: 218334.52259937208, Test Loss: 31575352.0\n",
      "Epoch [42/100], Training Loss: 213531.03696463478, Test Loss: 30946924.0\n",
      "Epoch [43/100], Training Loss: 208950.1440969137, Test Loss: 30348288.0\n",
      "Epoch [44/100], Training Loss: 204570.1198981103, Test Loss: 29774920.0\n",
      "Epoch [45/100], Training Loss: 200384.09877969316, Test Loss: 29226004.0\n",
      "Epoch [46/100], Training Loss: 196370.05423256915, Test Loss: 28697420.0\n",
      "Epoch [47/100], Training Loss: 192491.22794858125, Test Loss: 28190328.0\n",
      "Epoch [48/100], Training Loss: 188769.0041170547, Test Loss: 27702848.0\n",
      "Epoch [49/100], Training Loss: 185191.52508737633, Test Loss: 27232456.0\n",
      "Epoch [50/100], Training Loss: 181747.9610805047, Test Loss: 26778210.0\n",
      "Epoch [51/100], Training Loss: 178440.48332444762, Test Loss: 26345422.0\n",
      "Epoch [52/100], Training Loss: 175269.33365914342, Test Loss: 25930090.0\n",
      "Epoch [53/100], Training Loss: 172222.64060186007, Test Loss: 25534730.0\n",
      "Epoch [54/100], Training Loss: 169285.94061370773, Test Loss: 25153164.0\n",
      "Epoch [55/100], Training Loss: 166447.9171850009, Test Loss: 24787782.0\n",
      "Epoch [56/100], Training Loss: 163717.08260766542, Test Loss: 24434496.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [57/100], Training Loss: 161080.82877199218, Test Loss: 24091590.0\n",
      "Epoch [58/100], Training Loss: 158521.88646999586, Test Loss: 23762164.0\n",
      "Epoch [59/100], Training Loss: 156025.2897340205, Test Loss: 23443842.0\n",
      "Epoch [60/100], Training Loss: 153583.16195723, Test Loss: 23138852.0\n",
      "Epoch [61/100], Training Loss: 151211.62330430662, Test Loss: 22840530.0\n",
      "Epoch [62/100], Training Loss: 148911.6346188022, Test Loss: 22553702.0\n",
      "Epoch [63/100], Training Loss: 146676.16645933298, Test Loss: 22274082.0\n",
      "Epoch [64/100], Training Loss: 144510.43652627213, Test Loss: 22004640.0\n",
      "Epoch [65/100], Training Loss: 142413.58050470945, Test Loss: 21748406.0\n",
      "Epoch [66/100], Training Loss: 140384.7657721699, Test Loss: 21501476.0\n",
      "Epoch [67/100], Training Loss: 138421.9129494698, Test Loss: 21263270.0\n",
      "Epoch [68/100], Training Loss: 136515.5072862982, Test Loss: 21031836.0\n",
      "Epoch [69/100], Training Loss: 134655.62813221966, Test Loss: 20809630.0\n",
      "Epoch [70/100], Training Loss: 132837.36558853148, Test Loss: 20592312.0\n",
      "Epoch [71/100], Training Loss: 131056.17093181683, Test Loss: 20381198.0\n",
      "Epoch [72/100], Training Loss: 129310.91842900302, Test Loss: 20174124.0\n",
      "Epoch [73/100], Training Loss: 127606.88251584621, Test Loss: 19977120.0\n",
      "Epoch [74/100], Training Loss: 125956.61584029382, Test Loss: 19787560.0\n",
      "Epoch [75/100], Training Loss: 124367.08574728985, Test Loss: 19609498.0\n",
      "Epoch [76/100], Training Loss: 122833.95237248979, Test Loss: 19440584.0\n",
      "Epoch [77/100], Training Loss: 121349.77659795036, Test Loss: 19277310.0\n",
      "Epoch [78/100], Training Loss: 119911.48199158818, Test Loss: 19123846.0\n",
      "Epoch [79/100], Training Loss: 118515.12478526153, Test Loss: 18973800.0\n",
      "Epoch [80/100], Training Loss: 117157.52307327765, Test Loss: 18833530.0\n",
      "Epoch [81/100], Training Loss: 115834.59334458859, Test Loss: 18695476.0\n",
      "Epoch [82/100], Training Loss: 114542.61305609858, Test Loss: 18561346.0\n",
      "Epoch [83/100], Training Loss: 113282.68950299153, Test Loss: 18432554.0\n",
      "Epoch [84/100], Training Loss: 112060.47902967833, Test Loss: 18308644.0\n",
      "Epoch [85/100], Training Loss: 110881.18603755701, Test Loss: 18191794.0\n",
      "Epoch [86/100], Training Loss: 109747.7564125348, Test Loss: 18083076.0\n",
      "Epoch [87/100], Training Loss: 108654.21183579172, Test Loss: 17979364.0\n",
      "Epoch [88/100], Training Loss: 107598.17666311238, Test Loss: 17881620.0\n",
      "Epoch [89/100], Training Loss: 106576.24914104615, Test Loss: 17786198.0\n",
      "Epoch [90/100], Training Loss: 105585.92235353356, Test Loss: 17696490.0\n",
      "Epoch [91/100], Training Loss: 104626.71417570049, Test Loss: 17609198.0\n",
      "Epoch [92/100], Training Loss: 103689.7371156922, Test Loss: 17527238.0\n",
      "Epoch [93/100], Training Loss: 102777.04567264972, Test Loss: 17445630.0\n",
      "Epoch [94/100], Training Loss: 101884.76150701972, Test Loss: 17370564.0\n",
      "Epoch [95/100], Training Loss: 101007.20700787868, Test Loss: 17293812.0\n",
      "Epoch [96/100], Training Loss: 100143.46285765061, Test Loss: 17223940.0\n",
      "Epoch [97/100], Training Loss: 99300.89776968189, Test Loss: 17152718.0\n",
      "Epoch [98/100], Training Loss: 98476.79220721521, Test Loss: 17087822.0\n",
      "Epoch [99/100], Training Loss: 97674.0562022392, Test Loss: 17018950.0\n",
      "Epoch [100/100], Training Loss: 96897.1345003258, Test Loss: 16965042.0\n",
      "Epoch [1/100], Training Loss: 1497615.0038504829, Test Loss: 298580352.0\n",
      "Epoch [2/100], Training Loss: 1463876.7864463006, Test Loss: 283348000.0\n",
      "Epoch [3/100], Training Loss: 1324135.4929210355, Test Loss: 241585376.0\n",
      "Epoch [4/100], Training Loss: 1079498.1084059002, Test Loss: 190416384.0\n",
      "Epoch [5/100], Training Loss: 868135.8201528345, Test Loss: 160056256.0\n",
      "Epoch [6/100], Training Loss: 761761.6029856051, Test Loss: 146183280.0\n",
      "Epoch [7/100], Training Loss: 704072.4824358746, Test Loss: 136958400.0\n",
      "Epoch [8/100], Training Loss: 659743.6324862271, Test Loss: 129266328.0\n",
      "Epoch [9/100], Training Loss: 620988.1469107281, Test Loss: 122457800.0\n",
      "Epoch [10/100], Training Loss: 585920.4776968189, Test Loss: 116325176.0\n",
      "Epoch [11/100], Training Loss: 553849.1473253954, Test Loss: 110766520.0\n",
      "Epoch [12/100], Training Loss: 524155.35762099404, Test Loss: 105610784.0\n",
      "Epoch [13/100], Training Loss: 496282.2256975298, Test Loss: 100908600.0\n",
      "Epoch [14/100], Training Loss: 471143.9189621468, Test Loss: 96787472.0\n",
      "Epoch [15/100], Training Loss: 448716.7660683609, Test Loss: 93122328.0\n",
      "Epoch [16/100], Training Loss: 428564.9390438955, Test Loss: 89838200.0\n",
      "Epoch [17/100], Training Loss: 410385.15111664, Test Loss: 86869672.0\n",
      "Epoch [18/100], Training Loss: 393939.8489425982, Test Loss: 84168920.0\n",
      "Epoch [19/100], Training Loss: 379041.91232746874, Test Loss: 81702328.0\n",
      "Epoch [20/100], Training Loss: 365546.1678810497, Test Loss: 79443512.0\n",
      "Epoch [21/100], Training Loss: 353317.03346958116, Test Loss: 77368288.0\n",
      "Epoch [22/100], Training Loss: 342222.4701143297, Test Loss: 75456384.0\n",
      "Epoch [23/100], Training Loss: 332148.50814525207, Test Loss: 73692312.0\n",
      "Epoch [24/100], Training Loss: 322986.4716545228, Test Loss: 72055496.0\n",
      "Epoch [25/100], Training Loss: 314618.06006753154, Test Loss: 70528696.0\n",
      "Epoch [26/100], Training Loss: 306929.75019252417, Test Loss: 69094024.0\n",
      "Epoch [27/100], Training Loss: 299816.16509685444, Test Loss: 67738920.0\n",
      "Epoch [28/100], Training Loss: 293201.34977193293, Test Loss: 66454884.0\n",
      "Epoch [29/100], Training Loss: 287007.7145607488, Test Loss: 65232044.0\n",
      "Epoch [30/100], Training Loss: 281163.05029322905, Test Loss: 64058768.0\n",
      "Epoch [31/100], Training Loss: 275607.55734257447, Test Loss: 62927376.0\n",
      "Epoch [32/100], Training Loss: 270298.5332048605, Test Loss: 61833628.0\n",
      "Epoch [33/100], Training Loss: 265194.71302425803, Test Loss: 60774728.0\n",
      "Epoch [34/100], Training Loss: 260266.94831837568, Test Loss: 59745868.0\n",
      "Epoch [35/100], Training Loss: 255483.8713568509, Test Loss: 58741944.0\n",
      "Epoch [36/100], Training Loss: 250824.14424130679, Test Loss: 57761732.0\n",
      "Epoch [37/100], Training Loss: 246281.20189117943, Test Loss: 56805096.0\n",
      "Epoch [38/100], Training Loss: 241839.21165807714, Test Loss: 55870028.0\n",
      "Epoch [39/100], Training Loss: 237480.93245364612, Test Loss: 54954636.0\n",
      "Epoch [40/100], Training Loss: 233204.70363130147, Test Loss: 54057712.0\n",
      "Epoch [41/100], Training Loss: 229011.6185652509, Test Loss: 53178668.0\n",
      "Epoch [42/100], Training Loss: 224896.8892838102, Test Loss: 52316260.0\n",
      "Epoch [43/100], Training Loss: 220859.5122474972, Test Loss: 51470532.0\n",
      "Epoch [44/100], Training Loss: 216894.5664356377, Test Loss: 50642476.0\n",
      "Epoch [45/100], Training Loss: 213015.79678336592, Test Loss: 49835340.0\n",
      "Epoch [46/100], Training Loss: 209224.8693797761, Test Loss: 49050312.0\n",
      "Epoch [47/100], Training Loss: 205519.75233990877, Test Loss: 48284748.0\n",
      "Epoch [48/100], Training Loss: 201907.8619750015, Test Loss: 47538964.0\n",
      "Epoch [49/100], Training Loss: 198395.15555209998, Test Loss: 46810108.0\n",
      "Epoch [50/100], Training Loss: 194972.0419776672, Test Loss: 46097068.0\n",
      "Epoch [51/100], Training Loss: 191628.9068035069, Test Loss: 45400684.0\n",
      "Epoch [52/100], Training Loss: 188367.65600379126, Test Loss: 44721832.0\n",
      "Epoch [53/100], Training Loss: 185188.86804691664, Test Loss: 44059460.0\n",
      "Epoch [54/100], Training Loss: 182090.09444790002, Test Loss: 43410288.0\n",
      "Epoch [55/100], Training Loss: 179074.07898302824, Test Loss: 42776224.0\n",
      "Epoch [56/100], Training Loss: 176139.02336576625, Test Loss: 42158764.0\n",
      "Epoch [57/100], Training Loss: 173282.21420902197, Test Loss: 41556288.0\n",
      "Epoch [58/100], Training Loss: 170509.66299760086, Test Loss: 40967888.0\n",
      "Epoch [59/100], Training Loss: 167815.28218485278, Test Loss: 40392888.0\n",
      "Epoch [60/100], Training Loss: 165194.57860723595, Test Loss: 39831128.0\n",
      "Epoch [61/100], Training Loss: 162643.69713731413, Test Loss: 39280796.0\n",
      "Epoch [62/100], Training Loss: 160159.49808494016, Test Loss: 38742848.0\n",
      "Epoch [63/100], Training Loss: 157737.6333345366, Test Loss: 38217736.0\n",
      "Epoch [64/100], Training Loss: 155378.61337728068, Test Loss: 37706628.0\n",
      "Epoch [65/100], Training Loss: 153077.86560542363, Test Loss: 37206440.0\n",
      "Epoch [66/100], Training Loss: 150830.91614509287, Test Loss: 36716796.0\n",
      "Epoch [67/100], Training Loss: 148639.14505484854, Test Loss: 36238276.0\n",
      "Epoch [68/100], Training Loss: 146505.57118076767, Test Loss: 35772872.0\n",
      "Epoch [69/100], Training Loss: 144434.7939987151, Test Loss: 35320132.0\n",
      "Epoch [70/100], Training Loss: 142421.12074989078, Test Loss: 34880248.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [71/100], Training Loss: 140461.88786413267, Test Loss: 34452468.0\n",
      "Epoch [72/100], Training Loss: 138559.90133389493, Test Loss: 34037304.0\n",
      "Epoch [73/100], Training Loss: 136713.74808482447, Test Loss: 33633992.0\n",
      "Epoch [74/100], Training Loss: 134916.23323125156, Test Loss: 33240942.0\n",
      "Epoch [75/100], Training Loss: 133162.51682573048, Test Loss: 32857784.0\n",
      "Epoch [76/100], Training Loss: 131452.34960370572, Test Loss: 32482838.0\n",
      "Epoch [77/100], Training Loss: 129779.219450214, Test Loss: 32115434.0\n",
      "Epoch [78/100], Training Loss: 128143.42156515646, Test Loss: 31757470.0\n",
      "Epoch [79/100], Training Loss: 126552.93959231162, Test Loss: 31410390.0\n",
      "Epoch [80/100], Training Loss: 125006.06164428574, Test Loss: 31072418.0\n",
      "Epoch [81/100], Training Loss: 123501.60074380961, Test Loss: 30743298.0\n",
      "Epoch [82/100], Training Loss: 122035.43506799435, Test Loss: 30422322.0\n",
      "Epoch [83/100], Training Loss: 120599.98784830209, Test Loss: 30110078.0\n",
      "Epoch [84/100], Training Loss: 119195.17237343389, Test Loss: 29803016.0\n",
      "Epoch [85/100], Training Loss: 117823.2209325573, Test Loss: 29506166.0\n",
      "Epoch [86/100], Training Loss: 116485.96218474171, Test Loss: 29214384.0\n",
      "Epoch [87/100], Training Loss: 115184.28637225283, Test Loss: 28933522.0\n",
      "Epoch [88/100], Training Loss: 113916.8501370809, Test Loss: 28658422.0\n",
      "Epoch [89/100], Training Loss: 112684.17680380309, Test Loss: 28390604.0\n",
      "Epoch [90/100], Training Loss: 111483.10958788727, Test Loss: 28130392.0\n",
      "Epoch [91/100], Training Loss: 110314.13206322936, Test Loss: 27878218.0\n",
      "Epoch [92/100], Training Loss: 109172.62338159395, Test Loss: 27630656.0\n",
      "Epoch [93/100], Training Loss: 108052.80599351712, Test Loss: 27389786.0\n",
      "Epoch [94/100], Training Loss: 106955.39070089893, Test Loss: 27152564.0\n",
      "Epoch [95/100], Training Loss: 105876.68707746505, Test Loss: 26921726.0\n",
      "Epoch [96/100], Training Loss: 104814.0543080053, Test Loss: 26694252.0\n",
      "Epoch [97/100], Training Loss: 103764.15838257509, Test Loss: 26470618.0\n",
      "Epoch [98/100], Training Loss: 102727.26053514305, Test Loss: 26251608.0\n",
      "Epoch [99/100], Training Loss: 101707.12057101919, Test Loss: 26039840.0\n",
      "Epoch [100/100], Training Loss: 100709.94191324567, Test Loss: 25832620.0\n",
      "Epoch [1/100], Training Loss: 1164497.1203127776, Test Loss: 299429120.0\n",
      "Epoch [2/100], Training Loss: 1151132.6343226111, Test Loss: 291250688.0\n",
      "Epoch [3/100], Training Loss: 1086769.300396896, Test Loss: 264805344.0\n",
      "Epoch [4/100], Training Loss: 947067.0159350749, Test Loss: 221130768.0\n",
      "Epoch [5/100], Training Loss: 773750.1403945264, Test Loss: 179841600.0\n",
      "Epoch [6/100], Training Loss: 646807.7047568272, Test Loss: 156697648.0\n",
      "Epoch [7/100], Training Loss: 582251.4486108642, Test Loss: 145180240.0\n",
      "Epoch [8/100], Training Loss: 544825.0236360405, Test Loss: 137200624.0\n",
      "Epoch [9/100], Training Loss: 515499.2322729696, Test Loss: 130413584.0\n",
      "Epoch [10/100], Training Loss: 489500.9632130798, Test Loss: 124251224.0\n",
      "Epoch [11/100], Training Loss: 465483.79693146143, Test Loss: 118514952.0\n",
      "Epoch [12/100], Training Loss: 442932.0789052781, Test Loss: 113125472.0\n",
      "Epoch [13/100], Training Loss: 421652.77080741664, Test Loss: 108057824.0\n",
      "Epoch [14/100], Training Loss: 401605.398258397, Test Loss: 103310304.0\n",
      "Epoch [15/100], Training Loss: 382775.9523724898, Test Loss: 98856704.0\n",
      "Epoch [16/100], Training Loss: 364917.99324684555, Test Loss: 94629992.0\n",
      "Epoch [17/100], Training Loss: 348055.92488596647, Test Loss: 90711616.0\n",
      "Epoch [18/100], Training Loss: 332607.6246667851, Test Loss: 87167960.0\n",
      "Epoch [19/100], Training Loss: 318624.6788697352, Test Loss: 83945408.0\n",
      "Epoch [20/100], Training Loss: 305897.5108109709, Test Loss: 80991152.0\n",
      "Epoch [21/100], Training Loss: 294258.8558734672, Test Loss: 78271784.0\n",
      "Epoch [22/100], Training Loss: 283587.8338960962, Test Loss: 75763032.0\n",
      "Epoch [23/100], Training Loss: 273782.3465434512, Test Loss: 73439976.0\n",
      "Epoch [24/100], Training Loss: 264749.57111545524, Test Loss: 71284488.0\n",
      "Epoch [25/100], Training Loss: 256409.49789704403, Test Loss: 69280912.0\n",
      "Epoch [26/100], Training Loss: 248695.9338901724, Test Loss: 67413520.0\n",
      "Epoch [27/100], Training Loss: 241547.0902197737, Test Loss: 65673656.0\n",
      "Epoch [28/100], Training Loss: 234901.088798057, Test Loss: 64047588.0\n",
      "Epoch [29/100], Training Loss: 228698.11136780996, Test Loss: 62520176.0\n",
      "Epoch [30/100], Training Loss: 222883.23950002962, Test Loss: 61080212.0\n",
      "Epoch [31/100], Training Loss: 217411.54552455424, Test Loss: 59718908.0\n",
      "Epoch [32/100], Training Loss: 212229.82749837096, Test Loss: 58427928.0\n",
      "Epoch [33/100], Training Loss: 207304.80729814584, Test Loss: 57200896.0\n",
      "Epoch [34/100], Training Loss: 202611.94905515076, Test Loss: 56030268.0\n",
      "Epoch [35/100], Training Loss: 198126.5936852082, Test Loss: 54911676.0\n",
      "Epoch [36/100], Training Loss: 193825.6497837806, Test Loss: 53839816.0\n",
      "Epoch [37/100], Training Loss: 189685.53960073454, Test Loss: 52809880.0\n",
      "Epoch [38/100], Training Loss: 185702.62472602335, Test Loss: 51821820.0\n",
      "Epoch [39/100], Training Loss: 181869.9594810734, Test Loss: 50869180.0\n",
      "Epoch [40/100], Training Loss: 178177.24234346306, Test Loss: 49951560.0\n",
      "Epoch [41/100], Training Loss: 174618.29654641313, Test Loss: 49064568.0\n",
      "Epoch [42/100], Training Loss: 171191.31887921333, Test Loss: 48207096.0\n",
      "Epoch [43/100], Training Loss: 167891.08168947336, Test Loss: 47378472.0\n",
      "Epoch [44/100], Training Loss: 164708.89200876726, Test Loss: 46577256.0\n",
      "Epoch [45/100], Training Loss: 161635.65985427404, Test Loss: 45802816.0\n",
      "Epoch [46/100], Training Loss: 158662.85516260884, Test Loss: 45052684.0\n",
      "Epoch [47/100], Training Loss: 155786.9683075647, Test Loss: 44324884.0\n",
      "Epoch [48/100], Training Loss: 153005.37160120846, Test Loss: 43618540.0\n",
      "Epoch [49/100], Training Loss: 150313.6436230081, Test Loss: 42931604.0\n",
      "Epoch [50/100], Training Loss: 147704.46821870742, Test Loss: 42263948.0\n",
      "Epoch [51/100], Training Loss: 145174.03406196315, Test Loss: 41617052.0\n",
      "Epoch [52/100], Training Loss: 142721.52147384634, Test Loss: 40987568.0\n",
      "Epoch [53/100], Training Loss: 140344.62828031514, Test Loss: 40373876.0\n",
      "Epoch [54/100], Training Loss: 138037.0164089805, Test Loss: 39777592.0\n",
      "Epoch [55/100], Training Loss: 135797.61193057284, Test Loss: 39197608.0\n",
      "Epoch [56/100], Training Loss: 133626.02867128723, Test Loss: 38634656.0\n",
      "Epoch [57/100], Training Loss: 131523.0663467804, Test Loss: 38087360.0\n",
      "Epoch [58/100], Training Loss: 129484.3411527753, Test Loss: 37556068.0\n",
      "Epoch [59/100], Training Loss: 127510.22907410699, Test Loss: 37041344.0\n",
      "Epoch [60/100], Training Loss: 125598.23476097388, Test Loss: 36543268.0\n",
      "Epoch [61/100], Training Loss: 123743.50767134648, Test Loss: 36060424.0\n",
      "Epoch [62/100], Training Loss: 121945.80587642912, Test Loss: 35591352.0\n",
      "Epoch [63/100], Training Loss: 120201.65458207452, Test Loss: 35134536.0\n",
      "Epoch [64/100], Training Loss: 118504.03726082577, Test Loss: 34690804.0\n",
      "Epoch [65/100], Training Loss: 116853.37669569338, Test Loss: 34259624.0\n",
      "Epoch [66/100], Training Loss: 115247.93845151353, Test Loss: 33840208.0\n",
      "Epoch [67/100], Training Loss: 113685.52656833126, Test Loss: 33431822.0\n",
      "Epoch [68/100], Training Loss: 112164.45885907233, Test Loss: 33033246.0\n",
      "Epoch [69/100], Training Loss: 110681.21183579172, Test Loss: 32645662.0\n",
      "Epoch [70/100], Training Loss: 109233.58793910313, Test Loss: 32267796.0\n",
      "Epoch [71/100], Training Loss: 107820.82714294177, Test Loss: 31899764.0\n",
      "Epoch [72/100], Training Loss: 106441.97156566555, Test Loss: 31541420.0\n",
      "Epoch [73/100], Training Loss: 105098.53213672175, Test Loss: 31191966.0\n",
      "Epoch [74/100], Training Loss: 103793.1990403412, Test Loss: 30852180.0\n",
      "Epoch [75/100], Training Loss: 102523.1365440436, Test Loss: 30520488.0\n",
      "Epoch [76/100], Training Loss: 101283.39914696997, Test Loss: 30195172.0\n",
      "Epoch [77/100], Training Loss: 100068.22208399976, Test Loss: 29878558.0\n",
      "Epoch [78/100], Training Loss: 98880.88472246905, Test Loss: 29571044.0\n",
      "Epoch [79/100], Training Loss: 97723.12588116818, Test Loss: 29271920.0\n",
      "Epoch [80/100], Training Loss: 96597.36366329009, Test Loss: 28980662.0\n",
      "Epoch [81/100], Training Loss: 95501.98045139507, Test Loss: 28697726.0\n",
      "Epoch [82/100], Training Loss: 94437.67140572242, Test Loss: 28423270.0\n",
      "Epoch [83/100], Training Loss: 93405.50530181862, Test Loss: 28157516.0\n",
      "Epoch [84/100], Training Loss: 92401.96990699603, Test Loss: 27899710.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [85/100], Training Loss: 91425.17498963332, Test Loss: 27648398.0\n",
      "Epoch [86/100], Training Loss: 90474.6918429003, Test Loss: 27402398.0\n",
      "Epoch [87/100], Training Loss: 89548.35519222794, Test Loss: 27163036.0\n",
      "Epoch [88/100], Training Loss: 88641.52295480126, Test Loss: 26929304.0\n",
      "Epoch [89/100], Training Loss: 87754.71393874771, Test Loss: 26700382.0\n",
      "Epoch [90/100], Training Loss: 86887.15627036314, Test Loss: 26476828.0\n",
      "Epoch [91/100], Training Loss: 86039.30685385937, Test Loss: 26257974.0\n",
      "Epoch [92/100], Training Loss: 85208.22232095255, Test Loss: 26042028.0\n",
      "Epoch [93/100], Training Loss: 84394.40874355784, Test Loss: 25831748.0\n",
      "Epoch [94/100], Training Loss: 83595.52165156092, Test Loss: 25625460.0\n",
      "Epoch [95/100], Training Loss: 82808.15520407559, Test Loss: 25423556.0\n",
      "Epoch [96/100], Training Loss: 82030.74101060364, Test Loss: 25225816.0\n",
      "Epoch [97/100], Training Loss: 81262.34061963153, Test Loss: 25031250.0\n",
      "Epoch [98/100], Training Loss: 80501.10775427996, Test Loss: 24837810.0\n",
      "Epoch [99/100], Training Loss: 79745.7676677922, Test Loss: 24647036.0\n",
      "Epoch [100/100], Training Loss: 79000.74172146199, Test Loss: 24458166.0\n",
      "Epoch [1/100], Training Loss: 582422.6687992418, Test Loss: 300034528.0\n",
      "Epoch [2/100], Training Loss: 581912.7985308927, Test Loss: 299413216.0\n",
      "Epoch [3/100], Training Loss: 579281.9979859013, Test Loss: 297033248.0\n",
      "Epoch [4/100], Training Loss: 571774.214323796, Test Loss: 291349120.0\n",
      "Epoch [5/100], Training Loss: 556481.6738344885, Test Loss: 281023776.0\n",
      "Epoch [6/100], Training Loss: 531414.9967418992, Test Loss: 265458960.0\n",
      "Epoch [7/100], Training Loss: 496488.8155914934, Test Loss: 245257104.0\n",
      "Epoch [8/100], Training Loss: 454216.5843255731, Test Loss: 222416624.0\n",
      "Epoch [9/100], Training Loss: 409577.3454179255, Test Loss: 199944176.0\n",
      "Epoch [10/100], Training Loss: 368573.04330312187, Test Loss: 180772832.0\n",
      "Epoch [11/100], Training Loss: 335719.69622652687, Test Loss: 166398496.0\n",
      "Epoch [12/100], Training Loss: 312013.35371127306, Test Loss: 156387760.0\n",
      "Epoch [13/100], Training Loss: 295415.5910194894, Test Loss: 149300272.0\n",
      "Epoch [14/100], Training Loss: 283149.35750251764, Test Loss: 143848112.0\n",
      "Epoch [15/100], Training Loss: 273224.4118239441, Test Loss: 139265632.0\n",
      "Epoch [16/100], Training Loss: 264570.12594040635, Test Loss: 135175408.0\n",
      "Epoch [17/100], Training Loss: 256674.68941413422, Test Loss: 131398888.0\n",
      "Epoch [18/100], Training Loss: 249289.49280255908, Test Loss: 127845512.0\n",
      "Epoch [19/100], Training Loss: 242281.59184882412, Test Loss: 124464288.0\n",
      "Epoch [20/100], Training Loss: 235572.7324210651, Test Loss: 121224320.0\n",
      "Epoch [21/100], Training Loss: 229114.54913808423, Test Loss: 118106256.0\n",
      "Epoch [22/100], Training Loss: 222877.05775724186, Test Loss: 115098272.0\n",
      "Epoch [23/100], Training Loss: 216842.3429891594, Test Loss: 112193448.0\n",
      "Epoch [24/100], Training Loss: 211000.61323381317, Test Loss: 109388216.0\n",
      "Epoch [25/100], Training Loss: 205347.85498489425, Test Loss: 106681352.0\n",
      "Epoch [26/100], Training Loss: 199883.6497837806, Test Loss: 104072208.0\n",
      "Epoch [27/100], Training Loss: 194606.5368165393, Test Loss: 101557960.0\n",
      "Epoch [28/100], Training Loss: 189499.0405781648, Test Loss: 99119856.0\n",
      "Epoch [29/100], Training Loss: 184527.06522125466, Test Loss: 96751424.0\n",
      "Epoch [30/100], Training Loss: 179690.2764054262, Test Loss: 94463928.0\n",
      "Epoch [31/100], Training Loss: 175032.3229666489, Test Loss: 92269280.0\n",
      "Epoch [32/100], Training Loss: 170575.41425271015, Test Loss: 90183016.0\n",
      "Epoch [33/100], Training Loss: 166352.49380960842, Test Loss: 88207120.0\n",
      "Epoch [34/100], Training Loss: 162353.8375688644, Test Loss: 86344400.0\n",
      "Epoch [35/100], Training Loss: 158577.87761388544, Test Loss: 84580640.0\n",
      "Epoch [36/100], Training Loss: 155004.6994846277, Test Loss: 82907968.0\n",
      "Epoch [37/100], Training Loss: 151617.9574669747, Test Loss: 81316832.0\n",
      "Epoch [38/100], Training Loss: 148403.1382027131, Test Loss: 79802576.0\n",
      "Epoch [39/100], Training Loss: 145347.83673952965, Test Loss: 78358112.0\n",
      "Epoch [40/100], Training Loss: 142439.43486760263, Test Loss: 76977664.0\n",
      "Epoch [41/100], Training Loss: 139667.97369824062, Test Loss: 75656768.0\n",
      "Epoch [42/100], Training Loss: 137024.15378235886, Test Loss: 74390968.0\n",
      "Epoch [43/100], Training Loss: 134499.2621290208, Test Loss: 73176776.0\n",
      "Epoch [44/100], Training Loss: 132085.11012380783, Test Loss: 72011168.0\n",
      "Epoch [45/100], Training Loss: 129773.31698359102, Test Loss: 70890520.0\n",
      "Epoch [46/100], Training Loss: 127557.74705289971, Test Loss: 69812944.0\n",
      "Epoch [47/100], Training Loss: 125430.81168177241, Test Loss: 68773256.0\n",
      "Epoch [48/100], Training Loss: 123383.57064154967, Test Loss: 67768112.0\n",
      "Epoch [49/100], Training Loss: 121413.5295302411, Test Loss: 66797752.0\n",
      "Epoch [50/100], Training Loss: 119517.43166874, Test Loss: 65861212.0\n",
      "Epoch [51/100], Training Loss: 117691.35880575795, Test Loss: 64955640.0\n",
      "Epoch [52/100], Training Loss: 115928.67105029323, Test Loss: 64078812.0\n",
      "Epoch [53/100], Training Loss: 114226.28138143475, Test Loss: 63230480.0\n",
      "Epoch [54/100], Training Loss: 112582.74201765298, Test Loss: 62409564.0\n",
      "Epoch [55/100], Training Loss: 110993.37716959896, Test Loss: 61612712.0\n",
      "Epoch [56/100], Training Loss: 109453.72276523903, Test Loss: 60838432.0\n",
      "Epoch [57/100], Training Loss: 107960.07843137255, Test Loss: 60085844.0\n",
      "Epoch [58/100], Training Loss: 106509.70914045376, Test Loss: 59353748.0\n",
      "Epoch [59/100], Training Loss: 105099.95450506486, Test Loss: 58642184.0\n",
      "Epoch [60/100], Training Loss: 103728.62105325513, Test Loss: 57949948.0\n",
      "Epoch [61/100], Training Loss: 102390.74474261004, Test Loss: 57274876.0\n",
      "Epoch [62/100], Training Loss: 101084.80196670814, Test Loss: 56615440.0\n",
      "Epoch [63/100], Training Loss: 99809.4871156922, Test Loss: 55972068.0\n",
      "Epoch [64/100], Training Loss: 98563.53557253718, Test Loss: 55343692.0\n",
      "Epoch [65/100], Training Loss: 97346.18233516972, Test Loss: 54728728.0\n",
      "Epoch [66/100], Training Loss: 96154.94425685682, Test Loss: 54126244.0\n",
      "Epoch [67/100], Training Loss: 94988.55115218293, Test Loss: 53536236.0\n",
      "Epoch [68/100], Training Loss: 93846.06990107222, Test Loss: 52958508.0\n",
      "Epoch [69/100], Training Loss: 92726.41182394407, Test Loss: 52392424.0\n",
      "Epoch [70/100], Training Loss: 91628.59877969314, Test Loss: 51837044.0\n",
      "Epoch [71/100], Training Loss: 90551.04958237072, Test Loss: 51291856.0\n",
      "Epoch [72/100], Training Loss: 89493.02541318642, Test Loss: 50756484.0\n",
      "Epoch [73/100], Training Loss: 88453.96161364848, Test Loss: 50230312.0\n",
      "Epoch [74/100], Training Loss: 87434.1919317576, Test Loss: 49713356.0\n",
      "Epoch [75/100], Training Loss: 86434.01990403412, Test Loss: 49205716.0\n",
      "Epoch [76/100], Training Loss: 85453.11509981636, Test Loss: 48705696.0\n",
      "Epoch [77/100], Training Loss: 84490.56501392098, Test Loss: 48215032.0\n",
      "Epoch [78/100], Training Loss: 83545.9649309875, Test Loss: 47733832.0\n",
      "Epoch [79/100], Training Loss: 82618.80717966946, Test Loss: 47262264.0\n",
      "Epoch [80/100], Training Loss: 81709.71411646229, Test Loss: 46800692.0\n",
      "Epoch [81/100], Training Loss: 80818.25922634915, Test Loss: 46348476.0\n",
      "Epoch [82/100], Training Loss: 79944.43480836443, Test Loss: 45904140.0\n",
      "Epoch [83/100], Training Loss: 79087.80747586043, Test Loss: 45468404.0\n",
      "Epoch [84/100], Training Loss: 78246.83040104259, Test Loss: 45040444.0\n",
      "Epoch [85/100], Training Loss: 77421.79396955157, Test Loss: 44619912.0\n",
      "Epoch [86/100], Training Loss: 76613.70937740656, Test Loss: 44207292.0\n",
      "Epoch [87/100], Training Loss: 75821.763995024, Test Loss: 43802672.0\n",
      "Epoch [88/100], Training Loss: 75046.05544695219, Test Loss: 43405472.0\n",
      "Epoch [89/100], Training Loss: 74286.26088501867, Test Loss: 43015604.0\n",
      "Epoch [90/100], Training Loss: 73541.98874474262, Test Loss: 42632592.0\n",
      "Epoch [91/100], Training Loss: 72811.85581422901, Test Loss: 42257092.0\n",
      "Epoch [92/100], Training Loss: 72095.00799715657, Test Loss: 41888212.0\n",
      "Epoch [93/100], Training Loss: 71391.10277827142, Test Loss: 41524732.0\n",
      "Epoch [94/100], Training Loss: 70699.08737634026, Test Loss: 41166296.0\n",
      "Epoch [95/100], Training Loss: 70018.80954919732, Test Loss: 40813192.0\n",
      "Epoch [96/100], Training Loss: 69350.12404478408, Test Loss: 40465616.0\n",
      "Epoch [97/100], Training Loss: 68692.51703098157, Test Loss: 40123304.0\n",
      "Epoch [98/100], Training Loss: 68044.80611338191, Test Loss: 39786300.0\n",
      "Epoch [99/100], Training Loss: 67408.0979799775, Test Loss: 39455448.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Training Loss: 66782.20768911795, Test Loss: 39129508.0\n",
      "Epoch [1/100], Training Loss: 7320930.70860731, Test Loss: 151733504.0\n",
      "Epoch [2/100], Training Loss: 4160224.99638647, Test Loss: 120808216.0\n",
      "Epoch [3/100], Training Loss: 3367911.2873644927, Test Loss: 99749480.0\n",
      "Epoch [4/100], Training Loss: 2782972.880457319, Test Loss: 84170264.0\n",
      "Epoch [5/100], Training Loss: 2341925.408802796, Test Loss: 72339384.0\n",
      "Epoch [6/100], Training Loss: 2012031.0058053434, Test Loss: 63466844.0\n",
      "Epoch [7/100], Training Loss: 1769684.8862922813, Test Loss: 56940176.0\n",
      "Epoch [8/100], Training Loss: 1591601.833836858, Test Loss: 52027220.0\n",
      "Epoch [9/100], Training Loss: 1453673.308542148, Test Loss: 48063280.0\n",
      "Epoch [10/100], Training Loss: 1339165.9224868196, Test Loss: 44677596.0\n",
      "Epoch [11/100], Training Loss: 1240040.4398732302, Test Loss: 41679184.0\n",
      "Epoch [12/100], Training Loss: 1152377.3664178662, Test Loss: 38978496.0\n",
      "Epoch [13/100], Training Loss: 1074413.209629169, Test Loss: 36533024.0\n",
      "Epoch [14/100], Training Loss: 1005097.2251051478, Test Loss: 34322220.0\n",
      "Epoch [15/100], Training Loss: 943447.1543599312, Test Loss: 32342982.0\n",
      "Epoch [16/100], Training Loss: 888671.6813281204, Test Loss: 30580984.0\n",
      "Epoch [17/100], Training Loss: 839985.374015165, Test Loss: 29015838.0\n",
      "Epoch [18/100], Training Loss: 796676.9538386351, Test Loss: 27618716.0\n",
      "Epoch [19/100], Training Loss: 758191.5847846691, Test Loss: 26364872.0\n",
      "Epoch [20/100], Training Loss: 724009.4770451988, Test Loss: 25249158.0\n",
      "Epoch [21/100], Training Loss: 693378.4288253066, Test Loss: 24251074.0\n",
      "Epoch [22/100], Training Loss: 665551.0080415852, Test Loss: 23348488.0\n",
      "Epoch [23/100], Training Loss: 640115.0162905041, Test Loss: 22532908.0\n",
      "Epoch [24/100], Training Loss: 616762.6619350157, Test Loss: 21797864.0\n",
      "Epoch [25/100], Training Loss: 595240.6786624015, Test Loss: 21135720.0\n",
      "Epoch [26/100], Training Loss: 575311.2311326343, Test Loss: 20538980.0\n",
      "Epoch [27/100], Training Loss: 556896.8393015816, Test Loss: 20005262.0\n",
      "Epoch [28/100], Training Loss: 539873.4135862804, Test Loss: 19522356.0\n",
      "Epoch [29/100], Training Loss: 524024.9880486938, Test Loss: 19089410.0\n",
      "Epoch [30/100], Training Loss: 509251.347831882, Test Loss: 18688084.0\n",
      "Epoch [31/100], Training Loss: 495465.2035276346, Test Loss: 18331558.0\n",
      "Epoch [32/100], Training Loss: 482548.4396362775, Test Loss: 17998494.0\n",
      "Epoch [33/100], Training Loss: 470493.41573366505, Test Loss: 17694750.0\n",
      "Epoch [34/100], Training Loss: 459179.60192672233, Test Loss: 17407336.0\n",
      "Epoch [35/100], Training Loss: 448585.1611056809, Test Loss: 17152256.0\n",
      "Epoch [36/100], Training Loss: 438646.0584829098, Test Loss: 16911310.0\n",
      "Epoch [37/100], Training Loss: 429300.7175226586, Test Loss: 16698669.0\n",
      "Epoch [38/100], Training Loss: 420536.12037201587, Test Loss: 16500703.0\n",
      "Epoch [39/100], Training Loss: 412301.21022155083, Test Loss: 16334887.0\n",
      "Epoch [40/100], Training Loss: 404613.57551389135, Test Loss: 16165999.0\n",
      "Epoch [41/100], Training Loss: 397338.46453853446, Test Loss: 16031847.0\n",
      "Epoch [42/100], Training Loss: 390475.3687355607, Test Loss: 15898034.0\n",
      "Epoch [43/100], Training Loss: 383947.88135329663, Test Loss: 15792637.0\n",
      "Epoch [44/100], Training Loss: 377740.7527693857, Test Loss: 15691494.0\n",
      "Epoch [45/100], Training Loss: 371830.5629183698, Test Loss: 15601900.0\n",
      "Epoch [46/100], Training Loss: 366233.17855873465, Test Loss: 15526065.0\n",
      "Epoch [47/100], Training Loss: 360899.91474883, Test Loss: 15451389.0\n",
      "Epoch [48/100], Training Loss: 355865.3085125289, Test Loss: 15387955.0\n",
      "Epoch [49/100], Training Loss: 351052.3597831882, Test Loss: 15332722.0\n",
      "Epoch [50/100], Training Loss: 346525.40769207984, Test Loss: 15268425.0\n",
      "Epoch [51/100], Training Loss: 342185.69334606954, Test Loss: 15201780.0\n",
      "Epoch [52/100], Training Loss: 338105.2212472602, Test Loss: 15152683.0\n",
      "Epoch [53/100], Training Loss: 334206.73975179193, Test Loss: 15074147.0\n",
      "Epoch [54/100], Training Loss: 330527.2941583733, Test Loss: 15063410.0\n",
      "Epoch [55/100], Training Loss: 327053.90552988567, Test Loss: 14978917.0\n",
      "Epoch [56/100], Training Loss: 323745.65918043954, Test Loss: 14971900.0\n",
      "Epoch [57/100], Training Loss: 320565.45727074816, Test Loss: 14888045.0\n",
      "Epoch [58/100], Training Loss: 317545.038238256, Test Loss: 14879795.0\n",
      "Epoch [59/100], Training Loss: 314679.52137758426, Test Loss: 14810670.0\n",
      "Epoch [60/100], Training Loss: 311953.90404893074, Test Loss: 14803365.0\n",
      "Epoch [61/100], Training Loss: 309341.4438051656, Test Loss: 14734559.0\n",
      "Epoch [62/100], Training Loss: 306831.18497126945, Test Loss: 14734365.0\n",
      "Epoch [63/100], Training Loss: 304435.4178810497, Test Loss: 14675828.0\n",
      "Epoch [64/100], Training Loss: 302156.7321730052, Test Loss: 14666147.0\n",
      "Epoch [65/100], Training Loss: 299961.16513017594, Test Loss: 14622201.0\n",
      "Epoch [66/100], Training Loss: 297816.0355355133, Test Loss: 14575222.0\n",
      "Epoch [67/100], Training Loss: 295827.2249200284, Test Loss: 14597721.0\n",
      "Epoch [68/100], Training Loss: 293886.97678973404, Test Loss: 14553844.0\n",
      "Epoch [69/100], Training Loss: 292031.6279545051, Test Loss: 14535096.0\n",
      "Epoch [70/100], Training Loss: 290271.4103207748, Test Loss: 14503193.0\n",
      "Epoch [71/100], Training Loss: 288578.8397495705, Test Loss: 14486574.0\n",
      "Epoch [72/100], Training Loss: 286941.7688044251, Test Loss: 14499437.0\n",
      "Epoch [73/100], Training Loss: 285388.1752228837, Test Loss: 14436650.0\n",
      "Epoch [74/100], Training Loss: 283813.79348083644, Test Loss: 14453459.0\n",
      "Epoch [75/100], Training Loss: 282332.7932364789, Test Loss: 14426178.0\n",
      "Epoch [76/100], Training Loss: 280835.30746105086, Test Loss: 14405295.0\n",
      "Epoch [77/100], Training Loss: 279406.82181520644, Test Loss: 14420078.0\n",
      "Epoch [78/100], Training Loss: 278027.0183564362, Test Loss: 14374627.0\n",
      "Epoch [79/100], Training Loss: 276640.03386943904, Test Loss: 14368831.0\n",
      "Epoch [80/100], Training Loss: 275330.0512595522, Test Loss: 14420327.0\n",
      "Epoch [81/100], Training Loss: 274071.1705911972, Test Loss: 14353442.0\n",
      "Epoch [82/100], Training Loss: 272824.276890439, Test Loss: 14360337.0\n",
      "Epoch [83/100], Training Loss: 271620.00872282445, Test Loss: 14330082.0\n",
      "Epoch [84/100], Training Loss: 270434.3242365677, Test Loss: 14330683.0\n",
      "Epoch [85/100], Training Loss: 269273.5448581245, Test Loss: 14331359.0\n",
      "Epoch [86/100], Training Loss: 268176.7554832356, Test Loss: 14309977.0\n",
      "Epoch [87/100], Training Loss: 267053.25179565785, Test Loss: 14291330.0\n",
      "Epoch [88/100], Training Loss: 266013.2760907233, Test Loss: 14345077.0\n",
      "Epoch [89/100], Training Loss: 264981.440128695, Test Loss: 14292347.0\n",
      "Epoch [90/100], Training Loss: 263999.81266660744, Test Loss: 14303154.0\n",
      "Epoch [91/100], Training Loss: 262971.1095758545, Test Loss: 14285166.0\n",
      "Epoch [92/100], Training Loss: 262009.94279811622, Test Loss: 14287044.0\n",
      "Epoch [93/100], Training Loss: 261083.1091389728, Test Loss: 14295908.0\n",
      "Epoch [94/100], Training Loss: 260152.12262676973, Test Loss: 14225150.0\n",
      "Epoch [95/100], Training Loss: 259233.37936881703, Test Loss: 14268119.0\n",
      "Epoch [96/100], Training Loss: 258367.59287068303, Test Loss: 14258955.0\n",
      "Epoch [97/100], Training Loss: 257471.24416874, Test Loss: 14231823.0\n",
      "Epoch [98/100], Training Loss: 256616.47886307092, Test Loss: 14269603.0\n",
      "Epoch [99/100], Training Loss: 255783.50703823825, Test Loss: 14211983.0\n",
      "Epoch [100/100], Training Loss: 254878.09095654878, Test Loss: 14236722.0\n",
      "Epoch [1/100], Training Loss: 4390411.151235117, Test Loss: 230941904.0\n",
      "Epoch [2/100], Training Loss: 2675665.9282033057, Test Loss: 144864032.0\n",
      "Epoch [3/100], Training Loss: 2074908.9636869854, Test Loss: 125212160.0\n",
      "Epoch [4/100], Training Loss: 1798192.463005746, Test Loss: 109596952.0\n",
      "Epoch [5/100], Training Loss: 1568991.2821515312, Test Loss: 96701760.0\n",
      "Epoch [6/100], Training Loss: 1379820.889520763, Test Loss: 86201104.0\n",
      "Epoch [7/100], Training Loss: 1224454.6922575678, Test Loss: 77546208.0\n",
      "Epoch [8/100], Training Loss: 1097159.1883182277, Test Loss: 70416128.0\n",
      "Epoch [9/100], Training Loss: 993646.7115099817, Test Loss: 64567212.0\n",
      "Epoch [10/100], Training Loss: 909813.2716071323, Test Loss: 59802940.0\n",
      "Epoch [11/100], Training Loss: 841721.1217344945, Test Loss: 55906844.0\n",
      "Epoch [12/100], Training Loss: 785634.7091404537, Test Loss: 52636480.0\n",
      "Epoch [13/100], Training Loss: 737829.3798353178, Test Loss: 49790580.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/100], Training Loss: 695784.0528997098, Test Loss: 47246588.0\n",
      "Epoch [15/100], Training Loss: 657946.2919258338, Test Loss: 44927636.0\n",
      "Epoch [16/100], Training Loss: 623444.0113144956, Test Loss: 42790336.0\n",
      "Epoch [17/100], Training Loss: 591725.7155677981, Test Loss: 40799436.0\n",
      "Epoch [18/100], Training Loss: 562449.75525739, Test Loss: 38939548.0\n",
      "Epoch [19/100], Training Loss: 535390.5744031752, Test Loss: 37200232.0\n",
      "Epoch [20/100], Training Loss: 510406.7438540371, Test Loss: 35578516.0\n",
      "Epoch [21/100], Training Loss: 487385.05420295004, Test Loss: 34075972.0\n",
      "Epoch [22/100], Training Loss: 466153.9683668029, Test Loss: 32686650.0\n",
      "Epoch [23/100], Training Loss: 446588.2342278301, Test Loss: 31403140.0\n",
      "Epoch [24/100], Training Loss: 428570.1220603045, Test Loss: 30218472.0\n",
      "Epoch [25/100], Training Loss: 411994.7444168, Test Loss: 29126324.0\n",
      "Epoch [26/100], Training Loss: 396747.55627628695, Test Loss: 28117180.0\n",
      "Epoch [27/100], Training Loss: 382683.83228185534, Test Loss: 27183394.0\n",
      "Epoch [28/100], Training Loss: 369739.06763521116, Test Loss: 26321994.0\n",
      "Epoch [29/100], Training Loss: 357814.4201024821, Test Loss: 25526990.0\n",
      "Epoch [30/100], Training Loss: 346767.01894141344, Test Loss: 24788938.0\n",
      "Epoch [31/100], Training Loss: 336464.86391505244, Test Loss: 24100372.0\n",
      "Epoch [32/100], Training Loss: 326805.13857295184, Test Loss: 23461044.0\n",
      "Epoch [33/100], Training Loss: 317710.6844973639, Test Loss: 22864352.0\n",
      "Epoch [34/100], Training Loss: 309138.9681742788, Test Loss: 22310786.0\n",
      "Epoch [35/100], Training Loss: 301047.1218529708, Test Loss: 21797710.0\n",
      "Epoch [36/100], Training Loss: 293393.6597506072, Test Loss: 21319146.0\n",
      "Epoch [37/100], Training Loss: 286154.59484035306, Test Loss: 20873060.0\n",
      "Epoch [38/100], Training Loss: 279319.1853859369, Test Loss: 20458652.0\n",
      "Epoch [39/100], Training Loss: 272840.92690006515, Test Loss: 20072780.0\n",
      "Epoch [40/100], Training Loss: 266701.01966708136, Test Loss: 19713376.0\n",
      "Epoch [41/100], Training Loss: 260889.61822463124, Test Loss: 19379850.0\n",
      "Epoch [42/100], Training Loss: 255380.74401694213, Test Loss: 19067534.0\n",
      "Epoch [43/100], Training Loss: 250142.09835021623, Test Loss: 18776352.0\n",
      "Epoch [44/100], Training Loss: 245149.8505420295, Test Loss: 18503230.0\n",
      "Epoch [45/100], Training Loss: 240391.47452757537, Test Loss: 18245372.0\n",
      "Epoch [46/100], Training Loss: 235847.24563118297, Test Loss: 18005050.0\n",
      "Epoch [47/100], Training Loss: 231525.734198211, Test Loss: 17781618.0\n",
      "Epoch [48/100], Training Loss: 227390.8757330727, Test Loss: 17567398.0\n",
      "Epoch [49/100], Training Loss: 223438.26254368818, Test Loss: 17368236.0\n",
      "Epoch [50/100], Training Loss: 219646.75251762336, Test Loss: 17180128.0\n",
      "Epoch [51/100], Training Loss: 216029.14008352585, Test Loss: 17003990.0\n",
      "Epoch [52/100], Training Loss: 212553.25456134116, Test Loss: 16839712.0\n",
      "Epoch [53/100], Training Loss: 209213.30863100526, Test Loss: 16687329.0\n",
      "Epoch [54/100], Training Loss: 206012.08909424796, Test Loss: 16544801.0\n",
      "Epoch [55/100], Training Loss: 202960.4438273799, Test Loss: 16413810.0\n",
      "Epoch [56/100], Training Loss: 200042.17617439726, Test Loss: 16295707.0\n",
      "Epoch [57/100], Training Loss: 197268.10529589478, Test Loss: 16177401.0\n",
      "Epoch [58/100], Training Loss: 194598.22546057697, Test Loss: 16082351.0\n",
      "Epoch [59/100], Training Loss: 192042.6115603341, Test Loss: 15980407.0\n",
      "Epoch [60/100], Training Loss: 189585.17510810972, Test Loss: 15897766.0\n",
      "Epoch [61/100], Training Loss: 187223.39551863042, Test Loss: 15811617.0\n",
      "Epoch [62/100], Training Loss: 184957.52419880338, Test Loss: 15741638.0\n",
      "Epoch [63/100], Training Loss: 182782.74326165512, Test Loss: 15668843.0\n",
      "Epoch [64/100], Training Loss: 180685.85002369527, Test Loss: 15607270.0\n",
      "Epoch [65/100], Training Loss: 178679.7222469048, Test Loss: 15547009.0\n",
      "Epoch [66/100], Training Loss: 176761.3379242936, Test Loss: 15492087.0\n",
      "Epoch [67/100], Training Loss: 174907.16132782417, Test Loss: 15437164.0\n",
      "Epoch [68/100], Training Loss: 173120.134855755, Test Loss: 15388535.0\n",
      "Epoch [69/100], Training Loss: 171414.12984272258, Test Loss: 15339502.0\n",
      "Epoch [70/100], Training Loss: 169753.38288608496, Test Loss: 15291262.0\n",
      "Epoch [71/100], Training Loss: 168174.4176144778, Test Loss: 15244843.0\n",
      "Epoch [72/100], Training Loss: 166652.6267179077, Test Loss: 15193782.0\n",
      "Epoch [73/100], Training Loss: 165202.5195337954, Test Loss: 15151124.0\n",
      "Epoch [74/100], Training Loss: 163795.65193116522, Test Loss: 15100340.0\n",
      "Epoch [75/100], Training Loss: 162443.4913141994, Test Loss: 15066543.0\n",
      "Epoch [76/100], Training Loss: 161136.0353355844, Test Loss: 15024063.0\n",
      "Epoch [77/100], Training Loss: 159882.30234435163, Test Loss: 14971292.0\n",
      "Epoch [78/100], Training Loss: 158674.85300041467, Test Loss: 14965824.0\n",
      "Epoch [79/100], Training Loss: 157532.1888143475, Test Loss: 14905216.0\n",
      "Epoch [80/100], Training Loss: 156409.4193397903, Test Loss: 14889900.0\n",
      "Epoch [81/100], Training Loss: 155352.00085895386, Test Loss: 14850337.0\n",
      "Epoch [82/100], Training Loss: 154301.28903797167, Test Loss: 14821806.0\n",
      "Epoch [83/100], Training Loss: 153316.8819456786, Test Loss: 14796654.0\n",
      "Epoch [84/100], Training Loss: 152349.69021384988, Test Loss: 14768650.0\n",
      "Epoch [85/100], Training Loss: 151430.1561370772, Test Loss: 14749215.0\n",
      "Epoch [86/100], Training Loss: 150533.62930957883, Test Loss: 14719069.0\n",
      "Epoch [87/100], Training Loss: 149680.61326343226, Test Loss: 14708411.0\n",
      "Epoch [88/100], Training Loss: 148847.0398228778, Test Loss: 14660037.0\n",
      "Epoch [89/100], Training Loss: 148035.41665185711, Test Loss: 14677711.0\n",
      "Epoch [90/100], Training Loss: 147242.09446270956, Test Loss: 14646813.0\n",
      "Epoch [91/100], Training Loss: 146492.6802988567, Test Loss: 14607284.0\n",
      "Epoch [92/100], Training Loss: 145741.93471950715, Test Loss: 14612548.0\n",
      "Epoch [93/100], Training Loss: 145040.77981162255, Test Loss: 14595051.0\n",
      "Epoch [94/100], Training Loss: 144331.1478807535, Test Loss: 14572199.0\n",
      "Epoch [95/100], Training Loss: 143660.6180098928, Test Loss: 14563359.0\n",
      "Epoch [96/100], Training Loss: 142982.42264231975, Test Loss: 14533820.0\n",
      "Epoch [97/100], Training Loss: 142333.151183283, Test Loss: 14527961.0\n",
      "Epoch [98/100], Training Loss: 141692.0045169125, Test Loss: 14504918.0\n",
      "Epoch [99/100], Training Loss: 141065.06752413957, Test Loss: 14493847.0\n",
      "Epoch [100/100], Training Loss: 140455.3038401161, Test Loss: 14468764.0\n",
      "Epoch [1/100], Training Loss: 2318154.1046146555, Test Loss: 292373760.0\n",
      "Epoch [2/100], Training Loss: 2070546.011729163, Test Loss: 229870144.0\n",
      "Epoch [3/100], Training Loss: 1486552.549967419, Test Loss: 162911616.0\n",
      "Epoch [4/100], Training Loss: 1170043.590308631, Test Loss: 142581184.0\n",
      "Epoch [5/100], Training Loss: 1053746.150820449, Test Loss: 130794544.0\n",
      "Epoch [6/100], Training Loss: 967208.217048753, Test Loss: 120761624.0\n",
      "Epoch [7/100], Training Loss: 890552.4902553166, Test Loss: 111728360.0\n",
      "Epoch [8/100], Training Loss: 821017.1681772406, Test Loss: 103567576.0\n",
      "Epoch [9/100], Training Loss: 757073.9785557728, Test Loss: 96022432.0\n",
      "Epoch [10/100], Training Loss: 699555.7342574492, Test Loss: 89565464.0\n",
      "Epoch [11/100], Training Loss: 650213.7213435223, Test Loss: 83978784.0\n",
      "Epoch [12/100], Training Loss: 607312.0530774243, Test Loss: 79080624.0\n",
      "Epoch [13/100], Training Loss: 569904.3099342457, Test Loss: 74783336.0\n",
      "Epoch [14/100], Training Loss: 537314.5327883419, Test Loss: 71001688.0\n",
      "Epoch [15/100], Training Loss: 508863.7099697885, Test Loss: 67659136.0\n",
      "Epoch [16/100], Training Loss: 483943.94727800484, Test Loss: 64705700.0\n",
      "Epoch [17/100], Training Loss: 462003.66637047566, Test Loss: 62086768.0\n",
      "Epoch [18/100], Training Loss: 442545.34233753924, Test Loss: 59746140.0\n",
      "Epoch [19/100], Training Loss: 425148.8290978023, Test Loss: 57641352.0\n",
      "Epoch [20/100], Training Loss: 409445.03098157694, Test Loss: 55728944.0\n",
      "Epoch [21/100], Training Loss: 395127.97713405604, Test Loss: 53969824.0\n",
      "Epoch [22/100], Training Loss: 381896.35359279666, Test Loss: 52337376.0\n",
      "Epoch [23/100], Training Loss: 369553.57650613115, Test Loss: 50807616.0\n",
      "Epoch [24/100], Training Loss: 357965.4358154138, Test Loss: 49369756.0\n",
      "Epoch [25/100], Training Loss: 347042.8766068361, Test Loss: 48014048.0\n",
      "Epoch [26/100], Training Loss: 336743.81221491616, Test Loss: 46732604.0\n",
      "Epoch [27/100], Training Loss: 326999.01030744624, Test Loss: 45519428.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/100], Training Loss: 317736.5742550797, Test Loss: 44363764.0\n",
      "Epoch [29/100], Training Loss: 308892.4294176885, Test Loss: 43253516.0\n",
      "Epoch [30/100], Training Loss: 300444.8817605592, Test Loss: 42188140.0\n",
      "Epoch [31/100], Training Loss: 292383.8896984776, Test Loss: 41164396.0\n",
      "Epoch [32/100], Training Loss: 284683.4064925064, Test Loss: 40182452.0\n",
      "Epoch [33/100], Training Loss: 277320.63710680645, Test Loss: 39241324.0\n",
      "Epoch [34/100], Training Loss: 270269.0521296132, Test Loss: 38339876.0\n",
      "Epoch [35/100], Training Loss: 263534.18731117825, Test Loss: 37477080.0\n",
      "Epoch [36/100], Training Loss: 257134.46958118596, Test Loss: 36655116.0\n",
      "Epoch [37/100], Training Loss: 251060.3339849535, Test Loss: 35872972.0\n",
      "Epoch [38/100], Training Loss: 245285.35436289318, Test Loss: 35125664.0\n",
      "Epoch [39/100], Training Loss: 239769.22356495468, Test Loss: 34405896.0\n",
      "Epoch [40/100], Training Loss: 234480.39423019963, Test Loss: 33716900.0\n",
      "Epoch [41/100], Training Loss: 229408.76612759908, Test Loss: 33055098.0\n",
      "Epoch [42/100], Training Loss: 224540.98673064393, Test Loss: 32420578.0\n",
      "Epoch [43/100], Training Loss: 219884.8447959244, Test Loss: 31813648.0\n",
      "Epoch [44/100], Training Loss: 215437.63716604467, Test Loss: 31232578.0\n",
      "Epoch [45/100], Training Loss: 211175.2454830875, Test Loss: 30675324.0\n",
      "Epoch [46/100], Training Loss: 207083.75499081807, Test Loss: 30141552.0\n",
      "Epoch [47/100], Training Loss: 203136.16462294888, Test Loss: 29625958.0\n",
      "Epoch [48/100], Training Loss: 199324.46741899176, Test Loss: 29129300.0\n",
      "Epoch [49/100], Training Loss: 195651.86244890705, Test Loss: 28649216.0\n",
      "Epoch [50/100], Training Loss: 192104.99558675435, Test Loss: 28184082.0\n",
      "Epoch [51/100], Training Loss: 188682.2989455601, Test Loss: 27737172.0\n",
      "Epoch [52/100], Training Loss: 185387.52488004265, Test Loss: 27307948.0\n",
      "Epoch [53/100], Training Loss: 182213.520466797, Test Loss: 26895782.0\n",
      "Epoch [54/100], Training Loss: 179161.04362893195, Test Loss: 26502032.0\n",
      "Epoch [55/100], Training Loss: 176230.1438007227, Test Loss: 26122084.0\n",
      "Epoch [56/100], Training Loss: 173407.47568272022, Test Loss: 25753910.0\n",
      "Epoch [57/100], Training Loss: 170652.96593803685, Test Loss: 25390458.0\n",
      "Epoch [58/100], Training Loss: 167931.80208518452, Test Loss: 25032998.0\n",
      "Epoch [59/100], Training Loss: 165266.62460754695, Test Loss: 24687152.0\n",
      "Epoch [60/100], Training Loss: 162694.7955097447, Test Loss: 24359362.0\n",
      "Epoch [61/100], Training Loss: 160224.07304069665, Test Loss: 24044272.0\n",
      "Epoch [62/100], Training Loss: 157833.4537645874, Test Loss: 23741804.0\n",
      "Epoch [63/100], Training Loss: 155502.3415082045, Test Loss: 23442706.0\n",
      "Epoch [64/100], Training Loss: 153228.57431431787, Test Loss: 23152484.0\n",
      "Epoch [65/100], Training Loss: 151000.87527397665, Test Loss: 22873388.0\n",
      "Epoch [66/100], Training Loss: 148840.16557076003, Test Loss: 22606996.0\n",
      "Epoch [67/100], Training Loss: 146762.1918429003, Test Loss: 22350334.0\n",
      "Epoch [68/100], Training Loss: 144769.6254961199, Test Loss: 22106804.0\n",
      "Epoch [69/100], Training Loss: 142853.2192405663, Test Loss: 21869122.0\n",
      "Epoch [70/100], Training Loss: 140992.68609679522, Test Loss: 21643962.0\n",
      "Epoch [71/100], Training Loss: 139200.76461702507, Test Loss: 21429766.0\n",
      "Epoch [72/100], Training Loss: 137464.601445412, Test Loss: 21222944.0\n",
      "Epoch [73/100], Training Loss: 135783.9338901724, Test Loss: 21025950.0\n",
      "Epoch [74/100], Training Loss: 134147.34198211005, Test Loss: 20833400.0\n",
      "Epoch [75/100], Training Loss: 132553.70016586696, Test Loss: 20649456.0\n",
      "Epoch [76/100], Training Loss: 131003.3042769978, Test Loss: 20470886.0\n",
      "Epoch [77/100], Training Loss: 129496.3025887092, Test Loss: 20298934.0\n",
      "Epoch [78/100], Training Loss: 128027.50068123927, Test Loss: 20134438.0\n",
      "Epoch [79/100], Training Loss: 126595.4021088798, Test Loss: 19975152.0\n",
      "Epoch [80/100], Training Loss: 125194.63301937089, Test Loss: 19822966.0\n",
      "Epoch [81/100], Training Loss: 123823.2692968426, Test Loss: 19672534.0\n",
      "Epoch [82/100], Training Loss: 122473.60966471181, Test Loss: 19528686.0\n",
      "Epoch [83/100], Training Loss: 121136.29913808424, Test Loss: 19384418.0\n",
      "Epoch [84/100], Training Loss: 119814.31967892898, Test Loss: 19243590.0\n",
      "Epoch [85/100], Training Loss: 118510.32207807594, Test Loss: 19105926.0\n",
      "Epoch [86/100], Training Loss: 117219.82930513595, Test Loss: 18970254.0\n",
      "Epoch [87/100], Training Loss: 115959.89648125111, Test Loss: 18840630.0\n",
      "Epoch [88/100], Training Loss: 114753.1899768971, Test Loss: 18719506.0\n",
      "Epoch [89/100], Training Loss: 113602.21509389255, Test Loss: 18605104.0\n",
      "Epoch [90/100], Training Loss: 112495.81618387536, Test Loss: 18498058.0\n",
      "Epoch [91/100], Training Loss: 111429.98027368047, Test Loss: 18394088.0\n",
      "Epoch [92/100], Training Loss: 110391.41283099343, Test Loss: 18295432.0\n",
      "Epoch [93/100], Training Loss: 109375.9835317813, Test Loss: 18198280.0\n",
      "Epoch [94/100], Training Loss: 108375.8449144008, Test Loss: 18107818.0\n",
      "Epoch [95/100], Training Loss: 107390.116195723, Test Loss: 18014164.0\n",
      "Epoch [96/100], Training Loss: 106423.59327054085, Test Loss: 17925024.0\n",
      "Epoch [97/100], Training Loss: 105476.6585214146, Test Loss: 17836494.0\n",
      "Epoch [98/100], Training Loss: 104562.25161424086, Test Loss: 17754564.0\n",
      "Epoch [99/100], Training Loss: 103682.65859546236, Test Loss: 17676616.0\n",
      "Epoch [100/100], Training Loss: 102838.46918132812, Test Loss: 17606930.0\n",
      "Epoch [1/100], Training Loss: 1497686.1257034536, Test Loss: 298639328.0\n",
      "Epoch [2/100], Training Loss: 1465089.8709792073, Test Loss: 283887872.0\n",
      "Epoch [3/100], Training Loss: 1329030.3877732363, Test Loss: 243037872.0\n",
      "Epoch [4/100], Training Loss: 1087677.9351934127, Test Loss: 192018896.0\n",
      "Epoch [5/100], Training Loss: 874403.9289141638, Test Loss: 160926768.0\n",
      "Epoch [6/100], Training Loss: 765234.2003435815, Test Loss: 146740224.0\n",
      "Epoch [7/100], Training Loss: 706820.8757775014, Test Loss: 137474112.0\n",
      "Epoch [8/100], Training Loss: 662478.5375273976, Test Loss: 129792040.0\n",
      "Epoch [9/100], Training Loss: 623830.1323381317, Test Loss: 122998960.0\n",
      "Epoch [10/100], Training Loss: 588859.7850838221, Test Loss: 116869448.0\n",
      "Epoch [11/100], Training Loss: 556624.7305254427, Test Loss: 111198192.0\n",
      "Epoch [12/100], Training Loss: 526226.0771281322, Test Loss: 105962232.0\n",
      "Epoch [13/100], Training Loss: 498413.8212191221, Test Loss: 101311184.0\n",
      "Epoch [14/100], Training Loss: 473494.2304365855, Test Loss: 97206424.0\n",
      "Epoch [15/100], Training Loss: 451117.86410757655, Test Loss: 93538856.0\n",
      "Epoch [16/100], Training Loss: 430947.1993365322, Test Loss: 90246776.0\n",
      "Epoch [17/100], Training Loss: 412720.49546827795, Test Loss: 87268600.0\n",
      "Epoch [18/100], Training Loss: 396208.90480421775, Test Loss: 84558536.0\n",
      "Epoch [19/100], Training Loss: 381239.4142527101, Test Loss: 82081976.0\n",
      "Epoch [20/100], Training Loss: 367665.47206919023, Test Loss: 79812272.0\n",
      "Epoch [21/100], Training Loss: 355349.9511877258, Test Loss: 77726640.0\n",
      "Epoch [22/100], Training Loss: 344178.7622771163, Test Loss: 75805856.0\n",
      "Epoch [23/100], Training Loss: 334033.47337243054, Test Loss: 74033824.0\n",
      "Epoch [24/100], Training Loss: 324804.88880990463, Test Loss: 72390576.0\n",
      "Epoch [25/100], Training Loss: 316381.7346128784, Test Loss: 70859400.0\n",
      "Epoch [26/100], Training Loss: 308648.5073159173, Test Loss: 69422712.0\n",
      "Epoch [27/100], Training Loss: 301509.3652627214, Test Loss: 68067304.0\n",
      "Epoch [28/100], Training Loss: 294881.72762277117, Test Loss: 66785500.0\n",
      "Epoch [29/100], Training Loss: 288691.8237959836, Test Loss: 65565320.0\n",
      "Epoch [30/100], Training Loss: 282855.4900405782, Test Loss: 64397388.0\n",
      "Epoch [31/100], Training Loss: 277320.9828616492, Test Loss: 63273604.0\n",
      "Epoch [32/100], Training Loss: 272045.5292405293, Test Loss: 62188992.0\n",
      "Epoch [33/100], Training Loss: 266982.92175161, Test Loss: 61138184.0\n",
      "Epoch [34/100], Training Loss: 262098.12985938333, Test Loss: 60118372.0\n",
      "Epoch [35/100], Training Loss: 257366.70986056808, Test Loss: 59125916.0\n",
      "Epoch [36/100], Training Loss: 252768.3525005924, Test Loss: 58158488.0\n",
      "Epoch [37/100], Training Loss: 248287.4129272555, Test Loss: 57213688.0\n",
      "Epoch [38/100], Training Loss: 243908.4386070138, Test Loss: 56291560.0\n",
      "Epoch [39/100], Training Loss: 239612.412253421, Test Loss: 55387276.0\n",
      "Epoch [40/100], Training Loss: 235397.47091404538, Test Loss: 54500064.0\n",
      "Epoch [41/100], Training Loss: 231263.21250222143, Test Loss: 53630836.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/100], Training Loss: 227208.63029441383, Test Loss: 52779196.0\n",
      "Epoch [43/100], Training Loss: 223233.55467685562, Test Loss: 51948092.0\n",
      "Epoch [44/100], Training Loss: 219339.92910668798, Test Loss: 51138940.0\n",
      "Epoch [45/100], Training Loss: 215532.70533439962, Test Loss: 50348160.0\n",
      "Epoch [46/100], Training Loss: 211818.32903856406, Test Loss: 49576836.0\n",
      "Epoch [47/100], Training Loss: 208191.33234109354, Test Loss: 48822804.0\n",
      "Epoch [48/100], Training Loss: 204651.63331556186, Test Loss: 48083200.0\n",
      "Epoch [49/100], Training Loss: 201194.73208044548, Test Loss: 47365456.0\n",
      "Epoch [50/100], Training Loss: 197821.90221254664, Test Loss: 46666960.0\n",
      "Epoch [51/100], Training Loss: 194533.6561592915, Test Loss: 45986324.0\n",
      "Epoch [52/100], Training Loss: 191330.83214116463, Test Loss: 45321336.0\n",
      "Epoch [53/100], Training Loss: 188214.30989722174, Test Loss: 44671276.0\n",
      "Epoch [54/100], Training Loss: 185178.59505509151, Test Loss: 44037304.0\n",
      "Epoch [55/100], Training Loss: 182227.9093655589, Test Loss: 43420068.0\n",
      "Epoch [56/100], Training Loss: 179364.0185378532, Test Loss: 42817232.0\n",
      "Epoch [57/100], Training Loss: 176584.87128465433, Test Loss: 42230792.0\n",
      "Epoch [58/100], Training Loss: 173888.9045357947, Test Loss: 41659200.0\n",
      "Epoch [59/100], Training Loss: 171274.7237889491, Test Loss: 41100640.0\n",
      "Epoch [60/100], Training Loss: 168725.7335549212, Test Loss: 40552984.0\n",
      "Epoch [61/100], Training Loss: 166232.71562703632, Test Loss: 40014884.0\n",
      "Epoch [62/100], Training Loss: 163793.83450513892, Test Loss: 39485608.0\n",
      "Epoch [63/100], Training Loss: 161407.46821315383, Test Loss: 38968984.0\n",
      "Epoch [64/100], Training Loss: 159074.4045068698, Test Loss: 38464260.0\n",
      "Epoch [65/100], Training Loss: 156803.99270109055, Test Loss: 37971276.0\n",
      "Epoch [66/100], Training Loss: 154601.48796238189, Test Loss: 37493832.0\n",
      "Epoch [67/100], Training Loss: 152465.89670969997, Test Loss: 37030000.0\n",
      "Epoch [68/100], Training Loss: 150392.23042870348, Test Loss: 36578392.0\n",
      "Epoch [69/100], Training Loss: 148374.99372850664, Test Loss: 36136988.0\n",
      "Epoch [70/100], Training Loss: 146414.0539405072, Test Loss: 35707564.0\n",
      "Epoch [71/100], Training Loss: 144504.77004822475, Test Loss: 35289340.0\n",
      "Epoch [72/100], Training Loss: 142646.68649688445, Test Loss: 34882664.0\n",
      "Epoch [73/100], Training Loss: 140842.38255599397, Test Loss: 34487196.0\n",
      "Epoch [74/100], Training Loss: 139091.4448989896, Test Loss: 34102640.0\n",
      "Epoch [75/100], Training Loss: 137392.93279970362, Test Loss: 33728800.0\n",
      "Epoch [76/100], Training Loss: 135739.54740409428, Test Loss: 33363034.0\n",
      "Epoch [77/100], Training Loss: 134114.25194167075, Test Loss: 33004630.0\n",
      "Epoch [78/100], Training Loss: 132517.03960651954, Test Loss: 32652768.0\n",
      "Epoch [79/100], Training Loss: 130944.72334743943, Test Loss: 32306380.0\n",
      "Epoch [80/100], Training Loss: 129399.60654313651, Test Loss: 31966932.0\n",
      "Epoch [81/100], Training Loss: 127887.77494691702, Test Loss: 31634154.0\n",
      "Epoch [82/100], Training Loss: 126416.65625694198, Test Loss: 31314812.0\n",
      "Epoch [83/100], Training Loss: 124990.10826104422, Test Loss: 31000802.0\n",
      "Epoch [84/100], Training Loss: 123605.33875131435, Test Loss: 30698072.0\n",
      "Epoch [85/100], Training Loss: 122251.0960542733, Test Loss: 30400604.0\n",
      "Epoch [86/100], Training Loss: 120915.60607802411, Test Loss: 30108974.0\n",
      "Epoch [87/100], Training Loss: 119597.11504057817, Test Loss: 29822740.0\n",
      "Epoch [88/100], Training Loss: 118309.2274635685, Test Loss: 29549004.0\n",
      "Epoch [89/100], Training Loss: 117063.73986841715, Test Loss: 29283746.0\n",
      "Epoch [90/100], Training Loss: 115857.37502360273, Test Loss: 29027954.0\n",
      "Epoch [91/100], Training Loss: 114681.7501754006, Test Loss: 28777734.0\n",
      "Epoch [92/100], Training Loss: 113532.91488257879, Test Loss: 28533576.0\n",
      "Epoch [93/100], Training Loss: 112404.9410293007, Test Loss: 28293044.0\n",
      "Epoch [94/100], Training Loss: 111292.84910550324, Test Loss: 28055748.0\n",
      "Epoch [95/100], Training Loss: 110190.41962487412, Test Loss: 27819750.0\n",
      "Epoch [96/100], Training Loss: 109098.39072727846, Test Loss: 27587264.0\n",
      "Epoch [97/100], Training Loss: 108018.14964355266, Test Loss: 27357780.0\n",
      "Epoch [98/100], Training Loss: 106954.5448627525, Test Loss: 27133316.0\n",
      "Epoch [99/100], Training Loss: 105908.8343635226, Test Loss: 26909796.0\n",
      "Epoch [100/100], Training Loss: 104882.03479503584, Test Loss: 26694610.0\n",
      "Epoch [1/100], Training Loss: 1164470.0091226823, Test Loss: 299431040.0\n",
      "Epoch [2/100], Training Loss: 1152030.017179077, Test Loss: 291971168.0\n",
      "Epoch [3/100], Training Loss: 1093980.021088798, Test Loss: 268162192.0\n",
      "Epoch [4/100], Training Loss: 966695.6796398318, Test Loss: 227704128.0\n",
      "Epoch [5/100], Training Loss: 800652.1841123156, Test Loss: 186390912.0\n",
      "Epoch [6/100], Training Loss: 667342.3607606185, Test Loss: 160677968.0\n",
      "Epoch [7/100], Training Loss: 594802.0685978319, Test Loss: 147843776.0\n",
      "Epoch [8/100], Training Loss: 554711.279663527, Test Loss: 139589824.0\n",
      "Epoch [9/100], Training Loss: 525029.257982347, Test Loss: 132805064.0\n",
      "Epoch [10/100], Training Loss: 499249.2975534625, Test Loss: 126715544.0\n",
      "Epoch [11/100], Training Loss: 475607.7763165689, Test Loss: 121069088.0\n",
      "Epoch [12/100], Training Loss: 453434.87328949705, Test Loss: 115750096.0\n",
      "Epoch [13/100], Training Loss: 432271.1538415971, Test Loss: 110620000.0\n",
      "Epoch [14/100], Training Loss: 411764.2563829157, Test Loss: 105718720.0\n",
      "Epoch [15/100], Training Loss: 392273.98969255376, Test Loss: 101062768.0\n",
      "Epoch [16/100], Training Loss: 373522.62828031514, Test Loss: 96619056.0\n",
      "Epoch [17/100], Training Loss: 355839.55405485455, Test Loss: 92520104.0\n",
      "Epoch [18/100], Training Loss: 339807.94431609503, Test Loss: 88870056.0\n",
      "Epoch [19/100], Training Loss: 325460.7365677389, Test Loss: 85577464.0\n",
      "Epoch [20/100], Training Loss: 312466.6952194775, Test Loss: 82565456.0\n",
      "Epoch [21/100], Training Loss: 300608.18588946154, Test Loss: 79796448.0\n",
      "Epoch [22/100], Training Loss: 289746.1964338606, Test Loss: 77245888.0\n",
      "Epoch [23/100], Training Loss: 279780.12487411883, Test Loss: 74890272.0\n",
      "Epoch [24/100], Training Loss: 270611.202298442, Test Loss: 72707664.0\n",
      "Epoch [25/100], Training Loss: 262165.32729103725, Test Loss: 70682600.0\n",
      "Epoch [26/100], Training Loss: 254390.52520585273, Test Loss: 68801128.0\n",
      "Epoch [27/100], Training Loss: 247211.01759374444, Test Loss: 67056764.0\n",
      "Epoch [28/100], Training Loss: 240576.51904508026, Test Loss: 65438660.0\n",
      "Epoch [29/100], Training Loss: 234439.46709318168, Test Loss: 63933828.0\n",
      "Epoch [30/100], Training Loss: 228753.17433801314, Test Loss: 62527048.0\n",
      "Epoch [31/100], Training Loss: 223458.32533617676, Test Loss: 61210020.0\n",
      "Epoch [32/100], Training Loss: 218489.98199158817, Test Loss: 59971404.0\n",
      "Epoch [33/100], Training Loss: 213818.27924885968, Test Loss: 58801448.0\n",
      "Epoch [34/100], Training Loss: 209410.20745216517, Test Loss: 57693756.0\n",
      "Epoch [35/100], Training Loss: 205223.48403530597, Test Loss: 56643996.0\n",
      "Epoch [36/100], Training Loss: 201235.19613766958, Test Loss: 55641932.0\n",
      "Epoch [37/100], Training Loss: 197435.4817842545, Test Loss: 54684476.0\n",
      "Epoch [38/100], Training Loss: 193804.26834903145, Test Loss: 53774072.0\n",
      "Epoch [39/100], Training Loss: 190330.70244653753, Test Loss: 52901896.0\n",
      "Epoch [40/100], Training Loss: 187001.6771518275, Test Loss: 52065688.0\n",
      "Epoch [41/100], Training Loss: 183808.59712102363, Test Loss: 51265328.0\n",
      "Epoch [42/100], Training Loss: 180741.57526212902, Test Loss: 50498760.0\n",
      "Epoch [43/100], Training Loss: 177793.8432557313, Test Loss: 49761204.0\n",
      "Epoch [44/100], Training Loss: 174957.09993483798, Test Loss: 49048812.0\n",
      "Epoch [45/100], Training Loss: 172229.7654167407, Test Loss: 48364180.0\n",
      "Epoch [46/100], Training Loss: 169605.350393934, Test Loss: 47701952.0\n",
      "Epoch [47/100], Training Loss: 167083.79266631123, Test Loss: 47065172.0\n",
      "Epoch [48/100], Training Loss: 164656.32569160595, Test Loss: 46453168.0\n",
      "Epoch [49/100], Training Loss: 162320.6350334696, Test Loss: 45864660.0\n",
      "Epoch [50/100], Training Loss: 160069.3484983117, Test Loss: 45297248.0\n",
      "Epoch [51/100], Training Loss: 157903.5663763995, Test Loss: 44750640.0\n",
      "Epoch [52/100], Training Loss: 155818.91096499024, Test Loss: 44224744.0\n",
      "Epoch [53/100], Training Loss: 153810.6198684912, Test Loss: 43719352.0\n",
      "Epoch [54/100], Training Loss: 151877.5606895326, Test Loss: 43230424.0\n",
      "Epoch [55/100], Training Loss: 150016.60008293347, Test Loss: 42755000.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [56/100], Training Loss: 148223.83993839228, Test Loss: 42301680.0\n",
      "Epoch [57/100], Training Loss: 146502.3082755761, Test Loss: 41862564.0\n",
      "Epoch [58/100], Training Loss: 144845.60049760086, Test Loss: 41438608.0\n",
      "Epoch [59/100], Training Loss: 143250.7493631894, Test Loss: 41029652.0\n",
      "Epoch [60/100], Training Loss: 141713.2450684201, Test Loss: 40635544.0\n",
      "Epoch [61/100], Training Loss: 140230.41069841833, Test Loss: 40253644.0\n",
      "Epoch [62/100], Training Loss: 138795.92844025829, Test Loss: 39886572.0\n",
      "Epoch [63/100], Training Loss: 137403.690243469, Test Loss: 39529552.0\n",
      "Epoch [64/100], Training Loss: 136047.65025768615, Test Loss: 39182048.0\n",
      "Epoch [65/100], Training Loss: 134727.98436111605, Test Loss: 38842764.0\n",
      "Epoch [66/100], Training Loss: 133442.5458799834, Test Loss: 38512688.0\n",
      "Epoch [67/100], Training Loss: 132192.4225460577, Test Loss: 38193260.0\n",
      "Epoch [68/100], Training Loss: 130978.05082637284, Test Loss: 37882548.0\n",
      "Epoch [69/100], Training Loss: 129793.20531959007, Test Loss: 37576788.0\n",
      "Epoch [70/100], Training Loss: 128636.29630946033, Test Loss: 37277852.0\n",
      "Epoch [71/100], Training Loss: 127518.30010070493, Test Loss: 36990632.0\n",
      "Epoch [72/100], Training Loss: 126445.6217048753, Test Loss: 36713244.0\n",
      "Epoch [73/100], Training Loss: 125406.29838279723, Test Loss: 36442492.0\n",
      "Epoch [74/100], Training Loss: 124374.20603044843, Test Loss: 36170204.0\n",
      "Epoch [75/100], Training Loss: 123334.96741899177, Test Loss: 35893132.0\n",
      "Epoch [76/100], Training Loss: 122295.54297731176, Test Loss: 35619228.0\n",
      "Epoch [77/100], Training Loss: 121289.05609857236, Test Loss: 35357924.0\n",
      "Epoch [78/100], Training Loss: 120336.13761033115, Test Loss: 35110616.0\n",
      "Epoch [79/100], Training Loss: 119428.39186067176, Test Loss: 34872336.0\n",
      "Epoch [80/100], Training Loss: 118555.43006930868, Test Loss: 34642864.0\n",
      "Epoch [81/100], Training Loss: 117710.61678810498, Test Loss: 34418428.0\n",
      "Epoch [82/100], Training Loss: 116890.730584681, Test Loss: 34203212.0\n",
      "Epoch [83/100], Training Loss: 116091.8726378769, Test Loss: 33991644.0\n",
      "Epoch [84/100], Training Loss: 115306.9944908477, Test Loss: 33787844.0\n",
      "Epoch [85/100], Training Loss: 114528.56288134589, Test Loss: 33581624.0\n",
      "Epoch [86/100], Training Loss: 113748.36621053255, Test Loss: 33375330.0\n",
      "Epoch [87/100], Training Loss: 112955.79953794206, Test Loss: 33163494.0\n",
      "Epoch [88/100], Training Loss: 112156.26603874179, Test Loss: 32953302.0\n",
      "Epoch [89/100], Training Loss: 111372.37112730289, Test Loss: 32748914.0\n",
      "Epoch [90/100], Training Loss: 110595.14400805639, Test Loss: 32544730.0\n",
      "Epoch [91/100], Training Loss: 109812.89254191103, Test Loss: 32340518.0\n",
      "Epoch [92/100], Training Loss: 109053.85498489426, Test Loss: 32146766.0\n",
      "Epoch [93/100], Training Loss: 108318.51092944731, Test Loss: 31955802.0\n",
      "Epoch [94/100], Training Loss: 107572.3228481725, Test Loss: 31759210.0\n",
      "Epoch [95/100], Training Loss: 106785.22232095255, Test Loss: 31551296.0\n",
      "Epoch [96/100], Training Loss: 105987.88614418577, Test Loss: 31352288.0\n",
      "Epoch [97/100], Training Loss: 105238.01332859427, Test Loss: 31166734.0\n",
      "Epoch [98/100], Training Loss: 104534.6458148214, Test Loss: 30995070.0\n",
      "Epoch [99/100], Training Loss: 103859.19595995497, Test Loss: 30823120.0\n",
      "Epoch [100/100], Training Loss: 103190.85462946507, Test Loss: 30652850.0\n",
      "Epoch [1/100], Training Loss: 582427.1244594514, Test Loss: 300036960.0\n",
      "Epoch [2/100], Training Loss: 581936.9942538949, Test Loss: 299450208.0\n",
      "Epoch [3/100], Training Loss: 579502.022865944, Test Loss: 297272128.0\n",
      "Epoch [4/100], Training Loss: 572697.4819027309, Test Loss: 292150848.0\n",
      "Epoch [5/100], Training Loss: 558960.1535454061, Test Loss: 282886304.0\n",
      "Epoch [6/100], Training Loss: 536426.2505775724, Test Loss: 268852320.0\n",
      "Epoch [7/100], Training Loss: 504732.50068123924, Test Loss: 250380432.0\n",
      "Epoch [8/100], Training Loss: 465630.6958118595, Test Loss: 228968656.0\n",
      "Epoch [9/100], Training Loss: 423043.35809489957, Test Loss: 207085152.0\n",
      "Epoch [10/100], Training Loss: 382158.6692731473, Test Loss: 187433024.0\n",
      "Epoch [11/100], Training Loss: 347592.67389372666, Test Loss: 171856912.0\n",
      "Epoch [12/100], Training Loss: 321424.61181209644, Test Loss: 160596608.0\n",
      "Epoch [13/100], Training Loss: 302757.58402938215, Test Loss: 152635440.0\n",
      "Epoch [14/100], Training Loss: 289220.1627865648, Test Loss: 146706960.0\n",
      "Epoch [15/100], Training Loss: 278665.68473431666, Test Loss: 141907264.0\n",
      "Epoch [16/100], Training Loss: 269754.5709377407, Test Loss: 137735920.0\n",
      "Epoch [17/100], Training Loss: 261789.03050767136, Test Loss: 133944376.0\n",
      "Epoch [18/100], Training Loss: 254424.959658788, Test Loss: 130408672.0\n",
      "Epoch [19/100], Training Loss: 247484.6163141994, Test Loss: 127061864.0\n",
      "Epoch [20/100], Training Loss: 240867.32918665957, Test Loss: 123864456.0\n",
      "Epoch [21/100], Training Loss: 234511.49055150762, Test Loss: 120791680.0\n",
      "Epoch [22/100], Training Loss: 228377.8065280493, Test Loss: 117827544.0\n",
      "Epoch [23/100], Training Loss: 222440.844973639, Test Loss: 114961352.0\n",
      "Epoch [24/100], Training Loss: 216681.80321071026, Test Loss: 112183376.0\n",
      "Epoch [25/100], Training Loss: 211074.7159528464, Test Loss: 109472808.0\n",
      "Epoch [26/100], Training Loss: 205571.0405781648, Test Loss: 106810368.0\n",
      "Epoch [27/100], Training Loss: 200161.6162549612, Test Loss: 104209856.0\n",
      "Epoch [28/100], Training Loss: 194877.54967122801, Test Loss: 101686192.0\n",
      "Epoch [29/100], Training Loss: 189745.8141105385, Test Loss: 99238864.0\n",
      "Epoch [30/100], Training Loss: 184773.34826135891, Test Loss: 96882816.0\n",
      "Epoch [31/100], Training Loss: 179993.74515727742, Test Loss: 94639312.0\n",
      "Epoch [32/100], Training Loss: 175436.81630235177, Test Loss: 92502360.0\n",
      "Epoch [33/100], Training Loss: 171095.8523784136, Test Loss: 90468872.0\n",
      "Epoch [34/100], Training Loss: 166980.50091819206, Test Loss: 88542800.0\n",
      "Epoch [35/100], Training Loss: 163084.8911794325, Test Loss: 86724592.0\n",
      "Epoch [36/100], Training Loss: 159397.29447307624, Test Loss: 85002608.0\n",
      "Epoch [37/100], Training Loss: 155906.2491558557, Test Loss: 83366064.0\n",
      "Epoch [38/100], Training Loss: 152595.07612108288, Test Loss: 81809296.0\n",
      "Epoch [39/100], Training Loss: 149450.61524791186, Test Loss: 80326176.0\n",
      "Epoch [40/100], Training Loss: 146459.56448077722, Test Loss: 78911504.0\n",
      "Epoch [41/100], Training Loss: 143612.10307446241, Test Loss: 77559128.0\n",
      "Epoch [42/100], Training Loss: 140896.75753806054, Test Loss: 76264880.0\n",
      "Epoch [43/100], Training Loss: 138305.5852141461, Test Loss: 75024552.0\n",
      "Epoch [44/100], Training Loss: 135830.1678810497, Test Loss: 73835112.0\n",
      "Epoch [45/100], Training Loss: 133462.47165452284, Test Loss: 72693368.0\n",
      "Epoch [46/100], Training Loss: 131197.4314317872, Test Loss: 71597880.0\n",
      "Epoch [47/100], Training Loss: 129028.6419051004, Test Loss: 70544512.0\n",
      "Epoch [48/100], Training Loss: 126951.11403352882, Test Loss: 69529456.0\n",
      "Epoch [49/100], Training Loss: 124956.56655411409, Test Loss: 68551160.0\n",
      "Epoch [50/100], Training Loss: 123042.83561400391, Test Loss: 67609096.0\n",
      "Epoch [51/100], Training Loss: 121206.39606658374, Test Loss: 66701932.0\n",
      "Epoch [52/100], Training Loss: 119442.04300693086, Test Loss: 65827208.0\n",
      "Epoch [53/100], Training Loss: 117745.67028019667, Test Loss: 64982560.0\n",
      "Epoch [54/100], Training Loss: 116112.49357265564, Test Loss: 64166208.0\n",
      "Epoch [55/100], Training Loss: 114537.61696581956, Test Loss: 63375948.0\n",
      "Epoch [56/100], Training Loss: 113016.9020792607, Test Loss: 62608032.0\n",
      "Epoch [57/100], Training Loss: 111545.73899650495, Test Loss: 61863592.0\n",
      "Epoch [58/100], Training Loss: 110119.80759433683, Test Loss: 61140564.0\n",
      "Epoch [59/100], Training Loss: 108737.15964694034, Test Loss: 60438028.0\n",
      "Epoch [60/100], Training Loss: 107391.08595462354, Test Loss: 59754268.0\n",
      "Epoch [61/100], Training Loss: 106079.60784313726, Test Loss: 59088812.0\n",
      "Epoch [62/100], Training Loss: 104802.64297138795, Test Loss: 58441592.0\n",
      "Epoch [63/100], Training Loss: 103555.17658906463, Test Loss: 57810400.0\n",
      "Epoch [64/100], Training Loss: 102335.35880575795, Test Loss: 57192752.0\n",
      "Epoch [65/100], Training Loss: 101144.90006516202, Test Loss: 56590444.0\n",
      "Epoch [66/100], Training Loss: 99983.41993957704, Test Loss: 56002460.0\n",
      "Epoch [67/100], Training Loss: 98849.25940406373, Test Loss: 55426628.0\n",
      "Epoch [68/100], Training Loss: 97741.59836502577, Test Loss: 54864668.0\n",
      "Epoch [69/100], Training Loss: 96660.96605651324, Test Loss: 54316164.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/100], Training Loss: 95605.166163142, Test Loss: 53778740.0\n",
      "Epoch [71/100], Training Loss: 94571.21852970796, Test Loss: 53253332.0\n",
      "Epoch [72/100], Training Loss: 93557.81209643978, Test Loss: 52738632.0\n",
      "Epoch [73/100], Training Loss: 92563.91730347728, Test Loss: 52233604.0\n",
      "Epoch [74/100], Training Loss: 91589.17801078135, Test Loss: 51738032.0\n",
      "Epoch [75/100], Training Loss: 90632.84023458326, Test Loss: 51250568.0\n",
      "Epoch [76/100], Training Loss: 89695.64338605532, Test Loss: 50771916.0\n",
      "Epoch [77/100], Training Loss: 88777.29992299035, Test Loss: 50305308.0\n",
      "Epoch [78/100], Training Loss: 87877.5332030093, Test Loss: 49848416.0\n",
      "Epoch [79/100], Training Loss: 86997.06510277827, Test Loss: 49401164.0\n",
      "Epoch [80/100], Training Loss: 86136.38872104733, Test Loss: 48964336.0\n",
      "Epoch [81/100], Training Loss: 85295.70392749245, Test Loss: 48538132.0\n",
      "Epoch [82/100], Training Loss: 84474.17273858184, Test Loss: 48120600.0\n",
      "Epoch [83/100], Training Loss: 83670.7638173094, Test Loss: 47712280.0\n",
      "Epoch [84/100], Training Loss: 82885.03512825069, Test Loss: 47313368.0\n",
      "Epoch [85/100], Training Loss: 82115.80202594634, Test Loss: 46922756.0\n",
      "Epoch [86/100], Training Loss: 81363.26189206801, Test Loss: 46539160.0\n",
      "Epoch [87/100], Training Loss: 80627.05112256383, Test Loss: 46162804.0\n",
      "Epoch [88/100], Training Loss: 79906.88596647118, Test Loss: 45793596.0\n",
      "Epoch [89/100], Training Loss: 79201.7288075351, Test Loss: 45432536.0\n",
      "Epoch [90/100], Training Loss: 78510.57496593804, Test Loss: 45078528.0\n",
      "Epoch [91/100], Training Loss: 77832.23304306617, Test Loss: 44730684.0\n",
      "Epoch [92/100], Training Loss: 77167.05550619039, Test Loss: 44388776.0\n",
      "Epoch [93/100], Training Loss: 76514.64877673124, Test Loss: 44053436.0\n",
      "Epoch [94/100], Training Loss: 75874.16835495527, Test Loss: 43722676.0\n",
      "Epoch [95/100], Training Loss: 75244.5203483206, Test Loss: 43396592.0\n",
      "Epoch [96/100], Training Loss: 74624.84746164326, Test Loss: 43075676.0\n",
      "Epoch [97/100], Training Loss: 74015.02873052545, Test Loss: 42760268.0\n",
      "Epoch [98/100], Training Loss: 73415.20691902138, Test Loss: 42450420.0\n",
      "Epoch [99/100], Training Loss: 72827.08808719863, Test Loss: 42145940.0\n",
      "Epoch [100/100], Training Loss: 72250.39985782832, Test Loss: 41848216.0\n"
     ]
    }
   ],
   "source": [
    "input_size = X_train.shape[1]\n",
    "hidden_size = 128\n",
    "output_size = 1  \n",
    "\n",
    "for network in nns:\n",
    "    for lr in lr_vals:\n",
    "        for batch in batch_vals:\n",
    "            model = NeuralNetwork3(input_size, hidden_size, output_size)\n",
    "            res = train_and_get_losses(lr, batch, model, X_train, X_test, y_train, y_test)\n",
    "            res_rmse[(lr,batch)] = res[1][-1]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7de6eb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "for comb in res_rmse:\n",
    "    res_rmse[comb] = sqrt(res_rmse[comb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "532afd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dict = dict(sorted(res_rmse.items(), key=lambda item: item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c428d354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0.004, 32): 3773.1580936928685,\n",
       " (0.001, 32): 3781.3319082037747,\n",
       " (0.4, 64): 3794.059040131031,\n",
       " (0.2, 64): 3798.582762031124,\n",
       " (0.004, 64): 3803.782853949473,\n",
       " (0.1, 32): 3804.9708277462523,\n",
       " (0.1, 64): 3807.489461574385,\n",
       " (0.002, 32): 3812.0598106535526,\n",
       " (0.002, 64): 3817.0860613824257,\n",
       " (0.3, 32): 3818.1062059612746,\n",
       " (0.4, 32): 3834.453676861933,\n",
       " (0.2, 32): 3853.8265399470174,\n",
       " (0.003, 64): 3884.37922968394,\n",
       " (0.3, 64): 3904.0234374296474,\n",
       " (0.003, 32): 3908.2149377944916,\n",
       " (0.001, 64): 3911.3714985922775,\n",
       " (0.001, 128): 3990.3572521768024,\n",
       " (0.003, 128): 4025.169685864187,\n",
       " (0.3, 128): 4051.8448884427944,\n",
       " (0.4, 128): 4118.864163819924,\n",
       " (0.002, 128): 4183.7772407239845,\n",
       " (0.004, 128): 4196.061248361372,\n",
       " (0.2, 128): 4222.319267890575,\n",
       " (0.1, 128): 4400.679947462664,\n",
       " (0.1, 256): 4697.313487516029,\n",
       " (0.002, 211): 4762.16463386137,\n",
       " (0.003, 211): 4827.896850596541,\n",
       " (0.4, 256): 4945.519790679236,\n",
       " (0.002, 256): 4967.153712137365,\n",
       " (0.2, 256): 5011.073537676333,\n",
       " (0.3, 211): 5031.9052057843855,\n",
       " (0.1, 211): 5070.6179110637,\n",
       " (0.4, 211): 5082.580053476777,\n",
       " (0.003, 256): 5115.592829770563,\n",
       " (0.001, 211): 5148.086440610725,\n",
       " (0.004, 211): 5166.6826881472025,\n",
       " (0.2, 211): 5192.10246432021,\n",
       " (0.3, 256): 5332.122091625434,\n",
       " (0.001, 256): 5460.654173265324,\n",
       " (0.004, 256): 5536.5016029980525,\n",
       " (0.2, 512): 5996.792475982473,\n",
       " (0.003, 512): 6073.254811054778,\n",
       " (0.002, 512): 6107.300549342565,\n",
       " (0.1, 512): 6218.898616314628,\n",
       " (0.4, 512): 6255.358343052778,\n",
       " (0.001, 512): 6279.632154832001,\n",
       " (0.004, 512): 6469.019709353188,\n",
       " (0.3, 512): 6559.871949969755}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97c9b7b",
   "metadata": {},
   "source": [
    "## Saving weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "74f1d758",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../weights/model_leaky.0.0.2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caffeb50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
